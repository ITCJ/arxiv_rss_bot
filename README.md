# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-06 06:36:49 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)

**Authors**: Jiyoung Park, Hankyu Jang, Changseok Song, Wookeun Jung  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2602.05145v1  

#### Abstract
Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptatio...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
- **Speculative Decoding åœ¨åŠ¨æ€è´Ÿè½½ä¸‹çš„æ€§èƒ½é€€åŒ–**ï¼šå°½ç®¡ speculative decoding èƒ½æ˜¾è‘—åŠ é€Ÿ LLM æ¨ç†ï¼Œä½†å…¶æ•ˆæœé«˜åº¦ä¾èµ–äº draft model å’Œ target model çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¾“å…¥ workloads éšæ—¶é—´ä¸æ–­å˜åŒ–ï¼ˆå¦‚ç”¨æˆ·è¡Œä¸ºã€prompt æ¨¡æ¿æ›´æ–°ï¼‰ï¼Œå¯¼è‡´ draft-target alignment ä¸‹é™ï¼Œacceptance rate é”å‡ï¼Œä»è€Œå‰Šå¼±ç”šè‡³æŠµæ¶ˆåŠ é€Ÿæ”¶ç›Šã€‚
- **åœ¨çº¿è®­ç»ƒå¼€é”€å¤§ä¸”éš¾ä»¥é›†æˆåˆ°é«˜æ€§èƒ½æ¨ç†ç³»ç»Ÿä¸­**ï¼šç°æœ‰åœ¨çº¿é€‚åº”æ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°è¿è¡Œ target model æ¥ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼ˆå¦‚ logits æˆ– hidden statesï¼‰ï¼Œå¸¦æ¥é¢å¤–è®¡ç®—å¼€é”€ï¼Œå¹¶å ç”¨å®è´µçš„æ¨ç†èµ„æºã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æå‡º **TIDE (Temporal Incremental Draft Engine)** â€”â€”ä¸€ç§**æœåŠ¡å¼•æ“åŸç”Ÿçš„è‡ªé€‚åº” speculative decoding æ¡†æ¶**ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒè®¾è®¡ï¼š

1. âœ… **é›¶å¼€é”€è®­ç»ƒä¿¡å·æå–ï¼ˆZero-overhead Training Signal Generationï¼‰**
   - å¤ç”¨ target model åœ¨éªŒè¯é˜¶æ®µå·²è®¡ç®—å‡ºçš„ä¸­é—´ hidden states ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œæ— éœ€é‡æ–°åŠ è½½æˆ–é‡ç®— target modelã€‚
   - å®ç°æ–¹å¼ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼‚æ­¥æå–å¹¶ä¼ è¾“ hidden states è‡³å…±äº«å­˜å‚¨ï¼Œä¸ä¸»æ¨ç†æµæ°´çº¿é‡å æ‰§è¡Œï¼Œå‡ ä¹æ— æ€§èƒ½æŸå¤±ã€‚

2. âœ… **è‡ªé€‚åº”è¿è¡Œæ—¶æ§åˆ¶ï¼ˆAdaptive Runtime Controlï¼‰**
   - åŠ¨æ€åˆ¤æ–­ä½•æ—¶å¯ç”¨ speculative decoding å’Œ draft model è®­ç»ƒã€‚
   - åŸºäº batch size å’Œ acceptance length å®æ—¶ä¼°ç®— speedupï¼Œä»…å½“æœ‰ç›Šæ—¶æ‰å¼€å¯ speculationã€‚
   - é€šè¿‡ç›‘æ§ acceptance rate çš„çŸ­æœŸä¸é•¿æœŸç§»åŠ¨å¹³å‡å€¼ï¼Œæ£€æµ‹åˆ†å¸ƒåç§»åè§¦å‘è®­ç»ƒï¼Œé¿å…æ— æ•ˆè®­ç»ƒã€‚

3. âœ… **å¼‚æ„ GPU åˆ©ç”¨ï¼ˆHeterogeneous GPU Utilizationï¼‰**
   - å°† inference serving ä¸ draft model training è§£è€¦ï¼Œåˆ†åˆ«éƒ¨ç½²åœ¨ä¸åŒç±»å‹çš„ GPU ä¸Šã€‚
   - ç¤ºä¾‹ï¼šH100 æ‰§è¡Œé«˜ååæ¨ç†ï¼ŒMI250 æ‰¿æ‹… draft model è®­ç»ƒä»»åŠ¡ï¼Œæå‡æ•´ä½“é›†ç¾¤åˆ©ç”¨ç‡ã€‚

4. âœ… **å¢é‡å¼åœ¨çº¿ draft model æ›´æ–°æœºåˆ¶**
   - draft model å¼‚æ­¥è®­ç»ƒå¹¶åœ¨éªŒè¯æ€§èƒ½æå‡åçƒ­æ›´æ–°è‡³æ¨ç†å¼•æ“ï¼Œå®ç°æŒç»­è‡ªæˆ‘ä¼˜åŒ–ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ SpecForgeï¼‰ | TIDE |
|------|------------------------|------|
| **è®­ç»ƒä¿¡å·è·å–** | éœ€ç¦»çº¿ prefill æˆ–åœ¨çº¿é‡ç®— target model è¾“å‡º | å¤ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„ hidden statesï¼Œé›¶é¢å¤–å¼€é”€ |
| **è®­ç»ƒæ•ˆç‡** | å­˜å‚¨å¼€é”€å¤§ï¼ˆofflineï¼‰æˆ–è®­ç»ƒæ…¢ï¼ˆonlineï¼‰ | è®­ç»ƒæ—¶é—´å‡å°‘ 1.67Ã—ï¼Œå­˜å‚¨éœ€æ±‚é™ä½ >20Ã— |
| **ç³»ç»Ÿé›†æˆæ€§** | å¤šä¸ºç‹¬ç«‹è®­ç»ƒæµç¨‹ï¼Œéš¾åµŒå…¥ç”Ÿäº§ç³»ç»Ÿ | åŸç”Ÿé›†æˆäºæ¨ç†å¼•æ“ï¼Œæ”¯æŒå®æ—¶è‡ªé€‚åº” |
| **èµ„æºåˆ©ç”¨** | é€šå¸¸ä½¿ç”¨åŒæ„ GPU | æ”¯æŒå¼‚æ„éƒ¨ç½²ï¼Œæå‡ç¡¬ä»¶åˆ©ç”¨ç‡ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **ShareGPT**ï¼šå¼€æ”¾å¯¹è¯æ•°æ®é›†ï¼ˆå¤šè¯­è¨€å­é›†ç”¨äºæ¨¡æ‹Ÿåˆ†å¸ƒåç§»ï¼‰
- **Science**ï¼šç§‘å­¦æ–‡æœ¬ï¼ˆæ¥è‡ª CAMEL-AI.org çš„ biology/chemistry/physics æ•°æ®ï¼‰
- **EvolCodeAlpaca**ï¼šä»£ç ç”Ÿæˆä»»åŠ¡
- **NuminaMath**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡
- **Alpaca å¤šè¯­è¨€å˜ä½“**ï¼ˆKorean, Arabic, Chinese, Frenchï¼‰ï¼šç”¨äºæµ‹è¯•è·¨è¯­è¨€åˆ†å¸ƒåç§»åœºæ™¯

### å®éªŒè®¾ç½®
- **Target Models**ï¼š
  - `gpt-oss-120b`, `Qwen3-235B-A22B`, `Llama-4-Scout-17B-16E`, `Llama-3.3-70B-Instruct`
- **Draft Model æ¶æ„**ï¼š
  - åŸºäº EAGLE-3ï¼Œå• decoder layer + LM headï¼Œé¢„æµ‹ next token åŸºäº target model çš„ low/middle/high å±‚ hidden states
- **å€™é€‰ token æ•°é‡ï¼ˆÎ³ï¼‰**ï¼šå›ºå®šä¸º 3ï¼ˆç»æ¶ˆèå®éªŒè¯æ˜æœ€ä¼˜ï¼‰
- **ç¡¬ä»¶é…ç½®**ï¼š
  - æ¨ç†èŠ‚ç‚¹ï¼šNVIDIA H100ï¼ˆ8 GPUsï¼‰
  - è®­ç»ƒèŠ‚ç‚¹ï¼šAMD Instinct MI250ï¼ˆ4 GPUsï¼‰
- **ç³»ç»Ÿå®ç°åŸºç¡€**ï¼š
  - æ¨ç†å¼•æ“åŸºäº **SGLang**
  - è®­ç»ƒå¼•æ“åŸºäº **SpecForge**

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Throughput** | å•ä½æ—¶é—´å†…å¤„ç†çš„ token æ•°é‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ |
| **Acceptance Length (E[l])** | å¹³å‡æ¯è½® speculation æˆåŠŸæ¥å—çš„ draft token æ•°é‡ |
| **Speedup** | ç›¸å¯¹äº vanilla autoregressive decoding çš„åŠ é€Ÿæ¯” |
| **Training Time** | å®Œæˆä¸€è½® draft model å¾®è°ƒæ‰€éœ€æ—¶é—´ |
| **Storage Overhead** | å­˜å‚¨è®­ç»ƒä¿¡å·æ‰€éœ€çš„ç£ç›˜ç©ºé—´ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **SpecForge Offline** | å…ˆ prefill è·å– hidden states å¹¶æŒä¹…åŒ–ï¼Œå†è®­ç»ƒï¼›å­˜å‚¨å¼€é”€å¤§ |
| **SpecForge Online** | æ¯æ¬¡è®­ç»ƒéƒ½é‡æ–°è¿è¡Œ target model ç”Ÿæˆ hidden statesï¼›è®¡ç®—å¼€é”€é«˜ |
| **TIDE-default** | å§‹ç»ˆå¯ç”¨ speculationï¼Œä¸è¿›è¡ŒåŠ¨æ€æ§åˆ¶ |
| **TIDE-adaptive** | å¯ç”¨ adaptive æ§åˆ¶é€»è¾‘ï¼ˆspeculation å¼€å…³ + selective trainingï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ğŸ”¹ ååé‡æå‡
- åœ¨å¤šä¸ªçœŸå® workload ä¸Šï¼ŒTIDE å®ç° **æœ€é«˜è¾¾ 1.15Ã— çš„ç«¯åˆ°ç«¯ throughput æå‡**ï¼ˆç›¸æ¯”é™æ€ speculative decodingï¼‰ã€‚
- æå‡å¹…åº¦å›  dataset è€Œå¼‚ï¼š
  - **Science**: +1.15Ã—
  - **NuminaMath**: +1.12Ã—
  - **EvolCodeAlpaca**: +1.10Ã—
  - **ShareGPT**: æå‡æœ‰é™ï¼ˆä»… ~1.02Ã—ï¼‰ï¼Œå› å…¶ high entropy å’Œ discourse å˜åŒ–é¢‘ç¹ï¼Œspeculative decoding æœ¬èº«å¢ç›Šè¾ƒå°ã€‚

#### ğŸ”¹ è®­ç»ƒæ•ˆç‡å¯¹æ¯”ï¼ˆä»¥ gpt-oss-120b + ShareGPT ä¸ºä¾‹ï¼‰
| æ–¹æ³• | Prefill æ—¶é—´ | Train æ—¶é—´ | æ€»è€—æ—¶ | Speedup |
|------|-------------|-----------|--------|---------|
| SpecForge Offline | 6.16 hr | 9.16 hr | 15.32 hr | 1.00Ã— |
| SpecForge Online | 18.48 hr | 9.16 hr | 27.64 hr | 0.55Ã— |
| **TIDE** | â€” | **9.16 hr** | **9.16 hr** | **1.67Ã—** |

> âœ… TIDE æ¶ˆé™¤äº† prefill å¼€é”€ï¼Œè®­ç»ƒé€Ÿåº¦æ˜¯ offline çš„ **1.67Ã—**ï¼Œæ˜¯ online çš„ **3.02Ã—**

#### ğŸ”¹ å­˜å‚¨å¼€é”€å¯¹æ¯”
| Target Model | SpecForge Offline | TIDE |
|--------------|--------------------|-------|
| gpt-oss-120b | 4.66 TB | 0.19 TB (**â†“96%**) |
| Qwen3-235B-A22B | 19.89 TB | 0.82 TB |
| Llama-3.3-70B-Instruct | 46.40 TB | 1.92 TB |

> âœ… TIDE ä»…éœ€ç¼“å­˜å½“å‰è®­ç»ƒæ‰¹æ¬¡çš„ hidden statesï¼Œå­˜å‚¨éœ€æ±‚é™ä½ä¸¤ä¸ªæ•°é‡çº§ã€‚

#### ğŸ”¹ å¼‚æ„ GPU åˆ©ç”¨æ•ˆæœ
- **æ¨ç†ååå·®è·å¤§**ï¼šH100 æ¨ç†ååæ˜¯ MI250 çš„ **6.76Ã—**
- **è®­ç»ƒååå·®è·å°**ï¼šH100 è®­ç»ƒååä»…ä¸º MI250 çš„ **2.44Ã—**
- å› æ­¤å°†è®­ç»ƒä»»åŠ¡äº¤ç»™ä½æ€§èƒ½ GPU æ›´åˆ’ç®—ã€‚

æœ€ç»ˆç³»ç»Ÿçº§ååæå‡ï¼š
- ä½¿ç”¨ H100ï¼ˆæ¨ç†ï¼‰+ MI250ï¼ˆè®­ç»ƒï¼‰ç»„åˆï¼ŒTIDE å®ç° **1.08â€“1.22Ã— çš„ç›¸å¯¹ throughput æå‡**ï¼ˆvs all-inference baselineï¼‰ã€‚

#### ğŸ”¹ è‡ªé€‚åº”æ§åˆ¶æœ‰æ•ˆæ€§ï¼ˆFigure 9ï¼‰
- åœ¨è¿ç»­è¯­è¨€åˆ‡æ¢ï¼ˆKorean â†’ Arabic â†’ Chinese â†’ Frenchï¼‰çš„å‹åŠ›æµ‹è¯•ä¸­ï¼š
  - **TIDE-default**ï¼šé‡åˆ°åˆ†å¸ƒåç§»æ—¶ throughput æ˜æ˜¾ä¸‹é™
  - **TIDE-adaptive**ï¼šè‡ªåŠ¨å…³é—­ speculationï¼Œé¿å…è´Ÿä¼˜åŒ–ï¼Œæ›´å¿«æ¢å¤æ€§èƒ½

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Speculative decoding çš„æ”¶ç›Šé«˜åº¦ä¾èµ– workload ç‰¹æ€§**ï¼š
   - ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ Scienceã€Mathï¼‰æ›´æ˜“å­¦ä¹ ï¼Œacceptance length æ›´é«˜ã€‚
   - å¼€æ”¾å¼å¯¹è¯ï¼ˆå¦‚ ShareGPTï¼‰éš¾ä»¥æœ‰æ•ˆ speculationã€‚

2. âœ… **é›¶å¼€é”€è®­ç»ƒä¿¡å·å¤ç”¨æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**ï¼š
   - åˆ©ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„å‰¯äº§å“ï¼ˆhidden statesï¼‰å¯å®Œå…¨æ›¿ä»£æ˜‚è´µçš„ re-computationã€‚

3. âœ… **è‡ªé€‚åº”æ§åˆ¶è‡³å…³é‡è¦**ï¼š
   - ä¸åŠ é€‰æ‹©åœ°å§‹ç»ˆ speculation æˆ– training ä¼šå¯¼è‡´èµ„æºæµªè´¹ç”šè‡³æ€§èƒ½å€’é€€ã€‚
   - åŸºäº acceptance length å’Œ batch size çš„åŠ¨æ€å†³ç­–èƒ½æ˜¾è‘—æå‡é²æ£’æ€§ã€‚

4. âœ… **å¼‚æ„ GPU åˆ†é…ç­–ç•¥å…·æœ‰å®é™…ä»·å€¼**ï¼š
   - é«˜ç«¯ GPU æ“…é•¿æ¨ç†ï¼Œä½ç«¯ GPU åœ¨è®­ç»ƒè½»é‡ draft model ä¸Šæ€§ä»·æ¯”æ›´é«˜ã€‚
   - è§£è€¦è®­ç»ƒä¸æ¨ç†ä½¿å¼‚æ„é›†ç¾¤åˆ©ç”¨ç‡æœ€å¤§åŒ–ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- â— **ä¾èµ–ç‰¹å®š draft model æ¶æ„ï¼ˆå¦‚ EAGLE-3ï¼‰**ï¼šå¿…é¡»åŸºäº target model çš„ hidden states è¿›è¡Œé¢„æµ‹ï¼Œé€šç”¨æ€§å—é™ã€‚
- â— **ä»…é€‚ç”¨äºæ”¯æŒ hidden state è¾“å‡ºçš„æ¨ç†æ¡†æ¶**ï¼šéœ€æ·±åº¦é›†æˆè‡³ SGLang/vLLM ç­‰ç³»ç»Ÿã€‚
- â— **å¯¹ extremely high-entropy workloads æ•ˆæœæœ‰é™**ï¼šå¦‚è‡ªç”±åˆ›ä½œç±»ä»»åŠ¡ï¼Œspeculative decoding æœ¬èº«å¢ç›Šå°ã€‚
- â— **å†·å¯åŠ¨é—®é¢˜**ï¼šåˆå§‹ draft model æ€§èƒ½å·®æ—¶å¯èƒ½æ— æ³•è§¦å‘æœ‰æ•ˆè®­ç»ƒä¿¡å·æ”¶é›†ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- ğŸ”„ **æ¢ç´¢æ›´é€šç”¨çš„ draft model æ¶æ„**ï¼Œé™ä½å¯¹ target model hidden states çš„ä¾èµ–ã€‚
- ğŸ§  **å¼•å…¥å¼ºåŒ–å­¦ä¹ æˆ– bandit ç®—æ³•**ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ– speculation å’Œ training çš„è§¦å‘ç­–ç•¥ã€‚
- âš™ï¸ **æ”¯æŒæ›´å¤šå¼‚æ„è®¾å¤‡ç±»å‹**ï¼ˆå¦‚ Intel GPUsã€å›½äº§åŠ é€Ÿå™¨ï¼‰ã€‚
- ğŸ“ˆ **æ‰©å±•è‡³å¤š draft model ååŒæœºåˆ¶**ï¼Œåº”å¯¹æ›´å¤æ‚çš„æ··åˆ workload åœºæ™¯ã€‚

--- 

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **TIDE å°† speculative decoding ä»â€œé™æ€åŠ é€ŸæŠ€å·§â€è½¬å˜ä¸ºâ€œåŠ¨æ€è‡ªä¼˜åŒ–ç³»ç»Ÿâ€ï¼Œé€šè¿‡é›¶å¼€é”€ä¿¡å·å¤ç”¨ã€è‡ªé€‚åº”æ§åˆ¶å’Œå¼‚æ„èµ„æºè°ƒåº¦ï¼Œåœ¨çœŸå®éå¹³ç¨³ workload ä¸­å®ç°äº†å¯æŒç»­çš„æ¨ç†åŠ é€Ÿã€‚**

</details>

---

### 2. [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)

**Authors**: Wentao Ni, Kangqi Zhang, Zhongming Yu, Oren Nelson, Mingu Lee, Hong Cai, Fatih Porikli, Jongryool Kim, Zhijian Liu, Jishen Zhao  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2602.05191v1  

#### Abstract
As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

åœ¨**é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†**ä¸­ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¯è¾¾æ•°ä¸‡ç”šè‡³æ•°åä¸‡ tokensï¼‰ï¼Œ**æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¼€é”€æ€¥å‰§ä¸Šå‡**ï¼Œæˆä¸ºè§£ç é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚ä¼ ç»Ÿçš„ **fixed-budget top-k sparse attention** æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š

- **æ— æ³•é€‚åº”æ³¨æ„åŠ›åˆ†å¸ƒçš„å¼‚è´¨æ€§**ï¼šä¸åŒå±‚ï¼ˆlayerï¼‰ã€å¤´ï¼ˆheadï¼‰å’Œè§£ç æ­¥ï¼ˆdecode stepï¼‰çš„æ³¨æ„åŠ›åˆ†å¸ƒå·®å¼‚æ˜¾è‘—ï¼Œå›ºå®šé¢„ç®—ï¼ˆå¦‚å›ºå®šé€‰æ‹© 256 ä¸ª tokenï¼‰ä¼šå¯¼è‡´æŸäº›æƒ…å†µä¸‹ä¿ç•™ä¸è¶³ï¼ˆç²¾åº¦æŸå¤±ï¼‰ï¼Œå¦ä¸€äº›æƒ…å†µè¿‡åº¦ä¿ç•™ï¼ˆæ•ˆç‡ä½ä¸‹ï¼‰ã€‚
- **ç¼ºä¹å¯¹æ³¨æ„åŠ›è´¨é‡çš„æ˜¾å¼æ§åˆ¶**ï¼štop-k æ–¹æ³•ä¸ä¿è¯ä¿ç•™çš„æ³¨æ„åŠ›è´¨é‡ï¼ˆå³ç´¯è®¡æ³¨æ„åŠ›è´¨é‡ massï¼‰ï¼Œè€Œ **top-p æ–¹æ³•é€šè¿‡è®¾å®šæ¦‚ç‡é˜ˆå€¼ p æ¥ä¿ç•™å‰ p% çš„æ³¨æ„åŠ›è´¨é‡**ï¼Œç†è®ºä¸Šèƒ½æä¾›æ›´å¼ºçš„ç²¾åº¦ä¿éšœã€‚

ç„¶è€Œï¼Œç°æœ‰ top-p æ–¹æ³•ï¼ˆå¦‚ *Twilight*ï¼‰ä¹Ÿå­˜åœ¨é—®é¢˜ï¼š
- **token-level ä¼°è®¡æˆæœ¬é«˜**ï¼šéœ€è¦å¯¹æ‰€æœ‰ token è¿›è¡Œè¿‘ä¼¼æ³¨æ„åŠ›æ‰“åˆ†ï¼Œè®¡ç®—å’Œæ’åºå¼€é”€éšä¸Šä¸‹æ–‡çº¿æ€§å¢é•¿ã€‚
- **ä¼°è®¡ä¸å¯é **ï¼šç”±äºä½¿ç”¨å›ºå®š token é¢„ç®—è¿›è¡Œä¼°è®¡ï¼Œå¯¼è‡´å®é™…ä¿ç•™çš„æ³¨æ„åŠ›è´¨é‡æ³¢åŠ¨å¤§ï¼Œå¸¸ä½äºç›®æ ‡ p å€¼ï¼ˆå¦‚ p=0.95ï¼‰ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šDouble-P

ä½œè€…æå‡º **Double-P**ï¼Œä¸€ç§**åˆ†å±‚çš„ top-p ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶**ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ top-p é€‰æ‹©å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„ç¨€ç–åŒ–ï¼š

#### åˆ›æ–°ç‚¹ 1ï¼š**ä¸¤é˜¶æ®µåˆ†å±‚ top-p è®¾è®¡**

1. **ç¬¬ä¸€é˜¶æ®µï¼šCluster-Level Top-P Estimation**
   - åœ¨é¢„å¡«å……é˜¶æ®µå°† KV Cache æŒ‰ key å‘é‡èšç±»ä¸ºå¤šä¸ª clusterã€‚
   - ä½¿ç”¨ **size-weighted centroid**ï¼ˆå¸¦å¤§å°æƒé‡çš„èšç±»ä¸­å¿ƒï¼‰å¿«é€Ÿä¼°ç®—æ¯ä¸ª cluster çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚
   - åœ¨ cluster ç²’åº¦ä¸Šæ‰§è¡Œ top-pï¼Œç­›é€‰å‡ºå¯èƒ½è´¡çŒ®é‡è¦æ³¨æ„åŠ›çš„ cluster é›†åˆã€‚

2. **ç¬¬äºŒé˜¶æ®µï¼šAdaptive Token-Level Top-P Refinement**
   - å¯¹é€‰ä¸­çš„ clusterï¼ŒåŠ¨æ€å†³å®šæ˜¯å¦è¿›è¡Œç²¾ç¡®çš„ token-level æ³¨æ„åŠ›è®¡ç®—ã€‚
   - å¼•å…¥ç¬¬äºŒä¸ª top-p å‚æ•° $p_2$ï¼Œä»…å¯¹é«˜å½±å“ cluster æ‰§è¡Œç²¾ç¡®è®¡ç®—ï¼Œå…¶ä½™ç”¨ centroid è¿‘ä¼¼ã€‚
   - å®ç°â€œæŒ‰éœ€ç²¾ç®—â€ï¼Œé¿å…ä¸å¿…è¦çš„ token çº§è®¡ç®—ã€‚

#### åˆ›æ–°ç‚¹ 2ï¼šé«˜æ•ˆçš„ GPU Kernel å®ç°
- è‡ªå®šä¹‰ **Top-P kernel**ï¼Œæ”¯æŒå¯¹å·²æ’åºå¼ é‡è¿›è¡Œå‰ç¼€å’Œ + æ—©åœã€‚
- èåˆ **token å’Œ cluster çš„ gather æ“ä½œ**ï¼Œæå‡å†…å­˜å±€éƒ¨æ€§ã€‚
- ä½¿ç”¨ **FlashAttention å˜ä½“**ç»Ÿä¸€å¤„ç†ç²¾ç¡®å’Œè¿‘ä¼¼éƒ¨åˆ†ï¼Œå‡å°‘ kernel launch å¼€é”€ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | ä¼˜åŠ¿ |
|------|------|
| **vs. Top-k (Quest, RetroInfer)** | æ”¯æŒè‡ªé€‚åº”é¢„ç®—ï¼Œæä¾› top-p ç²¾åº¦ä¿è¯ï¼Œé¿å…å›ºå®šé¢„ç®—çš„ä¸å¹³è¡¡é—®é¢˜ |
| **vs. Token-level Top-p (Twilight)** | æ˜¾è‘—é™ä½ä¼°è®¡å¼€é”€ï¼ˆä» token çº§é™åˆ° cluster çº§ï¼‰ï¼Œé¿å…çº¿æ€§å¢é•¿çš„ SpGEMV å’Œæ’åº |
| **vs. Cluster-based (RetroInfer)** | å¼•å…¥æ¦‚ç‡é©±åŠ¨çš„ top-p æ§åˆ¶ï¼Œè€Œéå›ºå®š cluster æ•°é‡ï¼Œç²¾åº¦æ›´é«˜ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†

- **RULER** (Hsieh et al., 2024)  
  åŒ…å« 13 ä¸ªä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä» 4K åˆ° 128Kï¼Œç”¨äºç»¼åˆè¯„ä¼°é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚
- **LongBench** (Bai et al., 2024)  
  åŒ…å« 21 ä¸ªçœŸå®åœºæ™¯ä»»åŠ¡ï¼Œæ¶µç›–é—®ç­”ã€æ‘˜è¦ã€æ¨ç†ç­‰ï¼Œå¹³å‡è¾“å…¥é•¿åº¦ 5Kâ€“15Kã€‚

### âš™ï¸ å®éªŒè®¾ç½®

- **æ¨¡å‹**ï¼šLLaMA-3.1-8B å’Œ Qwen-3-8Bï¼Œä¸Šä¸‹æ–‡é•¿åº¦æµ‹è¯•è‡³ 128Kã€‚
- **ç¡¬ä»¶**ï¼šå• NVIDIA H100 PCIe GPU (80GB)ï¼ŒCUDA 12.8ï¼ŒPyTorch 2.8ã€‚
- **ä¿ç•™ç­–ç•¥**ï¼šæ‰€æœ‰æ–¹æ³•å‡ä¿ç•™ **sink tokens (4)** å’Œ **sliding window tokens (64)**ï¼Œç¨€ç–æ³¨æ„åŠ›ä»…ä½œç”¨äºä¸­é—´ tokensã€‚

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Accuracy** | ä¸‹æ¸¸ä»»åŠ¡å¹³å‡å¾—åˆ†ï¼ˆå¦‚ RULER Avg., LongBench Avg.ï¼‰ |
| **End-to-End Decoding Latency** | æ¯è¾“å‡º token çš„å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰ |
| **Attention Latency Breakdown** | æ³¨æ„åŠ›å„é˜¶æ®µè€—æ—¶ï¼ˆSpGEMVã€Top-P é€‰æ‹©ã€ç¨€ç–æ³¨æ„åŠ›ï¼‰ |
| **Speedup** | ç›¸å¯¹äºåŸºçº¿æˆ–å…¨æ³¨æ„åŠ›çš„é€Ÿåº¦æå‡å€æ•° |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **Quest** | Page-level top-k | åŸºäº key bounds é€‰æ‹© top-k pagesï¼Œå– 25% tokens |
| **RetroInfer** | Cluster-based top-k | å›ºå®šæ•°é‡ cluster æ£€ç´¢ï¼Œç»“åˆ centroid è¿‘ä¼¼ |
| **Quest + Twilight** | Token-level top-p | åœ¨ Quest åŸºç¡€ä¸Šåº”ç”¨ Twilight çš„ top-p é€‰æ‹© |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### âœ… å‡†ç¡®ç‡è¡¨ç°ï¼ˆLLaMA-3.1-8Bï¼‰

| æ–¹æ³• | RULER (16K) | RULER (32K) | RULER (64K) | LongBench |
|------|------------|------------|------------|-----------|
| Full Attention | 93.25 | 90.00 | 85.36 | 39.33 |
| Quest | 89.24 | 85.95 | 83.61 | 38.76 |
| RetroInfer | 91.56 | 88.12 | 83.77 | 39.03 |
| Quest + Twilight | 86.73 | 86.50 | 81.87 | 38.97 |
| **Double-P (Ours)** | **92.87** | **89.91** | **84.55** | **39.06** |

- **ç›¸æ¯”æœ€å¼ºåŸºçº¿ RetroInfer**ï¼ŒDouble-P åœ¨ RULER ä¸Šåˆ†åˆ«æå‡ **+1.31**, **+1.79**, **+0.78** ç»å¯¹åˆ†ã€‚
- **å¹³å‡æå‡ +1.26 åˆ†**ï¼Œ**æ¥è¿‘å…¨æ³¨æ„åŠ›ç²¾åº¦**ï¼Œå®ç°â€œnear-zero accuracy dropâ€ã€‚

#### âš¡ æ•ˆç‡è¡¨ç°

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **Attention-level Speedup** | æœ€é«˜è¾¾ **1.74Ã— vs Quest-Twilight**, **1.78Ã— vs RetroInfer** |
| **End-to-End Decoding Speedup** | æœ€é«˜ **1.26Ã— vs RetroInfer**, **1.11Ã— vs Quest** |
| **ç›¸æ¯” Full Attention** | æœ€é«˜ **2.23Ã— åŠ é€Ÿ** |
| **Top-p Estimation Overhead** | Double-P å°†ä¼°è®¡å¼€é”€é™è‡³å¯å¿½ç•¥æ°´å¹³ï¼ˆè§ Figure 9ï¼‰ |

#### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰

- å›¾ 10 å±•ç¤ºäº†ä¸åŒ $(p_1, p_2)$ é…ç½®ä¸‹çš„ç²¾åº¦-å»¶è¿Ÿæƒè¡¡ï¼š
  - $p_1$: cluster-level top-p é˜ˆå€¼
  - $p_2$: token-level refinement é˜ˆå€¼
- å®è·µä¸­é€‰æ‹© $(p_1, p_2) = (0.95, 0.7)$ åœ¨ LLaMA-3.1-8B ä¸Šå–å¾—æœ€ä½³å¹³è¡¡ã€‚
- æ›´é«˜çš„ $p_1$ å’Œ $p_2$ æå‡ç²¾åº¦ä½†å¢åŠ å»¶è¿Ÿï¼ŒéªŒè¯äº†è®¾è®¡çš„å¯æ§æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **Fixed-budget top-k æ–¹æ³•æ— æ³•å¯é æ»¡è¶³ top-p è¦æ±‚**  
   å®éªŒè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨å¹³å‡é¢„ç®—ï¼ˆå¦‚ k=256ï¼‰ï¼Œä»æœ‰è¶…è¿‡ 20% çš„ attention head æ— æ³•è¾¾åˆ° p=0.95 çš„è´¨é‡è¦æ±‚ã€‚

2. **Token-level top-p ä¼°è®¡æˆæœ¬è¿‡é«˜**  
   SpGEMV å’Œæ’åºæ“ä½œå æ€»å»¶è¿Ÿçš„ **60% ä»¥ä¸Š**ï¼Œä¸¥é‡é™åˆ¶ç«¯åˆ°ç«¯åŠ é€Ÿæ½œåŠ›ã€‚

3. **Double-P å®ç°äº†ç²¾åº¦ä¸æ•ˆç‡çš„å¸•ç´¯æ‰˜å‰æ²¿**  
   å¦‚ Figure 1 æ‰€ç¤ºï¼ŒDouble-P åœ¨ç²¾åº¦-å»¶è¿Ÿå›¾ä¸­å½¢æˆ **Pareto frontier**ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚

4. **åˆ†å±‚è®¾è®¡æ˜¯å…³é”®**  
   å…ˆç”¨ cluster-level ä½æˆæœ¬ä¼°è®¡ç¼©å°æœç´¢ç©ºé—´ï¼Œå†ç”¨ adaptive token-level refinement ç²¾ç»†æ§åˆ¶ï¼Œå®ç°äº†â€œ**ç²—ä¸­æœ‰ç»†ï¼Œç»†ä¸­æ±‚çœ**â€ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

- **ä¾èµ–èšç±»è´¨é‡**ï¼šk-means èšç±»å‡è®¾è¯­ä¹‰ç›¸ä¼¼ token å¯è¢«èšåœ¨ä¸€èµ·ï¼Œæç«¯ç¨€ç–æˆ–å™ªå£°æ•°æ®å¯èƒ½å½±å“æ•ˆæœã€‚
- **é¢å¤–é¢„å¤„ç†å¼€é”€**ï¼šèšç±»è¿‡ç¨‹åœ¨ prefill é˜¶æ®µå¼•å…¥ä¸€å®šè®¡ç®—æˆæœ¬ï¼ˆä½†å¯æ¥å—ï¼‰ã€‚
- **è¶…å‚æ•°è°ƒä¼˜éœ€æ±‚**ï¼š$p_1$ å’Œ $p_2$ éœ€æ ¹æ®æ¨¡å‹å’Œä»»åŠ¡è°ƒæ•´ï¼Œè‡ªåŠ¨åŒ–è°ƒå‚å¯ä½œä¸ºæœªæ¥æ–¹å‘ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

- **åŠ¨æ€è°ƒæ•´ $p_1$, $p_2$**ï¼šåŸºäºå½“å‰ query æˆ–ä¸Šä¸‹æ–‡å¤æ‚åº¦è‡ªåŠ¨è°ƒèŠ‚é˜ˆå€¼ã€‚
- **ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ç»“åˆ**ï¼šå¦‚ KV Cache é‡åŒ–ï¼ˆKVQuantï¼‰ã€GQA/PagedAttention ç­‰ã€‚
- **æ‰©å±•åˆ°è®­ç»ƒé˜¶æ®µ**ï¼šæ¢ç´¢ Double-P åœ¨è®­ç»ƒä¸­çš„å¯è¡Œæ€§ï¼Œè¿›ä¸€æ­¥é™ä½é•¿åºåˆ—è®­ç»ƒæˆæœ¬ã€‚
- **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šé’ˆå¯¹ Double-P çš„è®¿é—®æ¨¡å¼è®¾è®¡ä¸“ç”¨åŠ é€Ÿå™¨æˆ– kernelã€‚

---

> **æ€»ç»“**ï¼šDouble-P é€šè¿‡**åˆ†å±‚ top-p ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶**ï¼ŒæˆåŠŸè§£å†³äº†é•¿ä¸Šä¸‹æ–‡ LLM æ¨ç†ä¸­**ç²¾åº¦ä¸æ•ˆç‡éš¾ä»¥å…¼é¡¾**çš„é—®é¢˜ã€‚å®ƒä¸ä»…æä¾›äº†**æ›´å¼ºçš„ top-p ç²¾åº¦ä¿è¯**ï¼Œè¿˜é€šè¿‡**cluster-level ä¼°è®¡ + adaptive refinement** æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œåœ¨å¤šä¸ªåŸºå‡†ä¸Šå®ç°äº†**æ¥è¿‘å…¨æ³¨æ„åŠ›çš„ç²¾åº¦**å’Œ**æœ€é«˜ 2.23Ã— çš„ç«¯åˆ°ç«¯åŠ é€Ÿ**ï¼Œä¸ºæœªæ¥é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†åšå®åŸºç¡€ã€‚

</details>

---

### 3. [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)

**Authors**: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu, Yuyu Luo  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.05711v1  

#### Abstract
Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-design...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **Mixture-of-Experts (MoE)** æ¶æ„åœ¨**ä¸“å®¶ç²’åº¦**ä¸Šé¢ä¸´æ ¹æœ¬æ€§æƒè¡¡ï¼š
- **ç²—ç²’åº¦ MoE**ï¼ˆå¦‚ DeepSeekMoEï¼‰ï¼šç¡¬ä»¶æ•ˆç‡é«˜ï¼ˆdense matmulï¼‰ï¼Œä½†æ¿€æ´»ä¸ç²¾ç¡®ï¼Œå­˜åœ¨å†—ä½™è®¡ç®—ã€‚
- **ç»†ç²’åº¦ MoE**ï¼ˆå¦‚ PEERï¼‰ï¼šå‚æ•°åˆ©ç”¨ç‡é«˜ï¼Œä½†å› å†…å­˜è®¿é—®åˆ†æ•£å¯¼è‡´ä¸¥é‡çš„ **memory-bound** é—®é¢˜ï¼Œæ¨ç†å»¶è¿Ÿæé«˜ã€‚

OmniMoE çš„ç›®æ ‡æ˜¯ï¼š**åœ¨ä¿æŒç»†ç²’åº¦ä¸“å®¶é«˜å‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°ç²—ç²’åº¦æ¶æ„çº§åˆ«çš„ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

OmniMoE æ˜¯ä¸€ä¸ª **system-algorithm co-designed** æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ååŒè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼š

#### ï¼ˆ1ï¼‰Atomic Experts + Dynamic Expert Assembly (DEA)
- å°†ä¸“å®¶ç²’åº¦æ¨å‘é€»è¾‘æé™â€”â€”**Atomic Expert**ï¼šæ¯ä¸ªä¸“å®¶ä»…ç”±ä¸€å¯¹å‘é‡ $(w_{\text{in}}, w_{\text{out}})$ å‚æ•°åŒ–ï¼Œæ„æˆæœ€å°å¯è·¯ç”±å•å…ƒã€‚
- å¼•å…¥ **Dynamic Expert Assembly (DEA)**ï¼šå¯¹æ¯ä¸ª token åŠ¨æ€æ£€ç´¢å¹¶ç»„åˆå¤šä¸ª Atomic Expertsï¼Œå½¢æˆ token-specific çš„é«˜æ•ˆéçº¿æ€§å˜æ¢ã€‚
- æ‰€æœ‰ä¸“å®¶å‚æ•°é›†ä¸­å­˜å‚¨ä¸ºå…¨å±€çŸ©é˜µ $W, V \in \mathbb{R}^{N \times d}$ï¼Œæ”¯æŒé«˜æ•ˆ gather æ“ä½œã€‚

> âœ… ä¼˜åŠ¿ï¼šæå¤§æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¸é•¿å°¾çŸ¥è¯†æ£€ç´¢ç²¾åº¦ï¼ŒåŒæ—¶é¿å…é™æ€ embedding ç±»è®¾è®¡ï¼ˆå¦‚ PKMï¼‰ç¼ºä¹éçº¿æ€§å˜æ¢çš„é—®é¢˜ã€‚

#### ï¼ˆ2ï¼‰Cartesian Product Router
- é¢å¯¹ç™¾ä¸‡çº§ä¸“å®¶å¸¦æ¥çš„è·¯ç”±å¼€é”€ï¼ˆ$O(Nd)$ æŠ•å½±ä¸å¯è¡Œï¼‰ï¼Œæå‡ºå°†ä¸€ç»´ä¸“å®¶ç´¢å¼•ç©ºé—´åˆ†è§£ä¸ºäºŒç»´ç½‘æ ¼ï¼ˆ$N_r \times N_c$, $N = N_r N_c$ï¼‰ã€‚
- è·¯ç”±å™¨åˆ†åˆ«é¢„æµ‹è¡Œåˆ†å¸ƒ $p_r(i|x)$ å’Œåˆ—åˆ†å¸ƒ $p_c(j|x)$ï¼Œè”åˆå¾—åˆ† $p(i,j|x) \approx p_r(i|x) \cdot p_c(j|x)$ã€‚
- å®ç°æ–¹å¼ï¼šä¸¤ä¸ªä½ç»´æŠ•å½± $W_r, W_c$ æ›¿ä»£å•ä¸€ $W_g$ï¼Œå°†è·¯ç”±å¤æ‚åº¦ä» $O(Nd)$ é™è‡³ $O(\sqrt{N}d)$ã€‚

> âœ… ä¼˜åŠ¿ï¼šä½¿å¤§è§„æ¨¡ç»†ç²’åº¦è·¯ç”±å˜å¾—å¯è¡Œä¸”é«˜æ•ˆï¼›æ”¯æŒé«˜è¾¾ç™¾ä¸‡çº§ä¸“å®¶æ± ã€‚

#### ï¼ˆ3ï¼‰Expert-Centric Scheduling
- æ”¹å˜ä¼ ç»Ÿ **token-centric** æ‰§è¡Œé¡ºåºï¼ˆæ¯ä¸ª token ç‹¬ç«‹æ‹‰å–å‚æ•°ï¼‰ï¼Œè½¬ä¸º **expert-centric** æ‰§è¡ŒèŒƒå¼ã€‚
- æ­¥éª¤ï¼š
  1. æ”¶é›†æ‰€æœ‰ token çš„è·¯ç”±è¯·æ±‚ï¼›
  2. æŒ‰æ¿€æ´»ä¸“å®¶åˆ†ç»„ï¼›
  3. åœ¨æ¯ç»„å†…æŒ‰ token ID æ’åºï¼›
  4. ä½¿ç”¨ **Grouped GEMM** æ‰¹é‡å¤„ç†åŒä¸€ä¸“å®¶ä¸‹çš„å¤šä¸ª tokenã€‚
- æ•ˆæœï¼šå°†éšæœºå†…å­˜è®¿é—®ï¼ˆscatter/gatherï¼‰è½¬åŒ–ä¸ºè¿ç»­ã€å¯é‡ç”¨çš„å¯†é›†çŸ©é˜µè¿ç®—ã€‚

> âœ… ä¼˜åŠ¿ï¼šå½»åº•ç¼“è§£ memory bandwidth ç“¶é¢ˆï¼ŒGPU åˆ©ç”¨ç‡æ˜¾è‘—æå‡ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç²—ç²’åº¦ MoEï¼ˆå¦‚ DeepSeekMoEï¼‰ | ç»†ç²’åº¦ MoEï¼ˆå¦‚ PEERï¼‰ | **OmniMoEï¼ˆæœ¬å·¥ä½œï¼‰** |
|------|-------------------------------|------------------------|--------------------------|
| å‚æ•°æ•ˆç‡ | ä½ï¼ˆå¤§å—æ¿€æ´»ï¼Œå†—ä½™å¤šï¼‰ | é«˜ï¼ˆç²¾å‡†æ§åˆ¶ï¼‰ | â­ æé«˜ï¼ˆatomic levelï¼‰ |
| è¡¨è¾¾èƒ½åŠ› | é«˜ï¼ˆå®Œæ•´ FFN ç»“æ„ï¼‰ | ä½ï¼ˆå¸¸ä¸ºçº¿æ€§èšåˆï¼‰ | â­ é«˜ï¼ˆåŠ¨æ€ç»„è£…éçº¿æ€§å—ï¼‰ |
| è·¯ç”±æ•ˆç‡ | é«˜ï¼ˆå°ä¸“å®¶æ•°ï¼‰ | ä½ï¼ˆå…¨æŠ•å½±ä»£ä»·å¤§ï¼‰ | â­ é«˜ï¼ˆfactorized routingï¼‰ |
| å†…å­˜è®¿é—® | è¿ç»­ï¼ˆdenseï¼‰ | åˆ†æ•£ï¼ˆrandom I/Oï¼‰ | â­ è¿ç»­ï¼ˆgrouped coalescingï¼‰ |
| æ¨ç†é€Ÿåº¦ | å¿« | æ…¢ï¼ˆmemory-boundï¼‰ | â­â­ æå¿«ï¼ˆcompute-boundï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **é¢„è®­ç»ƒè¯­æ–™**ï¼šSmolLMCorpusï¼ˆ400äº¿ tokenï¼‰
  - åŒ…å« Webã€Textbookã€Codeã€Math å››ç±»é«˜è´¨é‡æ–‡æœ¬ã€‚
- **ä¸‹æ¸¸è¯„ä¼°åŸºå‡†**ï¼ˆ7é¡¹é›¶æ ·æœ¬ä»»åŠ¡ï¼‰ï¼š
  - **MMLU**ï¼ˆå¤šä»»åŠ¡çŸ¥è¯†ï¼‰
  - **TriviaQA**ï¼ˆäº‹å®å›å¿†ï¼‰
  - **ARC**ï¼ˆç§‘å­¦æ¨ç†ï¼‰
  - **PIQA**ï¼ˆç‰©ç†å¸¸è¯†ï¼‰
  - **HellaSwag**ï¼ˆå¸¸è¯†æ¨æ–­ï¼‰
  - **OBQA**ï¼ˆå¼€æ”¾ä¹¦æœ¬é—®ç­”ï¼‰
  - **Winogrande**ï¼ˆå…±æŒ‡æ¶ˆè§£ï¼‰

> ä½¿ç”¨ Hugging Face LightEval å·¥å…·åŒ…ç»Ÿä¸€è¯„æµ‹ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹è§„æ¨¡**ï¼šä¸»æ¯”è¾ƒä½¿ç”¨ **6.4B æ€»å‚æ•° / 1.7B æ¿€æ´»å‚æ•°** çš„ MoE æ¨¡å‹ã€‚
- **éª¨å¹²ç½‘ç»œä¸€è‡´**ï¼šæ‰€æœ‰æ–¹æ³•å…±äº«ç›¸åŒçš„ Transformer ç»“æ„ï¼ˆdepth, width, GQA ç­‰ï¼‰ï¼Œä»…æ›¿æ¢ FFN æ¨¡å—ã€‚
- **å…¬å¹³å¯¹æ¯”åŸåˆ™**ï¼š
  - æ‰€æœ‰æ¨¡å‹ä»å¤´é¢„è®­ç»ƒï¼ˆscratch trainingï¼‰ï¼Œæ’é™¤ checkpoint å·®å¼‚å½±å“ã€‚
  - æ§åˆ¶ç›¸åŒæ¿€æ´»å‚æ•°é¢„ç®—ã€è®­ç»ƒ FLOPsã€æ•°æ®é›†ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **ä¸‹æ¸¸æ€§èƒ½**ï¼šzero-shot accuracy å¹³å‡å€¼
  - **ç³»ç»Ÿæ•ˆç‡**ï¼šinference latencyï¼ˆmsï¼‰ã€peak memoryï¼ˆGBï¼‰
  - **æ‰©å±•æ€§åˆ†æ**ï¼šscaling lawsï¼ˆperplexity vs. FLOPs / Act Paramsï¼‰

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Dense** | å…¨æ¿€æ´» MLP | åŸºå‡†ä¸Šé™ï¼ˆæ—  MoEï¼‰ |
| **Gshard** | Coarse-grained MoE | Top-K è·¯ç”±ï¼Œæ ‡å‡†å®ç° |
| **DeepSeekMoE** | Coarse-grained MoE | å« shared expertï¼Œå½“å‰ä¸»æµ |
| **PKM** | Fine-grained MoE | Product Key Memory è®¾è®¡ |
| **PEER** | Fine-grained MoE | ç™¾ä¸‡çº§è½»é‡ä¸“å®¶ï¼Œstate-of-the-art ç»†ç²’åº¦æ–¹æ¡ˆ |
| **OmniMoE (Ours)** | Hybrid Fine-grained | Atomic Experts + Cartesian Router + Expert-Centric Scheduling |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ï¼ˆZero-Shot Accuracyï¼‰
| Model | MMLU | TriviaQA | ARC | PIQA | HellaSwag | OBQA | Winogrande | **Avg** |
|-------|------|----------|-----|------|-----------|--------|------------|--------|
| Dense | 35.4 | 9.4 | 53.4 | 72.9 | 56.1 | 37.0 | 57.3 | 45.9 |
| DeepSeekMoE | 37.1 | 17.4 | 60.7 | 77.2 | 61.2 | 38.9 | 59.1 | 50.2 |
| PEER | 37.4 | 16.9 | 57.4 | 75.9 | 56.3 | 39.1 | 59.4 | 48.9 |
| **OmniMoE (Ours)** | **37.5** | **18.5** | **61.0** | **78.7** | **60.9** | **40.3** | **59.7** | **50.9** |

> âœ… OmniMoE åœ¨ **7é¡¹å¹³å‡å‡†ç¡®ç‡ä¸Šè¾¾åˆ° 50.9%**ï¼Œè¶…è¶Šæœ€å¼ºåŸºçº¿ DeepSeekMoEï¼ˆ+0.7ï¼‰å’Œç»†ç²’åº¦ PEERï¼ˆ+2.0ï¼‰ã€‚

#### ï¼ˆ2ï¼‰æ¨ç†æ•ˆç‡ï¼ˆLatency & Memoryï¼‰
- è¾“å…¥é•¿åº¦ï¼š**4,096 tokens**
- æ¿€æ´»å‚æ•°ç›¸è¿‘ï¼ˆ~28Mï¼‰

| Method | Latency (ms) | Speedup vs PEER |
|--------|---------------|------------------|
| PEER | 73.0 | 1Ã— |
| DeepSeekMoE | 102.0 | â€” |
| **OmniMoE (Ours)** | **6.7** | **10.9Ã— faster** |

> âš¡ OmniMoE å®ç° **10.9å€äº PEER çš„æ¨ç†åŠ é€Ÿ**ï¼Œä¸”å†…å­˜å ç”¨ä¸ç²—ç²’åº¦ MoE ç›¸å½“ã€‚

#### ï¼ˆ3ï¼‰Scaling Laws è¡¨ç°
- åœ¨ä¸åŒè§„æ¨¡ä¸‹ï¼ˆ80M â†’ 1.7B æ¿€æ´»å‚æ•°ï¼‰ï¼ŒOmniMoE å§‹ç»ˆä»¥æ›´ä½ FLOPs è¾¾åˆ°æ›´ä¼˜ validation perplexityã€‚
- è¡¨æ˜å…¶å…¼å…· **æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼ˆcompute efficiencyï¼‰å’Œå‚æ•°æ•ˆç‡ï¼ˆparameter efficiencyï¼‰**ã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| æ–¹æ³•å˜ä½“ | Latencyâ†‘ | Memoryâ†‘ | PPLâ†‘ | Knowledgeâ†“ | Reasoningâ†“ | Expert Usageâ†“ | Unevennessâ†‘ |
|---------|--------|--------|------|-------------|--------------|----------------|--------------|
| Full Model | 1.0x | 1.0x | 1.0Ã— | 1.0x | 1.0x | 100% | 0.24 |
| w/o Shared MLP | 0.86x | 0.98x | 1.2x | 0.91x | 0.79x | 100% | 0.27 |
| w/o Cartesian Router | 30.6x | 337.5x | 1.4x | 0.66x | 0.79x | 4% | 0.77 |
| w/o Expert-Centric Sched | 24.8x | 417.7x | 1.0x | 1.0x | 1.0x | 100% | 0.24 |

> ğŸ’¡ å‘ç°ï¼š
- **Cartesian Router** å¯¹é™ä½è·¯ç”±å¼€é”€è‡³å…³é‡è¦ï¼Œå¦åˆ™å†…å­˜æš´æ¶¨ 300+ å€ã€‚
- **Expert-Centric Scheduling** æ˜¯æ€§èƒ½é£è·ƒçš„å…³é”®ï¼Œæ¶ˆé™¤ memory bottleneckã€‚
- **Shared MLP** è™½è½»å¾®å¢åŠ æˆæœ¬ï¼Œä½†æ˜¾è‘—æå‡æ³›åŒ–ä¸æ¨ç†èƒ½åŠ›ï¼Œä¸å¯æˆ–ç¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ç»†ç²’åº¦ MoE å¯ä»¥æ—¢å¿«åˆå‡†**ï¼šé€šè¿‡ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡ï¼ŒOmniMoE æˆåŠŸæ‰“ç ´â€œç»†ç²’åº¦=æ…¢â€çš„å›ºæœ‰è®¤çŸ¥ã€‚
2. **Atomic Expert + DEA** æä¾›äº†æè‡´çµæ´»çš„å‚æ•°ç»„åˆæœºåˆ¶ï¼Œå®ç° token-level ç²¾ç»†åŒ–æ¿€æ´»ã€‚
3. **Cartesian Product Router** æœ‰æ•ˆè§£å†³äº†ç™¾ä¸‡çº§ä¸“å®¶ä¸‹çš„è·¯ç”±çˆ†ç‚¸é—®é¢˜ï¼Œå¤æ‚åº¦é™è‡³ $O(\sqrt{N})$ã€‚
4. **Expert-Centric Scheduling** æ˜¯æ€§èƒ½çªç ´çš„æ ¸å¿ƒï¼Œå°† scattered I/O è½¬æ¢ä¸º Grouped GEMMï¼Œé‡Šæ”¾ Tensor Core æ½œèƒ½ã€‚
5. **æ··åˆæ¶æ„ä¼˜è¶Šæ€§**ï¼šshared dense MLP å¤„ç†é€šç”¨è¯­ä¹‰ï¼Œrouted atomic experts ä¸“æ³¨é•¿å°¾çŸ¥è¯†ï¼ŒäºŒè€…äº’è¡¥ã€‚

---

### âš ï¸ å±€é™æ€§
- å½“å‰å®ç°ä¾èµ– Triton è‡ªå®šä¹‰ kernelï¼Œåœ¨é€šç”¨æ€§ä¸Šå¯èƒ½å—é™äºç‰¹å®šç¡¬ä»¶å¹³å°ï¼ˆå¦‚ NVIDIA GPUï¼‰ã€‚
- è™½ç„¶é€šä¿¡å¼€é”€é¥±å’Œï¼ˆè§ Appendix Cï¼‰ï¼Œä½†åœ¨è¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­ä»éœ€è¿›ä¸€æ­¥éªŒè¯ç¨³å®šæ€§ã€‚
- å¯¹ extremely sparse åœºæ™¯ï¼ˆæå°‘æ•° token æ¿€æ´»æŸä¸“å®¶ï¼‰çš„ Grouped GEMM åˆ©ç”¨ç‡å¯èƒ½ä¸‹é™ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´çµæ´»çš„ **multi-level routing hierarchy**ï¼Œç»“åˆ coarse + atomic ä¸“å®¶ã€‚
- å°† OmniMoE æ€è·¯æ¨å¹¿è‡³ **vision, multimodal, and agent-based models**ã€‚
- å¼€å‘è‡ªåŠ¨ç¼–è¯‘å™¨æ”¯æŒ **automatic scheduling optimization**ï¼Œé™ä½éƒ¨ç½²é—¨æ§›ã€‚
- ç ”ç©¶å¦‚ä½•åŠ¨æ€è°ƒæ•´ Atomic Expert æ•°é‡ä¸ç»“æ„ï¼Œå®ç° lifelong learningã€‚

---

> ğŸ”— **ä»£ç å·²å¼€æº**ï¼š[https://github.com/flash-algo/omni-moe](https://github.com/flash-algo/omni-moe)  
> ğŸ“„ Preprint å‘å¸ƒæ—¶é—´ï¼šFebruary 6, 2026

</details>

---

### 4. [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)

**Authors**: Jian Chen, Yesheng Liang, Zhijian Liu  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.06036v1  

#### Abstract
Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the targ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDFlash: Block Diffusion for Flash Speculative Decoding

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶é‡‡ç”¨**è‡ªå›å½’ç”Ÿæˆ**ï¼ˆautoregressive generationï¼‰ï¼Œé€ä¸ª token ç”Ÿæˆè¾“å‡ºï¼Œå¯¼è‡´ä¸¥é‡çš„**åºåˆ—åŒ–ç“¶é¢ˆ**ï¼Œè¡¨ç°ä¸ºï¼š
- æ¨ç†å»¶è¿Ÿé«˜
- GPU åˆ©ç”¨ç‡ä½
- é•¿æ–‡æœ¬ç”Ÿæˆæ•ˆç‡å·®

å°½ç®¡å·²æœ‰**æŠ•æœºè§£ç **ï¼ˆspeculative decodingï¼‰æŠ€æœ¯é€šè¿‡è½»é‡çº§ draft model åŠ é€Ÿæ¨ç†ï¼Œä½†ä¸»æµæ–¹æ³•ï¼ˆå¦‚ EAGLE-3ï¼‰ä»ä¾èµ–**è‡ªå›å½’ drafting**ï¼Œæ— æ³•çªç ´åºåˆ—ç”Ÿæˆçš„é™åˆ¶ã€‚

åŒæ—¶ï¼Œè™½ç„¶**æ‰©æ•£è¯­è¨€æ¨¡å‹**ï¼ˆdLLMsï¼‰æ”¯æŒå¹¶è¡Œç”Ÿæˆï¼Œä½†å…¶ç‹¬ç«‹ç”Ÿæˆè´¨é‡é€šå¸¸ä½äºè‡ªå›å½’æ¨¡å‹ï¼Œä¸”å¤šæ­¥å»å™ªè¿‡ç¨‹æ‹–æ…¢é€Ÿåº¦ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒåˆ›æ–°

**DFlash** æ˜¯ä¸€ç§åŸºäº**å—æ‰©æ•£æ¨¡å‹**ï¼ˆblock diffusion modelï¼‰çš„æ–°å‹æŠ•æœºè§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> å°†æ‰©æ•£æ¨¡å‹ä½œä¸º**é«˜æ•ˆå¹¶è¡Œ draft model**ï¼Œåˆ©ç”¨ç›®æ ‡ LLM çš„éšè—ç‰¹å¾è¿›è¡Œæ¡ä»¶å¼•å¯¼ï¼Œå®ç°é«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„å—çº§ token é¢„æµ‹ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

1. **å¹¶è¡Œ drafting æ¶æ„**
   - ä½¿ç”¨è½»é‡çº§ block diffusion model åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å¹¶è¡Œé¢„æµ‹å¤šä¸ª tokenï¼ˆblock-wise generationï¼‰
   - æ˜¾è‘—é™ä½ drafting latencyï¼Œæ‰“ç ´ autoregressive drafting çš„ä¸²è¡Œç“¶é¢ˆ

2. **åŸºäºç›®æ ‡æ¨¡å‹ä¸Šä¸‹æ–‡çš„å¼ºæ¡ä»¶å»ºæ¨¡**
   - ä»ç›®æ ‡ LLM ä¸­æå–å¤šå±‚ hidden featuresï¼Œèåˆä¸º **target context feature**
   - é€šè¿‡ **KV æ³¨å…¥æœºåˆ¶**ï¼ˆKV injectionï¼‰å°†è¯¥ç‰¹å¾æ³¨å…¥åˆ° draft model çš„æ¯ä¸€å±‚ Key å’Œ Value æŠ•å½±ä¸­
   - ä½¿ draft model èƒ½å¤Ÿâ€œç»§æ‰¿â€ç›®æ ‡æ¨¡å‹å¯¹æœªæ¥ token çš„éšå«é¢„æµ‹èƒ½åŠ›

3. **è®­ç»ƒç­–ç•¥ä¼˜åŒ–**
   - **éšæœºé”šç‚¹é‡‡æ ·**ï¼šè®­ç»ƒæ—¶éšæœºé€‰æ‹©å“åº”ä¸­çš„ token ä½œä¸º block èµ·å§‹ç‚¹ï¼Œæå‡æ•°æ®å¤šæ ·æ€§
   - **ä½ç½®åŠ æƒæŸå¤±å‡½æ•°**ï¼šå¯¹ block å†…é å‰çš„ token åˆ†é…æ›´é«˜æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰ï¼Œå› ä¸ºæ—©æœŸé”™è¯¯ä¼šé˜»æ–­æ•´ä¸ª block çš„æ¥å—
   - **å…±äº«åµŒå…¥å±‚ä¸ LM Head**ï¼šä¸ç›®æ ‡æ¨¡å‹å…±äº« token embedding å’Œè¾“å‡ºå¤´ï¼Œå‡å°‘å‚æ•°é‡å¹¶å¢å¼ºå¯¹é½

4. **è½»é‡åŒ–è®¾è®¡**
   - draft model ä»…éœ€ 5 å±‚ Transformerï¼ˆQwen3-Coder ä¸º 8 å±‚ï¼‰ï¼Œå‚æ•°æå°‘
   - æ”¯æŒé«˜æ•ˆé•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸éƒ¨ç½²

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | ç±»å‹ | Drafting æ–¹å¼ | æ˜¯å¦å¹¶è¡Œ | Acceptance Length | Latency | Memory |
|------|------|----------------|-----------|--------------------|---------|--------|
| EAGLE-3 | Autoregressive | Tree-based | âŒ ä¸²è¡Œ | ä¸­ç­‰ (~3â€“4) | è¾ƒé«˜ | ä½ |
| DiffuSpec / SpecDiff-2 | Diffusion | Full dLLM (7B) | âœ… å¹¶è¡Œ | é«˜ | é«˜ï¼ˆå¤§æ¨¡å‹ï¼‰ | é«˜ |
| PARD | AR mimic diffusion | Parallel AR | âœ… | ä½ | ä½ | ä½ |
| **DFlash** | **Block Diffusion** | **Parallel block** | âœ…âœ… | **æé«˜ (~6â€“8)** | **æä½** | **ä½** |

> âœ… DFlash å®ç°äº†**é«˜ acceptance length** ä¸**ä½ drafting latency** çš„å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†

- **è®­ç»ƒæ•°æ®**ï¼š
  - æ··åˆçº¦ 80 ä¸‡æ ·æœ¬
  - æ¥æºï¼šNVIDIA Nemotron Post-Training Dataset V2ã€CodeAlpaca
  - ä½¿ç”¨ç›®æ ‡æ¨¡å‹ç”Ÿæˆå“åº”ä»¥ä¿è¯å¯¹é½ï¼ˆtarget-aligned responsesï¼‰

- **è¯„ä¼°ä»»åŠ¡åˆ†ç±»**ï¼š
  - **Math**ï¼šGSM8Kã€MATH-500ã€AIME25
  - **Code**ï¼šHumanEvalã€MBPPã€LiveCodeBench (LCB)
  - **Chat**ï¼šMT-Benchã€Alpaca

---

### âš™ï¸ å®éªŒè®¾ç½®

- **æ¨¡å‹**ï¼š
  - Qwen3 ç³»åˆ—ï¼šQwen3-4Bã€Qwen3-8Bã€Qwen3-Coder-30B-A3B-Instruct
  - LLaMA-3.1-8B-Instruct

- **ç¡¬ä»¶å¹³å°**ï¼š
  - ä¸»è¦ä½¿ç”¨ NVIDIA H200 å’Œ B200 GPU
  - SGLang æ¡†æ¶ + FlashAttention-4ï¼ˆFA4ï¼‰åç«¯ç”¨äºçœŸå®æœåŠ¡åœºæ™¯æµ‹è¯•

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **å¹³å‡æ¥å—é•¿åº¦**ï¼ˆAverage Acceptance Length, $\bar{T}$ï¼‰ï¼šæ¯è½®éªŒè¯æˆåŠŸæ¥å—çš„ token æ•°
  - **ç«¯åˆ°ç«¯åŠ é€Ÿæ¯”**ï¼ˆEnd-to-end Speedupï¼‰ï¼šç›¸å¯¹äºæ ‡å‡† autoregressive decoding çš„ååæå‡
  - **Throughput (tokens/sec)**ï¼šåœ¨å¹¶å‘è¯·æ±‚ä¸‹çš„å®é™…ååé‡

- **åŸºçº¿æ–¹æ³•å¯¹æ¯”**ï¼š
  - **Baseline**ï¼šæ ‡å‡† autoregressive decoding
  - **EAGLE-3**ï¼šå½“å‰æœ€å…ˆè¿›çš„ speculative decoding æ–¹æ³•ï¼ˆtree-based, autoregressive draftingï¼‰
  - ï¼ˆæœªæ¯”è¾ƒå…¶ä»– dLLM-based æ–¹æ³•å› ç¼ºä¹å¼€æºå®ç°ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Table 3ï¼‰

#### åœ¨ Qwen3-8B ä¸Šçš„è¡¨ç°ï¼ˆgreedy decoding, temperature=0ï¼‰ï¼š

| æ–¹æ³• | å¹³å‡åŠ é€Ÿæ¯” | æœ€é«˜åŠ é€Ÿæ¯” | å¹³å‡æ¥å—é•¿åº¦ $\bar{T}$ |
|------|------------|-------------|--------------------------|
| EAGLE-3 (16) | ~1.8â€“2.2Ã— | â€” | ~3.0â€“3.7 |
| **DFlash (16)** | **~4.9Ã—** | **6.1Ã—** | **~6.5â€“7.9** |

> âœ… DFlash å®ç° **è¶…è¿‡ 6Ã— çš„ lossless åŠ é€Ÿ**ï¼Œæ˜¯ EAGLE-3 çš„ **2.5 å€ä»¥ä¸Š**

#### åœ¨ SGLang æ¡†æ¶ä¸‹çš„çœŸå®æœåŠ¡è¡¨ç°ï¼ˆQwen3-8B, concurrency=16ï¼‰ï¼š

| æ–¹æ³• | Throughput (tok/s) | Speedup | $\bar{T}$ |
|------|---------------------|---------|-----------|
| Baseline | 868 | 1.0Ã— | â€” |
| DFlash | **4858** | **5.1Ã—** | 8.0 |

> âœ… å³ä½¿åœ¨é«˜å¹¶å‘ä¸‹ä»ä¿æŒæ˜¾è‘—åŠ é€Ÿï¼ŒéªŒè¯äº†å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§

#### åœ¨ LLaMA-3.1-8B ä¸Šçš„ç»“æœï¼ˆSGLang, FA4 backendï¼‰ï¼š

| æ–¹æ³• | Task | Speedup (@concurrency=1) | $\bar{T}$ |
|------|------|----------------------------|-----------|
| EAGLE-3 (60) | HumanEval | 2.0Ã— | 4.65 |
| **DFlash (10)** | HumanEval | **2.8Ã—** | **4.91** |

> âœ… åœ¨ä¸åŒæ¶æ„ä¸Šå‡ä¼˜äº EAGLE-3ï¼Œæ³›åŒ–èƒ½åŠ›å¼º

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### â–¶ï¸ ä¸åŒ draft model æ·±åº¦çš„å½±å“ï¼ˆTable 5ï¼‰

| å±‚æ•° | Math500 Speedup | $\bar{T}$ |
|------|------------------|-----------|
| 3-L | 4.69Ã— | 5.64 |
| **5-L** | **4.71Ã—** | **5.99** |
| 8-L | 4.64Ã— | 6.33 |

> âœ… **5 å±‚æ¨¡å‹å–å¾—æœ€ä½³å¹³è¡¡**ï¼šæ›´æ·±è™½èƒ½æé«˜ $\bar{T}$ï¼Œä½† drafting latency ä¸Šå‡åè€Œé™ä½æ•´ä½“ speedup

#### â–¶ï¸ ç›®æ ‡æ¨¡å‹éšè—å±‚æ•°é‡çš„å½±å“ï¼ˆTable 6ï¼‰

| æå– hidden features å±‚æ•° | $\bar{T}$ |
|--------------------------|----------|
| 3 | ~4.5 |
| **5** | **~5.6â€“5.8** |

> âœ… æ›´å¤šå±‚ç‰¹å¾æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡ acceptance length

#### â–¶ï¸ è®­ç»ƒä¸æ¨ç† block size åŒ¹é…æ€§ï¼ˆTable 7ï¼‰

| Train â†’ Test | Math500 $\bar{T}$ |
|--------------|---------------------|
| 16 â†’ 16 | 6.33 |
| 16 â†’ 8 | 5.09 |
| 8 â†’ 16 | 5.02 |
| 8 â†’ 8 | 5.21 |

> âœ… **å¤§ block è®­ç»ƒæ¨¡å‹å¯è‰¯å¥½æ³›åŒ–è‡³å° block æ¨ç†**ï¼Œæ”¯æŒåŠ¨æ€è°ƒåº¦ï¼›åä¹‹ä¸è¡Œ

#### â–¶ï¸ æŸå¤±å‡½æ•°åŠ æƒ vs å‡åŒ€åŠ æƒï¼ˆFigure 5ï¼‰

- ä½¿ç”¨æŒ‡æ•°è¡°å‡çš„ä½ç½®åŠ æƒæŸå¤± â†’ **æ”¶æ•›æ›´å¿«ã€acceptance length æ›´é«˜**

#### â–¶ï¸ æ˜¯å¦ä½¿ç”¨ç›®æ ‡æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆTable 8ï¼‰

- æ—  context feature çš„ diffusion drafterï¼š
  - ä»…è¾¾åˆ° ~2.8â€“3.7Ã— speedup
  - $\bar{T} \approx 3.3â€“4.6$
> â—è¯æ˜ï¼š**ç›®æ ‡æ¨¡å‹çš„ hidden features æ˜¯å®ç°é«˜è´¨é‡ drafting çš„å…³é”®**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **æ‰©æ•£æ¨¡å‹ä¸å¿…è¿½æ±‚ç«¯åˆ°ç«¯ç”Ÿæˆè´¨é‡**ï¼Œå¯åœ¨ speculative decoding ä¸­ä½œä¸ºé«˜æ€§èƒ½ draft model å‘æŒ¥ç‹¬ç‰¹ä¼˜åŠ¿ã€‚
2. **ç›®æ ‡æ¨¡å‹çš„ hidden features å«æœ‰ä¸°å¯Œçš„æœªæ¥ token ä¿¡æ¯**ï¼Œå¯ç”¨äºæŒ‡å¯¼ draft model è¿›è¡Œé«˜è´¨é‡å¹¶è¡Œé¢„æµ‹ã€‚
3. **KV injection + block diffusion** æ¶æ„å®ç°äº† drafting latency ä¸ acceptance length çš„åŒé‡ä¼˜åŒ–ã€‚
4. DFlash åœ¨å¤šç§æ¨¡å‹ã€ä»»åŠ¡ã€æ¡†æ¶ä¸‹å‡å®ç° **>6Ã— lossless åŠ é€Ÿ**ï¼Œè¿œè¶… EAGLE-3 ç­‰ SOTA æ–¹æ³•ã€‚
5. å…¶è®¾è®¡å…è®¸çµæ´»è°ƒæ•´ block sizeã€depth ç­‰å‚æ•°ï¼Œåœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸­ä¿æŒé«˜æ•ˆã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ–ç›®æ ‡æ¨¡å‹çš„ hidden states æå–**ï¼Œéœ€ä¿®æ”¹æˆ– hook ç›®æ ‡æ¨¡å‹å†…éƒ¨ç»“æ„ï¼Œå¯èƒ½å¢åŠ é›†æˆå¤æ‚åº¦ã€‚
2. å½“å‰å®ç°ä¸»è¦é’ˆå¯¹ decoder-only æ¨¡å‹ï¼Œå¯¹ encoder-decoder æ¶æ„é€‚é…å°šä¸æ˜ç¡®ã€‚
3. è™½ç„¶ draft model å¾ˆå°ï¼Œä½†è®­ç»ƒé˜¶æ®µéœ€è¦ç¼“å­˜å¤§é‡ target hidden featuresï¼Œå­˜å‚¨å¼€é”€è¾ƒå¤§ï¼ˆå°¤å…¶ç¦»çº¿è®­ç»ƒï¼‰ã€‚
4. æœªå¼€æ”¾ä¸å…¶ä»– dLLM-based speculative æ–¹æ³•ï¼ˆå¦‚ DiffuSpecï¼‰çš„ç›´æ¥å¯¹æ¯”ï¼ˆå› æ— å¼€æºä»£ç ï¼‰ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **è‡ªé€‚åº” block size è°ƒåº¦**ï¼šæ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´ block å¤§å°ä»¥æœ€å¤§åŒ–åå
2. **zero-shot transferability**ï¼šæ¢ç´¢ä¸€ä¸ªé€šç”¨ draft model æ˜¯å¦å¯è·¨å¤šä¸ªç›®æ ‡æ¨¡å‹ä½¿ç”¨
3. **è’¸é¦æˆ–å‹ç¼© target context feature**ï¼šé™ä½è®­ç»ƒå’Œéƒ¨ç½²æ—¶çš„å†…å­˜å ç”¨
4. **æ”¯æŒæ›´å¤š generation pattern**ï¼šå¦‚ streamingã€function calling ç­‰å¤æ‚åœºæ™¯
5. **æ¢ç´¢ diffusion drafter çš„æ¶æ„æœç´¢**ï¼šå¯»æ‰¾æ›´ä¼˜çš„è½»é‡ç»“æ„

---

## æ€»ç»“

> **DFlash æˆåŠŸå°† diffusion LLM çš„å¹¶è¡Œæ€§ä¸ speculative decoding çš„å¯é æ€§ç»“åˆï¼Œæå‡ºäº†ä¸€ç§â€œè½»é‡æ‰©æ•£ draft + å¼ºå¤§è‡ªå›å½’éªŒè¯â€çš„æ–°èŒƒå¼ï¼Œä¸ä»…å¤§å¹…æå‡äº†æ¨ç†é€Ÿåº¦ï¼ˆæœ€é«˜ >6Ã—ï¼‰ï¼Œè¿˜æ­ç¤ºäº† diffusion æ¨¡å‹åœ¨ LLM åŠ é€Ÿä¸­çš„å…¨æ–°è§’è‰²â€”â€”ä¸å†æ˜¯æ›¿ä»£è€…ï¼Œè€Œæ˜¯é«˜æ•ˆçš„ååŒè€…ã€‚**

è¿™ä¸€å·¥ä½œæœ‰æœ›æ¨åŠ¨ diffusion LLM çš„å®ç”¨åŒ–è¿›ç¨‹ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£é«˜æ•ˆ LLM inference æ¡†æ¶æä¾›é‡è¦å‚è€ƒã€‚

</details>

---

### 5. [Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics](https://arxiv.org/abs/2602.04928)

**Authors**: Ruizhe Zhong, Jiesong Lian, Xiaoyue Mi, Zixiang Zhou, Yuan Zhou, Qinglin Lu, Junchi Yan  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2602.04928v1  

#### Abstract
While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠEuphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamicsã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰åŸºäº **Reinforcement Learning (RL)** çš„è§†é¢‘ç”Ÿæˆåè®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ Flow-GRPOã€DanceGRPOï¼‰åœ¨å¯¹é½äººç±»åå¥½æ–¹é¢å­˜åœ¨**æ¢ç´¢æ•ˆç‡ä½ä¸‹**çš„é—®é¢˜ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºæ— å¯¼å‘çš„éšæœºæ‰°åŠ¨ï¼ˆundirected stochasticityï¼‰è¿›è¡Œç­–ç•¥æ¢ç´¢ï¼Œå¹¶ä»…åœ¨å®Œæ•´è§†é¢‘ç”Ÿæˆåè·å¾—ç¨€ç–çš„ç»“æœå¥–åŠ±ï¼ˆoutcome rewardsï¼‰ï¼Œå¯¼è‡´ï¼š
- é«˜è´¨é‡æ ·æœ¬éš¾ä»¥è¢«å‘ç°ï¼›
- è®­ç»ƒè¿‡ç¨‹æ•°æ®åˆ©ç”¨ç‡ä½ï¼›
- æ”¶æ•›é€Ÿåº¦æ…¢ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡º **Euphonium**ï¼Œä¸€ç§é€šè¿‡**è¿‡ç¨‹å¥–åŠ±æ¢¯åº¦å¼•å¯¼çš„éšæœºåŠ¨åŠ›å­¦**æ¥ä¸»åŠ¨å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹çš„æ–°æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

#### âœ… **Guided Exploration via Process Reward Gradient**
å°†é‡‡æ ·è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªç†è®ºä¸Šæœ‰æ®å¯ä¾çš„ **Stochastic Differential Equation (SDE)**ï¼Œæ˜¾å¼åœ°å°† **Process Reward Model (PRM)** çš„æ¢¯åº¦æ³¨å…¥åˆ° flow drift ä¸­ï¼š
$$
dX_t = \left[u_\theta(X_t,t) - \epsilon_t \nabla U_t(X_t)\right]dt + \sqrt{2\epsilon_t}dW_t
$$
å…¶ä¸­ $U_t(x)$ æ˜¯ç»“åˆäº† flow prior å’Œ PRM çš„å¢å¼ºåŠ¿èƒ½å‡½æ•°ã€‚è¿™å®ç°äº†**æ¯ä¸€æ­¥çš„å¯†é›†å¼•å¯¼**ï¼Œä½¿æ¨¡å‹åœ¨æ½œç©ºé—´ä¸­ä¸»åŠ¨å‘é«˜å¥–åŠ±åŒºåŸŸç§»åŠ¨ã€‚

#### âœ… **Dual-Reward Optimization**
å¼•å…¥åŒå¥–åŠ±æœºåˆ¶ï¼š
- **Latent-space Process Reward**ï¼šæ¥è‡ª PRMï¼Œåœ¨ä¸­é—´æ—¶é—´æ­¥æä¾›ç»†ç²’åº¦åé¦ˆï¼Œæå‡ä¿¡ç”¨åˆ†é…æ•ˆç‡ï¼›
- **Pixel-space Outcome Reward**ï¼šæ¥è‡ª ORMï¼ˆOutcome Reward Modelï¼‰ï¼Œç¡®ä¿æœ€ç»ˆè§†è§‰è´¨é‡å’Œæç¤ºä¸€è‡´æ€§ã€‚

#### âœ… **Reward-Gradient-Free Inference**
è®¾è®¡äº†ä¸€ä¸ª**ç­–ç•¥è’¸é¦ç›®æ ‡ï¼ˆPolicy Distillationï¼‰**ï¼Œå°†è®­ç»ƒé˜¶æ®µçš„å¥–åŠ±æ¢¯åº¦ä¿¡å·å†…åŒ–åˆ° flow network æƒé‡ä¸­ï¼Œä»è€Œåœ¨æ¨ç†æ—¶æ— éœ€åŠ è½½å¤–éƒ¨ PRMï¼Œä¿æŒä¸åŸºç¡€ç”Ÿæˆå™¨ç›¸åŒçš„éƒ¨ç½²æ–¹å¼ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | Euphonium | Flow-GRPO / DanceGRPO |
|------|----------|------------------------|
| æ¢ç´¢æ–¹å¼ | **æœ‰å‘å¼•å¯¼æ¢ç´¢**ï¼ˆreward gradient æ˜¾å¼å¼•å¯¼ï¼‰ | æ— å¯¼å‘éšæœºæ¢ç´¢ |
| å¥–åŠ±å¯†åº¦ | **å¯†é›†è¿‡ç¨‹å¥–åŠ± + ç»“æœå¥–åŠ±** | ä»…ç¨€ç–ç»“æœå¥–åŠ± |
| æ¨ç†ä¾èµ– | âŒ ä¸éœ€è¦ PRMï¼ˆè’¸é¦åï¼‰ | âœ… é€šå¸¸ä¸ä¾èµ–ï¼Œä½†æ— æ³•åˆ©ç”¨è¿‡ç¨‹ä¿¡å· |
| æ”¶æ•›é€Ÿåº¦ | â¬†ï¸ **å¿« 1.66Ã—** | åŸºå‡†æ°´å¹³ |
| å¯¹é½æ•ˆæœ | â¬†ï¸ æ›´ä¼˜ï¼ˆVBench2 æ€»åˆ†æœ€é«˜ï¼‰ | è¾ƒå¼± |

æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ç†è®ºä¸Šç»Ÿä¸€äº†ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ Flow-GRPOã€DanceGRPO å¯è§†ä¸º reward-free ç‰¹ä¾‹ï¼‰ï¼Œæä¾›äº†æ›´å¹¿ä¹‰çš„è§†è§’ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **Reward Model è®­ç»ƒæ•°æ®**ï¼š
  - åŒ…å« 200,000 ä¸ªç”± 20,000 ä¸ªå”¯ä¸€ prompt ç”Ÿæˆçš„è§†é¢‘æ ·æœ¬ï¼›
  - é‡‡ç”¨æˆå¯¹æ ‡æ³¨ï¼ˆpairwise preference annotationsï¼‰ï¼ŒåŒºåˆ†æ­£è´Ÿæ ·æœ¬ï¼ˆåŸºäºè§†è§‰è´¨é‡ä¸è¿åŠ¨è¿è´¯æ€§ï¼‰ã€‚
- **GRPO è®­ç»ƒæ•°æ®**ï¼š
  - ä½¿ç”¨ 10,000 ä¸ª promptï¼ˆæ¥è‡ª DanceGRPO å’Œå†…éƒ¨äººåƒç±»æ•°æ®æºï¼‰ï¼›
  - ä¸¥æ ¼éš”ç¦»äº reward model çš„è®­ç»ƒé›†ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **ä¸»å¹²æ¨¡å‹**ï¼šHunyuanVideo-14Bï¼ˆå¼€æºå¤§æ¨¡å‹ï¼‰
- **é‡‡æ ·æ­¥æ•°ï¼ˆè®­ç»ƒï¼‰**ï¼š16 æ­¥ Euler-Maruyama ç¦»æ•£åŒ–
- **è¯„ä¼°åˆ†è¾¨ç‡ä¸å¸§æ•°**ï¼š640Ã—640, 81 framesï¼ˆé«˜åˆ†è¾¨ç‡é•¿åºåˆ—ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š**VBench2**ï¼ˆæƒå¨è§†é¢‘ç”Ÿæˆè¯„æµ‹å¥—ä»¶ï¼‰ï¼ŒåŒ…å«ä»¥ä¸‹å­é¡¹ï¼š
  - Total Score
  - Creativity
  - Commonsense
  - Controllability
  - Human Fidelity
  - Physics

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ |
|------|------|
| Base Model (HunyuanVideo) | æœªç»è¿‡ RL å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ |
| Flow-GRPO (Liu et al., 2025b) | å¼•å…¥ SDE è¿›è¡Œéšæœºæ¢ç´¢çš„ RL æ–¹æ³• |
| DanceGRPO (Xue et al., 2025) | ä½¿ç”¨å…±äº«å™ªå£°ç­–ç•¥æ”¹è¿›ä¿¡ç”¨åˆ†é…çš„ RL æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆVBench2 æ€»åˆ†ï¼‰
| æ–¹æ³• | Total Score |
|------|-------------|
| Base Model | 51.09 |
| Flow-GRPO | 51.52 |
| DanceGRPO | 51.85 |
| **Euphonium (Ours)** | **54.24** âœ… |

> **æå‡å¹…åº¦**ï¼šç›¸æ¯”æœ€å¼ºåŸºçº¿ DanceGRPO æå‡ **+2.39 åˆ†**ï¼Œç»å¯¹é¢†å…ˆã€‚

### å„ç»´åº¦è¡¨ç°ï¼ˆéƒ¨åˆ†çªå‡ºé¡¹ï¼‰
| ç»´åº¦ | Euphonium | æœ€ä½³åŸºçº¿ | æå‡ |
|------|-----------|----------|------|
| **Commonsense** | **67.17** | 62.87 (Base) | +4.3 |
| **Controllability** | **26.88** | 25.08 (DanceGRPO) | +1.8 |
| **Human Fidelity** | **88.91** | 88.10 (DanceGRPO) | +0.81 |
| **Physics** | **46.84** | 45.15 (Base) | +1.69 |

> åœ¨ **4/5 å­ç»´åº¦**ä¸Šå–å¾—ç¬¬ä¸€ï¼Œä»…åœ¨ Creativity ä¸Šç•¥ä½äº Flow-GRPOï¼ˆ41.42 vs 42.42ï¼‰ï¼Œä½†ä»å…·ç«äº‰åŠ›ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **æ”¶æ•›é€Ÿåº¦**ï¼šè¾¾åˆ°ç›¸åŒæ€§èƒ½æ°´å¹³æ‰€éœ€è®­ç»ƒæ­¥æ•°å‡å°‘ **1.66Ã—**ï¼ˆè§ Figure 1ï¼‰ï¼›
- **é‡‡æ ·æ•ˆç‡æ›´é«˜**ï¼šå¾—ç›Šäºè¿‡ç¨‹å¥–åŠ±çš„å¯†é›†æŒ‡å¯¼ï¼Œæ›´å¿«æ‰¾åˆ°é«˜è´¨é‡è½¨è¿¹ï¼›
- **è§†è§‰è´¨é‡æ›´ä¼˜**ï¼šFigure 2 æ˜¾ç¤º Euphonium ç”Ÿæˆçš„è§†é¢‘åœ¨åŠ¨ä½œè¿è´¯æ€§ã€ç»†èŠ‚è¿˜åŸå’Œ prompt adherence ä¸Šæ˜æ˜¾ä¼˜äºåŸºçº¿ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ğŸ”¹ ç§»é™¤ä¸»åŠ¨å¼•å¯¼ï¼ˆw/o Active Steeringï¼‰
| è®¾ç½® | VBench2 Total |
|------|---------------|
| å®Œæ•´ Euphonium | 54.24 |
| w/o Reward Gradient Guidance | 53.61 |
| **ä¸‹é™ Î”** | **-0.63** |

> è¡¨æ˜ reward gradient å¼•å¯¼å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚

#### ğŸ”¹ ç§»é™¤åŒå¥–åŠ±ç»„ä»¶
| è®¾ç½® | VBench2 Total |
|------|---------------|
| w/o PRM Advantageï¼ˆæ— è¿‡ç¨‹å¥–åŠ±ä¼˜åŠ¿ï¼‰ | 53.95 |
| w/o ORM Advantageï¼ˆæ— ç»“æœå¥–åŠ±ä¼˜åŠ¿ï¼‰ | 53.59 |

> è¯´æ˜ä¸¤è€…å‡é‡è¦ï¼Œå°¤å…¶æ˜¯ ORM å¯¹æœ€ç»ˆè§†è§‰ä¿çœŸåº¦çš„å…³é”®ä½œç”¨ã€‚

#### ğŸ”¹ Reward-Gradient Guidance è¶…å‚æ•°åˆ†æ
| æŒ‡å¯¼å¼ºåº¦ Î» | æ€»åˆ† |
|------------|------|
| 0.01ï¼ˆå¤ªå¼±ï¼‰ | 53.61 |
| **0.1ï¼ˆé€‚ä¸­ï¼‰** | **54.24** âœ… |
| 1.0ï¼ˆå¤ªå¼ºï¼‰ | 52.86 |

> è¿‡å¼ºå¼•å¯¼ä¼šç ´å flow dynamicsï¼Œå¯¼è‡´ç”Ÿæˆå¤±çœŸã€‚

| æŒ‡å¯¼æ—¶é—´çª—å£ | æ€»åˆ† |
|--------------|------|
| æ— æŒ‡å¯¼ | 53.61 |
| å…¨ç¨‹æŒ‡å¯¼ (0â‰¤tâ‰¤1) | 53.64 |
| **ååŠæ®µæŒ‡å¯¼ (0.5â‰¤tâ‰¤1)** | **54.24** âœ… |
| åå››åˆ†ä¹‹ä¸€ (0.75â‰¤tâ‰¤1) | 54.14 |

> ååŠæ®µæŒ‡å¯¼æœ€ä¼˜â€”â€”é¿å¼€æ—©æœŸç»“æ„å½¢æˆå¹²æ‰°ï¼Œä¿ç•™è¶³å¤Ÿä¼˜åŒ–çª—å£ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æœ‰å‘æ¢ç´¢æ˜¾è‘—ä¼˜äºæ— å¯¼å‘æ¢ç´¢**ï¼šé€šè¿‡å°† PRM æ¢¯åº¦æ³¨å…¥ SDE driftï¼Œå®ç° step-level å¯†é›†å¼•å¯¼ï¼Œå¤§å¹…æå‡æ¢ç´¢æ•ˆç‡ã€‚
2. **åŒå¥–åŠ±æœºåˆ¶ååŒå¢æ•ˆ**ï¼š
   - Latent PRM æä¾›é«˜æ•ˆä¿¡ç”¨åˆ†é…ï¼›
   - Pixel ORM é”šå®šæœ€ç»ˆæ„ŸçŸ¥è´¨é‡ã€‚
3. **ç­–ç•¥è’¸é¦æ˜¯å®ç”¨éƒ¨ç½²çš„å…³é”®**ï¼š
   - â€œInference RGGâ€ å› éœ€åŒæ—¶åŠ è½½ PRM å¯¼è‡´ OOMï¼ˆå•å¡ H20 ä¸Šå¤±è´¥ï¼‰ï¼›
   - **Distilled æ¨¡å‹æ— éœ€å¤–éƒ¨ LRMï¼Œæ¨ç†è½»é‡ä¸”æ€§èƒ½æœ€ä½³ï¼ˆ54.24ï¼‰**ã€‚
4. **ç†è®ºç»Ÿä¸€æ€§**ï¼šEuphonium çš„ SDE å½¢å¼åœ¨ reward=0 æ—¶é€€åŒ–ä¸º Flow-GRPO/DanceGRPOï¼Œè¯æ˜å…¶ä¸ºé€šç”¨æ¡†æ¶ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **Latent PRM çš„æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼š
   - å½“å‰ PRM ä¾èµ–ç‰¹å®š VAE çš„ latent spaceï¼Œéš¾ä»¥è·¨æ¶æ„è¿ç§»ï¼›
   - å¯¹ä¸åŒç”Ÿæˆå™¨éœ€é‡æ–°è®­ç»ƒ PRMã€‚
2. **Latent Space Reward çš„å¯é æ€§å‡è®¾**ï¼š
   - è™½ç„¶ PRM åœ¨å„å™ªå£°çº§åˆ«ä¸‹å‡†ç¡®ç‡ >70%ï¼Œä½†ä»å¯èƒ½è¯¯åˆ¤å¤æ‚è¯­ä¹‰çŠ¶æ€ã€‚
3. **è®¡ç®—å¼€é”€ä»å­˜åœ¨è¾¹é™…å¢åŠ **ï¼š
   - å°½ç®¡ overhead å¾ˆå°ï¼ˆå»¶è¿Ÿ +2.4%ï¼Œæ˜¾å­˜ +8.5%ï¼‰ï¼Œä½†åœ¨æè‡´æˆæœ¬æ•æ„Ÿåœºæ™¯ä»éœ€æƒè¡¡ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘é€šç”¨ Latent Reward Model**ï¼š
   - åˆ©ç”¨ **Representation Autoencoder (RAE)** æˆ–å›ºå®šè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ DINOv2ï¼‰æ„å»ºè·¨æ¨¡å‹å…±äº« latent spaceï¼›
   - å®ç°â€œå³æ’å³ç”¨â€çš„ backbone-agnostic PRMã€‚
2. **åŠ¨æ€è°ƒæ•´æŒ‡å¯¼å¼ºåº¦**ï¼š
   - æ ¹æ®ç”Ÿæˆé˜¶æ®µè‡ªé€‚åº”è°ƒèŠ‚ Î» æˆ–æ¿€æ´»çª—å£ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–å¼•å¯¼èŠ‚å¥ã€‚
3. **æ‰©å±•è‡³å…¶ä»–ç”Ÿæˆä»»åŠ¡**ï¼š
   - åº”ç”¨äº text-to-audioã€3D generation ç­‰éœ€è¦é•¿æœŸä¸€è‡´æ€§æ§åˆ¶çš„ä»»åŠ¡ã€‚
4. **æ¢ç´¢æ›´é«˜æ•ˆçš„æ¢¯åº¦ä¼°è®¡æ–¹å¼**ï¼š
   - å¦‚ low-rank approximation æˆ– implicit differentiationï¼Œé™ä½ PRM è®­ç»ƒæˆæœ¬ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **Euphonium é€šè¿‡å¼•å…¥â€œè¿‡ç¨‹å¥–åŠ±æ¢¯åº¦å¼•å¯¼ + åŒå¥–åŠ±ä¼˜åŒ– + ç­–ç•¥è’¸é¦â€çš„é—­ç¯è®¾è®¡ï¼Œåœ¨ä¸å¢åŠ æ¨ç†è´Ÿæ‹…çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´é«˜æ•ˆã€æ›´ç²¾å‡†çš„äººç±»åå¥½å¯¹é½ï¼Œæ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆ RL å¾®è°ƒæŠ€æœ¯çš„å‘å±•ã€‚**

</details>

---

### 6. [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)

**Authors**: Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar, Xiaoxiao Li, Weijie Xu, Xin Chen, Jindong Wang  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.03955v1  

#### Abstract
While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weigh...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šAgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systems, MASï¼‰é€šè¿‡å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„è¾©è®ºã€æ‰¹åˆ¤å’Œå…±è¯†æœºåˆ¶ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼ŒMAS å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼š
- **é«˜è®¡ç®—æˆæœ¬**ï¼šæ¨ç†æ—¶éœ€å¤šæ¬¡è°ƒç”¨å¤šä¸ªæ¨¡å‹ï¼Œå¯¼è‡´å»¶è¿Ÿé«˜ã€èµ„æºæ¶ˆè€—å¤§ã€‚
- **é”™è¯¯ä¼ æ’­é£é™©**ï¼šä¸ªä½“å¹»è§‰æˆ–åè§å¯èƒ½åœ¨äº¤äº’ä¸­è¢«æ”¾å¤§ï¼Œå½±å“æ•´ä½“é²æ£’æ€§ã€‚

å› æ­¤ï¼Œå¦‚ä½•å°† MAS çš„ååŒæ¨ç†èƒ½åŠ›â€œå†…åŒ–â€åˆ°å•ä¸ªæ¨¡å‹ä¸­ï¼Œä½¿å…¶å…·å¤‡å¤šæ™ºèƒ½ä½“çš„æ€ç»´æ¨¡å¼ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ¨ç†ï¼Œæˆä¸ºä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯**
æœ¬æ–‡æå‡ºäº† **AgentArk**ï¼Œä¸€ä¸ªå°†å¤šæ™ºèƒ½ä½“æ¨ç†åŠ¨æ€è’¸é¦åˆ°å•ä¸€ LLM ä¸­çš„é€šç”¨æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†å¤šæ™ºèƒ½ä½“åœ¨æµ‹è¯•æ—¶çš„æ˜¾å¼äº¤äº’è¿‡ç¨‹ï¼Œè½¬åŒ–ä¸ºå•ä¸ªæ¨¡å‹å†…éƒ¨éšå«çš„æ¨ç†èƒ½åŠ›ã€‚

ä¸ºæ­¤ï¼ŒAgentArk è®¾è®¡äº†ä¸‰ä¸ªå±‚æ¬¡é€’è¿›çš„è’¸é¦ç­–ç•¥ï¼š

| æ–¹æ³• | æè¿° |
|------|------|
| **Reasoning-Enhanced SFT (RSFT)** | åœ¨ç›‘ç£å¾®è°ƒä¸­å¼•å…¥å®Œæ•´çš„å¤šæ™ºèƒ½ä½“æ¨ç†è½¨è¿¹ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ ç”Ÿæˆé«˜è´¨é‡çš„ CoT é“¾æ¡ã€‚ |
| **Data Augmentation (DA)** | ä»å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­æå–å¤šæ ·åŒ–çš„æ­£ç¡®æ¨ç†è·¯å¾„è¿›è¡Œæ•°æ®å¢å¼ºï¼Œæå‡æ¨¡å‹å¯¹ä¸åŒè§£é¢˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ |
| **Process-Aware Distillation (PAD)** | åˆ©ç”¨ **Process Reward Model (PRM)** å¯¹æ¯ä¸€æ­¥æ¨ç†æ‰“åˆ†ï¼Œå¹¶ç»“åˆ **Group Relative Policy Optimization (GRPO)** è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè®©å­¦ç”Ÿæ¨¡å‹å­¦ä¼šè‡ªæˆ‘çº é”™ä¸åæ€ã€‚ |

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
- **å»è€¦åˆè®¾è®¡**ï¼šä¸ä¾èµ–ç‰¹å®šçš„ MAS æ¶æ„æˆ–è§’è‰²è®¾å®šï¼Œé€‚ç”¨äºä»»æ„åŸºäºäº¤äº’çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚
- **è¿‡ç¨‹çº§ç›‘ç£**ï¼šè¶…è¶Šä»…æ¨¡ä»¿æœ€ç»ˆç­”æ¡ˆçš„ä¼ ç»Ÿè’¸é¦ï¼Œæ•æ‰â€œå†²çª-ä¿®æ­£â€çš„è¾©è¯æ¨ç†åŠ¨æ€ã€‚
- **å¯æ‰©å±•æ€§å¼º**ï¼šæ”¯æŒè·¨æ¨¡å‹æ—ï¼ˆcross-familyï¼‰ã€è·¨è§„æ¨¡ï¼ˆteacherâ†’studentï¼‰ç”šè‡³è·¨æ¨¡æ€ï¼ˆmultimodalï¼‰çš„çŸ¥è¯†è¿ç§»ã€‚
- **æ•ˆç‡ä¼˜åŠ¿**ï¼šè®­ç»ƒå¼€é”€å‰ç½®ï¼Œæ¨ç†é˜¶æ®µä»…ä¸ºå•æ¬¡å‰å‘ä¼ æ’­ï¼Œæ˜¾è‘—é™ä½éƒ¨ç½²æˆæœ¬ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
| æ•°æ®é›† | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| **GSM8K** | æ•°å­¦æ¨ç† | å¤šæ­¥ç®—æœ¯åº”ç”¨é¢˜ |
| **MATH** | æ•°å­¦æ¨ç† | æ›´éš¾çš„æ•°å­¦ç«èµ›é¢˜ |
| **MetaMathQA (MMQA)** | å¢å¼ºæ•°å­¦ | åŒ…å«å¤šæ ·åŒ–è§£æ³•çš„æ•°å­¦é—®ç­” |
| **MedMCQA** | åŒ»ç–—é¢†åŸŸ | åŒ»å­¦è€ƒè¯•é€‰æ‹©é¢˜ï¼Œå¼ºè°ƒä¸“ä¸šçŸ¥è¯† |
| **HotpotQA / QASPER / QMSum** | å¼€æ”¾åŸŸæ¨ç† | ç”¨äºè¯„ä¼°é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ˆæœªå‚ä¸è®­ç»ƒï¼‰ |

### **å®éªŒè®¾ç½®**
- **æ•™å¸ˆæ¨¡å‹**ï¼š`Qwen3-32B`, `Gemma3-27B-it`, `Qwen3-8B`
- **å­¦ç”Ÿæ¨¡å‹**ï¼š`Qwen3-8B`, `Qwen3-1.7B`, `Qwen3-0.6B`, `Llama3-8B`, `Gemma-7B`
- **è’¸é¦æ–¹å¼**ï¼šä»å¤§æ¨¡å‹å‘å°æ¨¡å‹ã€åŒæ—/å¼‚æ—ä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»
- **å¤šæ™ºèƒ½ä½“é…ç½®**ï¼š5~20 ä¸ª agent å‚ä¸è¾©è®ºï¼Œæœ€å¤š 3 è½®è¿­ä»£
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - ä¸»è¦æŒ‡æ ‡ï¼š**Accuracy**
  - æ¨ç†è´¨é‡åˆ†æï¼šPerplexityã€Step Decompositionã€Intermediate Verificationã€Error Localizationã€Coherenceï¼ˆç”± InternLM-2.5-20b è‡ªåŠ¨è¯„åˆ†ï¼‰
  - æ³›åŒ–æ€§ï¼šåœ¨ OOD æ•°æ®é›†ä¸Šçš„è¡¨ç°
  - é²æ£’æ€§ï¼šåœ¨ TruthfulQA ä¸Šçš„ BLEU/ROUGE/BERTScore

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿ | è¯´æ˜ |
|------|------|
| **Single Agent** | åŸå§‹å­¦ç”Ÿæ¨¡å‹ï¼Œæ— ä»»ä½•è’¸é¦ |
| **Vanilla Multi-Agent Debate** | å¤šæ™ºèƒ½ä½“ç›´æ¥åä½œæ¨ç†ï¼ˆé«˜æˆæœ¬ï¼‰ |
| **Standard SFT** | ä»…ç”¨æ ‡å‡†è¾“å…¥è¾“å‡ºå¯¹å¾®è°ƒ |
| **RSFT / DA / PAD** | ä¸‰ç§è’¸é¦ç­–ç•¥å•ç‹¬åŠç»„åˆä½¿ç”¨ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
- AgentArk å¹³å‡å°†å•ä¸ª agent çš„æ€§èƒ½æå‡ **4.8%**ï¼Œæ¥è¿‘åŸå§‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ°´å¹³ï¼Œä½†æ¨ç†æˆæœ¬ä»…ä¸ºåè€…çš„æå°éƒ¨åˆ†ã€‚
- åœ¨ **GSM8K** ä¸Šï¼Œ`Qwen3-0.6B` ç» PAD è’¸é¦åå‡†ç¡®ç‡ä» 41.93 æå‡è‡³ **44.61**ï¼ˆâ†‘2.68ï¼‰ï¼Œè€Œå¤šæ™ºèƒ½ä½“å¹³å‡ä¸º ~45ã€‚
- åœ¨ **MedMCQA** ä¸Šï¼Œ`Qwen3-8B` ä» 59.65 æå‡è‡³ **63.12**ï¼ˆâ†‘3.47ï¼‰ï¼Œæ˜¾ç¤ºå¯¹ä¸“ä¸šé¢†åŸŸçš„æœ‰æ•ˆè¿ç§»ã€‚

#### **ä¸åŒæ–¹æ³•æ¯”è¾ƒï¼ˆä»¥ Qwen3-32B â†’ Qwen3-8B ä¸ºä¾‹ï¼‰**
| æ–¹æ³• | GSM8K â†‘ | MedMCQA â†‘ |
|------|---------|-----------|
| Single Agent | 88.17 | 59.65 |
| RSFT | 89.05 | 60.04 |
| DA | 89.57 | 59.86 |
| **PAD** | **89.02** | **63.12** âœ… |

> ğŸ” **PAD åœ¨ MedMCQA ä¸Šæå‡æœ€å¤§**ï¼Œè¡¨æ˜å…¶å¯¹å¤æ‚é€»è¾‘å’Œé”™è¯¯æ£€æµ‹æ›´æœ‰æ•ˆã€‚

#### **è·¨å®¶æ—è’¸é¦æ•ˆæœæ›´å¼º**
- å½“ teacher å’Œ student å±äºä¸åŒæ¨¡å‹æ—ï¼ˆå¦‚ `Qwen â†’ Llama` æˆ– `Gemma â†’ Qwen`ï¼‰æ—¶ï¼Œå¢ç›Šæ›´å¤§ã€‚
- è¡¨æ˜å¼‚æ„æ¶æ„æ›´èƒ½å—ç›Šäºå¤–éƒ¨æ¨ç†æ¨¡å¼æ³¨å…¥ã€‚

#### **æ¶ˆèå®éªŒç»“æœ**
| å‘ç° | å†…å®¹ |
|------|------|
| âœ… **PRM å®¹é‡æ›´é‡è¦** | ä½¿ç”¨æ›´å¤§çš„ PRMï¼ˆå¦‚ 8Bï¼‰å³ä½¿è®­ç»ƒå°æ¨¡å‹ï¼ˆ0.6Bï¼‰ä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—æå‡ï¼›åä¹‹å¼± PRM é™åˆ¶ä¸Šé™ã€‚ |
| âš ï¸ **å­¦ç”Ÿå®¹é‡æ˜¯ç“¶é¢ˆ** | å°æ¨¡å‹ï¼ˆå¦‚ 0.6Bï¼‰æ— æ³•å¸æ”¶è¿‡å¤šæ•™å¸ˆå¤šæ ·æ€§ï¼Œè¶…è¿‡ 5 ä¸ª agent åæ€§èƒ½ä¸å†ä¸Šå‡ç”šè‡³ä¸‹é™ã€‚ |
| ğŸ“ˆ **PAD æœ€ç¨³å®š** | éšç€è®­ç»ƒæ•°æ®å¢åŠ ï¼ŒRSFT å’Œ DA å‡ºç°æ³¢åŠ¨ç”šè‡³é€€åŒ–ï¼Œè€Œ PAD è¡¨ç°ç¨³å¥ï¼Œè¯´æ˜**è´¨é‡ä¼˜äºæ•°é‡**ã€‚ |
| ğŸ”— **æ–¹æ³•å…¼å®¹æ€§å¥½** | RSFT+DAã€PAD+DA ç­‰ç»„åˆèƒ½è¿›ä¸€æ­¥å°å¹…æå‡æ€§èƒ½ï¼ˆè§ Table 7ï¼‰ã€‚ |

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **å•æ¨¡å‹å¯ä»¥å†…åŒ–å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›**  
   é€šè¿‡åˆç†çš„è’¸é¦ç­–ç•¥ï¼Œå•ä¸ª LLM å¯ä»¥å­¦ä¼šç±»ä¼¼â€œå†…å¿ƒè¾©è®ºâ€çš„è‡ªæˆ‘åæ€æœºåˆ¶ï¼Œå®ç°æ¥è¿‘å¤šæ™ºèƒ½ä½“çš„æ¨ç†è´¨é‡ã€‚

2. âœ… **è¿‡ç¨‹ç›‘ç£ï¼ˆPADï¼‰ä¼˜äºç»“æœç›‘ç£**  
   å¼•å…¥ PRM å¯¹ä¸­é—´æ­¥éª¤è¿›è¡Œå¥–åŠ±å»ºæ¨¡ï¼Œæ¯”å•çº¯æ¨¡ä»¿æœ€ç»ˆç­”æ¡ˆæˆ–è½¨è¿¹æ›´æœ‰æ•ˆåœ°ä¼ é€’æ¨ç†è¡Œä¸ºã€‚

3. âœ… **æ¨ç†è´¨é‡ > æ•°æ®æ•°é‡**  
   å•çº¯å †å æ›´å¤šæ¨ç†è½¨è¿¹ä¸ä¼šæŒç»­ææ•ˆï¼Œåè€Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼›é«˜è´¨é‡ã€é«˜ä¿¡å·çš„è¿‡ç¨‹åé¦ˆæ‰æ˜¯å…³é”®ã€‚

4. âœ… **å¢å¼ºé²æ£’æ€§ä¸æ³›åŒ–æ€§**  
   è’¸é¦åçš„æ¨¡å‹åœ¨ TruthfulQA å’Œ OOD ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¯´æ˜å…¶å­¦åˆ°çš„æ˜¯é€šç”¨æ¨ç†èƒ½åŠ›è€Œéè¡¨é¢æ¨¡å¼åŒ¹é…ã€‚

5. âœ… **å¯æ‰©å±•è‡³å¤šæ¨¡æ€ LLMï¼ˆMLLMï¼‰**  
   åˆæ­¥å®éªŒæ˜¾ç¤ºï¼ŒAgentArk å¯æˆåŠŸè’¸é¦è‡³ `Qwen2.5-VL-3B`ï¼Œå°½ç®¡å¢ç›Šè¾ƒå°ï¼Œä½†ä»éªŒè¯äº†è·¨æ¨¡æ€æ½œåŠ›ã€‚

### **å±€é™æ€§**
- å®éªŒé›†ä¸­åœ¨æ•°å­¦å’ŒåŒ»ç–—ç­‰ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ï¼Œå°šæœªè¦†ç›–å·¥å…·è°ƒç”¨ã€é•¿æœŸè®°å¿†ç­‰å¤æ‚åœºæ™¯ã€‚
- å½“å‰æ¡†æ¶å¯¹ PRM å’Œ GRPO çš„ä¾èµ–è¾ƒé«˜ï¼Œè®­ç»ƒæˆæœ¬è¾ƒå¤§ï¼ˆçº¦ 20 å°æ—¶ on 8Ã—H100ï¼‰ã€‚
- å¯¹è¶…å°å‹æ¨¡å‹ï¼ˆå¦‚ <1Bï¼‰æå‡æœ‰é™ï¼Œå­˜åœ¨å®¹é‡å¤©èŠ±æ¿ã€‚
- æœªå……åˆ†æ¢ç´¢é™¤â€œè¾©è®ºâ€å¤–çš„å…¶ä»– MAS èŒƒå¼ï¼ˆå¦‚åä½œã€åˆ†å·¥ï¼‰ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢è‡ªé€‚åº”è’¸é¦ç­–ç•¥ï¼šæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€é€‰æ‹©æ˜¯å¦å¯ç”¨ PADã€‚
- æ„å»ºæ¨¡å—åŒ– PRMï¼šé’ˆå¯¹ä¸åŒæ¨ç†ç¯èŠ‚ï¼ˆåˆ†è§£ã€éªŒè¯ã€çº é”™ï¼‰è®¾è®¡ä¸“ç”¨å¥–åŠ±æ¨¡å‹ã€‚
- æ‰©å±•è‡³çœŸå®ä¸–ç•Œä»£ç†ä»»åŠ¡ï¼šå¦‚å·¥å…·ä½¿ç”¨ã€ç¯å¢ƒäº¤äº’ã€å®‰å…¨å†³ç­–æ”¯æŒã€‚
- ç ”ç©¶è½»é‡åŒ–ç‰ˆæœ¬ï¼šé™ä½ PAD çš„è®­ç»ƒé—¨æ§›ï¼Œä¾¿äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚
- æ¢ç´¢åå‘è’¸é¦ï¼šè®©å°æ¨¡å‹æŒ‡å¯¼å¤§æ¨¡å‹ï¼Œå½¢æˆé—­ç¯ä¼˜åŒ–ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **AgentArk æˆåŠŸåœ°å°†â€œç¾¤ä½“æ™ºæ…§â€å‹ç¼©è¿›â€œä¸ªä½“å¤§è„‘â€ï¼Œå®ç°äº†é«˜æ•ˆã€é²æ£’ä¸”å¯æ³›åŒ–çš„å•æ¨¡å‹é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä¸ºæœªæ¥ä½æˆæœ¬ã€é«˜æ€§èƒ½çš„ AI Agent éƒ¨ç½²æä¾›äº†æ–°èŒƒå¼ã€‚**

ğŸ”— ä»£ç åœ°å€ï¼š[https://github.com/AIFrontierLab/AgentArk](https://github.com/AIFrontierLab/AgentArk)

</details>

---

### 7. [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)

**Authors**: Xiaolin Hu, Hang Yuan, Xinzhu Sang, Binbin Yan, Zhou Yu, Cong Huang, Kai Chen  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.04913v1  

#### Abstract
Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**AÂ²-LLM: An End-to-end Conversational Audio Avatar Large Language Model**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰çš„å¯¹è¯å¼æ•°å­—äººç³»ç»Ÿæ™®éé‡‡ç”¨**çº§è”æ¶æ„**ï¼ˆcascaded pipelineï¼‰ï¼Œä¾‹å¦‚ï¼š  
`ASR â†’ LLM â†’ TTS â†’ Animation`ã€‚è¿™ç§æ¶æ„å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š
- **é«˜å»¶è¿Ÿ**ï¼ˆhigh latencyï¼‰ï¼šæ¨¡å—é—´ä¸²è¡Œå¤„ç†å¯¼è‡´å“åº”æ…¢ã€‚
- **è¯¯å·®ç´¯ç§¯**ï¼ˆaccumulated errorsï¼‰ï¼šæ¯ä¸ªæ¨¡å—ç‹¬ç«‹è®­ç»ƒï¼Œé”™è¯¯é€å±‚ä¼ æ’­ã€‚
- **è¯­ä¹‰-æƒ…æ„Ÿé¸¿æ²Ÿ**ï¼ˆSemantic-Emotion Gapï¼‰ï¼šé¢éƒ¨åŠ¨ç”»ä»…ä¾èµ–éŸ³é¢‘ä¿¡å·ï¼Œç¼ºä¹å¯¹ä¸Šä¸‹æ–‡è¯­ä¹‰çš„ç†è§£ï¼Œå¯¼è‡´è¡¨æƒ…åƒµç¡¬ã€ä¸è‡ªç„¶ï¼ˆå¦‚â€œå“ˆå“ˆâ€æ—¶å˜´å”‡åŠ¨ä½†è„¸ä¸Šæ— ç¬‘æ„ï¼‰ã€‚

æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šåªç”Ÿæˆè¯­éŸ³æˆ–2Dè§†é¢‘ï¼Œéš¾ä»¥æ»¡è¶³VR/XRç­‰æ²‰æµ¸å¼åœºæ™¯æ‰€éœ€çš„**å‡ ä½•ä¸€è‡´æ€§3Dé¢éƒ¨åŠ¨ç”»**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

#### ï¼ˆ1ï¼‰**AÂ²-LLMï¼šç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§æ¨¡å‹æ¡†æ¶**
- é¦–æ¬¡å°†è¯­è¨€ç†è§£ã€è¯­éŸ³ç”Ÿæˆä¸**3D facial motion generation**ç»Ÿä¸€åœ¨ä¸€ä¸ªLLMä¸­è¿›è¡Œè”åˆå»ºæ¨¡ã€‚
- ä¸å†ä¾èµ–ä¸­é—´æ–‡æœ¬è¡¨ç¤ºï¼Œè€Œæ˜¯ç›´æ¥ä»è¾“å…¥éŸ³é¢‘ç”ŸæˆåŒæ­¥çš„è¾“å‡ºéŸ³é¢‘å’Œ3Dé¢éƒ¨åŠ¨ä½œå‚æ•°ï¼ˆFLAME parametersï¼‰ã€‚
- å¼•å…¥ **Motion Connector** æ¨¡å—ï¼Œé€šè¿‡ cross-attention å°† LLM çš„ audio-aligned hidden states æ˜ å°„ä¸º facial motion tokensã€‚

#### ï¼ˆ2ï¼‰**Residual Motion Tokenization**
- ä½¿ç”¨ **RVQ-VAE** å¯¹è¿ç»­çš„ FLAME å‚æ•°åºåˆ—è¿›è¡Œåˆ†å±‚ç¦»æ•£åŒ–ç¼–ç ï¼Œå½¢æˆ hierarchical motion tokensã€‚
- ä½¿ facial dynamics å¯ä»¥åƒè¯­è¨€ä¸€æ ·è¢« autoregressively ç”Ÿæˆï¼Œå®ç°ä¸éŸ³é¢‘å’Œæ–‡æœ¬çš„ç»Ÿä¸€ token æµå¤„ç†ã€‚

#### ï¼ˆ3ï¼‰**FLAME-QA æ•°æ®é›†**
- æ„å»ºé¦–ä¸ªé¢å‘æŒ‡ä»¤å¾®è°ƒçš„é«˜è´¨é‡å¤šæ¨¡æ€é—®ç­”æ•°æ®é›†ï¼Œæ ¼å¼ä¸º `(Question, Response)` ä¸‰å…ƒç»„ï¼š
  - `Q_audio`, `Q_text`
  - `R_audio`, `R_text`, `R_visual`ï¼ˆå³ FLAME å‚æ•°ï¼‰
- æ‰€æœ‰æ ·æœ¬å‡ç»è¿‡è¯­ä¹‰æ¸…æ´—ï¼Œå¹¶ç”± LLM è‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜ï¼Œç¡®ä¿ facial è¡¨æƒ…å—è¯­ä¹‰é©±åŠ¨è€Œéä»…è·Ÿéšå£°å­¦ç‰¹å¾ã€‚

#### ï¼ˆ4ï¼‰**ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼ˆCurriculum Training Strategyï¼‰**
1. **Stage 1**: å†»ç»“ LLMï¼Œé¢„è®­ç»ƒ Motion Connectorï¼›
2. **Stage 2**: LoRA Reset â€”â€” é‡ç½® LoRA æƒé‡ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ï¼›
3. **Stage 3**: åœ¨é«˜åŠ¨æ€æƒ…æ„Ÿå­é›†ä¸Šè¿›è¡Œæƒ…æ„ŸæŒ‡ä»¤å¾®è°ƒï¼Œæå‡è¡¨è¾¾åŠ›ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼ ç»Ÿçº§è”ç³»ç»Ÿ | AÂ²-LLM |
|------|-------------|--------|
| æ¶æ„ | Cascaded (æ¨¡å—è§£è€¦) | End-to-end (ç»Ÿä¸€å»ºæ¨¡) |
| å»¶è¿Ÿ | >3sï¼ˆæµå¼ï¼‰| ~500ms TTFA |
| æƒ…æ„Ÿè¡¨è¾¾ | ä¾èµ–æ˜¾å¼æ ‡ç­¾æˆ–åå¤„ç† | ç”±è¯­ä¹‰æ·±åº¦é©±åŠ¨ï¼Œæ— éœ€é¢å¤–æ¡ä»¶ |
| åŒæ­¥æ€§ | æ˜“å‡ºç°å£å‹ä¸è¡¨æƒ…è„±èŠ‚ | éŸ³é¢‘-é¢éƒ¨åŠ¨ä½œé«˜åº¦ååŒ |
| å‡ ä½•ä¸€è‡´æ€§ | å¤šæ•°ä¸º2Dåƒç´ åˆæˆ | åŸç”Ÿæ”¯æŒ3D FLAMEæ¨¡å‹ï¼Œé€‚ç”¨äºVR/XR |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

#### **FLAME-QA**ï¼ˆæœ¬æ–‡æå‡ºï¼‰
- è§„æ¨¡ï¼šçº¦ **100k** é«˜è´¨é‡å¤šæ¨¡æ€ QA æ ·æœ¬ã€‚
- æ¥æºï¼šåŸºäº **VoxCeleb** åŸå§‹è§†é¢‘ï¼Œä½¿ç”¨ SMIRK æå– FLAME å‚æ•°ã€‚
- æ„é€ æµç¨‹ï¼š
  1. Whisper è¿›è¡Œ ASR è·å¾—è½¬å½•æ–‡æœ¬ï¼›
  2. GPT-5.1 æ¸…æ´—æ–‡æœ¬å¹¶ç”Ÿæˆå¯¹åº”é—®é¢˜ï¼›
  3. IndexTTS2 åˆæˆé—®é¢˜éŸ³é¢‘ï¼›
  4. æœ€ç»ˆå¾—åˆ° `(Q_audio, Q_text, R_audio, R_text, R_visual)` å®Œæ•´ä¸‰å…ƒç»„ã€‚
- ç‰¹è‰²å­é›†ï¼šçº¦ **1k é«˜åŠ¨æ€æƒ…æ„Ÿæ ·æœ¬**ï¼Œç”± InfiniteTalk ç”Ÿæˆï¼ŒåŒ…å«ä¸°å¯Œæƒ…ç»ªï¼ˆç¬‘ã€æƒŠè®¶ã€è½»è”‘ç­‰ï¼‰ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®

#### æ¨¡å‹æ¶æ„
- **Backbone**: Step-Audio-2-miniï¼ˆåŸºäº Qwen2.5-7B å’Œ Qwen2-Audio ç¼–ç å™¨ï¼‰
- **Motion Tokenizer**: RVQ-VAEï¼ˆå‹ç¼©ç‡ G=5ï¼ŒNq=6 å±‚é‡åŒ–å™¨ï¼‰
- **Motion Connector**: 6-layer Transformer decoderï¼Œæ¥æ”¶é™é‡‡æ ·åçš„ LLM hidden states ä½œä¸º Queryï¼Œå†å² motion embeddings ä½œä¸º KV
- **è®­ç»ƒæ–¹å¼**ï¼šLoRA å¾®è°ƒï¼ˆrank=64ï¼‰ï¼Œé…åˆ Motion Connector è”åˆä¼˜åŒ–

#### æ¨ç†æ¨¡å¼
- è‡ªå›å½’ç”Ÿæˆ interleaved æ–‡æœ¬ä¸éŸ³é¢‘ tokensï¼›
- Audio-Anchored Motion Generationï¼šåœ¨æ¯æ®µéŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶é¢„æµ‹ facial motion tokensã€‚

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

#### ï¼ˆ1ï¼‰å®æ—¶æ€§èƒ½
- **TTFT**ï¼ˆTime To First Tokenï¼‰ï¼šé¦–tokenå»¶è¿Ÿ
- **TTFA**ï¼ˆTime To First Actionï¼‰ï¼šé¦–æ¬¡é¢éƒ¨åŠ¨ä½œå»¶è¿Ÿ
- **RTF**ï¼ˆReal-Time Factorï¼‰ï¼šç”Ÿæˆæ—¶é—´ / å†…å®¹æ—¶é•¿ï¼Œè¶Šä½è¶Šå¥½

#### ï¼ˆ2ï¼‰è¯­è¨€èƒ½åŠ›
ä½¿ç”¨ **OpenVoiceBench**ï¼š
- AlpacaEvalï¼ˆæŒ‡ä»¤éµå¾ªï¼‰
- TriviaQAã€WebQuestionsï¼ˆçŸ¥è¯†é—®ç­”ï¼‰
- Reasoning QAï¼ˆé€»è¾‘æ¨ç†ï¼‰

#### ï¼ˆ3ï¼‰é¢éƒ¨åŠ¨ç”»è´¨é‡
##### ç©ºé—´ç»´åº¦
- **MOD**ï¼ˆMouth Opening Distanceï¼‰ï¼šå£å‹å‚ç›´å¼€åˆ MAEï¼ˆmmï¼‰ï¼Œè¶Šå°è¶Šå¥½
- **UFD**ï¼ˆUpper Face Dynamicsï¼‰ï¼šä¸Šè„¸åŠ¨æ€å¼ºåº¦ï¼ˆå‚è€ƒè‡ªç”±æŒ‡æ ‡ï¼‰ï¼Œè¶Šé«˜è¶Šå¥½

##### æ—¶é—´ç»´åº¦
- **Temporal Correlation**ï¼šæ•´ä½“èŠ‚å¥åŒæ­¥æ€§ï¼ˆPCCï¼‰
- **Velocity Correlation**ï¼šè¿åŠ¨æ–¹å‘ä¸€è‡´æ€§
- **Lip Width Correlation**ï¼šæ¨ªå‘æ‹‰ä¼¸åŒæ­¥æ€§ï¼ˆå¾®ç¬‘ç­‰ï¼‰
- **Liveliness Ratio**ï¼šåŠ¨ä½œæ´»åŠ›æ¯”ï¼ˆæ¥è¿‘1.0æœ€ä½³ï¼‰
- **Peak Align**ï¼šæœ€å¤§å¼€å£æ—¶é—´å·®ï¼ˆmsï¼‰ï¼Œè¶Šå°è¶Šå¥½

#### ï¼ˆ4ï¼‰ä¸»è§‚è¯„ä»·
- ç”¨æˆ·åå¥½ç ”ç©¶ï¼ˆN=60ï¼‰ï¼šä¸¤ä¸¤å¯¹æ¯”ï¼Œæ‰“åˆ†è¡¨è¾¾åŠ›

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| ç±»å‹ | æ–¹æ³• |
|------|------|
| çº§è”ç³»ç»Ÿ | ASR â†’ LLM â†’ TTS â†’ Animation pipeline |
| éŸ³é¢‘é©±åŠ¨åŠ¨ç”» | ARTalk, CodeTalker, FaceFormer |
| é«˜ä¿çœŸæ‰©æ•£æ¨¡å‹ | DiffPoseTalkï¼ˆpseudo-oracleï¼‰|

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **TTFA** | **535.53 ms**ï¼ˆä¼˜åŒ–åï¼‰ |
| **RTF** | **0.703x**ï¼ˆå¿«äºå®æ—¶ï¼‰ |
| **è¯­è¨€æ€§èƒ½ï¼ˆAlpacaEvalï¼‰** | **74.20**ï¼ˆSOTA among audio-native modelsï¼‰ |
| **MODï¼ˆå£å‹ç²¾åº¦ï¼‰** | **5.08 Â± 0.88 mm**ï¼ˆä¼˜äºå¤šæ•°åŸºçº¿ï¼‰ |
| **UFDï¼ˆä¸Šè„¸è¡¨ç°åŠ›ï¼‰** | **11.13 Â± 1.48**ï¼ˆè¿œè¶…åŸºçº¿ï¼‰ |
| **Temporal Correlation** | **0.464**ï¼ˆ+112% vs ARTalkï¼‰ |
| **Liveliness Ratio** | **1.087**ï¼ˆæ¥è¿‘çœŸå®åŠ¨æ€å¹…åº¦ï¼‰ |
| **Peak Align** | **114.3 ms**ï¼ˆæä½³éŸ³ç”»åŒæ­¥ï¼‰ |

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ

#### ï¼ˆ1ï¼‰è¯­è¨€èƒ½åŠ›ï¼ˆTable 2ï¼‰
- AÂ²-LLM åœ¨æ‰€æœ‰ audio-native æ¨¡å‹ä¸­è¡¨ç°æœ€ä¼˜ï¼š
  - AlpacaEval: **74.20**ï¼ˆvs ç¬¬äºŒå Qwen2.5-Omni: 72.76ï¼‰
  - TriviaQA: **79.90**
- æ€§èƒ½æ¥è¿‘çº¯æ–‡æœ¬æ¨¡å‹ï¼ˆQwen3-8B: 78.19ï¼‰ï¼Œè¯æ˜å¼•å…¥ motion token æœªæŸå®³è¯­è¨€èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰é¢éƒ¨åŠ¨ç”»ç©ºé—´è´¨é‡ï¼ˆTable 3ï¼‰

| Model | MOD â†“ | UFD â†‘ |
|-------|-------|-------|
| ARTalk | 4.60 | 9.40 |
| CodeTalker | 5.29 | 2.38 |
| FaceFormer | 5.75 | 3.14 |
| **AÂ²-LLM (Ours)** | **5.08** | **11.13** |

ğŸ‘‰ **ç»“è®º**ï¼šAÂ²-LLM åœ¨ä¿æŒè‰¯å¥½ lip-sync çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ä¸Šè„¸æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ã€‚

#### ï¼ˆ3ï¼‰æ—¶é—´åŠ¨æ€åˆ†æï¼ˆTable 4ï¼‰

| Metric | ARTalk | AÂ²-LLM |
|--------|--------|--------|
| Temporal Correlation | 0.218 | **0.464** |
| Velocity Correlation | -0.309 | **0.111**ï¼ˆæ­£ç›¸å…³ï¼ï¼‰ |
| Lip Width Correlation | 0.477 | **0.604** |
| Liveliness Ratio | 0.804 | **1.087** |
| Peak Align (ms) | 116.6 | **114.3** |

ğŸ‘‰ **ç»“è®º**ï¼šAÂ²-LLM åŠ¨ä½œæ›´è‡ªç„¶ã€èŠ‚å¥ä¸€è‡´ã€èƒ½é‡å……æ²›ï¼Œæ— â€œè¿‡å¹³æ»‘â€ç°è±¡ã€‚

#### ï¼ˆ4ï¼‰ç”¨æˆ·åå¥½ç ”ç©¶ï¼ˆTable 5ï¼‰

| å¯¹æ¯”å¯¹è±¡ | èµ¢ç‡ï¼ˆWin %ï¼‰ | å¹³å±€ | è¾“ |
|----------|----------------|------|----|
| vs DiffPoseTalk | **71.7%** | 10.0% | 18.3% |
| vs ARTalk | **75.0%** | 5.0% | 20.0% |

ğŸ‘‰ å³ä¾¿é¢å¯¹é«˜ä¿çœŸ diffusion æ¨¡å‹ï¼Œäººç±»ä»è®¤ä¸º AÂ²-LLM æ›´å…·è¡¨ç°åŠ›ã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆTable 6ï¼‰

| æŒ‡æ ‡ | Adapter-Onlyï¼ˆå†»ç»“LLMï¼‰ | Joint Trainingï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰ |
|------|--------------------------|----------------------------|
| Temporal Correlation | 0.028 | **0.464** |
| Lip Width Correlation | 0.057 | **0.604** |
| Peak Align (ms) | 515.05 | **114.30** |

ğŸ‘‰ **å…³é”®å‘ç°**ï¼šå¿…é¡»å¯¹ LLM è¿›è¡Œå¾®è°ƒæ‰èƒ½å®ç°ç²¾ç¡®çš„ç›¸ä½å¯¹é½ï¼›å¦åˆ™ä¼šå‡ºç°ä¸¥é‡æ»åï¼ˆ>500msï¼‰ï¼Œå‡ ä¹æ— æ³•åŒæ­¥ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦ç»“è®º

1. **ç«¯åˆ°ç«¯å»ºæ¨¡å¯æœ‰æ•ˆå¼¥åˆ Semantic-Emotion Gap**  
   AÂ²-LLM åˆ©ç”¨ LLM çš„æ·±å±‚è¯­ä¹‰ç†è§£ï¼Œé©±åŠ¨ä¸Šä¸‹è„¸åè°ƒçš„è¡¨æƒ…ï¼Œè€Œä¸ä»…æ˜¯æœºæ¢° lip-syncã€‚

2. **motion tokenization æ˜¯å¯è¡Œè·¯å¾„**  
   å°† facial dynamics ç¦»æ•£åŒ–ä¸º tokens å¹¶ä¸ text/audio ç»Ÿä¸€å»ºæ¨¡ï¼Œæ˜¯å®ç°å¤šæ¨¡æ€è”åˆç”Ÿæˆçš„æœ‰æ•ˆèŒƒå¼ã€‚

3. **é«˜è´¨é‡ instruction-following å¤šæ¨¡æ€æ•°æ®è‡³å…³é‡è¦**  
   FLAME-QA çš„ QA ç»“æ„è¿«ä½¿æ¨¡å‹å°† facial è¡¨æƒ…ä¸å¯¹è¯æ„å›¾ç»‘å®šï¼Œè€Œéç®€å•æ¨¡ä»¿å£°å­¦ä¿¡å·ã€‚

4. **å®æ—¶æ€§ä¸è¡¨ç°åŠ›å¯ä»¥å…¼å¾—**  
   åœ¨ä»… **500ms å·¦å³å»¶è¿Ÿ**ä¸‹ï¼Œå®ç°äº†ä¼˜äºç°æœ‰éå®æ—¶æ¨¡å‹çš„æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ã€‚

---

### âš ï¸ å±€é™æ€§

1. **è¯­è¨€é™åˆ¶**ï¼šç›®å‰ä»…æ”¯æŒè‹±è¯­ï¼Œå°šæœªæ‰©å±•è‡³å¤šè¯­è¨€åœºæ™¯ã€‚
2. **èº«ä½“åŠ¨ä½œç¼ºå¤±**ï¼šä»…å»ºæ¨¡é¢éƒ¨ï¼Œæœªæ¶‰åŠæ‰‹åŠ¿ã€å¤´éƒ¨å§¿æ€æˆ–å…¨èº«åŠ¨ä½œã€‚
3. **èº«ä»½å›ºå®š**ï¼šä½¿ç”¨å›ºå®š identity shapeï¼Œä¸ªæ€§åŒ–å®šåˆ¶èƒ½åŠ›æœ‰é™ã€‚
4. **æ•°æ®ä¾èµ–æ€§å¼º**ï¼šFLAME-QA ä¾èµ–å¤–éƒ¨ TTS å’Œ LLM ç”Ÿæˆé—®é¢˜ï¼Œå¯èƒ½å­˜åœ¨åå·®ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ„å»ºè·¨è¯­è¨€ç‰ˆæœ¬çš„ FLAME-QA-Xã€‚
2. **å…¨èº«ä½“åŠ¨ç”»æ‰©å±•**ï¼šå°† end-to-end èŒƒå¼æ¨å¹¿è‡³ full-body gesture generationã€‚
3. **ä¸ªæ€§åŒ–å¯æ§æ€§**ï¼šæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ avatar identity ä¸ personalityã€‚
4. **äº¤äº’å¼åé¦ˆé—­ç¯**ï¼šç»“åˆ gazeã€ç‚¹å¤´ç­‰éè¯­è¨€è¡Œä¸ºï¼Œå¢å¼ºåŒå‘äº’åŠ¨ä½“éªŒã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> AÂ²-LLM æˆåŠŸå°† **LLM çš„è¯­ä¹‰ç†è§£èƒ½åŠ›**æ³¨å…¥ **3D æ•°å­—äººé¢éƒ¨åŠ¨ç”»**ï¼Œå®ç°äº†**ä½å»¶è¿Ÿã€é«˜è¡¨è¾¾åŠ›ã€è¯­ä¹‰ä¸€è‡´**çš„ç«¯åˆ°ç«¯å¯¹è¯å¼ avatar ç”Ÿæˆï¼Œä¸ºä¸‹ä¸€ä»£æ²‰æµ¸å¼ HCI æä¾›äº†åšå®åŸºç¡€ã€‚

</details>

---

### 8. [Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics](https://arxiv.org/abs/2602.04975)

**Authors**: Jos\'e Afonso, Vasco Guerra, Pedro Viegas  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.04975v1  

#### Abstract
This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simula...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Stochastic Hierarchical Data-Driven Optimization: Application to Plasma-Surface Kinetics*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**å¤æ‚ç‰©ç†ç³»ç»Ÿå»ºæ¨¡ä¸­çš„å‚æ•°æ ¡å‡†éš¾é¢˜**ï¼Œå°¤å…¶æ˜¯åœ¨ä»¥ä¸‹æŒ‘æˆ˜ä¸‹ï¼š
- **é«˜ç»´ä¸”ç—…æ€ï¼ˆill-conditionedï¼‰çš„ä¼˜åŒ–æ™¯è§‚**ï¼šæ¨¡å‹å‚æ•°ç©ºé—´ç»´åº¦é«˜ï¼Œä½†ä»…æœ‰å°‘æ•°â€œåˆšæ€§â€ï¼ˆstiffï¼‰å‚æ•°ç»„åˆä¸»å¯¼ç³»ç»Ÿè¡Œä¸ºï¼Œå…¶ä½™â€œæ¾æ•£â€ï¼ˆsloppyï¼‰å‚æ•°å¯¹è¾“å‡ºå½±å“å¾®å¼±ã€‚
- **è®¡ç®—æˆæœ¬é«˜æ˜‚**ï¼šåŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼ˆsimulatorï¼‰è€—æ—¶ä¸¥é‡ï¼Œæ— æ³•æ‰¿å—å¤§é‡é‡‡æ ·ã€‚
- **ç¼ºä¹æ¢¯åº¦ä¿¡æ¯**ï¼šæ¨¡æ‹Ÿå™¨é€šå¸¸ä¸æä¾›è§£ææˆ–æ•°å€¼æ¢¯åº¦ï¼Œé™åˆ¶äº†åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•çš„åº”ç”¨ã€‚
- **æ•°æ®ç¨€ç–æ€§**ï¼šå®éªŒæµ‹é‡æœ‰é™ï¼Œå¯¼è‡´åé—®é¢˜ï¼ˆinverse problemï¼‰é«˜åº¦ä¸é€‚å®šã€‚

è¿™äº›é—®é¢˜åœ¨ç­‰ç¦»å­ä½“-è¡¨é¢ç›¸äº’ä½œç”¨ï¼ˆplasma-surface interactionsï¼‰å»ºæ¨¡ä¸­å°¤ä¸ºçªå‡ºï¼Œä¾‹å¦‚è¡¨é¢ååº”å‚æ•°ï¼ˆå¦‚å¸é™„ç³»æ•°ã€èƒ½å’ï¼‰éš¾ä»¥é€šè¿‡ç¬¬ä¸€æ€§åŸç†æˆ–å®éªŒç²¾ç¡®è·å¾—ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§**å—â€œSloppy Modelâ€ç†è®ºå¯å‘çš„éšæœºåˆ†å±‚ä¼˜åŒ–æ¡†æ¶**ï¼ˆStochastic Hierarchical Optimization Frameworkï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- å°†ä¼˜åŒ–è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªå­ç©ºé—´ï¼š**åˆšæ€§å­ç©ºé—´**ï¼ˆstiff subspaceï¼‰å’Œ**æ¾æ•£å­ç©ºé—´**ï¼ˆsloppy subspaceï¼‰ã€‚
- åˆ©ç”¨**ç®€åŒ–Hessianè¿‘ä¼¼**ï¼ˆreduced Hessian approximationï¼‰æ¥è¯†åˆ«ä¸»å¯¼ç³»ç»Ÿè¡Œä¸ºçš„ä½ç»´æµå½¢ï¼ˆlow-dimensional latent manifoldï¼‰ã€‚
- é‡‡ç”¨**åˆ†æ­¥ä¼˜åŒ–ç­–ç•¥**ï¼š
  1. **åˆšæ€§ä¼˜åŒ–**ï¼šåœ¨ç”±æœ€å¤§æ›²ç‡æ–¹å‘å¼ æˆçš„å­ç©ºé—´ä¸­å¿«é€Ÿæ”¶æ•›åˆ°ä½èƒ½é‡è°·åº•ï¼›
  2. **æ¾æ•£å†å¯¹é½ä¸ä¼˜åŒ–**ï¼šåœ¨æ­£äº¤çš„æ¾æ•£å­ç©ºé—´ä¸­è¿›è¡Œåæ ‡ç³»æ—‹è½¬å¹¶è¿›ä¸€æ­¥æœç´¢ã€‚

è¯¥æ–¹æ³•çš„å…³é”®æŠ€æœ¯æ˜¯**éšæœºä½ç§©Hessianä»£ç†**ï¼ˆstochastic reduced Hessian proxyï¼‰ï¼Œå®ƒé€šè¿‡åœ¨ä¸€ä¸ªéšæœºä½ç»´å­ç©ºé—´ $\Omega \subset \mathbb{R}^n$ ä¸ŠæŠ•å½± Gauss-Newton Hessian æ¥éšå¼ä¼°è®¡ä¸»æ›²ç‡æ–¹å‘ï¼Œä»…éœ€ $k+1$ æ¬¡æ¨¡æ‹Ÿè°ƒç”¨ï¼ˆ$k \ll n$ï¼‰ï¼Œè€Œéå®Œæ•´çš„ $n+1$ æ¬¡ã€‚

æ­¤å¤–ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ª**åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ principled æ¦‚ç‡ç›®æ ‡å‡½æ•°**ï¼Œä»¥ä¸¥æ ¼å¤„ç†å®éªŒå™ªå£°å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **æ ·æœ¬æ•ˆç‡**ï¼ˆSample Efficiencyï¼‰ | æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå…¨å±€ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚DEã€CMA-ESï¼‰å’Œå±€éƒ¨æ–¹æ³•ï¼ˆå¦‚Powellï¼‰ï¼Œåœ¨ç›¸åŒæ¨¡æ‹Ÿæ¬¡æ•°ä¸‹æ›´å¿«è¾¾åˆ°æ›´ä½æŸå¤±ã€‚ |
| **å¯æ‰©å±•æ€§** | éšæœºHessianç­–ç•¥å°†è®¡ç®—å¼€é”€ä» $O(n)$ é™ä½è‡³ $O(k)$ï¼Œé€‚ç”¨äºé«˜ç»´å‚æ•°ç©ºé—´ï¼ˆå³ä½¿ $n$ å¾ˆå¤§ï¼‰ã€‚ |
| **æ— éœ€ä»£ç†æ¨¡å‹** | ä¸ä¾èµ–é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ç­‰ç»Ÿè®¡ä»£ç†æ¨¡å‹ï¼Œé¿å…å› ä»£ç†å¤±é…å¯¼è‡´é™·å…¥è™šå‡æå°å€¼çš„é£é™©ã€‚ |
| **å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›** | ä¸»åŠ¨åˆ©ç”¨æŸå¤±æ™¯è§‚çš„å„å‘å¼‚æ€§ç»“æ„ï¼Œåœ¨ç‹­çª„ã€ç»†é•¿çš„å±±è°·ä¸­é«˜æ•ˆå¯¼èˆªã€‚ |
| **é²æ£’æ¨æ–­** | ç»“åˆHessianåˆ†ææä¾›å‚æ•°ä¸ç¡®å®šæ€§é‡åŒ–ï¼ŒåŒºåˆ†â€œå¯è¯†åˆ«â€ä¸â€œä¸å¯è¯†åˆ«â€å‚æ•°ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **å®éªŒæ•°æ®æ¥æº**ï¼šæ•´åˆè‡ªæ–‡çŒ® [18â€“20] åŠæœªå‘è¡¨æµ‹é‡ï¼ˆæ¥è‡ªLaboratoire de Physique des Plasmasï¼‰ã€‚
- **æ•°æ®è§„æ¨¡**ï¼šå…± **225 ç»„ç¨³æ€æ¡ä»¶**ä¸‹çš„å®éªŒæ•°æ®ã€‚
- **å˜é‡èŒƒå›´**ï¼š
  - æ°”ä½“ï¼šOâ‚‚/COâ‚‚æ··åˆæ°”ï¼Œæ€»æµé‡ 7.4 sccm
  - å‹åŠ›ï¼š0.2 â€“ 10 Torr
  - æ”¾ç”µç”µæµï¼š10 â€“ 40 mA
  - å£æ¸©ï¼š-20Â°C è‡³ 50Â°C
- **è§‚æµ‹é‡**ï¼ˆObservableï¼‰ï¼šåŸå­æ°§çš„æœ‰æ•ˆå¤åˆæ¦‚ç‡ $y_o$

> æ•°æ®æŒ‰ **80%è®­ç»ƒé›†ï¼ˆN=180ï¼‰ / 20%æµ‹è¯•é›†ï¼ˆN=45ï¼‰** åˆ†å‰²ç”¨äºäº¤å‰éªŒè¯ã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ä¼˜åŒ–ç›®æ ‡
æœ€å°åŒ–åŸºäºæœ€å¤§ä¼¼ç„¶æ¨å¯¼çš„ç›®æ ‡å‡½æ•°ï¼š
$$
\mathcal{L}(\theta) = \frac{1}{2} \sum_{i} \left( \frac{r_i(\theta)}{\sigma_i} \right)^2
$$
å…¶ä¸­ $r_i = E_i - M_i(\theta)$ æ˜¯æ®‹å·®ï¼Œ$\sigma_i$ ä¸ºå®éªŒè¯¯å·®ã€‚

#### å‚æ•°è®¾ç½®
- ä¼˜åŒ–å‚æ•°æ•°é‡ï¼š**29ä¸ªä¸ç¡®å®šåº¦æœ€é«˜çš„å‚æ•°**
  - èƒ½å’ï¼ˆEaï¼‰
  - æ–œå› å­ï¼ˆsteric factors, $k_0$ï¼‰
  - ç‰©ç†å¸é™„ç‰©ç§è„±é™„é¢‘ç‡å‚æ•°ï¼ˆA, B, Eï¼‰

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **Differential Evolution (DE)** | å…¨å±€æ¢ç´¢ | ç§ç¾¤å‹å¯å‘å¼ç®—æ³•ï¼Œç”¨äºè¯„ä¼°å…¨å±€æœç´¢èƒ½åŠ› |
| **CMA-ES** | è‡ªé€‚åº”æ¼”åŒ–ç­–ç•¥ | èƒ½é€‚åº”ç—…æ€åœ°å½¢ï¼Œå­¦ä¹ åæ–¹å·®çŸ©é˜µ |
| **Trust Region Reflective (TRF)** | å±€éƒ¨ä¼˜åŒ–ï¼ˆå¸¦è¾¹ç•Œçº¦æŸï¼‰ | Levenberg-Marquardt å˜ä½“ï¼Œæœ‰é™å·®åˆ†ä¼°è®¡Jacobian |
| **Powellâ€™s Method** | æ— æ¢¯åº¦å±€éƒ¨ä¼˜åŒ– | è¿­ä»£çº¿æœç´¢ï¼Œæ— éœ€æ¢¯åº¦ |
| **Gaussian Process (GP)** | ä»£ç†æ¨¡å‹ä¼˜åŒ– | ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œæ¯”è¾ƒ |

æ‰€æœ‰ç®—æ³•å‡ä»åŒä¸€åˆå§‹çŒœæµ‹å‡ºå‘ï¼ˆ$\mathcal{L}(\theta^{(0)}) \sim 700$ vs é»˜è®¤å€¼ $\sim 0.1$ï¼‰ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

#### è¯„ä¼°æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**ï¼ˆ$ \mathcal{L}_{\text{train}} $ï¼‰éšæ¨¡æ‹Ÿè°ƒç”¨æ¬¡æ•°çš„å˜åŒ– â†’ è¡¡é‡**æ ·æœ¬æ•ˆç‡**
- **æµ‹è¯•æŸå¤±**ï¼ˆ$ \mathcal{L}_{\text{test}} $ï¼‰â†’ è¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ”¶æ•›é€Ÿåº¦ä¸æœ€ç»ˆç²¾åº¦**
- **Hessianç‰¹å¾è°±åˆ†æ** â†’ éªŒè¯æ¨¡å‹çš„â€œsloppyâ€æ€§è´¨

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- **Hierarchical æ–¹æ³•ï¼ˆExact & Stochasticï¼‰** åœ¨å‰å‡ åæ¬¡è¿­ä»£ä¸­è¡¨ç°å‡º**æœ€å¿«ä¸‹é™é€Ÿåº¦**ã€‚
- **Stochastic Reduced Hessian ($k=18$)** åœ¨æå°‘æ¨¡æ‹Ÿè°ƒç”¨ä¸‹å³å¯é€¼è¿‘ Exact æ–¹æ³•æ€§èƒ½ã€‚
- æœ€ç»ˆæµ‹è¯•æŸå¤±ä¸å…¶ä»–ä¼˜ç§€æ–¹æ³•ï¼ˆå¦‚TRFï¼‰ç›¸å½“ï¼Œè¡¨æ˜**è‰¯å¥½æ³›åŒ–æ€§**ã€‚
- äº”æ¬¡ç‹¬ç«‹äº¤å‰éªŒè¯ä¸­ï¼Œæµ‹è¯•é›† $R^2 = 0.736$ï¼Œ$\mathcal{L}_{\text{test}}$ èŒƒå›´ä¸º **0.054 â€“ 0.087**ï¼Œæ˜¾ç¤ºç»“æœç¨³å®šã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| æ–¹æ³• | æ ·æœ¬æ•ˆç‡ | æœ€ç»ˆç²¾åº¦ | å¤‡æ³¨ |
|------|----------|----------|------|
| **Hierarchical (Exact)** | â­â­â­â­â˜† | â­â­â­â­â˜† | å¿«é€Ÿæ”¶æ•›ï¼Œä½†æ¯æ¬¡è¿­ä»£æˆæœ¬è¾ƒé«˜ |
| **Hierarchical (Stochastic, k=18)** | â­â­â­â­â­ | â­â­â­â­â˜† | **æœ€ä¼˜å¹³è¡¡ç‚¹**ï¼Œæé«˜æ•ˆåœ°æ•æ‰ä¸»æ›²ç‡ |
| **TRF** | â­â­â­â˜†â˜† | â­â­â­â­â˜† | å¼ºåŠ²å¯¹æ‰‹ï¼Œæœ€ç»ˆå¯è¾¾ç›¸ä¼¼ç²¾åº¦ï¼Œä½†å‰æœŸè¾ƒæ…¢ |
| **CMA-ES** | â­â­â˜†â˜†â˜† | â­â­â˜†â˜†â˜† | å¯¹ç—…æ€åœ°å½¢æœ‰ä¸€å®šé€‚åº”ï¼Œä½†ä»æ…¢äºåˆ†å±‚æ³• |
| **DE** | â­â˜†â˜†â˜†â˜† | â­â­â˜†â˜†â˜† | å› å„å‘åŒæ€§æœç´¢éš¾ä»¥ç©¿è¶Šç»†é•¿å±±è°· |
| **Powell** | â­â­â˜†â˜†â˜† | â­â­â˜†â˜†â˜† | æ˜“é™·å±€éƒ¨æå°ï¼Œæ•ˆç‡ä½ |
| **GP** | â­â­â˜†â˜†â˜† | â­â­â˜†â˜†â˜† | ä»£ç†æ¨¡å‹æ„å»ºæœ¬èº«ä»£ä»·é«˜ï¼Œä¸”å­˜åœ¨å¤±é…é£é™© |

> å›¾2(a) æ˜¾ç¤ºï¼šHierarchical æ–¹æ³•åœ¨çº¦ **50æ¬¡æ¨¡æ‹Ÿè°ƒç”¨å†…** è¾¾åˆ°å…¶ä»–æ–¹æ³•éœ€æ•°ç™¾æ¬¡æ‰èƒ½è¾¾åˆ°çš„æŸå¤±æ°´å¹³ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
è™½ç„¶æ–‡ä¸­æœªæ˜ç¡®æ ‡æ³¨â€œablationâ€ï¼Œä½†ä»¥ä¸‹å®éªŒè¯å®äº†å…³é”®è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼š
- **ä¸åŒ $k$ å€¼çš„éšæœºå­ç©ºé—´æ¯”è¾ƒ**ï¼ˆå›¾2bï¼‰ï¼š
  - å³ä½¿ $k=3$ æˆ– $k=5$ï¼ˆè¿œå°äº $n=29$ï¼‰ï¼Œä¹Ÿèƒ½å®ç°å¿«é€ŸåˆæœŸä¸‹é™ã€‚
  - è¯æ˜ï¼šåªéœ€å°‘é‡æ–¹å‘å³å¯æ•è·ä¸»å¯¼å‡ ä½•ç»“æ„ï¼Œæ”¯æŒæ–¹æ³•çš„**å¯æ‰©å±•æ€§å‡è®¾**ã€‚
- **Hessianç‰¹å¾è°±åˆ†æ**ï¼ˆå›¾2dï¼‰ï¼š
  - ç‰¹å¾å€¼å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè¯å®æ¨¡å‹å…·æœ‰å…¸å‹çš„ **sloppy structure**ã€‚
  - åˆšæ€§æ¨¡å¼ï¼ˆå‰å‡ ä¸ªå¤§ç‰¹å¾å€¼ï¼‰ä¸æ¾æ•£æ¨¡å¼ä¹‹é—´å­˜åœ¨æ˜æ˜¾èƒ½éš™ï¼ˆspectral gapï¼‰ï¼Œä¸ºåˆ†å±‚ä¼˜åŒ–æä¾›äº†ç†è®ºåŸºç¡€ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **ç‰©ç†æ¨¡å‹æ™®éå­˜åœ¨ sloppy structure**ï¼šå°½ç®¡å‚æ•°ä¼—å¤šï¼Œä½†ç³»ç»Ÿè¡Œä¸ºç”±å°‘æ•°åˆšæ€§ç»„åˆå†³å®šï¼Œå…¶ä½™å‚æ•°é«˜åº¦ä¸ç¡®å®šã€‚
2. **å‡ ä½•å¼•å¯¼çš„ä¼˜åŒ–æ˜¾è‘—æå‡æ•ˆç‡**ï¼šé€šè¿‡æ˜¾å¼è¯†åˆ«å¹¶ä¼˜å…ˆä¼˜åŒ–åˆšæ€§å­ç©ºé—´ï¼Œå¯åœ¨æå°‘æ•°æ¨¡æ‹Ÿè°ƒç”¨ä¸‹é€¼è¿‘æœ€ä¼˜è§£ã€‚
3. **éšæœºä½ç§©Hessianæ˜¯é«˜æ•ˆçš„å‡ ä½•æ¢æµ‹å™¨**ï¼šä½œä¸ºâ€œçº¿æ€§è‡ªç¼–ç å™¨â€ï¼Œå®ƒèƒ½ä»¥ $O(k)$ æˆæœ¬æœ‰æ•ˆæå–æœ¬åœ°æ›²ç‡ä¸»è½´ï¼Œé€‚åˆæ˜‚è´µæ¨¡æ‹Ÿå™¨ã€‚
4. **å‚æ•°ä¸ç¡®å®šæ€§å¯é€šè¿‡Hessianå®šé‡åˆ»ç”»**ï¼š
   - åˆšæ€§å‚æ•°ï¼ˆå¦‚è„±é™„é¢‘ç‡A/B/Eã€COåŒ–å­¦å¸é™„æ–œå› å­$k_{0.32}, k_{0.37}, k_{0.39}$ã€äºšç¨³æ€èƒ½é‡$E_c, E_{\min}$ï¼‰è¢«ç´§å¯†çº¦æŸã€‚
   - æ¾æ•£å‚æ•°ï¼ˆå¦‚éƒ¨åˆ†äºšç¨³æ€ååº”é€Ÿç‡ï¼‰åˆ™å…·æœ‰å®½æ³›ç½®ä¿¡åŒºé—´ï¼Œè¿™æ˜¯æ¨¡å‹ç»“æ„æ€§ç¼ºé™·è€Œéæ•°æ®ä¸è¶³æ‰€è‡´ã€‚

> å¦‚å›¾4æ‰€ç¤ºï¼Œè¿™äº›â€œuncolored barsâ€å¯¹åº” sloppy parametersï¼Œå…¶ä¸ç¡®å®šæ€§æœ¬è´¨ä¸Šä¸å¯æ¶ˆé™¤ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **å±€éƒ¨æ–¹æ³•æœ¬è´¨**ï¼šä¸èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜ï¼Œä½†åœ¨ sloppy models ä¸­ï¼Œè¿‘ä¼˜è§£é›†åˆ $S_\epsilon$ é€šå¸¸æ˜¯è¿é€šçš„å¤§åŒºåŸŸï¼Œå› æ­¤å±€éƒ¨æ”¶æ•›å·²è¶³å¤Ÿã€‚
- **ä¾èµ–æ®‹å·®è¾ƒå°çš„å‰æ**ï¼šGauss-Newton Hessian çš„æœ‰æ•ˆæ€§å»ºç«‹åœ¨æ®‹å·®æ¥è¿‘é›¶çš„åŸºç¡€ä¸Šï¼Œè‹¥åˆå§‹çŒœæµ‹å¤ªå·®å¯èƒ½å¤±æ•ˆã€‚
- **éœ€è¦æ‰‹åŠ¨è®¾å®šé˜ˆå€¼**ï¼šå¦‚ stiff subspace variance threshold $\gamma=0.9$ å’Œ reduced sloppy threshold $\tau=10^{-4}$ï¼Œè™½æœ‰ç»éªŒä¾æ®ï¼Œä½†ä»å±è¶…å‚è°ƒèŠ‚ã€‚
- **å®ç°å¤æ‚åº¦è¾ƒé«˜**ï¼šç›¸æ¯”æ ‡å‡†ä¼˜åŒ–å™¨ï¼Œéœ€è‡ªè¡Œå®ç°åˆ†å±‚é€»è¾‘ä¸Hessianä¼°è®¡ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ·±å…¥ç‰©ç†è§£é‡Š**ï¼šåˆ©ç”¨æ•°æ®é©±åŠ¨ç»“æœæ¢ç©¶Pyrexè¡¨é¢ä¸ŠåŸå­æ°§å¤åˆçš„å¾®è§‚æœºåˆ¶ã€‚
2. **å¤–æ¨èƒ½åŠ›ç ”ç©¶**ï¼šå½“å‰éªŒè¯é›†ä¸­äºæ’å€¼ä»»åŠ¡ï¼Œæœªæ¥å°†æµ‹è¯•æ¨¡å‹åœ¨è®­ç»ƒåŸŸä¹‹å¤–çš„æ“ä½œæ¡ä»¶ä¸‹æ˜¯å¦ä»å…·é¢„æµ‹åŠ›ã€‚
3. **æ¡†æ¶é€šç”¨åŒ–æ¨å¹¿**ï¼šå°†æ­¤æ–¹æ³•åº”ç”¨äºå…¶ä»–å¤æ‚ååº”ç½‘ç»œï¼Œå¦‚ç”ŸåŒ–ç³»ç»Ÿã€ç‡ƒçƒ§åŒ–å­¦ã€å‚¬åŒ–è¿‡ç¨‹ç­‰ã€‚
4. **ç»“åˆä¸»åŠ¨å­¦ä¹ **ï¼šåŠ¨æ€é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„å®éªŒæ¡ä»¶è¿›è¡Œæ¨¡æ‹Ÿï¼Œè¿›ä¸€æ­¥å‡å°‘æ€»æŸ¥è¯¢æ•°ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºçš„ **stochastic hierarchical optimization** æ¡†æ¶é€šè¿‡èåˆ **Sloppy Model ç†è®º** ä¸ **reduced Hessian æŠ€æœ¯**ï¼Œå®ç°äº†åœ¨**æé«˜è®¡ç®—æˆæœ¬ä¸æ•°æ®ç¨€ç¼ºåŒé‡é™åˆ¶ä¸‹**å¯¹å¤æ‚ç‰©ç†æ¨¡å‹çš„é«˜æ•ˆã€ç¨³å¥å‚æ•°æ ¡å‡†ï¼Œä¸º plasma-surface kinetics ç­‰é¢†åŸŸçš„å»ºæ¨¡æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œå¹¶å…·å¤‡å¹¿æ³›çš„å¯è¿ç§»æ½œåŠ›ã€‚

</details>

---

### 9. [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)

**Authors**: Shizheng Wen, Mingyuan Chi, Tianwei Yu, Ben Moseley, Mike Yan Michelis, Pu Ren, Hao Sun, Siddhartha Mishra  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.05052v1  

#### Abstract
We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel hig...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLearning, Solving and Optimizing PDEs with TensorGalerkin

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»ŸåŸºäº **Galerkin** æˆ– **æœ‰é™å…ƒæ³• (FEM)** çš„ PDE æ•°å€¼æ±‚è§£åœ¨ç°ä»£è‡ªåŠ¨å¾®åˆ†ï¼ˆADï¼‰æ¡†æ¶ï¼ˆå¦‚ PyTorchï¼‰ä¸­é¢ä¸´ä¸¥é‡çš„æ•ˆç‡ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨ GPU ä¸Šè¿è¡Œæ—¶ã€‚ä¸»è¦é—®é¢˜åŒ…æ‹¬ï¼š
- **Python å¾ªç¯å¼€é”€å¤§**ï¼šä¼ ç»Ÿ FEM ç»„è£…è¿‡ç¨‹ä¾èµ–å¯¹ç½‘æ ¼å…ƒç´ ï¼ˆelementï¼‰çš„é€ä¸ªå¾ªç¯ï¼Œåœ¨ Python å±‚é¢æ‰§è¡Œæ—¶å¼•å…¥æ˜¾è‘—è§£é‡Šå™¨å¼€é”€ã€‚
- **è®¡ç®—å›¾ç¢ç‰‡åŒ–**ï¼šå±€éƒ¨åŸºå‡½æ•°ç´¢å¼•çš„å¾ªç¯å¯¼è‡´è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿç”Ÿæˆé«˜åº¦ç¢ç‰‡åŒ–çš„è®¡ç®—å›¾ï¼Œä¸¥é‡æ‹–æ…¢åå‘ä¼ æ’­é€Ÿåº¦ã€‚
- **AD å¼€é”€é«˜**ï¼šç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆå¦‚ PINNsï¼‰ä¾èµ– `torch.autograd` è®¡ç®—ç©ºé—´å¯¼æ•°ï¼Œå¸¦æ¥å¤§é‡è®¡ç®—å›¾åµŒå¥—å’Œå†…å­˜æ¶ˆè€—ã€‚

è¿™äº›é—®é¢˜é™åˆ¶äº†å…¶åœ¨ **å¤šæŸ¥è¯¢ä»»åŠ¡**ï¼ˆå¦‚ PDE-constrained optimizationã€operator learningï¼‰ä¸­çš„åº”ç”¨ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šTENSORGALERKIN
ä½œè€…æå‡º **TENSORGALERKIN** â€”â€” ä¸€ç§é«˜æ•ˆçš„ã€é«˜æ€§èƒ½çš„ Galerkin ç»„è£…ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¼ ç»Ÿçš„â€œå¾ªç¯-æ•£åˆ—åŠ â€ï¼ˆscatter-addï¼‰ç»„è£…è¿‡ç¨‹é‡æ„ä¸º **çº¯å¼ é‡åŒ–çš„ Map-Reduce èŒƒå¼**ã€‚

#### åˆ›æ–°æ¶æ„
- **Stage I: Batch-Mapï¼ˆå…¨å¼ é‡åŒ–ç‰©ç†è®¡ç®—ï¼‰**
  - å°†æ‰€æœ‰å•å…ƒçš„å±€éƒ¨åˆšåº¦çŸ©é˜µ $ K_{\text{local}} $ å’Œè½½è·å‘é‡ $ F_{\text{local}} $ çš„è®¡ç®—ç»Ÿä¸€ä¸ºä¸€ä¸ª **å¯†é›†å¼ é‡æ”¶ç¼©ï¼ˆdense tensor contractionï¼‰** æ“ä½œã€‚
  - ä½¿ç”¨ `torch.einsum` å®ç°ï¼Œèåˆäº† quadrature ç‚¹ã€åŸºå‡½æ•°ç´¢å¼•å’Œå…ƒç´ ç»´åº¦ï¼Œé¿å…ä»»ä½•æ˜¾å¼å¾ªç¯ã€‚
  - è¾“å‡ºä¸ºå½¢çŠ¶ä¸º `(E, k, k)` çš„å±€éƒ¨å¼ é‡ï¼Œå…¶ä¸­ E æ˜¯å…ƒç´ æ•°é‡ï¼Œk æ˜¯æ¯ä¸ªå…ƒç´ çš„è‡ªç”±åº¦ã€‚

- **Stage II: Sparse-Reduceï¼ˆæ‹“æ‰‘æ„ŸçŸ¥çš„ç¨€ç–è§„çº¦ï¼‰**
  - ä½¿ç”¨é¢„è®¡ç®—çš„ **Routing Matrices**ï¼ˆ$ S_{\text{mat}}, S_{\text{vec}} $ï¼‰å°†å±€éƒ¨å¼ é‡èšåˆä¸ºå…¨å±€ç¨€ç–çŸ©é˜µ $ K $ å’Œå‘é‡ $ F $ã€‚
  - å…¨å±€ç»„è£…é€šè¿‡ä¸€æ¬¡ **ç¨€ç–çŸ©é˜µä¹˜æ³• (SpMM)** å®Œæˆï¼š
    $$
    F = S_{\text{vec}} \cdot \text{vec}(F_{\text{local}}), \quad K = \text{CSR}(L, S_{\text{mat}} \cdot \text{vec}(K_{\text{local}}))
    $$
  - å®Œå…¨æ¶ˆé™¤åŸå­æ“ä½œï¼ˆatomic operationsï¼‰ï¼Œå®ç°ç¡®å®šæ€§ã€é«˜æ•ˆä¸”å¯å¾®çš„ç»„è£…ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | æ˜¾è‘—å‡å°‘ Python å¼€é”€å’Œ AD å›¾ç¢ç‰‡ï¼Œæå‡ GPU åˆ©ç”¨ç‡ï¼Œå®ç° **1â€“2 ä¸ªæ•°é‡çº§çš„é€Ÿåº¦æå‡**ã€‚ |
| **å¯å¾®æ€§** | æ•´ä¸ªæµç¨‹ç«¯åˆ°ç«¯å¯å¾®ï¼ˆend-to-end differentiableï¼‰ï¼Œå¤©ç„¶æ”¯æŒæ¢¯åº¦åå‘ä¼ æ’­ï¼Œæ— éœ€æ‰‹åŠ¨æ¨å¯¼ä¼´éšæ–¹ç¨‹ã€‚ |
| **çµæ´»æ€§** | æ”¯æŒåŠ¨æ€ç½‘æ ¼ï¼ˆdynamic meshï¼‰ï¼Œæ—  JIT é‡ç¼–è¯‘å»¶è¿Ÿï¼ˆç›¸æ¯” JAX-FEMï¼‰ã€‚ |
| **ç²¾åº¦** | ä½¿ç”¨è§£æå½¢çŠ¶å‡½æ•°æ¢¯åº¦ï¼ˆanalytical shape gradientsï¼‰ï¼Œé¿å… AD å¼•å…¥çš„æ•°å€¼è¯¯å·®ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä¸‹æ¸¸åº”ç”¨åœºæ™¯ä¸ä»»åŠ¡
TENSORGALERKIN è¢«éƒ¨ç½²äºä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œå½¢æˆå®Œæ•´å·¥å…·é“¾ï¼š
1. **TENSORMESH**ï¼šé«˜æ•ˆæ•°å€¼ PDE æ±‚è§£å™¨
2. **TENSORPILS**ï¼šç‰©ç†ä¿¡æ¯é©±åŠ¨çš„ç®—å­å­¦ä¹ ï¼ˆPhysics-Informed Learning Systemï¼‰
3. **TENSOROPT**ï¼šPDE çº¦æŸä¼˜åŒ–ä¸é€†è®¾è®¡

---

### æ•°æ®é›†ä¸é—®é¢˜è®¾ç½®
| ä»»åŠ¡ | PDE ç±»å‹ | å‡ ä½•åŸŸ | ç½‘æ ¼ç±»å‹ | è¾“å…¥åˆ†å¸ƒ |
|------|--------|--------|---------|----------|
| **Numerical Solver** | 3D Poisson, 3D Linear Elasticity | å•ä½ç«‹æ–¹ä½“ã€ç©ºå¿ƒç«‹æ–¹ä½“ | å››é¢ä½“ç½‘æ ¼ (tetrahedral) | å•ä¸€æºé¡¹ |
| **Neural PDE Solver** | 2D Poisson (checkerboard forcing) | å•ä½æ­£æ–¹å½¢ | éç»“æ„ä¸‰è§’ç½‘æ ¼ | å¤šå°ºåº¦ä¸è¿ç»­æºé¡¹ $ f_K(x,y) $ |
| **Operator Learning** | Wave Equation (hyperbolic), Allen-Cahn (parabolic) | åœ†å½¢åŸŸã€L å½¢åŸŸ | éç»“æ„ä¸‰è§’ç½‘æ ¼ | éšæœºåˆå€¼ï¼ˆmulti-frequency sine expansionï¼‰ |
| **Inverse Design** | 2D Linear Elasticity (SIMP) | æ‚¬è‡‚æ¢çŸ©å½¢åŸŸ | ç»“æ„åŒ–å››è¾¹å½¢å•å…ƒ (QUAD4) | å¯†åº¦åœºä¼˜åŒ– |

---

### è¯„ä¼°æŒ‡æ ‡
| ä»»åŠ¡ | ä¸»è¦æŒ‡æ ‡ |
|------|--------|
| æ•°å€¼æ±‚è§£å™¨ | è¿è¡Œæ—¶é—´ (runtime)ã€ç›¸å¯¹æ®‹å·® (RelRes)ã€ç›¸å¯¹è¯¯å·® (RelErr) |
| ç¥ç» PDE æ±‚è§£å™¨ | ç›¸å¯¹ L2 è¯¯å·® (%)ã€è®­ç»ƒååé‡ (it/s) |
| ç®—å­å­¦ä¹  | ç›¸å¯¹ L2 è¯¯å·®ï¼ˆID / OOD æµ‹è¯•é›†ï¼‰ |
| é€†è®¾è®¡ | æ€»è€—æ—¶ã€æ”¶æ•›æ­¥æ•°ã€åˆè§„æ€§ï¼ˆcomplianceï¼‰ä¸‹é™ç‡ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ä»»åŠ¡ | å¯¹æ¯”åŸºçº¿ |
|------|--------|
| æ•°å€¼æ±‚è§£ | FEniCS (CPU), scikit-fem (SKFEM, CPU), JAX-FEM (CPU/GPU), PINN |
| ç¥ç»æ±‚è§£å™¨ | PINN, VPINN, Deep Ritz |
| ç®—å­å­¦ä¹  | Data-Driven GNN, PI-DeepONet |
| é€†è®¾è®¡ | JAX-FEM + LU solver |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… æ•°å€¼æ±‚è§£å™¨æ€§èƒ½ï¼ˆTENSORMESHï¼‰
- **3D Poisson æ–¹ç¨‹**ï¼š
  - GPU ç‰ˆæœ¬æ¯” FEniCS å¿« **10 å€ä»¥ä¸Š**ã€‚
  - åœ¨ç™¾ä¸‡çº§ DoF ä¸‹ä»ä¿æŒç¨³å®šåŠ é€Ÿã€‚
- **3D å¼¹æ€§åŠ›å­¦**ï¼š
  - æ¯” CPU ç‰ˆ TENSORMESH å¿«è¿‘ **100 å€**ã€‚
- **æ®‹å·®åˆ†æ**ï¼š
  - TENSORMESH è¾¾åˆ°æœ€å°çº¿æ€§ç³»ç»Ÿæ®‹å·®ï¼Œç²¾åº¦ä¼˜äºæˆ–ç­‰äºå…¶ä»– FEM å·¥å…·ã€‚
- **æ‰¹å¤„ç†ç”Ÿæˆ**ï¼š
  - æ‰¹é‡ç”Ÿæˆ 7k DoF Poisson è§£æ—¶ï¼ŒCUDA ç‰ˆæœ¬åœ¨ batch size=100 æ—¶å‡ ä¹é›¶é¢å¤–å¼€é”€ï¼Œè¿œè¶… CPU åŸºçº¿ã€‚

> ğŸ”º å›¾è¡¨æ”¯æŒï¼šFig 2, Fig B.1, Fig B.4

---

### âœ… ç¥ç» PDE æ±‚è§£å™¨æ€§èƒ½ï¼ˆTENSORPILSï¼‰
| Method | Rel. L2 Error (K=8) | Speed (Adam it/s) |
|--------|---------------------|------------------|
| PINN | 34.77% | 20.1 |
| VPINN | 154.10% | 54.9 |
| Deep Ritz | 10.60% | 58.7 |
| **TENSORPILS (Ours)** | **10.05%** | **117.8** |

- **è¯¯å·®é™ä½ 50%+** äºæœ€è¿‘åŸºçº¿ï¼ˆDeep Ritzï¼‰ï¼ŒåŒæ—¶é€Ÿåº¦å¿« **2 å€ä»¥ä¸Š**ã€‚
- **å‰å‘æŸå¤±è®¡ç®—æ‰©å±•æ€§**ï¼š
  - PINN æŸå¤±éš DoF å¢é•¿å‘ˆæŒ‡æ•°ä¸Šå‡ï¼ˆAD å›¾å¼€é”€ï¼‰ã€‚
  - TENSORPILS å‡ ä¹æ’å®šå¼€é”€ï¼Œæ¥è¿‘æœ‰é™å·®åˆ†ï¼ˆFDMï¼‰æ°´å¹³ã€‚

> ğŸ”º è¡¨æ ¼æ”¯æŒï¼šTable 1, Fig 3

---

### âœ… ç‰©ç†ä¿¡æ¯ç®—å­å­¦ä¹ ï¼ˆTENSORPILSï¼‰
| Model | Wave (ID) | Wave (OOD) | AC (ID) | AC (OOD) |
|-------|-----------|------------|--------|---------|
| Data-Driven | 0.089Â±0.013 | 0.230Â±0.017 | 0.135Â±0.042 | 0.152Â±0.080 |
| PI-DeepONet | 0.626Â±0.033 | 0.863Â±0.018 | 0.743Â±0.163 | 8.536Â±6.306 |
| **TENSORPILS** | **0.085Â±0.010** | **0.090Â±0.006** | **0.110Â±0.014** | **0.083Â±0.013** |

- **OOD æ³›åŒ–èƒ½åŠ›æå¼º**ï¼šTENSORPILS åœ¨å¤–æ¨ä»»åŠ¡ä¸­è¯¯å·®ä»…è½»å¾®ä¸Šå‡ï¼Œè€Œæ•°æ®é©±åŠ¨æ¨¡å‹è¯¯å·®ç¿»å€ï¼ŒPI-DeepONet å®Œå…¨å¤±æ•ˆã€‚
- **æ— éœ€æ ‡ç­¾æ•°æ®**ï¼šTENSORPILS ä¸º data-free æ–¹æ³•ï¼Œå´ä¼˜äºä½¿ç”¨ 16 ä¸ªæ ·æœ¬è®­ç»ƒçš„æ•°æ®é©±åŠ¨æ¨¡å‹ã€‚

> ğŸ”º è¡¨æ ¼æ”¯æŒï¼šTable 2, Fig B.13â€“B.15

---

### âœ… PDE çº¦æŸé€†è®¾è®¡ï¼ˆTENSOROPTï¼‰
| Stage | JAX-FEM | TENSOROPT (Ours) | Speedup |
|-------|--------|------------------|---------|
| Setup Time | 2.62 s | 0.58 s | **4.5Ã—** |
| Optimization Loop | 28.51 s | 7.77 s | **3.7Ã—** |
| **Total Time** | **31.13 s** | **8.35 s** | **3.7Ã—** |

- æœ€ç»ˆè®¾è®¡æ‹“æ‰‘ä¸€è‡´ï¼Œåˆè§„æ€§å·®å¼‚ < 0.33%ï¼ŒéªŒè¯å‡†ç¡®æ€§ã€‚
- åŠ é€Ÿæºäºï¼šæ— å¾ªç¯ç»„è£… + é«˜æ•ˆå¯å¾®æ±‚è§£å™¨ + é¿å… JIT ç¼–è¯‘ã€‚

> ğŸ”º è¡¨æ ¼æ”¯æŒï¼šTable 3, Fig B.17â€“B.18

---

### âœ… æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
- **æ•°æ®æ•ˆç‡åˆ†æ**ï¼ˆFig B.16ï¼‰ï¼š
  - TENSORPILS åœ¨ä»… **1 ä¸ªè®­ç»ƒæ ·æœ¬** ä¸‹å³å¯è¾¾åˆ° ~10% è¯¯å·®ã€‚
  - æ•°æ®é©±åŠ¨æ–¹æ³•éœ€è¦æ›´å¤šæ ·æœ¬æ‰èƒ½æ”¶æ•›ï¼Œä¸”æ³›åŒ–å·®ã€‚
- **è¯æ˜ Galerkin æŸå¤±æœ¬èº«å…·æœ‰å¼ºå½’çº³åç½®**ï¼Œé€‚åˆå°æ•°æ®åœºæ™¯ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Galerkin ç»„è£…æ˜¯æ€§èƒ½ç“¶é¢ˆçš„å…³é”®**ï¼šä¼ ç»Ÿ scatter-add æ¨¡å¼åœ¨ç°ä»£ AD æ¡†æ¶ä¸­ä¸å¯æŒç»­ã€‚
2. **TENSORGALERKIN å®ç°ç»Ÿä¸€é«˜æ•ˆæ¡†æ¶**ï¼š
   - ä¸€å¥—å¼•æ“æ”¯æŒ **æ±‚è§£ã€å­¦ä¹ ã€ä¼˜åŒ–** ä¸‰å¤§ä»»åŠ¡ã€‚
   - é€šè¿‡ **Map-Reduce + SpMM** å®ç°æè‡´å¹¶è¡Œä¸å¯å¾®æ€§ã€‚
3. **è§£ææ¢¯åº¦ä¼˜äºè‡ªåŠ¨å¾®åˆ†**ï¼š
   - ä½¿ç”¨ shape function gradients æ›¿ä»£ `autograd.grad()` æå¤§æå‡æ•ˆç‡ä¸ç²¾åº¦ã€‚
4. **ç‰©ç†ä¿¡æ¯å…ˆéªŒè‡³å…³é‡è¦**ï¼š
   - åœ¨ä½æ•°æ®å’Œ OOD åœºæ™¯ä¸‹ï¼ŒTENSORPILS æ˜¾è‘—ä¼˜äºçº¯æ•°æ®é©±åŠ¨æ–¹æ³•ã€‚
5. **ç«¯åˆ°ç«¯å¯å¾®æ€§ç®€åŒ–ä¼˜åŒ–æµç¨‹**ï¼š
   - æ— éœ€æ‰‹åŠ¨å®ç°ä¼´éšå˜é‡æ³•ï¼Œæ¢¯åº¦è‡ªåŠ¨ä¼ æ’­ã€‚

---

### å±€é™æ€§
- **å‡è®¾ PDE å…·æœ‰å˜åˆ†ç»“æ„**ï¼ˆvariational structureï¼‰ï¼šä»…é€‚ç”¨äºèƒ½å†™æˆåŒçº¿æ€§å½¢å¼ $ a_p(u,v) = l_p(v) $ çš„ PDEã€‚
- ä¸ç›´æ¥æ”¯æŒéåè°ƒæ–¹æ³•ï¼ˆå¦‚ DGï¼‰ã€éçº¿æ€§å¤æ‚è€¦åˆç³»ç»Ÿã€‚
- å½“å‰å®ç°é›†ä¸­åœ¨ 2D/3D æ ‡é‡/çŸ¢é‡æ¤­åœ†ã€æŠ›ç‰©ã€åŒæ›²æ–¹ç¨‹ï¼Œæ›´å¤æ‚ç³»ç»Ÿéœ€è¿›ä¸€æ­¥æ‰©å±•ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³ **éåè°ƒæœ‰é™å…ƒæ–¹æ³•**ï¼ˆDiscontinuous Galerkin, Petrov-Galerkinï¼‰ã€‚
2. æ”¯æŒæ›´å¤æ‚çš„ **æ—¶é—´æ­¥è¿›ç­–ç•¥**ï¼ˆå¦‚è‡ªé€‚åº”æ—¶é—´æ­¥ã€éšå¼ RKï¼‰ã€‚
3. æ¢ç´¢ **ä¸‰ç»´å¤æ‚å‡ ä½•ä¸‹çš„å¤§è§„æ¨¡ç³»ç»Ÿ** åº”ç”¨ã€‚
4. åº”ç”¨äºçœŸå®ä¸–ç•Œåœºæ™¯ï¼šå¦‚æµä½“æ§åˆ¶ã€ææ–™è®¾è®¡ã€æ°”å€™å»ºæ¨¡ç­‰ PDE-constrained control é—®é¢˜ã€‚

---

> ğŸ“Œ **é¡¹ç›®ä¸»é¡µ**ï¼š[https://camlab-ethz.github.io/TensorGalerkin](https://camlab-ethz.github.io/TensorGalerkin)

</details>

---

### 10. [E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching](https://arxiv.org/abs/2602.05068)

**Authors**: Wenting Li, Saif R. Kazi, Russell Bent, Duo Zhou, Huan Zhang  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.05068v1  

#### Abstract
Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching  
**è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç¥ç»ç½‘ç»œåœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚ç”µåŠ›ç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ï¼‰çš„åº”ç”¨å—é™äºå…¶**é²æ£’æ€§éªŒè¯çš„å¯æ‰©å±•æ€§ä¸å®Œå¤‡æ€§ä¹‹é—´çš„æƒè¡¡**ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **MIP-based å®Œå…¨éªŒè¯å™¨**ï¼šè™½ç„¶èƒ½è·å¾—å…¨å±€æœ€ä¼˜è§£ $f^*$ï¼Œä½†è®¡ç®—å¤æ‚åº¦éšç½‘ç»œè§„æ¨¡æŒ‡æ•°å¢é•¿ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§æ¨¡å‹ã€‚
- **æ¾å¼›-based ä¸å®Œå…¨éªŒè¯å™¨**ï¼ˆå¦‚ CROWNï¼‰ï¼šé€Ÿåº¦å¿«ï¼Œä½†ä»…æä¾›ä¸‹ç•Œ $l \leq f^*$ï¼Œæ— æ³•é‡åŒ–ä¼˜åŒ–é—´éš™ï¼ˆoptimality gapï¼‰ï¼Œå¯¼è‡´å¤§é‡â€œæœªçŸ¥â€çŠ¶æ€ã€‚
- **å¯¹æŠ—æ”»å‡»æ–¹æ³•**ï¼ˆå¦‚ PGDï¼‰ï¼šå¯å‘å¼æœç´¢ä¸Šç•Œ $u \geq f^*$ï¼Œä½†æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä¸Šç•Œæ¾æ•£ä¸”ä¸å¯é ã€‚

æœ¬æ–‡æ—¨åœ¨é€šè¿‡**é«˜æ•ˆåœ°åŒæ—¶ç´§ç¼©ä¸Šä¸‹ç•Œ**ï¼Œå®ç°æ¥è¿‘å…¨å±€æœ€ä¼˜çš„ $\epsilon$-global éªŒè¯ï¼Œåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶å¤§å¹…æå‡æ•ˆç‡ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹

E-Globe æ˜¯ä¸€ä¸ªåŸºäº **Branch-and-Bound (BaB)** æ¡†æ¶çš„æ··åˆéªŒè¯å™¨ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### (i) **NLP-CC ä¸Šç•Œæ±‚è§£å™¨ï¼ˆTight Upper Boundingï¼‰**
- å°†æ¯ä¸ª ReLU æ¿€æ´»å‡½æ•°ç”¨ **Complementarity Constraints (CC)** ç²¾ç¡®å»ºæ¨¡ï¼Œæ„å»ºä¸€ä¸ªéçº¿æ€§è§„åˆ’é—®é¢˜ï¼ˆNLP-CCï¼‰ã€‚
- è¯¥ reformulation æ˜¯**ç²¾ç¡®ç­‰ä»·**çš„ï¼šä»»ä½•å¯è¡Œè§£éƒ½å¯¹åº”åŸå§‹ç½‘ç»œçš„ä¸€ä¸ªæœ‰æ•ˆæ¿€æ´»æ¨¡å¼ï¼Œè¾“å‡ºå€¼å³ä¸ºåˆæ³•ä¸Šç•Œ $u = f(x)$ã€‚
- åˆ©ç”¨ KKT æ¡ä»¶è§£é‡Š ReLU åˆ‡æ¢è¡Œä¸ºï¼Œç¡®ä¿è¾“å…¥-è¾“å‡ºå›¾ä¸å˜ï¼ˆinvariant feasible regionï¼‰ã€‚
- åœ¨æ»¡è¶³ä¸¥æ ¼äº’è¡¥æ€§ï¼ˆstrict complementarityï¼‰æ—¶ï¼Œä¸Šç•Œæ˜¯ç´§è‡´çš„ï¼ˆtightï¼‰ã€‚

#### (ii) **Warm-started NLP with Low-Rank KKT Updates**
- åœ¨ BaB è¿‡ç¨‹ä¸­å¤ç”¨çˆ¶èŠ‚ç‚¹çš„ NLP è§£ä½œä¸º warm-startã€‚
- åˆ†æ”¯ä»…æ”¹å˜å°‘é‡ç¥ç»å…ƒç›¸ä½ï¼Œå› æ­¤åªéœ€å¯¹ KKT ç³»ç»Ÿè¿›è¡Œ**ä½ç§©ä¿®æ­£**ï¼ˆrank â‰¤ 4ï¼‰ï¼Œæ˜¾è‘—åŠ é€Ÿåç»­ NLP æ±‚è§£ã€‚
- å®è·µä¸­å¸¦æ¥ **2â€“5Ã— çš„é€Ÿåº¦æå‡**ã€‚

#### (iii) **Pattern-Aligned Strong Branching**
- åˆ©ç”¨ NLP-CC è¿”å›çš„å½“å‰æœ€ä¼˜æ¿€æ´»æ¨¡å¼ $a_{\text{NLP}}$ ä½œä¸ºâ€œå¯¼èˆªä¿¡å·â€ã€‚
- æ”¹è¿›ä¼ ç»Ÿçš„ Filtered Smart Branching (FSB)ï¼Œå¼•å…¥æ­£åˆ™é¡¹ä½¿å…¶ä¼˜å…ˆé€‰æ‹©ä¸ $a_{\text{NLP}}$ å¯¹é½çš„åˆ†è£‚æ–¹å‘ï¼š
  $$
  s_a(C_i) = s(C_i) + \lambda \cdot m(a(C_i), a_{\text{NLP}})
  $$
  å…¶ä¸­ $m$ è¡¨ç¤ºä¸ç¨³å®šç¥ç»å…ƒç›¸ä½åŒ¹é…çš„æ¯”ä¾‹ã€‚
- æ˜¾è‘—å‡å°‘æ— æ•ˆåˆ†æ”¯ï¼Œæ›´å¿«æå‡ä¸‹ç•Œã€‚

#### (iv) **$\epsilon$-Global Verification Framework**
- åŒæ—¶ç»´æŠ¤ä¸Šä¸‹ç•Œ $[l, u]$ï¼Œå½“ $u - l \leq \epsilon$ æ—¶åœæ­¢ï¼Œè¿”å› $\epsilon$-optimal certificateã€‚
- è‹¥ $u < 0$ï¼šç«‹å³è¿”å› **Unsafe** å¹¶ç»™å‡ºåä¾‹ï¼›è‹¥ $l > 0$ï¼šè¿”å› **Safe**ã€‚
- å®ç°æ—©æœŸç»ˆæ­¢ï¼Œé¿å…ç©·ä¸¾æ‰€æœ‰å­é—®é¢˜ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | E-Globe | MIP | PGD | æ¾å¼›æ–¹æ³•ï¼ˆå¦‚ CROWNï¼‰ |
|------|--------|-----|-----|------------------|
| ä¸Šç•Œè´¨é‡ | âœ… ç´§è‡´ã€å¯é  | âœ… æœ€ä¼˜ | âŒ æ¾æ•£ã€å¯èƒ½å¤±è´¥ | âŒ æ— ä¸Šç•Œ |
| ä¸‹ç•Œè´¨é‡ | âœ… å¯è®¤è¯ï¼ˆvia B-CROWNï¼‰ | âœ… æœ€ä¼˜ | âŒ æ—  | âœ… å¯è®¤è¯ |
| æ•ˆç‡ | â­â­â­ é«˜ï¼ˆå°¤å…¶å¤§è§„æ¨¡ï¼‰ | â­ ææ…¢ï¼ˆæŒ‡æ•°çº§ï¼‰ | â­â­ å¿«ä½†ä¸å®Œæ•´ | â­â­â­ å¿«ä½†gapæœªçŸ¥ |
| å¯æ‰©å±•æ€§ | âœ… è‰¯å¥½ï¼ˆå¤šé¡¹å¼è¶‹åŠ¿ï¼‰ | âŒ å·® | âœ… è‰¯å¥½ | âœ… è‰¯å¥½ |
| è¾“å‡ºå®Œæ•´æ€§ | âœ… $\epsilon$-gap æˆ–æ˜ç¡®ç»“è®º | âœ… å®Œå¤‡ | âŒ ä¸å®Œå¤‡ | âŒ â€œæœªçŸ¥â€å¤š |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
- **MNIST**ï¼šè¾“å…¥ç»´åº¦ 784ï¼Œç”¨äºå°æ‰°åŠ¨ï¼ˆ$\delta \leq 0.01$ï¼‰å’Œå¤§æ‰°åŠ¨ï¼ˆ$\delta \leq 0.1$ï¼‰æµ‹è¯•ã€‚
- **CIFAR-10**ï¼šè¾“å…¥ç»´åº¦ 3072ï¼Œæ›´å…·æŒ‘æˆ˜æ€§ï¼Œæµ‹è¯•æ›´å¤§æ‰°åŠ¨åœºæ™¯ï¼ˆ$\delta = 0.01, 0.03$ï¼‰ã€‚

### ğŸ—ï¸ ç½‘ç»œæ¶æ„
- **MNIST**ï¼šå…¨è¿æ¥ç½‘ç»œ `NoSoftmaxNet`ï¼Œä¸¤å±‚éšè—å±‚ï¼ˆ50 unitsï¼‰ï¼Œæ—  softmax å±‚ã€‚
- **CIFAR-10**ï¼šå…¨è¿æ¥ ReLU MLPï¼Œä¸¤å±‚ï¼ˆ256 unitsï¼‰ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- æ‰°åŠ¨é›†ï¼š$\ell_\infty$ çƒ $C = \{x : \|x - x_0\|_\infty \leq \delta\}$ã€‚
- éªŒè¯ç›®æ ‡ï¼šæœ€å°åˆ†ç±» margin $f(x) = z_k - z_a$ï¼ˆ$k$: æ­£ç¡®ç±»ï¼Œ$a$: æ”»å‡»ç±»ï¼‰ã€‚
- ä½¿ç”¨ **B-CROWN** ä½œä¸ºä¸‹ç•Œä¼ æ’­æ–¹æ³•ã€‚
- NLP-CC ä½¿ç”¨ **IPOPT** æ±‚è§£ï¼ŒMIP ä½¿ç”¨ **Gurobi**ã€‚
- å®éªŒå¹³å°ï¼š
  - Mac M4ï¼ˆCPU/GPUï¼‰ç”¨äºä¸Šç•Œå®éªŒï¼›
  - AMD 32æ ¸æœåŠ¡å™¨ + 64 GPUs ç”¨äºå®Œæ•´ BaB å®éªŒã€‚

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| $\Delta_\delta = |u - f^*|$ æˆ– $|l - f^*|$ | ç»å¯¹è¯¯å·®ï¼ˆä»¥ MIP æˆ–é«˜ä¿çœŸæ±‚è§£å™¨ä¸º ground truthï¼‰ |
| $\Delta_\delta^{\text{rel}} = \Delta_\delta / |f^*|$ | ç›¸å¯¹è¯¯å·® |
| ä¸Šç•ŒæˆåŠŸç‡ $\phi(\%)$ | æˆåŠŸæ‰¾åˆ°æœ‰æ•ˆä¸Šç•Œçš„æ¡ˆä¾‹å æ¯” |
| Runtime | å•ä¸ªæ ·æœ¬å¹³å‡è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ |
| Speedup | ç›¸å¯¹äº MIP çš„åŠ é€Ÿæ¯” |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ä¸‹ç•Œæ–¹æ³•**ï¼šCROWN-IBP, CROWN, $\alpha$-CROWN
- **ä¸Šç•Œæ–¹æ³•**ï¼šPGD
- **å®Œå…¨éªŒè¯å™¨**ï¼šMIP
- **ç»„åˆéªŒè¯å™¨**ï¼š$\alpha$-B-CROWNï¼ˆç”¨äºå®Œæ•´ BaB å¯¹æ¯”ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“‰ ä¸Šç•Œç´§è‡´æ€§ï¼ˆTables 1â€“4ï¼‰
| æ–¹æ³• | MNIST ($\delta=0.1$) $\Delta_{0.1}$ | MNIST ($\delta=0.01$) $\Delta_{0.01}$ | CIFAR-10 ($\delta=0.03$) $\Delta_{0.03}$ |
|------|-------------------------------|----------------------------------|------------------------------------|
| CROWN-IBP | 68.01 | 2.57 | â€” |
| CROWN | 42.73 | 0.4719 | â€” |
| $\alpha$-CROWN | 35.53 | 0.4673 | â€” |
| **E-Globe$_u$** | **0.43** | **0.0004** | **0.003** |
| PGD | â€” | â€” | 0.204 |

> âœ… E-Globe$_u$ çš„ä¸Šç•Œè¿œä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå³ä½¿åœ¨å¤§æ‰°åŠ¨ä¸‹ä»ä¿æŒæå°è¯¯å·®ï¼ˆ< 0.05ï¼‰ï¼Œè€Œ PGD ä¸Šç•Œæ¾æ•£ä¸”æˆåŠŸç‡ä½ï¼ˆä»… 42% @ CIFAR-10ï¼‰ã€‚

### â±ï¸ æ•ˆç‡å¯¹æ¯”ï¼ˆFigure 6ï¼‰
- å½“ binary variables æ•°é‡ > 120 æ—¶ï¼Œ**MIP è¿è¡Œæ—¶é—´å‘ˆæŒ‡æ•°å¢é•¿**ï¼Œå¤šæ•°è¶…æ—¶ï¼ˆ>2500sï¼‰ã€‚
- **E-Globe$_u$** è¿è¡Œæ—¶é—´å‡ ä¹ä¸å—å˜é‡æ•°å½±å“ï¼Œç»´æŒåœ¨ 0â€“30 ç§’å†…ã€‚
- åœ¨ binary vars > 180 åœºæ™¯ä¸‹ï¼Œ**E-Globe æ¯” MIP å¿« 2â€“3 ä¸ªæ•°é‡çº§**ã€‚

### ğŸ”¥ Warm-start åŠ é€Ÿæ•ˆæœï¼ˆFigure 7ï¼‰
- ä½¿ç”¨ low-rank KKT warm-start åï¼Œæ¯è½® NLP æ±‚è§£æ—¶é—´ä¸‹é™ **2â€“5Ã—**ã€‚
- ç‰¹åˆ«æ˜¯åœ¨å‰å‡ è½®åˆ†æ”¯ä¸­ï¼Œwarm-start æ˜¾è‘—ç¼©çŸ­æ”¶æ•›æ—¶é—´ã€‚

### ğŸŒ± Pattern-Aligned Branching æ¶ˆèå®éªŒï¼ˆFigure 9ï¼‰
- å¼•å…¥ pattern alignmentï¼ˆ$\lambda > 0$ï¼‰åï¼Œ**ä¸‹ç•Œä¸Šå‡é€Ÿåº¦æ˜æ˜¾åŠ å¿«**ã€‚
- $\lambda = 0.1$ æ—¶è¡¨ç°æœ€ä½³ï¼Œåœ¨çº¦ 500 è½®åç¨³å®šé¢†å…ˆæ ‡å‡† FSB æ–¹æ³•ã€‚
- è¯´æ˜ NLP æä¾›çš„æ¿€æ´»æ¨¡å¼æ˜¯é«˜è´¨é‡å¼•å¯¼ä¿¡å·ã€‚

### ğŸ“ˆ å®Œæ•´ E-Globe æ€§èƒ½ï¼ˆFigure 8ï¼‰
- åœ¨éš¾ä¾‹ï¼ˆcase 42ï¼‰ä¸Šï¼ŒMIP è€—æ—¶ >2000s æ‰æ”¶æ•›ã€‚
- E-Globe åœ¨ä¸åŒ $\epsilon$ ä¸‹å‡å®ç°å¤§å¹…åŠ é€Ÿï¼š
  - $\epsilon = 0.1$ï¼šçº¦ **20Ã— speedup**
  - $\epsilon = 0.5$ï¼šå¯è¾¾ **>100Ã— speedup**
- gap éš branch round å¿«é€Ÿç¼©å°ï¼Œä¸»è¦å¾—ç›Šäºä¸‹ç•Œå¿«é€Ÿä¸Šå‡ï¼ˆB-CROWN + pattern-aware branchingï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **NLP-CC æ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„ä¸Šç•Œç”Ÿæˆæœºåˆ¶**ï¼šå®ƒä¿ç•™äº† ReLU çš„ç²¾ç¡®ç»“æ„ï¼Œäº§ç”Ÿçš„æ¯ä¸ªå¯è¡Œè§£éƒ½æ˜¯æœ‰æ•ˆçš„ counterexampleï¼Œå¹¶èƒ½æä¾›ç´§è‡´ä¸Šç•Œã€‚
2. **ä¸Šä¸‹ç•ŒååŒå¯æå¤§æå‡éªŒè¯æ•ˆç‡**ï¼šé€šè¿‡ tight upper bound å®ç°å¿«é€Ÿ reject unsafe æƒ…å†µï¼Œç»“åˆ pattern-guided branching å¿«é€Ÿ tighten lower boundï¼Œé¿å… exhaustive searchã€‚
3. **warm-start å’Œä½ç§©æ›´æ–°æ˜¾è‘—é™ä½ NLP å¼€é”€**ï¼šä½¿å¾—åœ¨ BaB ä¸­é¢‘ç¹è°ƒç”¨ NLP æˆä¸ºå¯è¡Œã€‚
4. **pattern-aligned branching æ˜¯å…³é”®è®¾è®¡**ï¼šåˆ©ç”¨ NLP æä¾›çš„å±€éƒ¨æœ€ä¼˜æ¨¡å¼æŒ‡å¯¼åˆ†æ”¯ï¼Œæ˜¾è‘—æé«˜åˆ†æ”¯æ•ˆç‡ã€‚
5. **E-Globe åœ¨å®è·µä¸­æ¥è¿‘ complete verifier çš„è¦†ç›–ç‡**ï¼Œä½†åœ¨è¿è¡Œæ—¶é—´ä¸Šå®ç°æ•°é‡çº§æå‡ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ç†è®ºä¸Šå±äº incomplete verifier**ï¼šå°½ç®¡å®è·µä¸­ç»å¤§å¤šæ•°æƒ…å†µéƒ½èƒ½è§£å†³ï¼Œä½†ä»å­˜åœ¨æ— æ³•åœ¨æ—¶é™å†…è¾¾åˆ° $\epsilon$-gap çš„æç«¯æ¡ˆä¾‹ã€‚
2. **ä¾èµ–é«˜è´¨é‡åˆå§‹ bounds**ï¼šéœ€å…ˆç”¨ a-CROWN è·å–ä¸­é—´å±‚ boundsï¼Œå¦åˆ™ NLP-CC å¯èƒ½éš¾ä»¥æ”¶æ•›ã€‚
3. **GPU batching æœªå®Œå…¨å¹¶è¡ŒåŒ–**ï¼šç›®å‰ B-CROWN æ”¯æŒ GPU batchï¼Œä½† NLP-CC æ±‚è§£ä»æ˜¯ä¸²è¡Œï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚
4. **å¯¹éå¸¸æ·±æˆ–å¤æ‚ç»“æ„ï¼ˆå¦‚ Transformerï¼‰æ”¯æŒæœ‰é™**ï¼šå½“å‰å®éªŒé›†ä¸­åœ¨ FC å’Œç®€å• CNN ç»“æ„ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **è¿›ä¸€æ­¥èåˆ local solver ä¸ convex relaxation**ï¼šæ¢ç´¢æ›´æ·±å±‚æ¬¡çš„ååŒæœºåˆ¶ï¼Œä¾‹å¦‚å°† NLP è§£ç”¨äºæ„é€ æ›´å¼ºçš„ convex surrogateã€‚
2. **æ‰©å±•è‡³å…¶ä»–æ¿€æ´»å‡½æ•°å’Œç½‘ç»œç»“æ„**ï¼šå¦‚ SiLUã€GeLUã€ResNetã€Vision Transformer ç­‰ã€‚
3. **å¼€å‘åˆ†å¸ƒå¼/å¹¶è¡ŒåŒ–ç‰ˆæœ¬**ï¼šåˆ©ç”¨å¤š GPU/CPU å¹¶è¡Œå¤„ç†å¤šä¸ª subdomainsã€‚
4. **åº”ç”¨äº real-time verification åœºæ™¯**ï¼šå¦‚è‡ªåŠ¨é©¾é©¶ä¸­çš„åœ¨çº¿é²æ£’æ€§ç›‘æ§ã€‚
5. **ç†è®ºåˆ†æ NLP-CC çš„ landscape properties**ï¼šä¸ºä½•å…¶å±€éƒ¨è§£å¸¸æ¥è¿‘å…¨å±€æœ€ä¼˜ï¼Ÿæ˜¯å¦å­˜åœ¨éšå¼çš„æ³›åŒ–ç»“æ„ï¼Ÿ

--- 

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **E-Globe é€šè¿‡ NLP-CC ç²¾ç¡®ä¸Šç•Œ + pattern-aware branching + warm-start æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ¯” MIP éªŒè¯å™¨å¿« 2â€“3 ä¸ªæ•°é‡çº§çš„é€Ÿåº¦ï¼Œæ¨åŠ¨ç¥ç»ç½‘ç»œéªŒè¯è¿ˆå‘å®ç”¨åŒ– $\epsilon$-global æœ€ä¼˜æ—¶ä»£ã€‚**

</details>

---

### 11. [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)

**Authors**: Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.04284v1  

#### Abstract
Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAgent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰çš„ **LLM Agent** åœ¨å¤šè½®ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­ï¼Œæ™®éå­˜åœ¨ç”Ÿæˆå†—ä½™ **Thought**ï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰å’Œç´¯ç§¯è¿‡å¤šå†å² **Observation**ï¼ˆç¯å¢ƒåé¦ˆï¼‰çš„é—®é¢˜ã€‚è¿™å¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦è¿…é€Ÿå¢é•¿ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ•ˆç‡ï¼ˆtoken cost é«˜ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

ç°æœ‰ç ”ç©¶é€šå¸¸å¯¹æ•´ä¸ªäº¤äº’è½¨è¿¹è¿›è¡Œç»Ÿä¸€å‹ç¼©æˆ–å‰ªæï¼ˆå¦‚å›ºå®šé•¿åº¦æˆªæ–­ã€LLM summarizationï¼‰ï¼Œå¿½ç•¥äº† **ä¸åŒäº¤äº’è½®æ¬¡ä¸­ Thought å’Œ Observation çš„å¿…è¦æ€§æ˜¯åŠ¨æ€å˜åŒ–çš„**ã€‚ä¾‹å¦‚ï¼š
- åˆå§‹è§„åˆ’é˜¶æ®µçš„ Thought è‡³å…³é‡è¦ï¼›
- ä¸­é—´æ‰§è¡Œé˜¶æ®µå¯èƒ½æ— éœ€å¤æ‚æ¨ç†ï¼›
- æ—©æœŸ Observations åœ¨æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæ—¶å¾€å¾€å·²æ— ç”¨ã€‚

å› æ­¤ï¼Œå¦‚ä½•å®ç°**è‡ªé€‚åº”åœ°ã€é€‰æ‹©æ€§åœ°çœç•¥å†—ä½™å†…å®¹**æˆä¸ºæå‡æ•ˆç‡çš„å…³é”®ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡º **Agent-Omit**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ï¼Œä½¿ LLM Agent èƒ½å¤Ÿé€šè¿‡ **Agentic Reinforcement Learning** å­¦ä¹ åˆ°è‡ªé€‚åº”çœç•¥ï¼ˆadaptive omissionï¼‰çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **ç»Ÿä¸€çš„åˆ†ææ¡†æ¶**  
   é¦–æ¬¡ä»â€œè½®æ¬¡çº§åˆ«â€ï¼ˆturn-levelï¼‰å®šé‡åˆ†æ Thought ä¸ Observation å¯¹ **Effectiveness**ï¼ˆä»»åŠ¡å‡†ç¡®ç‡ï¼‰å’Œ **Efficiency**ï¼ˆtoken å¼€é”€ï¼‰çš„å½±å“ï¼ŒéªŒè¯äº†â€œéå‡åŒ€å¿…è¦æ€§â€çš„å‡è®¾ã€‚

2. **ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**
   - **Agent Omission Behavior Synthesisï¼ˆå†·å¯åŠ¨å¾®è°ƒï¼‰**  
     æ„å»ºå•è½®ä¸å¤šè½®çœç•¥åœºæ™¯çš„åˆæˆæ•°æ®ï¼Œç”¨äº SFTï¼ˆSupervised Fine-Tuningï¼‰ï¼Œæ•™ä¼šæ¨¡å‹å¦‚ä½•æ‰§è¡Œ `<think></think>`ï¼ˆç©ºæ€è€ƒï¼‰å’Œ `<omit_tool_response_N_...>`ï¼ˆçœç•¥æŒ‡å®šè½®æ¬¡è§‚å¯Ÿï¼‰ç­‰æ ¼å¼åŒ–è¡Œä¸ºã€‚
   - **Omit-Aware Agentic RLï¼ˆçœç•¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼‰**
     å¼•å…¥åŒé‡‡æ ·æœºåˆ¶ï¼ˆDual Samplingï¼‰å’Œå®šåˆ¶åŒ–çš„çœç•¥å¥–åŠ±ï¼ˆOmission Rewardï¼‰ï¼Œè®©ç­–ç•¥èƒ½åœ¨ä¿ç•™åŸå§‹ä¸Šä¸‹æ–‡çš„åŒæ—¶å­¦ä¹ çœç•¥å†³ç­–ï¼Œé¿å…å› â€œä¸Šä¸‹æ–‡æ”¹å˜â€è€Œå¯¼è‡´æ— æ³•å­¦ä¹ ã€‚

3. **ç†è®ºä¿éšœ**
   è¯æ˜æ‰€å­¦çœç•¥ç­–ç•¥çš„æ€§èƒ½åå·®ç”± KL æ•£åº¦ä¸Šç•Œæ§åˆ¶ï¼Œä¸ºæ–¹æ³•ç¨³å®šæ€§æä¾›ç†è®ºæ”¯æŒã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç±»åˆ« | å…¸å‹æ–¹æ³• | å±€é™ | Agent-Omit ä¼˜åŠ¿ |
|------|--------|-------|----------------|
| **Thought Management (TM)** | DEPO, ToolLight, Thinking-Retention | å›ºå®šå‹ç¼©ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§ï¼›æ˜“ä¸¢å¤±å…³é”®ä¿¡æ¯ | è‡ªé€‚åº”åˆ¤æ–­æ˜¯å¦éœ€è¦æ¨ç† |
| **Observation Management (OM)** | Observation-Mask, DeepMiner | å¯å‘å¼è§„åˆ™ï¼ˆå¦‚æ»‘çª—ï¼‰ï¼Œä¸èƒ½æ³›åŒ– | åŠ¨æ€è¯†åˆ«å¯çœç•¥çš„å†å²è§‚æµ‹ |
| **TOMï¼ˆè”åˆç®¡ç†ï¼‰** | MEM-Agent, ReSum | ä¾èµ–å¤–éƒ¨ LLM summarizerï¼Œå¼•å…¥é¢å¤–å¼€é”€ä¸”ä¸ä¸»æ¨ç†è„±èŠ‚ | å†…ç”Ÿå¼å‹ç¼©ï¼Œç«¯åˆ°ç«¯ä¼˜åŒ– |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼šä¸æ˜¯ç®€å•åˆ å‡ï¼Œè€Œæ˜¯è®© Agent â€œå­¦ä¼šä½•æ—¶å¯ä»¥å®‰å…¨è·³è¿‡â€ï¼Œå®ç°äº†æ›´çµæ´»ã€é«˜æ•ˆã€å¯æ‰©å±•çš„ä¸Šä¸‹æ–‡ç®¡ç†æœºåˆ¶ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨äº”ä¸ªå¤šæ ·åŒ–åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¦†ç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼š

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æœ€å¤§å›åˆæ•° | æµ‹è¯•æ ·æœ¬æ•° |
|-------|---------|------------|-----------|
| **DeepSearch** | çŸ¥è¯†å¯†é›†å‹æœç´¢é—®ç­” | 8 | 400 |
| **WebShop** | ç”µå•†ç½‘ç«™å¯¼èˆªä¸è´­ä¹° | 12 | 200 |
| **TextCraft** | æ–‡æœ¬ç‰ˆ Minecraft é•¿ç¨‹è§„åˆ’ | 20 | 100 |
| **BabyAI** | ç½‘æ ¼ä¸–ç•ŒæŒ‡ä»¤è·Ÿéš | 10 | 90 |
| **SciWorld** | ç§‘å­¦å®éªŒæ¨¡æ‹Ÿä¸æ¨ç† | 10 | 200 |

è¿™äº›ä»»åŠ¡å‡æ¥è‡ª **AgentGym-RL** ç»Ÿä¸€è¯„æµ‹å¹³å°ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **Backbone æ¨¡å‹**ï¼šQwen3-4B / Qwen3-8B
- **è®­ç»ƒæµç¨‹**ï¼š
  1. **SFT å†·å¯åŠ¨**ï¼šä½¿ç”¨çº¦ 2â€“4K åˆæˆçœç•¥æ•°æ®ï¼Œå…¨å‚æ•°å¾®è°ƒã€‚
  2. **Agentic RL å¾®è°ƒ**ï¼šåŸºäº GRPO ç®—æ³•ï¼Œç»“åˆ dual sampling ä¸ omission rewardã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Pass@1**ï¼šä»»åŠ¡æˆåŠŸç‡ï¼ˆä¸»è¦è¡¡é‡ effectivenessï¼‰
  - **Avg Tok. â†“**ï¼šå¹³å‡æ¯è½®è¾“å‡º token æ•°é‡ï¼ˆè¡¡é‡ efficiencyï¼‰
  - **Effectiveness-Efficiency Trade-off**ï¼šç»¼åˆè€ƒé‡å‡†ç¡®ç‡ä¸æˆæœ¬

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
åˆ†ä¸ºä¸¤ç±»ï¼š

#### ï¼ˆ1ï¼‰å‰æ²¿ LLM Agentsï¼ˆFrontier Modelsï¼‰
- DeepSeek-R1-0528, DeepSeek-V3.2
- OpenAI o3 / o4-mini
- Qwen3-235B-A22B, Qwen3-Next-80B-A3B, Qwen3-32B

> ç›®æ ‡ï¼šéªŒè¯ Agent-Omit æ˜¯å¦èƒ½ä»¥å°æ¨¡å‹åª²ç¾ç”šè‡³è¶…è¶Šå¤§æ¨¡å‹æ€§èƒ½ã€‚

#### ï¼ˆ2ï¼‰é«˜æ•ˆ Agent æ„å»ºæ–¹æ³•ï¼ˆEfficient Agent Methodsï¼‰
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **TM** | Thinking-Retention, DEPO, Tool-Light |
| **OM** | Observation-Mask, DeepMiner |
| **TOM** | MEM-Agent, ReSum |
| **Ours** | Agent-Omit-8B-RL |

> ç›®æ ‡ï¼šéªŒè¯åœ¨ç›¸åŒ backbone ä¸‹ï¼ŒAgent-Omit çš„æ•ˆç‡å¢ç›Šæ˜¯å¦æœ€ä¼˜ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ Qwen3-8B ä¸ºåŸºç¡€ï¼‰

#### âœ… ä¸å‰æ²¿ LLM Agents å¯¹æ¯”ï¼ˆTable 2ï¼‰
| Model | DeepSearch (Pass@1) | DeepSearch (Tokâ†“) | WebShop (Pass@1) | WebShop (Tokâ†“) |
|-------|------------------------|--------------------|------------------|---------------|
| DeepSeek-R1-0528 | 25.25 | 6,412 | 19.37 | 11,308 |
| Qwen3-32B | 19.00 | 6,640 | 11.31 | 11,872 |
| **Agent-Omit-8B-RL** | **26.56** | **4,356** | **23.57** | **8,764** |

> ğŸ“Œ åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ° SOTA å‡†ç¡®ç‡ï¼ŒåŒæ—¶ token æ¶ˆè€—æ˜¾è‘—ä½äºå¤§å¤šæ•° reasoning-mode æ¨¡å‹ã€‚

#### âœ… ä¸é«˜æ•ˆ Agent æ–¹æ³•å¯¹æ¯”ï¼ˆTable 3ï¼‰
| Method | DeepSearch (Pass@1â†‘) | DeepSearch (Tokâ†“) | WebShop (Pass@1â†‘) | WebShop (Tokâ†“) |
|--------|------------------------|--------------------|------------------|---------------|
| Base (Qwen3-8B) | 17.75 | 8,281 | 6.93 | 16,741 |
| ReSum | 22.28 | 5,724 | 17.80 | 9,251 |
| **Agent-Omit-8B-RL** | **24.56** | **4,356** | **23.57** | **8,764** |

> âœ… **å”¯ä¸€åŒæ—¶å®ç°æœ€é«˜å‡†ç¡®ç‡ä¸æœ€ä½ token æˆæœ¬çš„æ–¹æ³•**ï¼Œå±•ç°å‡ºæœ€ä½³çš„ **effectiveness-efficiency trade-off**ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

åœ¨ WebShop ä¸Šå¯¹ Agent-Omit-8B è¿›è¡Œæ¶ˆèï¼ˆFigure 5ï¼‰ï¼š

| å˜ä½“ | Pass@1 | Avg Tok |
|------|--------|--------|
| Full Agent-Omit (SFT + RL) | **23.57** | **8,764** |
| w/o STO (æ— å•è½®çœç•¥æ•°æ®) | 21.2 | ~8,900 |
| w/o MTO (æ— å¤šè½®çœç•¥æ•°æ®) | 20.8 | ~9,100 |
| w/o PT (æ—  Partial Trajectory) | 20.1 | ~9,300 |
| w/o OR (æ—  Omission Reward) | 20.5 | ~10,200 |

> ğŸ” å‘ç°ï¼š
- **SFT é˜¶æ®µ**ï¼šå•è½®çœç•¥æ•°æ®ï¼ˆSTOï¼‰æœ€ä¸ºå…³é”®ï¼Œå¥ å®šåŸºç¡€èƒ½åŠ›ã€‚
- **RL é˜¶æ®µ**ï¼šPartial Trajectory é‡‡æ ·æ¯” Full æ›´é‡è¦ï¼›Omission Reward æ˜¯é©±åŠ¨ token ä¸‹é™çš„æ ¸å¿ƒåŠ¨åŠ›ã€‚
- **åŒé˜¶æ®µååŒå¢ç›Šæ˜æ˜¾**ï¼Œç¼ºä¸€ä¸å¯ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Thought ä¸ Observation çš„å¿…è¦æ€§éšè½®æ¬¡åŠ¨æ€å˜åŒ–**  
   å¹¶éæ‰€æœ‰è½®æ¬¡éƒ½éœ€è¦è¯¦ç»†æ¨ç†æˆ–å®Œæ•´å†å²ä¸Šä¸‹æ–‡ã€‚ä¸­é—´è½®æ¬¡æ˜¯çœç•¥çš„ä¸»è¦çª—å£æœŸã€‚

2. **è‡ªé€‚åº”çœç•¥å¯è¡Œä¸”æœ‰æ•ˆ**  
   é€šè¿‡é€‚å½“çš„è®­ç»ƒæœºåˆ¶ï¼ŒLLM Agent å¯å­¦ä¼šåœ¨ä¸å½±å“æ€§èƒ½çš„å‰æä¸‹ä¸»åŠ¨çœç•¥å†—ä½™å†…å®¹ã€‚

3. **Agent-Omit æ˜¾è‘—æå‡å°æ¨¡å‹ç«äº‰åŠ›**  
   Agent-Omit-8B-RL åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šæ›´å¤§è§„æ¨¡çš„ frontier modelsï¼Œå°¤å…¶åœ¨ token æ•ˆç‡æ–¹é¢ä¼˜åŠ¿å·¨å¤§ã€‚

4. **çœç•¥è¡Œä¸ºåˆ†å¸ƒç¬¦åˆé¢„æœŸ**  
   åˆ†ææ˜¾ç¤º Agent å¹³å‡æ¯æ¡è½¨è¿¹çœç•¥ **3â€“4 è½®**ï¼Œä¸”é›†ä¸­åœ¨ **ç¬¬ 3â€“10 è½®**ï¼ˆä¸­é—´æ‰§è¡Œé˜¶æ®µï¼‰ï¼Œä¸äººç±»ç›´è§‰ä¸€è‡´ï¼ˆFigure 6ï¼‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡åˆæˆæ•°æ®æ„å»ºå†·å¯åŠ¨é›†**ï¼šè‹¥åˆå§‹ omission rollouts ä¸å‡†ç¡®ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯æ¨¡å¼å›ºåŒ–ã€‚
- **å½“å‰ä»…é€‚ç”¨äºæ–‡æœ¬å‹äº¤äº’ç¯å¢ƒ**ï¼šå¯¹äºè§†è§‰æˆ–å¤šæ¨¡æ€ Agent å°šæœªéªŒè¯ã€‚
- **Omission Reward è®¾è®¡æ•æ„Ÿ**ï¼šéœ€ careful tuning æƒé‡ $ \rho $ï¼Œå¦åˆ™å¯èƒ½é™·å…¥ reward hackingã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å°† omission data synthesis æ‰©å±•è‡³é¢„è®­ç»ƒé˜¶æ®µ**ï¼Œæ¢ç´¢å¤§è§„æ¨¡ omission-aware pretrainingã€‚
2. **åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„ LLMs**ï¼ˆå¦‚ Qwen3-72B æˆ– DeepSeek-V3ï¼‰ï¼Œè¿›ä¸€æ­¥é‡Šæ”¾æ½œåŠ›ã€‚
3. **æ‰©å±•è‡³å¤šæ¨¡æ€ Agent**ï¼Œç ”ç©¶è§†è§‰/è¯­éŸ³ context çš„è‡ªé€‚åº”çœç•¥æœºåˆ¶ã€‚
4. **æ„å»ºé€šç”¨çš„ omission policy adapter**ï¼Œå®ç°è·¨ä»»åŠ¡è¿ç§»ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **Agent-Omit æå‡ºäº†ä¸€ç§â€œä¼šå·æ‡’â€çš„æ™ºèƒ½ä½“è®­ç»ƒæ–¹å¼â€”â€”å®ƒä¸ç›²ç›®æ¨ç†ä¹Ÿä¸æ­»è®°å†å²ï¼Œè€Œæ˜¯åœ¨æ°å½“çš„æ—¶å€™é€‰æ‹©â€œè·³è¿‡â€ï¼Œä»è€Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å¤§å¹…é™ä½æˆæœ¬ï¼Œä¸ºé«˜æ•ˆ LLM Agent è®¾è®¡æä¾›äº†æ–°èŒƒå¼ã€‚**

GitHub ä»£ç ä¸æ•°æ®å·²å¼€æºï¼š[https://github.com/usail-hkust/Agent-Omit](https://github.com/usail-hkust/Agent-Omit)

</details>

---

### 12. [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)

**Authors**: Zhilin Liang, Yuxiang Wang, Zimu Zhou, Hainan Zhang, Boyi Liu, Yongxin Tong  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.05235v1  

#### Abstract
Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. Thi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿ **Retrieval-Augmented Generation (RAG)** ä¾èµ–äºå°†æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æœ¬æ’å…¥ LLM çš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œç”Ÿæˆï¼Œè¿™åœ¨éšç§æ•æ„Ÿé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›é¢†åŸŸçš„æ•°æ®é€šå¸¸åˆ†æ•£åœ¨å¤šä¸ªæœºæ„ï¼ˆå³â€œæ•°æ®å­¤å²›â€ï¼‰ä¸­ï¼Œå—æ³•è§„ï¼ˆå¦‚ HIPAAã€GDPRï¼‰é™åˆ¶ï¼Œ**æ— æ³•å…±äº«åŸå§‹æ–‡æ¡£**ã€‚

ç°æœ‰çš„è”é‚¦ RAGï¼ˆFedRAGï¼‰æ–¹æ³•å¤§å¤šåŸºäº in-context RAGï¼Œä»éœ€ä¼ è¾“åŸå§‹æ–‡æœ¬ï¼Œè¿åäº†**æœ¬åœ°æ€§çº¦æŸ**ï¼ˆlocality constraintï¼‰ã€‚è€Œç›´æ¥åº”ç”¨ parametric RAG åˆ°è”é‚¦åœºæ™¯åˆé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **å­˜å‚¨ä¸é€šä¿¡å¼€é”€é«˜**ï¼šä¸ºæ¯ä¸ªæ–‡æ¡£è®­ç»ƒç‹¬ç«‹çš„ LoRA adapter å¯¼è‡´æ•°é‡çˆ†ç‚¸ã€‚
- **ç ´åæ€§èšåˆ**ï¼ˆdestructive aggregationï¼‰ï¼šç®€å•å¹³å‡æ¥è‡ªä¸åŒå­¤å²›çš„ adapters ä¼šå¼•å…¥å™ªå£°å’Œå‚æ•°å†²çªï¼Œé™ä½å‡†ç¡®æ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **FedMosaic** â€”â€” é¦–ä¸ªæ»¡è¶³ locality constraint çš„è”é‚¦ RAG æ¡†æ¶ï¼ŒåŸºäº **parametric adapters** æ„å»ºï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯è§£å†³ä¸Šè¿°é—®é¢˜ï¼š

#### âœ… åˆ›æ–°ç‚¹ä¸€ï¼šå¤šæ–‡æ¡£å‚æ•°åŒ–é€‚é…å™¨ï¼ˆMulti-Document Parametric Adaptersï¼‰
- å°†è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£èšç±»ï¼Œå¹¶ä¸ºæ¯ä¸ªèšç±»è®­ç»ƒä¸€ä¸ªå…±äº«çš„ LoRA adapterã€‚
- å¼•å…¥**æ–‡æ¡£çº§äºŒå€¼æ©ç **ï¼ˆdocument-specific binary masksï¼‰ï¼Œä½¿æ¯ä¸ªæ–‡æ¡£ä»…æ¿€æ´» adapter ä¸­ç‰¹å®šå­é›†çš„å‚æ•°ï¼Œä»è€Œä¿ç•™ç»†ç²’åº¦çŸ¥è¯†å¹¶ç¼“è§£**ç»„å†…å¹²æ‰°**ï¼ˆintra-silo interferenceï¼‰ã€‚
- é‡‡ç”¨ä½æ‰“åŒ…ï¼ˆbit-packingï¼‰è¿›ä¸€æ­¥å‹ç¼©æ©ç å­˜å‚¨ã€‚

> ğŸ“Œ *ä¼˜åŠ¿*ï¼šæ˜¾è‘—å‡å°‘ adapter æ•°é‡ï¼Œé™ä½å­˜å‚¨å’Œé€šä¿¡æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒ per-document specificityã€‚

#### âœ… åˆ›æ–°ç‚¹äºŒï¼šé€‰æ‹©æ€§é€‚é…å™¨èšåˆï¼ˆSelective Adapter Aggregationï¼‰
- åœ¨æ¨ç†é˜¶æ®µï¼Œå„ silo ä¸Šä¼ æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°åŠå…¶æ©ç ï¼ˆä¸ä¼  adapter æˆ–åŸæ–‡ï¼‰ã€‚
- æœåŠ¡å™¨åŸºäºç›¸å…³æ€§å’Œ**æ©ç é‡å åº¦**ï¼ˆoverlapï¼‰é€‰æ‹©æœ€ç›¸å…³ä¸”å‚æ•°å†²çªæœ€å°çš„ä¸€ç»„æ–‡æ¡£ã€‚
- è¯·æ±‚å¯¹åº”çš„ adapters åï¼Œåœ¨æ©ç æ§åˆ¶ä¸‹åŠ æƒèšåˆï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

> ğŸ“Œ *ä¼˜åŠ¿*ï¼šé¿å…æ— å…³æˆ–å†²çª adapters çš„è´Ÿé¢å½±å“ï¼Œæå‡å‡†ç¡®ç‡ï¼Œå®ç°â€œç›¸å…³æ€§æ„ŸçŸ¥ + å†²çªæ„ŸçŸ¥â€çš„èšåˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | FedMosaic | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ in-context FedRAG, PRAGï¼‰ |
|------|-----------|----------------------------|
| **éšç§ä¿æŠ¤** | âœ”ï¸ ä¸ä¼ è¾“ä»»ä½•åŸå§‹æ–‡æ¡£ | âŒ in-context æ–¹æ³•å¿…é¡»ä¼ åŸæ–‡ |
| **é€šä¿¡æ•ˆç‡** | â†“ é™ä½ 91.4% | é«˜æ˜‚ï¼ˆå°¤å…¶ per-document adapterï¼‰ |
| **å­˜å‚¨å¼€é”€** | â†“ é™ä½ 78.8%â€“86.3% | éšæ–‡æ¡£æ•°çº¿æ€§å¢é•¿ |
| **å‡†ç¡®æ€§** | â†‘ å¹³å‡æå‡ 10.9% F1 | æ˜“å—å™ªå£°å’Œå†²çªå½±å“ |
| **çµæ´»æ€§** | æ”¯æŒåŠ¨æ€ç»„åˆçŸ¥è¯† | è”é‚¦å¾®è°ƒéœ€é‡æ–°è®­ç»ƒ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒåœ¨å››ä¸ªä¸»æµé—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶µç›–å¤šç§æ¨ç†ç±»å‹ï¼š
- **HotpotQA**ï¼ˆHQAï¼‰ï¼šå¤šè·³æ¨ç†ï¼ˆmulti-hopï¼‰ï¼Œå« Bridge å’Œ Compare ç±»å‹
- **2WikiMultihopQA**ï¼ˆ2WQAï¼‰ï¼šå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œåˆ† Bridge / Compare / Inf / Compose å››ç±»
- **PopQA**ï¼ˆPQAï¼‰ï¼šå¸¸è¯†é—®ç­”
- **ComplexWebQuestions**ï¼ˆCWQï¼‰ï¼šå¤æ‚ Web æŸ¥è¯¢

æ­¤å¤–è¿˜è¿›è¡Œäº†éšç§æ”»å‡»è¯„ä¼°ä½¿ç”¨çš„æ•°æ®é›†ï¼š
- **Enron Emails**
- **WikiText**

### å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼š
  - ä¸»å¹² LLMï¼š`LLaMA3.2-1B-Instruct` å’Œ `LLaMA3-8B-Instruct`
  - é€‚é…å™¨æŠ€æœ¯ï¼šLoRAï¼ˆLow-Rank Adaptationï¼‰
- **è”é‚¦è®¾ç½®**ï¼š
  - æ•°æ®æŒ‰ä¸»é¢˜ä½¿ç”¨ Dirichlet åˆ†é…ï¼ˆÎ±=0.1ï¼‰åˆ’åˆ†ä¸ºå¤šä¸ª silo
  - æ¯ä¸ª silo æ‹¥æœ‰æœ¬åœ°æ–‡æ¡£åº“ $ \mathcal{D}_m $
- **ç¦»çº¿é˜¶æ®µ**ï¼š
  - æ–‡æ¡£èšç±» â†’ è®­ç»ƒ cluster-level adapter â†’ å­¦ä¹  document-specific mask
- **åœ¨çº¿é˜¶æ®µ**ï¼š
  - æŸ¥è¯¢å¹¿æ’­ â†’ æœ¬åœ°æ£€ç´¢ä¸é‡æ’åº â†’ ä¸ŠæŠ¥ relevance score + mask â†’ æœåŠ¡ç«¯é€‰æ‹© â†’ è·å– adapter â†’ æ©ç èšåˆ â†’ ç”Ÿæˆç­”æ¡ˆ

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Accuracy (F1 Score)** | å›ç­”æ­£ç¡®æ€§çš„ä¸»è¦è¡¡é‡æ ‡å‡† |
| **Privacy Protection Rate** | å¯¹æŠ— target/prefix æ•°æ®æå–æ”»å‡»çš„èƒ½åŠ› |
| **Communication Efficiency** | æ¯æ¬¡æŸ¥è¯¢ä» silo å‘é€åˆ° server çš„å‚æ•°é‡ |
| **Storage Overhead** | silo ä¾§é¢å¤–å­˜å‚¨çš„ adapter å’Œ mask å¤§å° |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å…±å››ç±» baseline è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼š
1. **Local RAG**  
   - Standard RAG, CoTRAG, ReAct, Dargin
2. **In-context FedRAG**  
   - FRAG, MKPQA, RAGRoute
3. **Federated Fine-Tuning (FedFT)**  
   - FedIT, FLoRA
4. **Parametric RAG**  
   - PRAG

> âš ï¸ æ³¨æ„ï¼šéšç§ä¿æŠ¤ prompt å·¥ç¨‹æ–¹æ³•ï¼ˆå¦‚ DP-Prompt, Sageï¼‰å› æ€§èƒ½ä¸¥é‡ä¸‹é™æœªä½œä¸ºä¸» baselineã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰

| æ–¹æ³• | Avg. F1 æå‡ |
|------|-------------|
| **FedMosaic (Ours)** | **0.3841**ï¼ˆæœ€é«˜ï¼‰ |
| æœ€å¼º Baseline (FLoRA) | ~0.3497 |
| âœ **å¹³å‡é«˜å‡º 10.9% F1** | âœ… |

å…·ä½“è¡¨ç°äº®ç‚¹ï¼š
- åœ¨ **2WQA-Bridge** ä¸Šè¾¾åˆ° **0.4453**ï¼Œæ¯”ç¬¬äºŒåé«˜çº¦ 13%
- åœ¨ **2WQA-Compose** ä¸Šè¾¾ **0.0940**ï¼Œè¿œè¶… PRAG çš„ 0.0462
- åœ¨ **CWQ** ä¸Šè¾¾ **0.3841**ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| å¯¹æ¯”ç»´åº¦ | ç»“æœ |
|--------|------|
| **vs In-context FedRAG** | å‡†ç¡®æ€§æ›´é«˜ + å®Œå…¨æ»¡è¶³ locality constraint |
| **vs Federated Fine-Tuning** | æ— éœ€é¢‘ç¹ retrainingï¼Œæ”¯æŒæŒ‰éœ€æ¿€æ´»çŸ¥è¯† |
| **vs Parametric RAG (PRAG)** | æ›´ä½å¼€é”€ + æ›´é«˜é²æ£’æ€§ï¼ˆé¿å…ç ´åæ€§èšåˆï¼‰ |

> ğŸ” ç‰¹åˆ«æŒ‡å‡ºï¼šPRAG åœ¨æ›´å¤§æ¨¡å‹ï¼ˆLLaMA3-8Bï¼‰ä¸Šæ€§èƒ½é€€åŒ–æ˜æ˜¾ï¼Œè€Œ FedMosaic è¡¨ç°ç¨³å®šã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰èšç±»å¯¹å¼€é”€çš„å½±å“ï¼ˆFig. 4ï¼‰
- å½“æ¯ cluster åŒ…å«æœ€å¤š 10 ä¸ªæ–‡æ¡£æ—¶ï¼š
  - **å­˜å‚¨æˆæœ¬é™è‡³æ— èšç±»ç‰ˆæœ¬çš„ 11.23%**
  - **å•æ¬¡æŸ¥è¯¢é€šä¿¡æˆæœ¬é™è‡³ 4.86%**
- æ©ç æœ¬èº«ä»…å  ~1% å­˜å‚¨ç©ºé—´ï¼Œå¯å¿½ç•¥

#### ï¼ˆ2ï¼‰æ©ç çš„æœ‰æ•ˆæ€§ï¼ˆFig. 5ï¼‰
- åŠ å…¥ document-specific mask åï¼š
  - next-token loss ä¸‹é™æ›´å¿«
  - æ¨¡å‹å‡†ç¡®ç‡æŒç»­é¢†å…ˆâ€œæ—  maskâ€å˜ä½“
  - éªŒè¯äº† LoRA å‚æ•°å…·æœ‰ç¨€ç–å¯åˆ†ç¦»æ€§å‡è®¾

#### ï¼ˆ3ï¼‰é€‰æ‹©æ€§èšåˆçš„æ•ˆæœï¼ˆTable 5ï¼‰
- éšç€é€‰æ‹©çš„ top-k å¢å¤§ï¼Œâ€œæ— é€‰æ‹©â€ç‰ˆæœ¬æ€§èƒ½ä¸‹é™ï¼ˆå› å™ªå£°å¢åŠ ï¼‰
- FedMosaic åœ¨ k=5 åè¶‹äºç¨³å®šï¼Œ**Inf ç±»ä»»åŠ¡æå‡è¾¾ 20.7%**

#### ï¼ˆ4ï¼‰top-k æ£€ç´¢çš„å½±å“ï¼ˆFig. 6ï¼‰
- FedMosaic åœ¨ä¸åŒ top-k è®¾ç½®ä¸‹è¡¨ç°æ›´**ç¨³å®šä¸”ä¸€è‡´é¢†å…ˆ**
- åœ¨ HQA-Compare ä¸Šå¹³å‡ä¼˜äºæœ€å¼º baseline **10.17%**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Parametric RAG æ˜¯å®ç°éšç§å®‰å…¨ FedRAG çš„å¯è¡Œè·¯å¾„**ï¼Œä½†éœ€ä¸“é—¨è®¾è®¡ä»¥åº”å¯¹è”é‚¦ç¯å¢ƒä¸‹çš„æ•ˆç‡ä¸ç²¾åº¦æŒ‘æˆ˜ã€‚
2. âœ… **æ–‡æ¡£èšç±» + æ©ç æœºåˆ¶** èƒ½æœ‰æ•ˆå¹³è¡¡å­˜å‚¨æ•ˆç‡ä¸çŸ¥è¯†ç‰¹å¼‚æ€§ã€‚
3. âœ… **é€‰æ‹©æ€§èšåˆç­–ç•¥** æ˜¾è‘—ä¼˜äºç›²ç›®å¹³å‡ï¼Œæ˜¯æå‡å‡†ç¡®ç‡çš„å…³é”®ã€‚
4. âœ… FedMosaic åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹è§„æ¨¡ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡è‰¯å¥½**å¯æ‰©å±•æ€§**ã€‚
5. âœ… å®éªŒè¯æ˜å…¶å¯¹æ•°æ®æå–æ”»å‡»å…·æœ‰æ›´å¼ºæŠµæŠ—åŠ›ï¼Œ**éšç§æ€§ä¼˜äº in-context æ–¹æ³•**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **èšç±»è´¨é‡ä¾èµ–åµŒå…¥è¡¨ç¤º**ï¼šè‹¥åˆå§‹æ–‡æ¡£å‘é‡ä¸èƒ½å¾ˆå¥½åæ˜ è¯­ä¹‰ï¼Œåˆ™å¯èƒ½å¯¼è‡´é”™è¯¯åˆ†ç»„ã€‚
- **æ©ç å­¦ä¹ å¼•å…¥é¢å¤–è®­ç»ƒå¼€é”€**ï¼šè™½ç„¶åªè®­ç»ƒ maskï¼Œä½†ä»éœ€é¢å¤–è®¡ç®—èµ„æºã€‚
- **NP-hard çš„é€‰æ‹©é—®é¢˜**ï¼šå…¨å±€æœ€ä¼˜é€‰æ‹©ä¸å¯è¡Œï¼Œå½“å‰ä½¿ç”¨è´ªå¿ƒç®—æ³•è¿‘ä¼¼æ±‚è§£ã€‚
- **å‡è®¾åŒæ„æ¨¡å‹**ï¼šè¦æ±‚æ‰€æœ‰ silo ä½¿ç”¨ç›¸åŒ base LLM å’Œ re-rankerï¼Œå¯èƒ½é™åˆ¶å®é™…éƒ¨ç½²çµæ´»æ€§ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- è®¾è®¡æ›´é«˜æ•ˆçš„æ©ç å­¦ä¹ æœºåˆ¶ï¼ˆå¦‚è”åˆä¼˜åŒ– adapter ä¸ maskï¼‰
- æ¢ç´¢å¼‚æ„è”é‚¦ RAG åœºæ™¯ä¸‹çš„è‡ªé€‚åº”å¯¹é½æ–¹æ³•
- æ‰©å±•è‡³ streaming document æ›´æ–°åœºæ™¯ï¼Œæ”¯æŒå¢é‡å¼ adapter æ›´æ–°
- ç»“åˆ compressed adapter æŠ€æœ¯è¿›ä¸€æ­¥å‹ç¼©é€šä¿¡è´Ÿè½½
- æ¢ç´¢åœ¨çœŸå®åŒ»ç–—/é‡‘èç³»ç»Ÿä¸­çš„è½åœ°åº”ç”¨ä¸åˆè§„å®¡è®¡æ”¯æŒ

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **FedMosaic æ˜¯é¦–ä¸ªçœŸæ­£æ»¡è¶³ locality constraint çš„é«˜æ•ˆã€å‡†ç¡®ã€éšç§å®‰å…¨çš„è”é‚¦ RAG æ¡†æ¶ï¼Œé€šè¿‡ multi-document adapters + selective aggregation å®ç°äº†çŸ¥è¯†é›†æˆçš„â€œé©¬èµ›å…‹å¼æ‹¼æ¥â€ï¼Œä¸ºåˆ†å¸ƒå¼çŸ¥è¯†ç³»ç»Ÿçš„æ„å»ºæä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 13. [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)

**Authors**: Siran Liu, Guoxia Wang, Sa Wang, Jinle Zeng, HaoYang Xie, Siyu Lou, JiaBin Yang, DianHai Yu, Haifeng Wang, Chao Yang  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.05853v1  

#### Abstract
The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating que...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´ **attention æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ $O(L^2)$** é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥éƒ¨ç½²äºè¶…é•¿åºåˆ—åœºæ™¯ï¼ˆå¦‚ 128K tokensï¼‰ã€‚è™½ç„¶å·²æœ‰åŠ¨æ€ç¨€ç– attention æ–¹æ³•è¯•å›¾ç¼“è§£è¯¥é—®é¢˜ï¼Œä½†æ™®éå­˜åœ¨ä»¥ä¸‹æƒè¡¡ï¼ˆtrade-offsï¼‰ï¼š

- éœ€è¦ç¦»çº¿é¢„è®­ç»ƒæˆ–æ¨¡å¼æœç´¢ï¼ˆpreprocessingï¼‰ï¼Œé™åˆ¶éƒ¨ç½²çµæ´»æ€§ï¼›
- ç¼ºä¹å…¨å±€è¯„ä¼°èƒ½åŠ›ï¼Œæ— æ³•æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼›
- è¿å query independenceï¼Œå¯¼è‡´æ³¨æ„åŠ›åˆ†å¸ƒè¢«æ±¡æŸ“ï¼›
- ä¸åŒ attention head é—´ç­–ç•¥ä¸ä¸€è‡´ï¼Œå¢åŠ å®ç°å¤æ‚æ€§ï¼›
- Softmax ç²’åº¦ç²—ï¼Œå½±å“ç²¾åº¦ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **RRAttention**ï¼Œä¸€ç§å…¨æ–°çš„åŠ¨æ€å—ç¨€ç– attention æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Per-Head Round-Robinï¼ˆå¤´è½®è¯¢ï¼‰é‡‡æ ·ç­–ç•¥**ã€‚

#### ä¸»è¦è®¾è®¡æ€æƒ³ï¼š
- åœ¨æ¯ä¸ª stride å†…å¯¹ä¸åŒ attention head è½®æµé€‰æ‹©ä¸åŒçš„ query ä½ç½®è¿›è¡Œé‡è¦æ€§ä¼°è®¡ã€‚
- å…¬å¼åŒ–ä¸ºï¼š  
  $$
  P(i, h) = iS + (S - 1 - (h \mod S))
  $$
  å…¶ä¸­ $S$ æ˜¯ stride å¤§å°ï¼Œ$h$ æ˜¯ head indexã€‚
- æ‰€æœ‰ head åˆä½œå®Œæˆä¸€ä¸ª stride å†…æ‰€æœ‰ä½ç½®çš„è¦†ç›–ï¼Œé¿å…ä¿¡æ¯é—æ¼ã€‚

#### ä¸‰é˜¶æ®µæµç¨‹ï¼š
1. **Query Sampling with Head Round-Robin Strategy**  
   æ¯ä¸ª head åœ¨å…¶å¯¹åº”ä½ç½®é‡‡æ · queryï¼Œä¿æŒ query independenceã€‚
2. **Stride-level Importance Estimation**  
   å¯¹ key è¿›è¡Œ stride çº§èšåˆï¼Œè®¡ç®—è·¨ stride çš„é‡è¦æ€§å¾—åˆ†ï¼Œå°†å¤æ‚åº¦ä» $O(L^2)$ é™è‡³ $O(L^2/S^2)$ã€‚
3. **Block-level Selection via Top-T Thresholding**  
   å°† stride å¾—åˆ†èšåˆåˆ° block çº§åˆ«ï¼Œå¹¶ä¿ç•™ç´¯è®¡é‡è¦æ€§è¶…è¿‡é˜ˆå€¼ $T$ çš„ blocksï¼›åŒæ—¶ä¿æŠ¤æœ€åä¸€ä¸ª query block ä»¥ç»´æŒç”Ÿæˆè´¨é‡ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | RRAttention | å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ XAttentionã€FlexPrefillï¼‰ |
|------|-------------|----------------------------------------|
| **æ— éœ€é¢„å¤„ç†ï¼ˆPreprocessing-freeï¼‰** | âœ… | âŒï¼ˆéƒ¨åˆ†éœ€ç¦»çº¿è®­ç»ƒ/æ¨¡å¼åˆ†é…ï¼‰ |
| **æ”¯æŒå…¨å±€è¯„ä¼°ï¼ˆGlobal Evaluationï¼‰** | âœ… | âŒï¼ˆå¦‚ FlexPrefill åªç”¨æœ€å queryï¼‰ |
| **ä¿æŒ query ç‹¬ç«‹æ€§ï¼ˆQuery Independenceï¼‰** | âœ… | âŒï¼ˆå¦‚ XAttention è·¨ query èšåˆï¼‰ |
| **æ¨¡å¼æ— å…³ï¼ˆPattern-agnosticï¼‰** | âœ… | âŒï¼ˆå¦‚ MInference/FlexPrefill åŒºåˆ†å‚ç›´/æ–œçº¿æ¨¡å¼ï¼‰ |
| **é«˜æ•ˆ softmax ç²’åº¦ï¼ˆStride-levelï¼‰** | âœ… | âš ï¸ï¼ˆtoken-level æ›´æ…¢ï¼‰ |

> âœ… RRAttention æ˜¯ç›®å‰å”¯ä¸€åŒæ—¶æ»¡è¶³è¿™äº”ä¸ªç†æƒ³å±æ€§çš„æ–¹æ³•ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| ç±»å‹ | æ•°æ®é›† | æè¿° |
|------|-------|------|
| **è‡ªç„¶è¯­è¨€ç†è§£** | [HELMET](https://arxiv.org/abs/2502.11089) | åŒ…å« 7 å¤§ç±»ä»»åŠ¡ï¼š<br>- åˆæˆå›å¿†ï¼ˆRecallï¼‰<br>- æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰<br>- å¤šæ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰<br>- å¼•ç”¨ç”Ÿæˆï¼ˆCiteï¼‰<br>- æ–‡æ¡£é‡æ’åºï¼ˆRerankï¼‰<br>- é•¿æ–‡æ¡£é—®ç­”ï¼ˆLongQAï¼‰<br>- æ‘˜è¦ï¼ˆSummarizationï¼‰ |
| **å¤šæ¨¡æ€è§†é¢‘ç†è§£** | [Video-MME](https://arxiv.org/abs/2405.21075) | åŒ…å« 900 ä¸ªè§†é¢‘ã€2700 æ¡å¤šé€‰é¢˜ï¼Œæ¶µç›–æ„ŸçŸ¥ã€æ¨ç†ã€ä¿¡æ¯æ•´åˆç­‰ 12 ç§ä»»åŠ¡ï¼Œæµ‹è¯•æ¨¡å‹å¯¹é•¿æ—¶é—´è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚ |

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **æ¨¡å‹**ï¼š
  - `Meta-LLaMA-3.1-8B-Instruct`ï¼ˆæ”¯æŒ 128Kï¼‰
  - `Qwen2.5-7B-Instruct`ï¼ˆåŸºäº YARN æ‰©å±•è‡³ 128Kï¼‰
  - `Qwen2-VL-7B-Instruct`ï¼ˆç”¨äº Video-MMEï¼‰
  - è¡¥å……å®éªŒè¿˜ç”¨äº† `Yi-9B-200K` å’Œ `Qwen3-30B-A3B`

- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š8K â†’ 128K tokens

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - å¹³å‡å‡†ç¡®ç‡ï¼ˆAvg. Scoreï¼‰
  - ç¨€ç–åº¦ï¼ˆSparsityï¼‰ï¼šè·³è¿‡çš„ attention block æ¯”ä¾‹
  - æ¨ç†é€Ÿåº¦ï¼ˆFPS / Timeï¼‰
  - æ¨¡å¼æœç´¢å¼€é”€ï¼ˆPattern Search Overheadï¼‰

- **ç¨€ç–é…ç½®**ï¼š
  - ä¿å®ˆè®¾ç½®ï¼š$T=0.95$, $\gamma=0.99$
  - æ¿€è¿›è®¾ç½®ï¼š$T=0.90$, $\gamma=0.95$

- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H100 GPUs

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | ç‰¹ç‚¹ |
|------|------|
| **FlashAttention** | å¯†é›† attention åŸºçº¿ï¼Œè¡¡é‡åŸå§‹æ€§èƒ½ä¸Šé™ |
| **FlexPrefill** | ä½¿ç”¨æœ€åä¸€ä¸ª query å‘ç° vertical/slash æ¨¡å¼ï¼Œä¾èµ– JS æ•£åº¦åˆ¤æ–­å¯é æ€§ |
| **XAttention** | ä½¿ç”¨ anti-diagonal é‡‡æ · + stride èšåˆï¼Œé€Ÿåº¦å¿«ä½†è¿å query independence |
| **RRAttention (Ours)** | æœ¬æ–‡æå‡ºæ–¹æ³•ï¼Œhead-round-robin + stride-level aggregation |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### ğŸ“Š åœ¨ HELMET ä¸Šçš„æ•´ä½“è¡¨ç°ï¼ˆ128K contextï¼‰

| æ–¹æ³• | æ¨¡å‹ | Avg. Score | Sparsity | ç›¸å¯¹äº Full Attention çš„æ¢å¤ç‡ |
|------|------|------------|----------|-------------------------------|
| FullAttention | Llama | 49.74 | 0% | 100% |
| FlexPrefill ($\gamma=0.99$) | Llama | 49.87 | 50.54% | ~100.3% |
| XAttention ($T=0.95$) | Llama | 49.45 | 66.22% | 99.4% |
| **RRAttention ($T=0.95$)** | **Llama** | **50.37** | **66.02%** | **>100% (è¾¾ 101.3%)** |

> ğŸ”¥ **RRAttention åœ¨ Llama ä¸Šæ¢å¤äº†è¶…è¿‡ 100% çš„ Full Attention æ€§èƒ½ï¼ŒåŒæ—¶ä»…è®¡ç®—çº¦ä¸€åŠçš„ attention blocksï¼**

#### ğŸš€ æ¨ç†æ•ˆç‡æå‡

- åœ¨ 128K context ä¸‹ï¼Œ**RRAttention å®ç° 2.4Ã— ç«¯åˆ°ç«¯åŠ é€Ÿ**ã€‚
- **æ¨¡å¼æœç´¢æ—¶é—´å‡å°‘ 18.2%** ç›¸æ¯” XAttentionï¼ˆè§ Figure 3bï¼‰ï¼Œå¾—ç›Šäºæ›´é«˜æ•ˆçš„ head-round-robin é‡‡æ ·ä¸ stride-level aggregationã€‚

#### ğŸ¯ å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆVideo-MMEï¼‰è¡¨ç°

| è®¾ç½® | æ–¹æ³• | Avg. Score (Long Videos) | Sparsity |
|------|------|-------------------------|----------|
| 1fps | FullAttention | 55.20 | 0% |
| 1fps | XAttention ($T=0.95$) | 56.10 | 37.50% |
| 1fps | **RRAttention ($T=0.95$)** | **56.20** | **34.70%** |

> âœ… RRAttention åœ¨è§†é¢‘ç†è§£ä¸­ä¹Ÿå–å¾—æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä¸­é•¿è§†é¢‘ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼Œè¯´æ˜å…¶å…¨å±€è¯„ä¼°èƒ½åŠ›å¯¹æ—¶ç©ºå»ºæ¨¡è‡³å…³é‡è¦ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

| å¯¹æ¯”ç»´åº¦ | RRAttention vs Baselines |
|---------|---------------------------|
| **å‡†ç¡®æ€§** | åœ¨æ‰€æœ‰ context é•¿åº¦ä¸‹å‡ä¼˜äº FlexPrefill å’Œ XAttentionï¼Œå¹³å‡é«˜å‡º 0.5â€“1.5 åˆ† |
| **ç¨€ç–æ€§-ç²¾åº¦æƒè¡¡** | åœ¨ç›¸åŒç¨€ç–åº¦ä¸‹ç²¾åº¦æ›´é«˜ï¼Œæˆ–åœ¨ç›¸åŒç²¾åº¦ä¸‹å®ç°æ›´é«˜ç¨€ç–åº¦ |
| **æ³›åŒ–æ€§** | åœ¨ Llamaã€Qwenã€Yiã€Qwen3 å››å¤§æ¶æ„ä¸Šå‡è¡¨ç°æœ€ä¼˜ï¼ŒéªŒè¯é€šç”¨æ€§ |
| **ç»†ç²’åº¦ä»»åŠ¡è¡¨ç°** | åœ¨ Recallã€LongQAã€Rerank ç­‰éœ€è¦å…¨å±€ç†è§£çš„ä»»åŠ¡ä¸Šæ˜¾è‘—é¢†å…ˆ |

> ğŸ’¡ ä¾‹å¦‚åœ¨ Qwen-128K ä¸Šï¼ŒRRAttention è¾¾åˆ° 38.51 åˆ†ï¼Œè€Œ FlexPrefill ä»…ä¸º 35.52 åˆ†ï¼ˆç›¸å·® +3.0 åˆ†ï¼‰ï¼Œä¸”ç¨€ç–åº¦æ›´é«˜ï¼ˆ60.97% vs 48.20%ï¼‰ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### âœ… æœ€å query block ä¿æŠ¤æœºåˆ¶ï¼ˆLast Q Block Protectionï¼‰

- åº”ç”¨äº XAttention åï¼Œæ€§èƒ½ä» 55.74 æå‡è‡³ 55.92ï¼ˆ+0.18ï¼‰ï¼Œä½† RRAttention æœ¬èº«å·²è¾¾ 56.24ã€‚
- ç»“è®ºï¼š**pattern discovery æ¯” protection æ›´å…³é”®**ã€‚

#### âœ… ä¸åŒ RR ç­–ç•¥æ¯”è¾ƒï¼ˆHead-RR vs Layer-RR vs Hybrid-RRï¼‰

| æ–¹æ³• | Avg. Score |
|------|------------|
| w/o RR | 55.65 |
| **Head-RR** | **55.80** âœ… |
| Layer-RR | 55.54 |
| Hybrid-RR | 55.61 |

> âœ… **Head-level RR æ•ˆæœæœ€å¥½**ï¼Œå› å…¶ç¡®ä¿æ¯ä¸ª stride å†…çš„ä½ç½®éƒ½èƒ½è¢«å……åˆ†é‡‡æ ·ã€‚

#### âœ… Stride å¤§å°å½±å“ï¼ˆS=4,8,16,32ï¼‰

- å½“ $S \leq 16$ æ—¶æ€§èƒ½ç¨³å®šï¼›
- $S=32$ æ—¶å› èšåˆè¿‡ç²—å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
- æ¨èä½¿ç”¨ $S=8$ æˆ– $S=16$ï¼Œå…¼é¡¾æ•ˆç‡ä¸ç²¾åº¦ã€‚

#### âœ… Block Selection å‡†ç¡®æ€§åˆ†æï¼ˆAppendix Dï¼‰

| æ–¹æ³• | Average Precision â†‘ | Recall | F1 Score â†‘ |
|------|---------------------|--------|------------|
| XAttention | 12.48% | 93.35% | 26.81 |
| **RRAttention** | **13.05%** (+0.57%) | 93.05% | **27.58** (+0.77) |

> âœ… RRAttention å…·æœ‰æ›´é«˜çš„ **precision** å’Œ **F1**ï¼Œè¡¨æ˜å…¶ block é€‰æ‹©æ›´ç²¾å‡†ï¼Œè¯¯æŠ¥æ›´å°‘ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **RRAttention æ˜¯é¦–ä¸ªåŒæ—¶æ»¡è¶³äº”å¤§ç†æƒ³å±æ€§çš„åŠ¨æ€ç¨€ç– attention æ–¹æ³•**ï¼š
   - Preprocessing-free
   - Global Evaluation
   - Query Independence
   - Pattern-agnostic
   - Stride-level Softmax

2. **é€šè¿‡ head-round-robin é‡‡æ ·å®ç°äº†â€œå®Œå…¨ä½ç½®è¦†ç›–â€ä¸â€œquery ç‹¬ç«‹æ€§â€çš„ç»Ÿä¸€**ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„ä¿¡æ¯ä¸¢å¤±ä¸å¹²æ‰°é—®é¢˜ã€‚

3. **åœ¨å¤šç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå‡æ¢å¤ >99% ç”šè‡³è¶…è¿‡ 100% çš„ Full Attention æ€§èƒ½**ï¼Œè¯æ˜å…¶ä¸ä»…èƒ½é€¼è¿‘åŸæ€§èƒ½ï¼Œè¿˜èƒ½èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ï¼Œè¿‡æ»¤å™ªå£°ã€‚

4. **åœ¨ 128K context ä¸‹å®ç° 2.4Ã— åŠ é€Ÿï¼Œä¸”æ¨¡å¼æœç´¢å¼€é”€æ›´ä½**ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚

5. **åœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸­è¡¨ç°ä¼˜å¼‚**ï¼Œè¯´æ˜å…¶å¯¹å¤æ‚æ—¶ç©ºä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼ºã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **æç«¯ stride é…ç½®ä¸‹çš„è¾¹ç•Œé—®é¢˜**ï¼šå½“ stride å¤§å° $S$ è¶…è¿‡ attention head æ•°é‡æ—¶ï¼Œæ— æ³•ä¿è¯æ¯ä¸ªä½ç½®éƒ½è¢«é‡‡æ ·ï¼Œå¯èƒ½å¯¼è‡´é‡è¦ä¿¡æ¯é—æ¼ã€‚
- **å½“å‰ä»…åº”ç”¨äº prefill é˜¶æ®µ**ï¼Œæœªæ‰©å±•è‡³ decoding é˜¶æ®µï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚
- **ä»æœ‰ä¸€å®šè¿è¡Œæ—¶å¼€é”€ç”¨äº pattern discovery**ï¼Œè™½å·²å¾ˆä½ï¼Œä½†ä¸å¦‚é™æ€æ–¹æ³•é›¶æˆæœ¬ã€‚

> âš ï¸ ä½†ä½œè€…æŒ‡å‡ºï¼šè¿™äº›æé™æƒ…å†µåœ¨å®è·µä¸­å¾ˆå°‘å‡ºç°ï¼Œæ¨èçš„ $S=8$ æˆ– $16$ å®Œå…¨å¯é¿å…æ­¤é—®é¢˜ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **å·¥ç¨‹ä¼˜åŒ–**ï¼š
   - å‡çº§è‡³ FlashAttention-3ï¼Œåˆ©ç”¨ warp specialization å’Œæ›´å¥½å†…å­˜è°ƒåº¦è¿›ä¸€æ­¥æé€Ÿã€‚

2. **è®­ç»ƒæ„ŸçŸ¥ç¨€ç–ï¼ˆTraining-aware Sparse Attentionï¼‰**ï¼š
   - åœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥ç¨€ç–ç›‘ç£ï¼Œè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ç¨€ç–æ¨¡å¼ï¼Œæ¶ˆé™¤æ¨ç†æ—¶ pattern search å¼€é”€ã€‚

3. **æ‰©å±•è‡³ decoding é˜¶æ®µ**ï¼š
   - å°† RR æ€æƒ³ç”¨äº KV Cache å‹ç¼©ï¼Œé™ä½æ¯ token çš„å»¶è¿Ÿå’Œæ˜¾å­˜å ç”¨ã€‚

4. **ç»“åˆå…¶ä»–åŠ é€ŸæŠ€æœ¯**ï¼š
   - ä¸ PagedAttentionã€KV Cache Quantization ç­‰æ­£äº¤æŠ€æœ¯è”åˆä½¿ç”¨ï¼Œæ„å»ºå…¨æ ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†å¼•æ“ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **RRAttention é€šè¿‡å·§å¦™çš„ head-round-robin é‡‡æ ·ç­–ç•¥ï¼Œåœ¨ä¸ç‰ºç‰²ä»»ä½•ç†è®ºæ€§è´¨çš„å‰æä¸‹ï¼Œå®ç°äº†å½“å‰æœ€å…ˆè¿›çš„åŠ¨æ€ç¨€ç– attention æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡ï¼Œæ˜¯è¿ˆå‘å®ç”¨åŒ–è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„é‡è¦ä¸€æ­¥ã€‚**

</details>

---

### 14. [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)

**Authors**: Lizhuo Luo, Shenggui Li, Yonggang Wen, Tianwei Zhang  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.05992v1  

#### Abstract
Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. How...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDSB: Dynamic Sliding Block Scheduling for Diffusion LLMs

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰åœ¨ **diffusion large language models (dLLMs)** ä¸­å¹¿æ³›ä½¿ç”¨çš„ **naive block-diffusion** æ¨ç†ç­–ç•¥å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š

- **å›ºå®šå—è°ƒåº¦ï¼ˆfixed, predefined block scheduleï¼‰** å¿½ç•¥äº†è¯­ä¹‰éš¾åº¦å’Œä¸Šä¸‹æ–‡åŠ¨æ€å˜åŒ–ã€‚
- å¼ºåˆ¶åœ¨ä½ç½®ä¿¡åº¦ä½ç½®æå‰è§£ç ï¼ˆpremature commitmentï¼‰ï¼Œå¯¼è‡´é”™è¯¯ä¼ æ’­ã€‚
- é«˜ç½®ä¿¡åº¦ä½†ä½äºå—è¾¹ç•Œå¤–çš„ä½ç½®è¢«å»¶è¿Ÿè§£ç ï¼Œé™ä½å¹¶è¡Œæ•ˆç‡ã€‚

è¿™ç§â€œä¸€åˆ€åˆ‡â€çš„å—åˆ’åˆ†æ–¹å¼é€ æˆäº† **generation quality** å’Œ **inference efficiency** ä¹‹é—´çš„æ¬¡ä¼˜æƒè¡¡ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯

ä½œè€…æå‡º **Dynamic Sliding Block (DSB)** â€”â€”ä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨æ€å—è°ƒåº¦æœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- ç»´æŠ¤ä¸€ä¸ª**æ»‘åŠ¨ä¸”å¤§å°å¯å˜çš„æ´»åŠ¨å—ï¼ˆactive blockï¼‰**ï¼Œè€Œéå›ºå®šçš„é™æ€å—ã€‚
- åœ¨æ¯ä¸€æ­¥è¿­ä»£ä¸­ï¼š
  - æ ¹æ®å½“å‰å·²è§£ç çŠ¶æ€åŠ¨æ€è°ƒæ•´å—çš„èµ·å§‹ä½ç½®ï¼ˆå·¦è¾¹ç•Œï¼‰ã€‚
  - åŠ¨æ€æ‰©å±•å³è¾¹ç•Œä»¥ä¿æŒè‡³å°‘ $ S_{\text{init}} $ ä¸ªæœªè§£ç  tokenï¼Œä¸Šé™ä¸º $ S_{\text{max}} $ã€‚
- å®ç°æ›´çµæ´»çš„ semi-autoregressive è§£ç ï¼šæ—¢ä¿ç•™å› æœæ€§ï¼Œåˆæå‡å±€éƒ¨å¹¶è¡Œæ€§ã€‚

æ­¤å¤–ï¼Œé’ˆå¯¹ DSB å¼•å…¥çš„ **KV-cache ä¸ç¨³å®šæ€§é—®é¢˜**ï¼Œæå‡ºäº†ä¸“ç”¨ç¼“å­˜æœºåˆ¶ï¼š

> **DSB Cache**ï¼šå¼•å…¥ä¸€ä¸ªä½äºæ´»åŠ¨å—å‰çš„ **prefix window**ï¼Œè¯¥çª—å£ä¸æ´»åŠ¨å—ä¸€èµ·åœ¨æ¯æ­¥åˆ·æ–° KV çŠ¶æ€ï¼Œå¹¶å‘¨æœŸæ€§æ‰§è¡Œå…¨å±€åˆ·æ–°ï¼Œä»è€Œç¨³å®šç¼“å­˜ã€é¿å…é¢‘ç¹å¤±æ•ˆã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | Naive Block | DSB |
|------|------------|-----|
| è°ƒåº¦çµæ´»æ€§ | âŒ å›ºå®šå—å¤§å°ä¸é¡ºåº | âœ… åŠ¨æ€æ»‘åŠ¨ + è‡ªé€‚åº”å°ºå¯¸ |
| è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ› | âŒ å®Œå…¨å¿½ç•¥ç½®ä¿¡åº¦ | âœ… å»¶è¿Ÿä½ç½®ä¿¡è§£ç ï¼Œä¼˜å…ˆé«˜ç½®ä¿¡è¾“å‡º |
| å¹¶è¡Œæ•ˆç‡ | âš ï¸ è¾¹ç•Œå¤„æµªè´¹å¹¶è¡Œæœºä¼š | âœ… æ›´æ—©é‡Šæ”¾æ˜“è§£ token |
| KV ç¼“å­˜å…¼å®¹æ€§ | âœ… æ”¯æŒ Dual/Prefx Cache | âœ… ä¸“ä¸ºæ»‘åŠ¨è®¾è®¡ï¼Œé¿å…çŠ¶æ€éœ‡è¡ |
| æ˜¯å¦éœ€è¦è®­ç»ƒ | âœ… æ˜¯ï¼ˆå¦‚ WeDLMï¼‰ | âœ… å¦ï¼ˆtraining-freeï¼‰ |

> âœ… **DSB æ˜¯é¦–ä¸ªå®Œå…¨ training-free çš„åŠ¨æ€æ»‘åŠ¨å—è°ƒåº¦æ–¹æ¡ˆ**ï¼Œæ˜¾è‘—ä¼˜äºå›ºå®šå—ç­–ç•¥ï¼ŒåŒæ—¶é¿å…äº†å¤æ‚è®­ç»ƒå¼€é”€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

æ¶µç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼Œå…± **5 ä¸ªåŸºå‡†æµ‹è¯•é›†**ï¼š

| æ•°æ®é›† | ç±»å‹ | ç¤ºä¾‹ä»»åŠ¡ |
|--------|------|----------|
| **GSM8K** (5-shot) | æ•°å­¦æ¨ç† | è§£æ•°å­¦æ–‡å­—é¢˜ |
| **MATH** (4-shot) | å¤æ‚æ•°å­¦ | é«˜ä¸­ç«èµ›çº§é¢˜ç›® |
| **HumanEval** (0-shot) | ä»£ç ç”Ÿæˆ | Python å‡½æ•°è¡¥å…¨ |
| **MBPP** (3-shot) | ç¼–ç¨‹ä»»åŠ¡ | å°è§„æ¨¡ç¼–ç¨‹é—®é¢˜ |
| **BBH** (3-shot) | ç»¼åˆæ¨ç† | Big-Bench Hard å­é›† |

---

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹
- **LLaDA-8B-Instruct**, **LLaDA-1.5**
- **Dream-v0-Base-7B**, **Dream-v0-Instruct-7B**

#### ç¡¬ä»¶
- å•å¼  **NVIDIA H200 140G GPU**

#### å‚æ•°é…ç½®
- ç”Ÿæˆé•¿åº¦ï¼š256
- åˆå§‹å—å¤§å° $ S_{\text{init}} $ï¼š32
- æœ€å¤§å—å¤§å° $ S_{\text{max}} $ï¼š32ï¼ˆDSB const.ï¼‰æˆ–æ— é™åˆ¶ï¼ˆDSB greedyï¼‰
- æœ€å° prefix window é•¿åº¦ $ l_{\text{pmin}} $ï¼š24ï¼ˆLLaDAï¼‰ï¼Œ4ï¼ˆDreamï¼‰
- å¹¶è¡Œè§£ç ç½®ä¿¡é˜ˆå€¼ï¼š0.9

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Accuracy (%)** | è¡¡é‡ç”Ÿæˆè´¨é‡ |
| **TPS (Tokens Per Second)** | è¡¡é‡æ¨ç†ååé‡ï¼Œåæ˜ æ•ˆç‡ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

ä»ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ¯”è¾ƒï¼š

| ç»´åº¦ | åŸºçº¿æ–¹æ³• |
|------|---------|
| **Decoding Strategy** | - Vanilla Top-1 Sampling<br>- Confidence-aware Parallel Decoding (Fast-dLLM) |
| **Block Scheduling** | - Naive Block Schedulingï¼ˆå›ºå®šå—ï¼‰ |
| **KV Caching** | - Dual Cacheï¼ˆç¼“å­˜éæ´»åŠ¨å—ï¼‰ |

> æ‰€æœ‰å¯¹æ¯”å‡åœ¨åŒä¸€æ¡†æ¶ä¸‹å®ç°ï¼Œç¡®ä¿å…¬å¹³æ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

#### åœ¨ **LLaDA-8B-Instruct + GSM8K** ä¸Šçš„è¡¨ç°ï¼š

| æ–¹æ³• | Accuracy â†‘ | TPS â†‘ |
|------|-----------|-------|
| Vanilla (no cache) | 77.79 | 14.94 |
| Naive Block + Dual Cache | 77.40 | 92.26 |
| **DSB (const.) + DSB Cache** | **80.14** | **98.10** |
| **DSB (greedy) + DSB Cache** | **80.29** | **99.61** |

âœ… **å‡†ç¡®ç‡æå‡çº¦ 2.9%ï¼Œååæå‡ >7%**

#### åœ¨ **Dream-v0-Instruct-7B + GSM8K** ä¸Šï¼š

| æ–¹æ³• | Accuracy | TPS |
|------|---------|-----|
| Naive Block + Dual Cache | 67.32 | 72.18 |
| **DSB (greedy) + DSB Cache** | **73.08** | **75.27** |

âœ… **å‡†ç¡®ç‡å¤§å¹…æå‡è¿‘ 6%ï¼ŒåŒæ—¶ç»´æŒæ›´é«˜åå**

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„æ•´ä½“å¯¹æ¯”ç»“è®º

- åœ¨å‡ ä¹æ‰€æœ‰æ¨¡å‹å’Œ benchmark ä¸Šï¼Œ**DSB + DSB Cache** å‡å®ç°äº†ï¼š
  - **æ›´é«˜çš„ Accuracy**
  - **æ›´é«˜çš„ TPS**
- ç‰¹åˆ«æ˜¯åœ¨ç»“åˆ KV Cache åï¼Œä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚
- å³ä½¿åœ¨ Dream ç³»åˆ—ä¸Šå›  AR åˆå§‹åŒ–è¡¨ç°æ³¢åŠ¨ï¼Œä»èƒ½åœ¨å¤šä¸ªåœºæ™¯ä¸‹å–å¾—å¢ç›Šã€‚

> ğŸ’¡ **DSB å®ç°äº† generation quality ä¸ inference speed çš„åŒé‡æå‡ï¼Œçªç ´ä¼ ç»Ÿ quality-speed trade-offã€‚**

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰DSB Cache ä¸­ prefix window çš„ä½œç”¨ï¼ˆTable 2ï¼‰

| æ–¹æ³• | GSM8K Acc / TPS |
|------|------------------|
| DSB (const.) + Dual Cacheï¼ˆæ—  prefix windowï¼‰ | 76.42 / 78.93 |
| **DSB (const.) + DSB Cacheï¼ˆå« prefix windowï¼‰** | **80.14 / 98.10** |

â¡ï¸ ç§»é™¤ prefix window å¯¼è‡´ï¼š
- **Accuracy â†“ 3.7 pts**
- **TPS â†“ 19.17**

> âœ… è¯æ˜ prefix window å¯¹ç¨³å®š KV-cache è‡³å…³é‡è¦ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒ $ S_{\text{init}} $ çš„å½±å“ï¼ˆFigure 4ï¼‰

- DSB å¯¹åˆå§‹å—é•¿åº¦é²æ£’æ€§å¼ºã€‚
- å½“ $ S_{\text{init}} = 64 $ æ—¶ï¼Œnaive block æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œè€Œ DSB ä»ä¿æŒç¨³å®šã€‚

#### ï¼ˆ3ï¼‰ä¸åŒç”Ÿæˆé•¿åº¦ $ L $ çš„è¡¨ç°ï¼ˆFigure 5ï¼‰

- éšç€ $ L $ å¢åŠ ï¼ŒDSB ä¾ç„¶ä¿æŒå¯¹ vanilla sampler çš„è´¨é‡å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚
- æ˜¾ç¤ºå…¶åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚

#### ï¼ˆ4ï¼‰$ S_{\text{max}} $ ä¸ $ l_{\text{pmin}} $ æ•æ„Ÿæ€§åˆ†æï¼ˆFigures 6 & 7ï¼‰

- $ S_{\text{max}} $ è¿‡å¤§ä¼šå‰Šå¼±å› æœçº¦æŸï¼Œç•¥å¾®ç‰ºç‰² accuracy æ¢å– TPSã€‚
- $ l_{\text{pmin}} $ å­˜åœ¨æœ€ä¼˜å€¼ï¼ˆå¦‚ 24 for LLaDAï¼‰ï¼Œè¿‡å¤§åè€Œé™ä½æ•ˆç‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **å›ºå®šå—è°ƒåº¦æ˜¯ç“¶é¢ˆ**ï¼šnaive block å¿½è§†è¯­ä¹‰éš¾åº¦ï¼Œé€ æˆè´¨é‡ä¸æ•ˆç‡åŒæŸã€‚
2. **DSB æ˜¾è‘—æ”¹å–„ semi-autoregressive æ¨ç†**ï¼š
   - åŠ¨æ€æ»‘åŠ¨å—èƒ½è‡ªé€‚åº”ä¸Šä¸‹æ–‡æ¼”åŒ–ã€‚
   - å»¶è¿Ÿä¸ç¡®å®š tokenï¼Œæå‰é‡Šæ”¾é«˜ç½®ä¿¡ tokenã€‚
3. **DSB Cache è§£å†³æ»‘åŠ¨å¸¦æ¥çš„ KV ä¸ç¨³å®šé—®é¢˜**ï¼š
   - prefix window + å‘¨æœŸåˆ·æ–°æœºåˆ¶æœ‰æ•ˆç»´æŒç¼“å­˜ä¸€è‡´æ€§ã€‚
4. **training-free è®¾è®¡æ›´å…·å®ç”¨æ€§**ï¼š
   - æ— éœ€é¢å¤–è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œé€‚ç”¨äºå„ç±» dLLM æ¶æ„ã€‚

> ğŸ¯ **DSB å°† dLLM æ¨ç†æ¨å‘æ–°çš„ quality-speed frontierã€‚**

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ–ç½®ä¿¡åº¦ä¼°è®¡**ï¼šæ€§èƒ½å—é™äºæ¨¡å‹è‡ªèº« confidence calibration èƒ½åŠ›ã€‚
2. **æç«¯é•¿æ–‡æœ¬å°šæœªéªŒè¯**ï¼šç›®å‰å®éªŒé›†ä¸­åœ¨ ~256 é•¿åº¦ï¼Œè¶…é•¿æ–‡æœ¬æ•ˆæœå¾…æ¢ç´¢ã€‚
3. **suffix window å°è¯•å¤±è´¥**ï¼ˆAppendix Aï¼‰ï¼š
   - å°è¯•æ·»åŠ åç¼€çª—å£æœªèƒ½å¸¦æ¥ä¸€è‡´æ”¶ç›Šï¼Œè¯´æ˜å‰å‘ä¸Šä¸‹æ–‡æ›´é‡è¦ã€‚
4. **å¯¹æŸäº›æ¶æ„å¢ç›Šæœ‰é™**ï¼šå¦‚ Dream ç³»åˆ—å›  AR åˆå§‹åŒ–å¯¼è‡´éƒ¨åˆ†åœºæ™¯å¢ç›Šä¸æ˜¾è‘—ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **å°† DSB æ€æƒ³èå…¥é¢„è®­ç»ƒæˆ–åè®­ç»ƒé˜¶æ®µ**ï¼š
   - è®­ç»ƒæ—¶æ¨¡æ‹ŸåŠ¨æ€å—æ©ç ï¼Œè¿›ä¸€æ­¥å¯¹é½è®­ç»ƒä¸æ¨ç†ã€‚
2. **ç»“åˆ early stopping æˆ– adaptive termination**ï¼š
   - åŠ¨æ€å†³å®šä½•æ—¶åœæ­¢ denoisingï¼Œè¿›ä¸€æ­¥æé€Ÿã€‚
3. **æ¢ç´¢æ›´æ™ºèƒ½çš„ block size æ§åˆ¶ç­–ç•¥**ï¼š
   - åŸºäºè¯­ä¹‰å•å…ƒï¼ˆå¥å­ã€çŸ­è¯­ï¼‰è‡ªåŠ¨åˆ’åˆ†å—å¤§å°ã€‚
4. **æ‰©å±•åˆ°å¤šæ¨¡æ€ diffusion æ¨¡å‹**ï¼š
   - å¦‚å›¾åƒ-æ–‡æœ¬è”åˆç”Ÿæˆä¸­åº”ç”¨åŠ¨æ€å—è°ƒåº¦ã€‚

---

> ğŸ”— **å¼€æºåœ°å€**ï¼š[https://github.com/lizhuo-luo/DSB](https://github.com/lizhuo-luo/DSB)  
> ğŸ“„ **è®ºæ–‡ç‰ˆæœ¬**ï¼šPreprint, February 6, 2026

</details>

---

### 15. [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)

**Authors**: Junhan Kim, Yeo Jeong Park, Seungwoo Son, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.04929v1  

#### Abstract
The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hour...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´é«˜å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬çš„æŒ‘æˆ˜ï¼Œ**Post-Training Quantization (PTQ)** æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰ä¸»æµæ–¹æ³•å­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š

- **GPTQ** è™½ç„¶é«˜æ•ˆï¼Œä½†å‡è®¾å±‚é—´ç‹¬ç«‹ï¼Œå¿½ç•¥äº†æ³¨æ„åŠ›æ¨¡å—ä¸­çš„è·¨å±‚ä¾èµ–å…³ç³»ï¼Œåœ¨ä½æ¯”ç‰¹ï¼ˆå¦‚ INT2ï¼‰é‡åŒ–ä¸‹ç²¾åº¦ä¸¥é‡ä¸‹é™ã€‚
- **BoA** é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æ„ŸçŸ¥çš„ Hessian è¿‘ä¼¼ï¼Œå»ºæ¨¡äº†è·¨å±‚ä¾èµ–ï¼Œæ˜¾è‘—æå‡äº†ç²¾åº¦ï¼Œä½†å…¶å¿…é¡»å¯¹è¾“å‡ºé€šé“ï¼ˆout-channelsï¼‰è¿›è¡Œ**é€ä¸ªä¸²è¡Œé‡åŒ–**ï¼Œå¯¼è‡´æ•ˆç‡è¿œä½äº GPTQã€‚

å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³ **â€œå¦‚ä½•åœ¨ä¿æŒç”šè‡³æå‡ BoA é«˜ç²¾åº¦çš„åŒæ—¶ï¼Œå¤§å¹…åŠ é€Ÿå…¶é‡åŒ–è¿‡ç¨‹â€** çš„æ ¸å¿ƒçŸ›ç›¾ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **TURBOBOA**ï¼Œä¸€ç§æ— éœ€åå‘ä¼ æ’­çš„ PTQ ç®—æ³•ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯å®ç°æ•ˆç‡ä¸ç²¾åº¦çš„åŒé‡çªç ´ï¼š

#### (i) **å¤šé€šé“è”åˆé‡åŒ–ï¼ˆJoint Quantization of Multiple Out-Channelsï¼‰**
- **æ ¸å¿ƒæ€æƒ³**ï¼šä¸å†é€ä¸ªé‡åŒ– out-channelsï¼Œè€Œæ˜¯**åŒæ—¶é‡åŒ– N ä¸ªé€šé“**ï¼Œå°†ä¸²è¡Œæ“ä½œè½¬æ¢ä¸ºå¹¶è¡Œå¤„ç†ï¼Œä»æ ¹æœ¬ä¸Šå‡å°‘è¿­ä»£æ¬¡æ•°ã€‚
- **å…³é”®æŠ€æœ¯**ï¼šæå‡ºä¸€ä¸ª**é—­å¼è¯¯å·®è¡¥å¿è§„åˆ™**ï¼ˆclosed-form error compensation ruleï¼‰ï¼Œåœ¨è”åˆé‡åŒ–åï¼Œæ˜¾å¼åœ°å°†è¿™äº›é€šé“é—´çš„ä¾èµ–å…³ç³»çº³å…¥è¯¯å·®è¡¥å¿ä¸­ï¼Œç¡®ä¿ç²¾åº¦ä¸å› å¹¶è¡ŒåŒ–è€ŒæŸå¤±ã€‚
- **æ•ˆæœ**ï¼šç›¸æ¯” BoA çš„å®Œå…¨ä¸²è¡Œï¼Œè¯¥ç­–ç•¥å®ç°äº†è¶…è¿‡ **3å€çš„é€Ÿåº¦æå‡**ã€‚

#### (ii) **å‰åºé‡åŒ–å±‚è¯¯å·®è¡¥å¿ï¼ˆError Compensation for Pre-Quantized Layersï¼‰**
- **é—®é¢˜**ï¼šBoA å¿½ç•¥äº†æ¥è‡ªå…ˆå‰å·²é‡åŒ–å±‚çš„è¯¯å·®ä¼ æ’­ï¼Œè¿™äº›è¯¯å·®ä¼šæ‰°åŠ¨å½“å‰å±‚çš„è¾“å…¥åˆ†å¸ƒï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯ã€‚
- **è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨è¯¯å·®è¡¥å¿ç›®æ ‡å‡½æ•°ä¸­ï¼Œæ˜¾å¼åŠ å…¥ç”±è¾“å…¥åå·® $ \Delta X $ å¼•èµ·çš„é¢å¤–å¤±çœŸé¡¹ $ GW\Delta X $ï¼Œä½¿é‡åŒ–æ¨¡å‹èƒ½æ›´å¿ å®åœ°å¤ç°å…¨ç²¾åº¦ï¼ˆFPï¼‰æ¨¡å‹çš„è¡Œä¸ºã€‚
- **æŠ€æœ¯åŒºåˆ«**ï¼šä¸åŒäº GPTAQ å‡è®¾ $ H_{out} = I $ï¼ˆå¿½ç•¥é€šé“é—´ç›¸å…³æ€§ï¼‰ï¼ŒTURBOBOA åœ¨ä¸€èˆ¬ä¸”å¯èƒ½ç¨ å¯†çš„ $ H_{out} $ ä¸‹æ¨å¯¼å‡ºæ›´æ–°è§„åˆ™ï¼Œä¿ç•™äº†æ³¨æ„åŠ›æ„ŸçŸ¥çš„é€šé“ä¾èµ–ã€‚

#### (iii) **è‡ªé€‚åº”ç½‘æ ¼é€‰æ‹©ä¸åæ ‡ä¸‹é™ç²¾ç‚¼ï¼ˆAdaptive Grid Selection with CD-based Refinementï¼‰**
- **é—®é¢˜**ï¼šBoA ä½¿ç”¨å›ºå®šçš„é‡åŒ–ç½‘æ ¼ï¼Œä½†åœ¨è¿­ä»£è¿‡ç¨‹ä¸­æƒé‡è¢«æŒç»­æ›´æ–°ï¼Œå¯¼è‡´åˆå§‹ç½‘æ ¼ä¸å®é™…æƒé‡åˆ†å¸ƒé”™ä½ï¼Œå°¤å…¶åœ¨ä½æ¯”ç‰¹ä¸‹å½±å“æ˜¾è‘—ã€‚
- **è§£å†³æ–¹æ¡ˆ**ï¼š
  1. **åŠ¨æ€ç½‘æ ¼è®¡ç®—**ï¼šåœ¨æ¯æ¬¡é‡åŒ–å‰ï¼ŒåŸºäº**æœ€æ–°æ›´æ–°çš„æƒé‡**é‡æ–°è®¡ç®—é‡åŒ–ç½‘æ ¼ï¼Œä¿è¯å¯¹é½ã€‚
  2. **ç½‘æ ¼ç²¾ç‚¼**ï¼šåœ¨æ‰€æœ‰æƒé‡æ•´æ•°é‡åŒ–å®Œæˆåï¼Œå†»ç»“æ•´æ•°æƒé‡ $ W_{int} $ï¼Œä»…é€šè¿‡**åæ ‡ä¸‹é™ï¼ˆCoordinate Descent, CDï¼‰** ä¼˜åŒ–ç¼©æ”¾å› å­ $ s $ï¼Œä»¥è¿›ä¸€æ­¥æœ€å°åŒ–æ³¨æ„åŠ›é‡å»ºè¯¯å·®ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | GPTQ | BoA | **TURBOBOA** |
| :--- | :--- | :--- | :--- |
| **æ•ˆç‡** | â­â­â­â­â­ (é«˜ï¼Œå¹¶è¡Œé‡åŒ–) | â­â­ (ä½ï¼Œä¸²è¡Œé‡åŒ–) | â­â­â­â­ (é«˜ï¼Œè”åˆé‡åŒ–) |
| **ç²¾åº¦** | â­â­ (ä½ï¼Œå¿½ç•¥è·¨å±‚ä¾èµ–) | â­â­â­â­ (é«˜ï¼Œæ³¨æ„åŠ›æ„ŸçŸ¥) | â­â­â­â­â­ (**æ›´é«˜**) |
| **è¯¯å·®ä¼ æ’­å¤„ç†** | âŒ | âŒ | âœ… (æ˜¾å¼è¡¥å¿) |
| **ç½‘æ ¼å¯¹é½** | âŒ | âŒ | âœ… (è‡ªé€‚åº”+ç²¾ç‚¼) |

**æ€»ç»“ä¼˜åŠ¿**ï¼šTURBOBOA æˆåŠŸæ‰“ç ´äº† BoA ä¸­â€œç²¾åº¦é«˜åˆ™é€Ÿåº¦æ…¢â€çš„å›ºæœ‰ trade-offï¼Œå®ç°äº†**é€Ÿåº¦ä¸ç²¾åº¦çš„åŒé‡è¶…è¶Š**ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **æ ¡å‡†æ•°æ®é›†ï¼ˆCalibration Dataï¼‰**ï¼šç”¨äºé‡åŒ–è¿‡ç¨‹çš„å¾®è°ƒå’Œå‚æ•°å­¦ä¹ ã€‚
  - `WikiText-2` (Wiki2)ï¼šéšæœºé‡‡æ · 128 æ¡é•¿åº¦ä¸º 2048 çš„åºåˆ—ã€‚
- **æµ‹è¯•æ•°æ®é›†ï¼ˆTest Setsï¼‰**ï¼šç”¨äºè¯„ä¼°é‡åŒ–åçš„æ¨¡å‹æ€§èƒ½ã€‚
  - `WikiText-2` (Wiki2)
  - `C4`

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **æ¨¡å‹**ï¼šLlama ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ `Llama3.2-1B`, `Llama3.2-3B`, `Llama3-8B`, `Llama3.1-70B`, `Llama2-7B`, `Llama2-13B`ã€‚
- **ç¡¬ä»¶**ï¼šNVIDIA H100 GPUs (80GB)ï¼Œ70B æ¨¡å‹ä½¿ç”¨åŒå¡ã€‚
- **é‡åŒ–é…ç½®**ï¼š
  - **æƒé‡é‡åŒ–**ï¼šINT2 å’Œ INT3ã€‚
  - **æƒæ¿€æ´»é‡åŒ–**ï¼šW2A4KV4 / W2A4KV16ï¼ˆæƒé‡2bitï¼Œæ¿€æ´»4bitï¼ŒKV Cache 4bit æˆ– 16bitï¼‰ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Perplexity (PPL)**ï¼šåœ¨ Wiki2 å’Œ C4 æµ‹è¯•é›†ä¸Šï¼Œè¶Šä½è¶Šå¥½ã€‚
  - **Zero-shot Accuracy**ï¼šåœ¨ 8 ä¸ªå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¶Šé«˜è¶Šå¥½ã€‚
  - **é‡åŒ–æ—¶é—´**ï¼šè¡¡é‡ç®—æ³•æ•ˆç‡ã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

- **åŸºç¡€é‡åŒ–å™¨**ï¼š`RTN` (Round-to-Nearest), `GPTQ`ã€‚
- **å…ˆè¿›æ–¹æ³•**ï¼š`BoA` (ç›´æ¥åŸºçº¿)ã€‚
- **å˜æ¢ç±»æ–¹æ³•**ï¼ˆTransformation-basedï¼‰ï¼š
  - `QuaRot`, `SpinQuant`, `OSTQuant` (ç”¨äºæŠ‘åˆ¶å¼‚å¸¸å€¼ï¼Œå¸¸ä¸ GPTQ/BoA ç»“åˆ)ã€‚
- **å…¶ä»–**ï¼š`GPTAQ` (ç”¨äºéªŒè¯é€šé“ä¾èµ–çš„é‡è¦æ€§)ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ**

#### **(1) é€Ÿåº¦å¯¹æ¯” (Table 2)**

- åœ¨ `Llama3.1-70B` æ¨¡å‹ä¸Šï¼Œå½“ $ N=16 $ æ—¶ï¼ŒTURBOBOA å°† BoA çš„é‡åŒ–æ—¶é—´ä» **16.99 å°æ—¶**ç¼©çŸ­è‡³ **5.636 å°æ—¶**ï¼Œå®ç°äº† **è¶…è¿‡ 3 å€çš„åŠ é€Ÿ**ã€‚
- å³ä½¿åœ¨è¾ƒå°æ¨¡å‹ä¸Šï¼ŒåŠ é€Ÿä¹Ÿååˆ†æ˜¾è‘—ï¼ˆå¦‚ 1B æ¨¡å‹ä» 13.32 åˆ†é’Ÿé™è‡³ 4.363 åˆ†é’Ÿï¼‰ã€‚

#### **(2) æƒé‡-ä»…é‡åŒ– (Weight-only Quantization) (Table 4)**

- **INT2 é‡åŒ–**ï¼šåœ¨ `Llama3.2-1B` ä¸Šï¼Œç»“åˆ `QuaRot`ï¼ŒTURBOBOA å°† Wiki2 PPL ä» BoA çš„ **40.86** æ˜¾è‘—é™ä½åˆ° **33.33**ã€‚
- **é›¶æ ·æœ¬å‡†ç¡®ç‡**ï¼šåœ¨ `Llama2-13B` ä¸Šï¼ŒTURBOBOA è¾¾åˆ°äº† **69.07%**ï¼Œéå¸¸æ¥è¿‘å…¨ç²¾åº¦åŸºçº¿ï¼ˆ69.83%ï¼‰ï¼Œä¸”æ¯” BoA é«˜å‡ºè‡³å°‘ 2 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### **(3) æƒé‡-æ¿€æ´»é‡åŒ– (Weight-Activation Quantization) (Table 5)**

- åœ¨ `W2A4KV16` è®¾ç½®ä¸‹ï¼Œç»“åˆ `OSTQuant`ï¼ŒTURBOBOA åœ¨ `Llama3.2-3B` ä¸Šå°† C4 PPL ä» BoA çš„ **74.04** é™ä½åˆ° **63.75**ã€‚
- **é›¶æ ·æœ¬å‡†ç¡®ç‡å¢ç›Šå·¨å¤§**ï¼šåœ¨ `Llama2-13B` çš„ `W2A4KV4` è®¾ç½®ä¸‹ï¼ŒTURBOBOA è¾¾åˆ° **55.86%**ï¼Œæ¯” BoA é«˜å‡º **3 ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Š**ï¼Œæ¯” GPTQ é«˜å‡º **15 ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Š**ã€‚

#### **(4) æ¶ˆèå®éªŒç»“æœ (Ablation Studies)**

- **è”åˆé‡åŒ– (F1)**ï¼šéªŒè¯äº† $ N=16 $ æ˜¯æ•ˆç‡ä¸ç²¾åº¦çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼Œæ›´å¤§çš„ $ N $ åŠ é€Ÿæ”¶ç›Šé€’å‡ã€‚
- **è¯¯å·®è¡¥å¿ (F2)**ï¼šåœ¨ `Llama3.2-1B` ä¸Šï¼Œå•ç‹¬åŠ å…¥ F2 å¯å°† Wiki2 PPL ä» 41.85 é™è‡³ 37.15ï¼Œè¯æ˜äº†å¤„ç†è¯¯å·®ä¼ æ’­çš„æœ‰æ•ˆæ€§ã€‚
- **è‡ªé€‚åº”ç½‘æ ¼ (F3)**ï¼šå•ç‹¬åŠ å…¥ F3 å¯å°† PPL é™è‡³ 39.45ï¼Œè¯æ˜äº†åŠ¨æ€ç½‘æ ¼å¯¹é½çš„é‡è¦æ€§ã€‚
- **ç»¼åˆæ•ˆæœ**ï¼šF2 å’Œ F3 çš„ç»„åˆå¸¦æ¥äº†æœ€ä½³æ€§èƒ½ï¼ˆPPL é™è‡³ 33.33ï¼‰ï¼Œè¡¨æ˜äºŒè€…äº’è¡¥ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **è”åˆé‡åŒ–æ˜¯å¯è¡Œçš„**ï¼šå³ä½¿åŒæ—¶é‡åŒ–å¤šä¸ª out-channelsï¼Œåªè¦é…åˆç²¾å¿ƒè®¾è®¡çš„é—­å¼è¯¯å·®è¡¥å¿è§„åˆ™ï¼Œä¹Ÿèƒ½æœ‰æ•ˆæ•æ‰é€šé“é—´ä¾èµ–ï¼Œé¿å…ç²¾åº¦å¤§å¹…ä¸‹é™ã€‚
2. **è¯¯å·®ä¼ æ’­ä¸å®¹å¿½è§†**ï¼šæ¥è‡ªå‰åºå±‚çš„é‡åŒ–è¯¯å·®ä¼šæ˜¾è‘—å½±å“æ·±å±‚ç½‘ç»œçš„æ€§èƒ½ï¼Œæ˜¾å¼è¡¥å¿æ˜¯æå‡å¤§æ¨¡å‹é‡åŒ–é²æ£’æ€§çš„å…³é”®ã€‚
3. **åŠ¨æ€å¯¹é½è‡³å…³é‡è¦**ï¼šå›ºå®šçš„é‡åŒ–ç½‘æ ¼åœ¨è¿­ä»£ PTQ ä¸­æ˜¯æ¬¡ä¼˜çš„ï¼Œæ ¹æ®æ›´æ–°åçš„æƒé‡åŠ¨æ€è°ƒæ•´ç½‘æ ¼å¹¶è¿›è¡Œç²¾ç‚¼ï¼Œèƒ½æŒç»­æå‡æœ€ç»ˆç²¾åº¦ã€‚
4. **TURBOBOA å®ç° SOTA**ï¼šåœ¨ç»“åˆ `QuaRot`ã€`SpinQuant` æˆ– `OSTQuant` ç­‰å˜æ¢æ–¹æ³•åï¼ŒTURBOBOA åœ¨ **weight-only** å’Œ **weight-activation** é‡åŒ–ä¸¤ä¸ªé¢†åŸŸå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ï¼ˆstate-of-the-artï¼‰æ€§èƒ½ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**

- **ç†è®ºåˆ†æä¸è¶³**ï¼šè™½ç„¶å®éªŒè¡¨æ˜è”åˆé‡åŒ– $ N $ å¾ˆå¤§æ—¶æ€§èƒ½ä¾ç„¶ç¨³å®šï¼Œä½†ç¼ºä¹å…³äº $ N $ ä¸ç²¾åº¦æŸå¤±ä¹‹é—´ä¸¥æ ¼çš„ç†è®ºè¯¯å·®ç•Œåˆ†æã€‚
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šç¨³å®šç³»æ•° $ \alpha $ éœ€è¦é’ˆå¯¹ä¸åŒæ¨¡å‹è¿›è¡Œè°ƒä¼˜ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦æœ‰å¾…æé«˜ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- ä¸ºè”åˆé‡åŒ–å‚æ•° $ N $ å»ºç«‹å½¢å¼åŒ–çš„ç†è®ºè¯¯å·®åˆ†ææ¡†æ¶ã€‚
- æ¢ç´¢æ›´é«˜æ•ˆçš„ç½‘æ ¼ç²¾ç‚¼ç®—æ³•ï¼Œæˆ–å°†å…¶ä¸é‡åŒ–è¿‡ç¨‹æ›´ç´§å¯†åœ°è€¦åˆã€‚
- å°† TURBOBOA çš„æ€æƒ³æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ç¥ç»ç½‘ç»œæ¶æ„æˆ–æ›´å¤æ‚çš„é‡åŒ–æ–¹æ¡ˆï¼ˆå¦‚æ··åˆç²¾åº¦ï¼‰ã€‚

</details>

---

### 16. [TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training](https://arxiv.org/abs/2602.05251)

**Authors**: Guanjie Cheng, Boyi Li, Lingyu Sun, Mengying Zhu, Yangyang Wu, Xinkui Zhao, Shuiguang Deng  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.05251v1  

#### Abstract
Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, sufferin...

---

### 17. [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)

**Authors**: Zhentao Tang, Yuqi Cui, Shixiong Kai, Wenqian Zhao, Ke Ye, Xing Li, Anxin Tian, Zehua Pei, Hui-Ling Zhen, Shoubo Hu, Xiaoguang Li, Yunhe Wang, Mingxuan Yuan  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.04496v1  

#### Abstract
Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidenc...

---

### 18. [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)

**Authors**: Chun-Wun Cheng, Carola-Bibiane Sch\"onlieb, Angelica I. Aviles-Rivero  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.05187v1  

#### Abstract
Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels ...

---

### 19. [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)

**Authors**: Boxiang Zhang, Baijian Yang  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.05243v1  

#### Abstract
Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot struc...

---

### 20. [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)

**Authors**: Qingyuan Wu, Yuhui Wang, Simon Sinong Zhan, Yanning Dai, Shilong Deng, Sarra Habchi, Qi Zhu, Matthias Gall\'e, Chao Huang  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.05494v1  

#### Abstract
Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likeliho...

---

### 21. [Exact Recovery in the Data Block Model](https://arxiv.org/abs/2602.05852)

**Authors**: Amir R. Asadi, Akbar Davoodi, Ramin Javadi, Farzad Parvaresh  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.05852v1  

#### Abstract
Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact r...

---

### 22. [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)

**Authors**: Hao Lu, Haoyuan Huang, Yulin Zhou, Chen Li, Ningxin Zhu  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.04248v1  

#### Abstract
Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and...

---

### 23. [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)

**Authors**: Jian Chen, Zhuoran Wang, Jiayu Qin, Ming Li, Meng Wang, Changyou Chen, Yin Chen, Qizhen Weng, Yirui Liu  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.05929v1  

#### Abstract
Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent n...

---

### 24. [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)

**Authors**: Jose Miguel Luna, Taha Bouhsine, Krzysztof Choromanski  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.04915v1  

#### Abstract
We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interaction...

---

### 25. [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)

**Authors**: Ioannis Anagnostides, Itai Zilberstein, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.04990v1  

#### Abstract
The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamenta...

---

### 26. [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)

**Authors**: Xiandong Zou, Jianshu Li, Jing Huang, Pan Zhou  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.05774v1  

#### Abstract
Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft...

---

### 27. [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)

**Authors**: Xiaofeng Lin, Sirou Zhu, Yilei Chen, Mingyu Chen, Hejian Sang, Ioannis Paschalidis, Zhipeng Wang, Aldo Pacchiano, Xuezhou Zhang  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.04089v1  

#### Abstract
Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction,...

---

### 28. [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)

**Authors**: Zelai Xu, Zhexuan Xu, Ruize Zhang, Chunyang Zhu, Shi Yu, Weilin Liu, Quanlu Zhang, Wenbo Ding, Chao Yu, Yu Wang  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.04634v1  

#### Abstract
Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In t...

---

### 29. [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)

**Authors**: Deepak Gupta, Davis Bartels, Dina Demner-Fuhsman  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.04982v1  

#### Abstract
With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge f...

---

### 30. [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)

**Authors**: Ji Zhao, Yufei Gu, Shitong Shao, Xun Zhou, Liang Xiang, Zeke Xie  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.05393v1  

#### Abstract
As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computati...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- kv cache, offload, State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
