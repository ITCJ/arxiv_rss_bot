# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2025-12-30 05:58:31 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)

**Authors**: Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 15.0  
**Type**: new  
**ArXiv ID**: 2512.22925v1  

#### Abstract
Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochasti...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šArgus: Token Aware Distributed LLM Inference Optimization**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¾¹ç¼˜-äº‘æ··åˆç³»ç»Ÿä¸­çš„æ¨ç†é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
- **æ¨ç†æ—¶é—´é«˜åº¦å¯å˜**ï¼šç”±äºLLMé‡‡ç”¨è‡ªå›å½’æ¶æ„ï¼Œè¾“å‡ºtokené•¿åº¦éšè¾“å…¥è¯­ä¹‰å¤æ‚åº¦å‰§çƒˆå˜åŒ–ï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿæ³¢åŠ¨æ˜¾è‘—ã€‚
- **åŠ¨æ€ä¸éšæœºç¯å¢ƒ**ï¼šçœŸå®åœºæ™¯ä¸­è¯·æ±‚æµé‡ã€å®¢æˆ·ç«¯æ´»è·ƒçŠ¶æ€å’Œç½‘ç»œæ¡ä»¶å‡æ—¶å˜ä¸”ä¸å¯é¢„æµ‹ã€‚
- **å¼‚æ„è®¾å¤‡ç¯å¢ƒ**ï¼šè¾¹ç¼˜ä¸äº‘ç«¯æœåŠ¡å™¨è®¡ç®—èƒ½åŠ›ã€é€šä¿¡å»¶è¿Ÿå’Œç²¾åº¦è¡¨ç°å·®å¼‚å¤§ï¼Œèµ„æºåˆ†é…å¤æ‚ã€‚

ç°æœ‰æ–¹æ³•å¤§å¤šå¿½ç•¥tokenç”Ÿæˆçš„åŠ¨æ€ç‰¹æ€§ï¼Œä»…é™æ€ä¼˜åŒ–æŸä¸€æ—¶åˆ»æ€§èƒ½ï¼Œæ— æ³•åº”å¯¹é•¿æœŸéšæœºæ€§å’Œå¼‚æ„æ€§ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡º **Argus**ï¼Œé¦–ä¸ª**tokenæ„ŸçŸ¥çš„åˆ†å¸ƒå¼è¾¹ç¼˜-äº‘LLMæ¨ç†æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒç”±ä¸¤ä¸ªæ¨¡å—æ„æˆï¼š

#### **(1) Length-Aware Semantics (LAS) æ¨¡å—**
- åˆ©ç”¨ä¸€ä¸ªå¾®è°ƒåçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆModernBERTï¼‰ï¼Œç»“åˆ**ç‰¹å¾é‡æ ¡å‡†æœºåˆ¶**ï¼ˆå—Squeeze-and-Excitationå¯å‘ï¼‰ï¼Œé¢„æµ‹æ¯ä¸ªè¾“å…¥promptçš„é¢„æœŸè¾“å‡ºtokené•¿åº¦ã€‚
- å¼•å…¥**token-length-sensitiveç‰¹å¾è°ƒåˆ¶**ï¼Œå¢å¼ºæ¨¡å‹å¯¹â€œè¯¦ç»†è§£é‡Šâ€æˆ–â€œç®€è¦åˆ—å‡ºâ€ç­‰æŒ‡ä»¤çš„æ•æ„Ÿåº¦ã€‚
- å®ç°**æ¨ç†å‰ç²¾å‡†è´Ÿè½½ä¼°è®¡**ï¼Œä¸ºåç»­ä»»åŠ¡è°ƒåº¦æä¾›ç»†ç²’åº¦ç”»åƒã€‚

#### **(2) Lyapunov-guided Offloading Optimization (LOO) æ¨¡å—**
- å°†é•¿æœŸQoEï¼ˆQuality-of-Experienceï¼‰ä¼˜åŒ–å»ºæ¨¡ä¸ºå¸¦é•¿æœŸçº¦æŸçš„éšæœºä¼˜åŒ–é—®é¢˜ã€‚
- åŸºäºLyapunovä¼˜åŒ–ç†è®ºï¼Œå°†åŸé—®é¢˜åˆ†è§£ä¸ºæ¯æ—¶éš™çš„å­é—®é¢˜ã€‚
- æå‡º **Iterative Offloading Algorithm with Damping and Congestion Control (IODCC)**ï¼š
  - åŠ¨æ€æ„å»ºæˆæœ¬çŸ©é˜µï¼ŒåŒ…å«åŸºç¡€å¼€é”€ï¼ˆé€šä¿¡+è®¡ç®—ï¼‰ä¸æ‹¥å¡æƒ©ç½šé¡¹ã€‚
  - è¿­ä»£æ±‚è§£æ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰ï¼Œå®ç°è´Ÿè½½å‡è¡¡ã€‚
  - å¼•å…¥**é˜»å°¼æ›´æ–°æœºåˆ¶**é˜²æ­¢éœ‡è¡ï¼Œæå‡æ”¶æ•›ç¨³å®šæ€§ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | Argusä¼˜åŠ¿ |
|------|----------|
| **å»ºæ¨¡ç²¾ç»†åº¦** | æ˜¾å¼å»ºæ¨¡LLMè‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ï¼Œè€ƒè™‘prefillä¸decodeé˜¶æ®µåŠtokené•¿åº¦å½±å“ |
| **é€‚åº”æ€§** | æ”¯æŒåŠ¨æ€ã€éšæœºã€å¼‚æ„ç¯å¢ƒä¸‹çš„é•¿æœŸç¨³å®šä¼˜åŒ– |
| **æ•ˆç‡ä¸æ€§èƒ½** | ä½å¤æ‚åº¦ç®—æ³•ï¼ˆIODCCï¼‰é€¼è¿‘æœ€ä¼˜è§£ï¼Œä¼˜äºå¼ºåŒ–å­¦ä¹ ç­‰é«˜å¼€é”€æ–¹æ³• |
| **å®ç”¨æ€§** | åœ¨çœŸå®LLMè¯·æ±‚è½¨è¿¹ä¸ŠéªŒè¯ï¼Œå…·å¤‡å·¥ç¨‹è½åœ°æ½œåŠ› |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- ä½¿ç”¨æ¥è‡ª **Alibaba Bailian** å¹³å°çš„çœŸå®LLMæŸ¥è¯¢æ—¥å¿—ï¼ˆ[3] ServeGenï¼‰ä½œä¸ºè¯·æ±‚traceã€‚
- åŒ…å«å¤šä¸ªå®¢æˆ·ç«¯åœ¨ä¸åŒæ—¶é—´æ®µçš„è¯·æ±‚æ¨¡å¼ï¼Œåæ˜ çœŸå®ä¸–ç•Œçš„å·¥ä½œè´Ÿè½½æ³¢åŠ¨ã€‚

---

### **å®éªŒè®¾ç½®**
- **ç³»ç»Ÿæ¶æ„**ï¼šå¼‚æ„è¾¹ç¼˜-äº‘ç³»ç»Ÿï¼ŒåŒ…å« `N` å°è¾¹ç¼˜æœåŠ¡å™¨å’Œ `U` å°äº‘æœåŠ¡å™¨ã€‚
- **ä»»åŠ¡ç±»å‹**ï¼š3ç±»ä»»åŠ¡ï¼ˆè½»é‡ã€ä¸­ç­‰ã€é‡é‡çº§ï¼‰ï¼Œå¯¹åº”ä¸åŒè®¡ç®—éœ€æ±‚ã€‚
- **æ¨¡å‹è§„æ¨¡**ï¼š
  - å°æ¨¡å‹ï¼šprefilléœ€2å•ä½ï¼Œdecodeéœ€1å•ä½
  - å¤§æ¨¡å‹ï¼šprefilléœ€8å•ä½ï¼Œdecodeéœ€4å•ä½
- **æ—¶é—´è·¨åº¦**ï¼šå…± `T=100` ä¸ªæ—¶éš™
- **èµ„æºå¼‚æ„æ€§**ï¼š
  - è¾¹ç¼˜æœåŠ¡å™¨ç®—åŠ› `f_e âˆˆ [2.5, 5]`
  - äº‘æœåŠ¡å™¨ç®—åŠ› `f_c âˆˆ [5, 7.5]`
- **é€šä¿¡ä¸ç²¾åº¦**ï¼š
  - è¾¹ç¼˜å»¶è¿Ÿä½ä½†ç²¾åº¦è¾ƒä½ `[0.1, 0.5]`
  - äº‘ç«¯å»¶è¿Ÿé«˜ä½†ç²¾åº¦é«˜ `[0.6, 1.0]`

---

### **è¯„ä¼°æŒ‡æ ‡**
- **ä¸»æŒ‡æ ‡**ï¼š**Lyapunov Reward**ï¼Œç»¼åˆè¡¡é‡QoEï¼ˆå»¶è¿Ÿã€å‡†ç¡®ç‡ï¼‰ä¸é•¿æœŸçº¦æŸæ»¡è¶³æƒ…å†µï¼ˆå¦‚é˜Ÿåˆ—ç¨³å®šæ€§ï¼‰ã€‚
- **è¾…åŠ©æŒ‡æ ‡**ï¼š
  - Tokené¢„æµ‹è¯¯å·®ï¼ˆL1 Lossï¼‰
  - å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆç”¨äºæ¯”è¾ƒæ¨¡å‹è½»é‡åŒ–ç¨‹åº¦ï¼‰
  - ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½å¢ç›Šï¼ˆæ¶ˆèå®éªŒï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
#### **Tokené¢„æµ‹éƒ¨åˆ†åŸºçº¿**
- **LoRA**ï¼šä¸»æµé«˜æ•ˆå¾®è°ƒæ–¹æ³•
- **LSTM**ï¼šç»å…¸åºåˆ—æ¨¡å‹
- **Transformers**ï¼šæ ‡å‡†æ³¨æ„åŠ›æ¶æ„
- **Qwen2.5-7B**ï¼šå¼ºå¤§LLMç›´æ¥ç”¨äºé¢„æµ‹

#### **ä»»åŠ¡å¸è½½éƒ¨åˆ†åŸºçº¿**
- **Greedy_Accuracy**ï¼šä¼˜å…ˆé€‰æ‹©æœ€é«˜ç²¾åº¦è®¾å¤‡
- **Greedy_Compute**ï¼šä¼˜å…ˆé€‰æ‹©æœ€å¼ºç®—åŠ›è®¾å¤‡
- **Greedy_Delay**ï¼šä¼˜å…ˆé€‰æ‹©æœ€ä½å»¶è¿Ÿè·¯å¾„
- **TransformerPPO**ï¼šåŸºäºTransformer + PPOçš„RLæ–¹æ³• + Lyapunovçº¦æŸ
- **DiffusionRL**ï¼šæ‰©æ•£æ¨¡å‹å¢å¼ºçš„RLæ–¹æ³• + Lyapunovçº¦æŸ

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **è¡¨ I & IIï¼šä¸åŒæœåŠ¡å™¨æ•°é‡ä¸‹çš„æ€§èƒ½å¯¹æ¯”**
| ç®—æ³• | U=15 (N=4) | U=20 (N=4) | N=15 (U=6) | N=20 (U=6) |
|------|------------|------------|------------|------------|
| **Ours (Argus)** | **36,602** | **30,757** | **21,080** | **28,961** |
| Greedy-Accuracy | -265,297 | -213,876 | -219,696 | -278,269 |
| Greedy-Compute | -267,139 | -154,887 | -181,459 | -125,089 |
| Greedy-Delay | 13,565 | 4,052 | 7,235 | 10,928 |
| TransformerPPO | -4,050,226 | -3,928,943 | -5,642,526 | -76,411 |
| DiffusionRL | 32,077 | 25,511 | 14,823 | 17,939 |

> âœ… **ç»“è®º**ï¼šArgusåœ¨æ‰€æœ‰é…ç½®ä¸‹å‡å–å¾—**æœ€é«˜Rewardå€¼**ï¼Œè¿œè¶…å„ç±»è´ªå©ªç­–ç•¥ä¸å…ˆè¿›RLæ–¹æ³•ã€‚

---

#### **å›¾4aï¼šTokené¢„æµ‹ç²¾åº¦ï¼ˆL1 Lossï¼‰**
| æ–¹æ³• | L1 Loss |
|------|--------|
| **LAS (ours)** | **91.85** |
| LoRA | 92.07 (+0.2%) |
| LSTM | 107.79 (+17.3%) |
| Transformers | 106.69 (+16.15%) |
| Qwen2.5-7B | 176.93 (+92.6%) |

> âœ… LASåœ¨æ›´ä½å‚æ•°é‡ä¸‹å®ç°äº†**æœ€ç²¾ç¡®çš„tokené•¿åº¦é¢„æµ‹**ã€‚

---

#### **å›¾4bï¼šå¯è®­ç»ƒå‚æ•°é‡**
- **LoRA**: 8.75M å‚æ•°
- **LAS**: ä»…å¼•å…¥ **0.09M** æ¿€æ´»å‚æ•°ï¼ˆâ†“99%ï¼‰

> âœ… LASæ›´è½»é‡ã€æ›´é€‚åˆéƒ¨ç½²åœ¨è¾¹ç¼˜ä¾§ã€‚

---

#### **è¡¨ IIIï¼šæ¶ˆèå®éªŒ â€” æ˜¯å¦å¯ç”¨tokené¢„æµ‹å™¨**
| é…ç½® | å«é¢„æµ‹å™¨ | æ— é¢„æµ‹å™¨ | æå‡å¹…åº¦ |
|------|---------|---------|----------|
| N=4, U=6 | 10,808 | 5,409 | **+99.8%** |
| N=4, U=8 | 14,324 | 11,519 | +24.3% |
| N=4, U=10 | 18,339 | 14,457 | +26.9% |

> âœ… **tokené•¿åº¦é¢„æµ‹å¯¹æ•´ä½“æ€§èƒ½è‡³å…³é‡è¦**ï¼Œå°¤å…¶åœ¨èµ„æºç´§å¼ æ—¶æå‡æ¥è¿‘ç¿»å€ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **tokené•¿åº¦æ˜¯å½±å“LLMæ¨ç†è°ƒåº¦çš„å…³é”®å› ç´ **ï¼Œå¿½ç•¥è¯¥å˜é‡ä¼šå¯¼è‡´ä¸¥é‡æ¬¡ä¼˜å†³ç­–ã€‚
2. **LASæ¨¡å—èƒ½ä»¥æä½å¼€é”€å®ç°é«˜ç²¾åº¦tokené¢„æµ‹**ï¼Œé€šè¿‡è¯­ä¹‰ç‰¹å¾é‡åŠ æƒæœºåˆ¶æ•æ‰é•¿åº¦ç›¸å…³æç¤ºã€‚
3. **LOO + IODCCæ¡†æ¶èƒ½åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°è¿‘ä¼¼æœ€ä¼˜çš„é•¿æœŸQoEä¼˜åŒ–**ï¼ŒåŒæ—¶ä¿è¯ç³»ç»Ÿç¨³å®šæ€§ï¼ˆæ»¡è¶³é•¿æœŸè®¡ç®—çº¦æŸï¼‰ã€‚
4. **ç›¸æ¯”RLæ–¹æ³•ï¼ŒIODCCæ— éœ€å¤§é‡è®­ç»ƒï¼Œæ”¶æ•›å¿«ã€ç¨³å®šæ€§å¥½**ï¼Œæ›´é€‚åˆå®é™…éƒ¨ç½²ã€‚
5. **çœŸå®traceéªŒè¯è¡¨æ˜ï¼ŒArgusåœ¨å¤šç§èµ„æºé…ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºstate-of-the-artåŸºçº¿**ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œtokené¢„æµ‹**ï¼šè‹¥è¾“å…¥é¢†åŸŸä¸è®­ç»ƒæ•°æ®åå·®è¾ƒå¤§ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚
- **å‡è®¾ä»»åŠ¡åªèƒ½å¸è½½åˆ°å•ä¸€è®¾å¤‡**ï¼šæœªè€ƒè™‘è·¨è®¾å¤‡æµæ°´çº¿å¹¶è¡Œï¼ˆå¦‚prefillåœ¨äº‘ã€decodeåœ¨è¾¹ï¼‰ã€‚
- **æœªæ˜¾å¼å»ºæ¨¡KV Cacheå¤ç”¨æˆ–å†…å­˜é™åˆ¶**ï¼šè™½ç„¶æåŠPagedAttentionç­‰å·¥ä½œï¼Œä½†æœªå°†å…¶é›†æˆè¿›è°ƒåº¦å†³ç­–ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. æ‰©å±•æ”¯æŒ**å¤šé˜¶æ®µæµæ°´çº¿å¸è½½**ï¼ˆsplit inferenceï¼‰ã€‚
2. ç»“åˆ**KV Cacheç®¡ç†æœºåˆ¶**ï¼ˆå¦‚PagedAttentionã€CacheGenï¼‰è¿›ä¸€æ­¥é™ä½ä¼ è¾“å¼€é”€ã€‚
3. æ¢ç´¢**åœ¨çº¿è‡ªé€‚åº”tokené¢„æµ‹å™¨**ï¼Œé€‚åº”ä¸åŒç”¨æˆ·è¡Œä¸ºæ¼‚ç§»ã€‚
4. å°†æ¡†æ¶æ‰©å±•è‡³**å¤šæ¨¡æ€å¤§æ¨¡å‹**ï¼ˆå¦‚LLaVAï¼‰çš„è¾¹ç¼˜-äº‘ååŒæ¨ç†ã€‚

---

> **æ€»ç»“**ï¼šArgusé¦–æ¬¡å°†LLMçš„**tokenç”ŸæˆåŠ¨æ€æ€§**çº³å…¥è¾¹ç¼˜-äº‘ååŒæ¨ç†ä¼˜åŒ–æ¡†æ¶ï¼Œæå‡º**LAS + LOO + IODCC**ä¸‰ä½ä¸€ä½“æ–¹æ¡ˆï¼Œåœ¨ç†è®ºä¿éšœä¸å®éªŒæ•ˆæœä¸Šå‡å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºä¸‹ä¸€ä»£åˆ†å¸ƒå¼LLMæœåŠ¡æä¾›äº†é‡è¦è®¾è®¡èŒƒå¼ã€‚

</details>

---

### 2. [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)

**Authors**: Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2512.22168v1  

#### Abstract
Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç©ºé—´æ•°æ®æµæ¶æ„ï¼ˆ**spatial dataflow architectures**ï¼‰è™½ç„¶åœ¨èƒ½æ•ˆå’Œååé‡ä¸Šæ½œåŠ›å·¨å¤§ï¼Œä½†ç”±äºå…¶ç¼–ç¨‹å¤æ‚æ€§é«˜ï¼Œæ€§èƒ½ä¸¥é‡ä¾èµ–äºè®¡ç®—ä»»åŠ¡åˆ°ç¡¬ä»¶çš„æ˜ å°„æ–¹å¼ï¼ˆmappingï¼‰ï¼Œç›®å‰ä¸»è¦ä¾èµ–å‚å•†æä¾›çš„æ‰‹å·¥ä¼˜åŒ–åº“ï¼ˆå¦‚ TTNNï¼‰ã€‚è¿™ç§ä¾èµ–é™åˆ¶äº†ç¨‹åºçš„å¯ç§»æ¤æ€§å’Œæ–°ç®—å­çš„å¼€å‘æ•ˆç‡ã€‚

ä¼ ç»Ÿç¼–è¯‘å™¨é€šå¸¸åªå…³æ³¨å•ä¸ª tile å†…çš„ä»£ç ç”Ÿæˆï¼Œè€Œå¿½ç•¥äº† tile å®ä¾‹åœ¨åˆ†å¸ƒå¼æ ¸å¿ƒé—´çš„è°ƒåº¦ã€æ•°æ®å¤ç”¨å’Œç‰‡ä¸Šç½‘ç»œï¼ˆNoCï¼‰é€šä¿¡ç­‰è·¨æ ¸é—®é¢˜ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šTL ç¼–è¯‘æ¡†æ¶
è®ºæ–‡æå‡ºäº† **TL**ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¼–è¯‘æ¡†æ¶ï¼Œç”¨äºå°†åŸºäº tile çš„è¯­è¨€ï¼ˆå¦‚ Tritonï¼‰è‡ªåŠ¨ç¼–è¯‘åˆ°ç©ºé—´æ•°æ®æµæ¶æ„ä¸Šã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ç»Ÿä¸€çš„ç¡¬ä»¶æŠ½è±¡æ¨¡å‹ï¼ˆHardware Representationï¼‰**  
  å¼•å…¥åŸºäº MLIR çš„ `df` Dialectï¼Œå¯¹å¤šçº§ç¡¬ä»¶ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼š
  - **Scale-out æ¶æ„**ï¼šæ ¸å¿ƒé˜µåˆ—æ‹“æ‰‘ã€NoC è¿æ¥æ–¹å¼
  - **å†…å­˜å±‚æ¬¡**ï¼šæœ¬åœ° scratchpadï¼ˆL1ï¼‰ã€DRAM é€šé“
  - **Intra-core è®¡ç®—å•å…ƒ**ï¼šçŸ©é˜µå•å…ƒï¼ˆmatï¼‰ã€å‘é‡å•å…ƒï¼ˆvecï¼‰
  æ”¯æŒå¤šç§æ¶æ„ï¼ˆå¦‚ 2D meshã€1D ringï¼‰ï¼Œæå‡å¯ç§»æ¤æ€§ã€‚

- **ç«¯åˆ°ç«¯çš„æ—¶ç©ºæ˜ å°„ï¼ˆSpatiotemporal Mappingï¼‰**  
  å°†é€»è¾‘ tile ç½‘æ ¼æ˜ å°„åˆ°ç‰©ç†æ ¸å¿ƒé˜µåˆ—å’Œæ—¶é—´ç»´åº¦ï¼š
  - ç©ºé—´ç»´åº¦ï¼šå†³å®š tile åˆ†å¸ƒåœ¨å“ªäº›æ ¸å¿ƒä¸Šå¹¶è¡Œæ‰§è¡Œ
  - æ—¶é—´ç»´åº¦ï¼šå†³å®š tile å¦‚ä½•åˆ†æ‰¹ï¼ˆwavesï¼‰éšæ—¶é—´è°ƒåº¦
  æ”¯æŒçµæ´»çš„æ•°æ®å¤ç”¨ç­–ç•¥ï¼ˆå¦‚å¹¿æ’­ã€ç¼“å†²é‡ç”¨ï¼‰ã€‚

- **æ•°æ®å¤ç”¨åˆ†æä¸é€šä¿¡è§„åˆ’ï¼ˆData Reuse & Movement Planningï¼‰**  
  - åˆ†æè®¿é—®æ¨¡å¼ä¸­çš„ **ç©ºé—´å¤ç”¨**ï¼ˆå¤šä¸ª core å…±äº«åŒä¸€æ•°æ®ï¼‰å’Œ **æ—¶é—´å¤ç”¨**ï¼ˆåŒä¸€ core å¤šæ¬¡ä½¿ç”¨ï¼‰
  - è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆçš„ NoC å¹¿æ’­æ–¹æ¡ˆï¼ˆbroadcastï¼‰æ›¿ä»£é‡å¤çš„å…¨å±€å†…å­˜åŠ è½½
  - æ”¯æŒ loop hoisting æ¥æå‡ç¼“å­˜åˆ©ç”¨ç‡

- **æ€§èƒ½æ¨¡å‹é©±åŠ¨çš„å€™é€‰é€‰æ‹©æœºåˆ¶ï¼ˆTop-k Selectionï¼‰**  
  ä½¿ç”¨è½»é‡çº§æ€§èƒ½æ¨¡å‹é¢„ä¼°ä¸åŒ mapping çš„æ‰§è¡Œæ—¶é—´ï¼Œé€‰å‡º top-k å€™é€‰ï¼Œå†é€šè¿‡å®é™… profiling ç¡®å®šæœ€ä¼˜é…ç½®ï¼Œå¹³è¡¡ç¼–è¯‘å¼€é”€ä¸æœ€ç»ˆæ€§èƒ½ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ TTNNï¼‰ | TL |
|------|------------------|-----|
| æ˜ å°„ç­–ç•¥ | å›ºå®šæ¨¡æ¿ï¼ˆå¦‚ TT-1D / TT-2Dï¼‰ | è‡ªåŠ¨æœç´¢å¤§è§„æ¨¡ mapping ç©ºé—´ |
| æ•°æ®å¤ç”¨ | æ‰‹å·¥è®¾è®¡ | ç¼–è¯‘å™¨è‡ªåŠ¨è¯†åˆ«å¹¶ä¼˜åŒ–ç©ºé—´/æ—¶é—´å¤ç”¨ |
| å¯ç§»æ¤æ€§ | ç»‘å®šç‰¹å®šæ¶æ„ | åŸºäº `df` æŠ½è±¡æ”¯æŒå¤šæ¶æ„ |
| æ–°ç®—å­æ”¯æŒ | éœ€æ‰‹åŠ¨å®ç° | æ”¯æŒä»»æ„ tile-based kernel |
| å¼€å‘æˆæœ¬ | é«˜ï¼ˆä¸“å®¶è°ƒä¼˜ï¼‰ | ä½ï¼ˆè‡ªåŠ¨åŒ–æµç¨‹ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ’» å®éªŒå¹³å°
- **ç¡¬ä»¶**ï¼šTenstorrent Wormhole n300d åŠ é€Ÿå¡ï¼ˆåŒ socketï¼Œæ¯ socket 64 Tensix cores @ 1GHzï¼Œå³°å€¼ 64 TFLOPsï¼‰
- **å†…å­˜**ï¼šæ¯ socket 96MB SRAM + 12GB GDDR6ï¼ˆå¸¦å®½ 288GB/sï¼‰

### âš™ï¸ æ¶æ„ç›®æ ‡ï¼ˆLogical Configurationsï¼‰
ä¸ºéªŒè¯æ³›åŒ–èƒ½åŠ›ï¼Œæµ‹è¯•ä¸‰ç§é€»è¾‘é…ç½®ï¼š
1. **1Ã—8 Ring**ï¼ˆ1D ç¯å½¢ï¼‰
2. **4Ã—8 Mesh**ï¼ˆéå¯¹ç§°çŸ©å½¢ï¼‰
3. **8Ã—8 Mesh**ï¼ˆæ ‡å‡†å¯¹ç§°ç½‘æ ¼ï¼‰

### ğŸ§ª å·¥ä½œè´Ÿè½½ï¼ˆWorkloadsï¼‰
- **GEMM**ï¼šé€šç”¨çŸ©é˜µä¹˜æ³•ï¼Œè¦†ç›–å¤šç§å½¢çŠ¶ï¼ˆM, N, K âˆˆ [256, 16384]ï¼‰
- **FlashAttention**ï¼šTransformer ä¸­çš„å…³é”®ç®—å­ï¼Œå…·æœ‰å¤æ‚çš„è®¿å­˜æ¨¡å¼

### ğŸ“ è¯„ä¼°æŒ‡æ ‡
- **æ€§èƒ½ï¼ˆThroughputï¼‰**ï¼šTFLOP/s
- **åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰**ï¼šç›¸å¯¹äºåŸºçº¿ï¼ˆTTNNï¼‰çš„æ€§èƒ½æå‡
- **DRAM è®¿é—®å‡å°‘ç‡**ï¼šè¡¡é‡æ•°æ®å¤ç”¨æ•ˆæœ
- **é¢„æµ‹å‡†ç¡®æ€§**ï¼šæ€§èƒ½æ¨¡å‹ vs å®æµ‹æ€§èƒ½è¯¯å·®

### ğŸ”„ åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **TTNN**ï¼šTenstorrent å®˜æ–¹åº“ï¼Œé‡‡ç”¨ä¸¤ç§å›ºå®šæ•°æ®æµæ¨¡æ¿ï¼š
  - **TT-1D**ï¼šä¸€ç»´å¹¿æ’­
  - **TT-2D**ï¼šäºŒç»´æ³¢å‰å¼ systolic æµæ°´
- TL åœ¨ç›¸åŒæ¡ä»¶ä¸‹è‡ªåŠ¨ç”Ÿæˆ mappingï¼Œå¹¶ä¸ TTNN åŠå…¶å­æ¨¡æ¿æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ GEMM æ€§èƒ½ç»“æœ
- åœ¨ **8Ã—8 mesh** ä¸Šï¼ŒTL ç›¸æ¯” TTNN å®ç°ï¼š
  - **å‡ ä½•å¹³å‡åŠ é€Ÿæ¯” 1.03Ã—**
  - åœ¨ **62.8% çš„é…ç½®ä¸‹ä¼˜äº TTNN**
  - æœ€é«˜æé€Ÿè¾¾ **2.19Ã—**
- åœ¨ä¸è§„åˆ™å½¢çŠ¶ä¸‹è¡¨ç°æ›´ä¼˜ï¼š
  - å½“ `N â‰ˆ M` æ—¶ï¼Œ2D å¹¿æ’­æ›´æœ‰åˆ©ï¼›å½“ `N << M` æ—¶ï¼Œ1D æ›´ä¼˜
  - TL èƒ½è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œè€Œ TTNN å› å›ºå®šç­–ç•¥å¯¼è‡´è¯¯åˆ¤ï¼ˆä¾‹å¦‚åœ¨ N=1024 æ—¶é”™è¯¯é€‰ç”¨ 2D æ¨¡æ¿ï¼‰

### âš¡ FlashAttention æ€§èƒ½ç»“æœ
- TL åœ¨æ‰€æœ‰æµ‹è¯•é…ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äº TTNNï¼š
  - **å¹³å‡æ€§èƒ½æå‡ 1.91Ã—**
  - æœ€é«˜è¾¾ **2.0Ã—**
- å…³é”®åŸå› ï¼šTL è‡ªåŠ¨å®ç°äº† key æ•°æ®çš„ on-chip å¤ç”¨ï¼Œå¤§å¹…å‡å°‘äº† DRAM é‡è½½æ¬¡æ•°ï¼Œè€Œ TTNN é»˜è®¤ mapping å­˜åœ¨å¤§é‡å†—ä½™è®¿é—®ã€‚

### ğŸ”¬ æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰ç©ºé—´å¤ç”¨ï¼ˆSpatial Reuseï¼‰çš„å½±å“
| é…ç½® (M=K=N) | ä»…ä» DRAM åŠ è½½ | å¯ç”¨ TL ç©ºé—´å¤ç”¨ | åŠ é€Ÿæ¯” |
|-------------|---------------|------------------|--------|
| 1024        | 11.15 TFLOP/s | 23.70 TFLOP/s     | 2.12Ã— |
| 2048        | 16.77         | 33.96            | 2.03Ã— |
| 4096        | 22.96         | 40.44            | 1.76Ã— |

- **å¹³å‡å‡å°‘ 70% DRAM è®¿é—®**
- å¯¹å°è§„æ¨¡é—®é¢˜æ”¶ç›Šæ˜æ˜¾ï¼ˆå†…å­˜å—é™ï¼‰ï¼Œå¤§é—®é¢˜è¶‹äºè®¡ç®—ç“¶é¢ˆåå¢ç›Šä¸‹é™

#### ï¼ˆ2ï¼‰æ—¶é—´å¤ç”¨ï¼ˆTemporal Reuseï¼‰çš„å½±å“
- åœ¨ M/N è¾ƒå¤§ã€K è¾ƒå°æ—¶ï¼Œé€šè¿‡ loop hoisting ç¼“å†² A/B tileï¼Œé¿å…é‡å¤åŠ è½½
- æœ€é«˜å¸¦æ¥ **1.12Ã— åŠ é€Ÿ**
- æ•ˆæœéš M/N å¢å¤§è€Œå¢å¼º

#### ï¼ˆ3ï¼‰æ€§èƒ½æ¨¡å‹å‡†ç¡®æ€§
- é¢„æµ‹æ€§èƒ½ä¸å®æµ‹æ€§èƒ½çš„ **å‡ ä½•å¹³å‡è¯¯å·®ä¸º 17%**
- èƒ½å‡†ç¡®æ•æ‰ **memory-bound ä¸ compute-bound çš„è½¬æ¢è¶‹åŠ¿**
- å³ä½¿é¢„æµ‹ä¸å‡†ï¼Œä¹Ÿèƒ½æœ‰æ•ˆç­›é€‰å‡ºé«˜è´¨é‡å€™é€‰ï¼ˆè§ top-k å®éªŒï¼‰

#### ï¼ˆ4ï¼‰Top-k é€‰æ‹©ç­–ç•¥çš„å½±å“
| top-k | 8Ã—8 Mesh å‡ ä½•å¹³å‡æ€§èƒ½ï¼ˆç›¸å¯¹ TTNNï¼‰ | ç¼–è¯‘æ—¶é—´ï¼ˆç§’ï¼‰ |
|-------|-------------------------------|--------------|
| 1     | -6.5%                         | 5.31         |
| 2     | -0.7%                         | 10.54        |
| 5     | +2.8%                         | 26.42        |

- **top-2 å³å¯æ¶ˆé™¤å¤§éƒ¨åˆ†æ€§èƒ½å·®è·**
- å° k å€¼å³å¯è·å¾—æ¥è¿‘æœ€ä¼˜çš„ç»“æœï¼Œé€‚åˆå®é™…éƒ¨ç½²

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ç¼–è¯‘å™¨å¯ä»¥åª²ç¾ç”šè‡³è¶…è¶Šæ‰‹å·¥ä¼˜åŒ–åº“**  
   TL åœ¨ GEMM å’Œ FlashAttention ä¸Šåˆ†åˆ«è¾¾åˆ° **1.03Ã— å’Œ 1.91Ã—** äº TTNNï¼Œè¯æ˜è‡ªåŠ¨åŒ– mapping æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„ã€‚

2. **å›ºå®šæ•°æ®æµæ¨¡æ¿å­˜åœ¨å±€é™æ€§**  
   ä¸åŒç®—å­å½¢çŠ¶å’Œç¡¬ä»¶æ‹“æ‰‘éœ€è¦ä¸åŒçš„æœ€ä¼˜ mappingï¼Œ**é™æ€æ¨¡æ¿æ— æ³•é€‚åº”å¤šæ ·æ€§**ï¼Œè€Œ TL çš„åŠ¨æ€æœç´¢æ›´å…·ä¼˜åŠ¿ã€‚

3. **ç©ºé—´ä¸æ—¶é—´å¤ç”¨æ˜¯æ€§èƒ½å…³é”®**  
   - ç©ºé—´å¤ç”¨ï¼ˆNoC å¹¿æ’­ï¼‰å¯å‡å°‘é«˜è¾¾ 70% çš„ DRAM è®¿é—®
   - æ—¶é—´å¤ç”¨ï¼ˆloop hoistingï¼‰åœ¨ç‰¹å®šå½¢çŠ¶ä¸‹è¿›ä¸€æ­¥æå‡æ€§èƒ½

4. **è½»é‡çº§æ€§èƒ½æ¨¡å‹è¶³ä»¥æŒ‡å¯¼é«˜è´¨é‡ mapping æœç´¢**  
   ä¸éœ€ç²¾ç¡®æ¨¡æ‹Ÿï¼Œåªéœ€ç›¸å¯¹æ’åºèƒ½åŠ›å³å¯æœ‰æ•ˆç¼©å°å€™é€‰ç©ºé—´ã€‚

5. **ç¡¬ä»¶æŠ½è±¡å±‚æå¤§æå‡äº†å¯ç§»æ¤æ€§**  
   é€šè¿‡ `df` Dialect æè¿°ç¡¬ä»¶ï¼ŒTL å¯è½»æ¾é€‚é…ä¸åŒç©ºé—´æ¶æ„ï¼ˆå¦‚ 1D ringã€2D meshï¼‰ã€‚

### âš ï¸ å±€é™æ€§
- **ä¾èµ–å‰ç«¯ affinization**ï¼šè¦æ±‚ Triton ç­‰ DSL æä¾› affine è¡¨è¾¾å¼ï¼Œå¦åˆ™éœ€é¢å¤– pass å¤„ç†
- **å°è§„æ¨¡ kernel æ€§èƒ½ä¸ä½³**ï¼šå—ç¡¬ä»¶å¯åŠ¨å¼€é”€å½±å“ï¼Œæ¨¡å‹æœªå®Œå…¨å»ºæ¨¡æ­¤ç±» overhead
- **ç¼ºä¹å¯¹ç¨€ç–æ€§å’ŒåŠ¨æ€æ§åˆ¶æµçš„æ”¯æŒ**ï¼šå½“å‰èšç„¦ dense tensor ops
- **æœªå¼€æ”¾å®Œæ•´ç¡¬ä»¶ç»†èŠ‚**ï¼šéƒ¨åˆ†å‚æ•°é€šè¿‡ microbenchmark åæ¨ï¼Œå¯èƒ½å¼•å…¥è¯¯å·®

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•æ”¯æŒæ›´å¤š tile-based DSLï¼ˆå¦‚ CuTileã€Taichiï¼‰
- æ”¯æŒç¨€ç–å¼ é‡å’ŒåŠ¨æ€è°ƒåº¦
- ç»“åˆç¡¬ä»¶è®¾è®¡æ¢ç´¢ï¼ˆco-designï¼‰ï¼Œç”¨äºèŠ¯ç‰‡æ—©æœŸæ¶æ„è¯„ä¼°
- æ¢ç´¢æ›´ç²¾ç»†çš„æ€§èƒ½å»ºæ¨¡ï¼ˆå¦‚è€ƒè™‘ç¼“å­˜å†²çªã€NoC æ‹¥å¡ï¼‰
- æ”¯æŒè·¨èŠ¯ç‰‡ï¼ˆmulti-chipï¼‰æ‰©å±• mapping

---

## æ€»ç»“
TL æ˜¯é¦–ä¸ªå®ç°ä» tile-based é«˜å±‚è¯­è¨€åˆ°ç©ºé—´æ•°æ®æµæ¶æ„ç«¯åˆ°ç«¯è‡ªåŠ¨ç¼–è¯‘çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å¼•å…¥ **ç¡¬ä»¶æŠ½è±¡ï¼ˆdf Dialectï¼‰**ã€**æ—¶ç©ºæ˜ å°„æœç´¢** å’Œ **æ€§èƒ½æ¨¡å‹å¼•å¯¼çš„ top-k é€‰æ‹©æœºåˆ¶**ï¼ŒæˆåŠŸå®ç°äº†åª²ç¾ç”šè‡³è¶…è¶Šå‚å•†æ‰‹å·¥åº“çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·å¤‡è‰¯å¥½çš„å¯ç§»æ¤æ€§å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦ã€‚è¯¥å·¥ä½œä¸ºä¸‹ä¸€ä»£ç©ºé—´è®¡ç®—æ¶æ„çš„ç¼–ç¨‹è‡ªåŠ¨åŒ–æä¾›äº†é‡è¦èŒƒå¼ã€‚

</details>

---

### 3. [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)

**Authors**: Jithin VG, Ditto PS  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.22125v1  

#### Abstract
The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardw...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šGPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
éšç€AIå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ç­‰GPUåŠ é€Ÿå·¥ä½œè´Ÿè½½çš„å¿«é€Ÿå¢é•¿ï¼Œäº‘ç¯å¢ƒå’Œå®¹å™¨åŒ–å¹³å°å¯¹é«˜æ•ˆå…±äº«GPUèµ„æºçš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµçš„ç¡¬ä»¶çº§è™šæ‹ŸåŒ–æŠ€æœ¯ï¼ˆå¦‚NVIDIA MIGï¼‰ä»…é™äºé«˜ç«¯æ•°æ®ä¸­å¿ƒGPUï¼ˆå¦‚A100ã€H100ï¼‰ï¼Œæ— æ³•åœ¨æ›´å¹¿æ³›çš„GPUå‹å·ä¸Šéƒ¨ç½²ã€‚

è½¯ä»¶å±‚é¢çš„è™šæ‹ŸåŒ–æ–¹æ¡ˆï¼ˆå¦‚HAMi-coreï¼‰è™½ç„¶æ”¯æŒæ›´å¤šGPUç±»å‹ï¼Œä½†ç¼ºä¹ç»Ÿä¸€ã€ç³»ç»ŸåŒ–çš„è¯„ä¼°æ ‡å‡†ï¼Œå¯¼è‡´å…¶æ€§èƒ½ã€éš”ç¦»æ€§å’Œé€‚ç”¨åœºæ™¯éš¾ä»¥é‡åŒ–æ¯”è¾ƒã€‚å› æ­¤ï¼Œè¯¥é¢†åŸŸå­˜åœ¨ä¸€ä¸ªæ˜¾è‘—çš„â€œ**Benchmarking Gap**â€â€”â€”å³ç¼ºå°‘æ ‡å‡†åŒ–ã€å¯å¤ç°ä¸”è¦†ç›–å¤šç»´åº¦çš„è¯„æµ‹æ¡†æ¶ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **GPU-Virt-Bench**ï¼Œä¸€ä¸ªé¢å‘è½¯ä»¶å‹GPUè™šæ‹ŸåŒ–ç³»ç»Ÿçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹äº”å¤§æ ¸å¿ƒè´¡çŒ®ï¼š

1. **æ„å»ºäº†ä¸€ä¸ªåŒ…å«56ä¸ªæŒ‡æ ‡çš„å®Œæ•´åº¦é‡ä½“ç³»ï¼ˆTaxonomyï¼‰**  
   å°†GPUè™šæ‹ŸåŒ–è´¨é‡åˆ’åˆ†ä¸º10ä¸ªç±»åˆ«ï¼š
   - Overheadï¼ˆå¼€é”€ï¼‰
   - Isolationï¼ˆéš”ç¦»æ€§ï¼‰
   - LLMï¼ˆå¤§æ¨¡å‹ç›¸å…³æ€§èƒ½ï¼‰
   - Memory Bandwidthï¼ˆå†…å­˜å¸¦å®½ï¼‰
   - Cache Behaviorï¼ˆç¼“å­˜è¡Œä¸ºï¼‰
   - PCIe Throughputï¼ˆä¸»æœºé€šä¿¡ï¼‰
   - NCCL/P2P Communicationï¼ˆå¤šGPUé€šä¿¡ï¼‰
   - Scheduling Efficiencyï¼ˆè°ƒåº¦æ•ˆç‡ï¼‰
   - Memory Fragmentationï¼ˆå†…å­˜ç¢ç‰‡ï¼‰
   - Error Recoveryï¼ˆé”™è¯¯æ¢å¤ï¼‰

2. **å¼€æºå®ç°äº†ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•å·¥å…·é“¾**  
   æ”¯æŒå¤šç§è™šæ‹ŸåŒ–åç«¯ï¼ˆNativeã€HAMi-coreã€BUD-FCSPã€MIG-Idealæ¨¡æ‹Ÿï¼‰ï¼Œä»£ç å…¬å¼€äºGitHubï¼ˆhttps://github.com/BudEcosystem/GPU-Virt-Benchï¼‰ï¼Œä¾¿äºç¤¾åŒºå¤ç°ä¸æ‰©å±•ã€‚

3. **æä¾›äº†é’ˆå¯¹LLMæ¨ç†çš„å…³é”®ä¸“é¡¹æµ‹è¯•**  
   åŒ…æ‹¬Attention Kernelååã€KV Cacheåˆ†é…é€Ÿåº¦ã€Tokenç”Ÿæˆå»¶è¿Ÿï¼ˆTTFT/ITLï¼‰ã€åŠ¨æ€æ‰¹å¤„ç†å½±å“ç­‰ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨AIåº”ç”¨å±‚çš„ç©ºç™½ã€‚

4. **æå‡ºäº†ä¸€å¥—åŸºäºMIGç†æƒ³è¡Œä¸ºçš„è¯„åˆ†æœºåˆ¶ï¼ˆScoring Methodologyï¼‰**  
   å¼•å…¥â€œMIG Parityâ€æ¦‚å¿µï¼Œå°†å„æŒ‡æ ‡å¾—åˆ†å½’ä¸€åŒ–å¹¶ä¸ç†æƒ³MIGè¡¨ç°å¯¹æ¯”ï¼Œè®¡ç®—æ€»ä½“åˆ†æ•°å¹¶èµ‹äºˆå­—æ¯ç­‰çº§ï¼ˆA+/B/Cç­‰ï¼‰ï¼Œä½¿ä¸åŒç³»ç»Ÿé—´å…·å¤‡æ¨ªå‘å¯æ¯”æ€§ã€‚

5. **å®è¯è¯„ä¼°äº†ä¸¤ç§ä¸»æµè½¯ä»¶è™šæ‹ŸåŒ–ç³»ç»Ÿï¼ˆHAMi-core å’Œ BUD-FCSPï¼‰**  
   æ­ç¤ºäº†å®ƒä»¬åœ¨çœŸå®è´Ÿè½½ä¸‹çš„æ€§èƒ½ç‰¹å¾ï¼Œä¸ºç”Ÿäº§éƒ¨ç½²æä¾›å†³ç­–ä¾æ®ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| å¯¹æ¯”é¡¹ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆMLPerf/Rodinia/å‚å•†å·¥å…·ï¼‰ | GPU-Virt-Bench |
|-------|-------------------------------|----------------|
| èšç„¦ç›®æ ‡ | åŸå§‹ç®—åŠ›æ€§èƒ½ | **è™šæ‹ŸåŒ–ç‰¹æ€§æœ¬èº«çš„è´¨é‡** |
| å¯å¤ç°æ€§ | å¤šä¸ºé—­æºæˆ–å®šåˆ¶è„šæœ¬ | **å®Œå…¨å¼€æºã€å‚æ•°å¯é…ç½®** |
| æµ‹è¯•ç»´åº¦ | å•ä¸€ç»´åº¦ï¼ˆå¦‚ååï¼‰ | **10ç±»å…±56é¡¹æŒ‡æ ‡å…¨é¢è¦†ç›–** |
| LLMé€‚é…æ€§ | ç¼ºä¹é’ˆå¯¹æ€§è®¾è®¡ | **ä¸“è®¾LLM-001~LLM-010ç³»åˆ—æŒ‡æ ‡** |
| éš”ç¦»æ€§è¯„ä¼° | å‡ ä¹æ—  | **ç³»ç»ŸåŒ–æµ‹é‡è·¨ç§Ÿæˆ·å¹²æ‰°ã€å…¬å¹³æ€§ã€QoSä¸€è‡´æ€§ç­‰** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒå¹³å°é…ç½®
- **GPU**: NVIDIA A100-40GB PCIe
- **CPU**: AMD EPYC 7742 (64æ ¸)
- **å†…å­˜**: 512GB DDR4-3200
- **å­˜å‚¨**: NVMe SSD
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS
- **CUDAç‰ˆæœ¬**: 12.0
- **é©±åŠ¨ç‰ˆæœ¬**: 525.105.17

### æ”¯æŒçš„è™šæ‹ŸåŒ–ç³»ç»Ÿï¼ˆExecution Modesï¼‰
| System       | Key     | æè¿° |
|--------------|---------|------|
| Native       | `native` | ç‰©ç†æœºåŸç”Ÿè¿è¡Œï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™åŸºçº¿ |
| HAMi-core    | `hami`   | åŸºäºCUDA APIæ‹¦æˆªçš„è½¯ä»¶è™šæ‹ŸåŒ–ä¸­é—´ä»¶ |
| BUD-FCSP     | `fcsp`   | ä½œè€…å›¢é˜Ÿæå‡ºçš„æ”¹è¿›ç‰ˆç»†ç²’åº¦SMæ§åˆ¶æ–¹æ¡ˆ |
| MIG-Ideal    | `mig`    | åŸºäºNVIDIAè§„æ ¼ä¹¦çš„ç†æƒ³åŒ–MIGè¡Œä¸ºæ¨¡æ‹Ÿï¼ˆéå®æµ‹ï¼‰ |

> æ³¨ï¼šMIG-Idealç”¨äºè®¾å®šç†è®ºæœ€ä¼˜è¾¹ç•Œï¼Œå³ä½¿åœ¨ä¸æ”¯æŒMIGçš„è®¾å¤‡ä¸Šä¹Ÿå¯è¿›è¡Œç›¸å¯¹è¯„ä¼°ã€‚

### è¯„ä¼°æŒ‡æ ‡ä½“ç³»ï¼ˆéƒ¨åˆ†å…³é”®æŒ‡æ ‡ï¼‰
#### Overhead Metricsï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- OH-001: Kernel Launch Latencyï¼ˆå†…æ ¸å¯åŠ¨å»¶è¿Ÿï¼‰
- OH-005: API Interception Overheadï¼ˆAPIé’©å­å¼€é”€ï¼‰
- OH-010: Total Throughput Degradationï¼ˆæ€»ååä¸‹é™ç™¾åˆ†æ¯”ï¼‰

#### Isolation Metricsï¼ˆè¶Šé«˜è¶Šå¥½æˆ–è¶Šä½è¶Šå¥½è§†æƒ…å†µè€Œå®šï¼‰
- IS-003: SM Utilization Accuracyï¼ˆSMåˆ©ç”¨ç‡å‡†ç¡®æ€§ï¼‰
- IS-008: Fairness Indexï¼ˆJainå…¬å¹³æŒ‡æ•°ï¼‰
- IS-009: Noisy Neighbor Impactï¼ˆå™ªå£°é‚»å±…å½±å“ï¼‰

#### LLM-Specific Metrics
- LLM-001: Attention Kernel Throughputï¼ˆæ³¨æ„åŠ›æ ¸æ€§èƒ½ï¼‰
- LLM-004: Token Generation Latencyï¼ˆé¦–tokenæ—¶é—´ TTFT + é—´éš” ITLï¼‰
- LLM-003: Batch Size Scalingï¼ˆæ‰¹é‡æ‰©å±•æ•ˆç‡ï¼‰

### ç»Ÿè®¡æ–¹æ³•
- æ¯é¡¹æŒ‡æ ‡é»˜è®¤æ‰§è¡Œ100æ¬¡è¿­ä»£ï¼Œå‰10æ¬¡ä¸ºé¢„çƒ­
- æŠ¥å‘Šå‡å€¼ï¼ˆmeanï¼‰ã€æ ‡å‡†å·®ï¼ˆstddevï¼‰ã€P95/P99ã€å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰
- æ‰€æœ‰ç»“æœä¸Native/MIG-Idealå¯¹æ¯”ï¼Œè®¡ç®—â–³MIGåå·®å’Œå¾—åˆ†

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### è¡¨1ï¼šOverhead æ€§èƒ½å¯¹æ¯”ï¼ˆå•ä½ï¼šÎ¼sï¼Œé™¤ç‰¹åˆ«æ ‡æ³¨å¤–ï¼‰
| Metric               | Native | HAMi-core | BUD-FCSP |
|----------------------|--------|-----------|----------|
| OH-001 (Launch)      | 4.2    | 15.3      | 8.7      |
| OH-002 (Alloc)       | 12.5   | 45.2      | 28.3     |
| OH-003 (Free)        | 8.1    | 32.4      | 18.6     |
| OH-004 (Context)     | 125    | 312       | 198      |
| OH-005 (Hook, ns)    | â€”      | 85        | 42       |
| OH-010 (Throughput â†“)| â€”      | 18.5%     | 9.2%     |

> **å‘ç°**ï¼šHAMi-coreå¼•å…¥çº¦3.6å€çš„Kernel Launchå»¶è¿Ÿï¼›BUD-FCSPé€šè¿‡ä¼˜åŒ–dlsymè·¯å¾„å’Œä»¤ç‰Œæ¡¶ç®—æ³•ï¼Œå°†å¤šæ•°å¼€é”€é™ä½40%ä»¥ä¸Šã€‚

#### è¡¨2ï¼šIsolation è´¨é‡å¯¹æ¯”ï¼ˆ4ä¸ªå¹¶å‘ç§Ÿæˆ·ï¼‰
| Metric                   | HAMi-core | BUD-FCSP |
|--------------------------|-----------|----------|
| IS-001 (Mem Accuracy, %) | 98.2      | 99.1     |
| IS-003 (SM Accuracy, %)  | 85.4      | 92.7     |
| IS-008 (Fairness Index)  | 0.87      | 0.94     |
| IS-009 (Noisy Neighbor â†“)| 24.3%     | 12.1%    |
| IS-010 (Fault Isolation) | Pass      | Pass     |

> **å‘ç°**ï¼šä¸¤è€…å‡å®ç°åŸºæœ¬å†…å­˜éš”ç¦»ï¼Œä½†SMæ§åˆ¶ç²¾åº¦æœ‰é™ï¼›BUD-FCSPåœ¨å…¬å¹³æ€§å’ŒæŠ—å¹²æ‰°æ–¹é¢æ˜æ˜¾ä¼˜äºHAMi-coreã€‚

#### è¡¨3ï¼šLLM å·¥ä½œè´Ÿè½½æ€§èƒ½ï¼ˆç›¸å¯¹äºNativeï¼‰
| Metric                  | HAMi-core | BUD-FCSP |
|-------------------------|-----------|----------|
| LLM-001 (Attention, %)  | 82.3%     | 91.5%    |
| LLM-002 (KV Cache, %)   | 76.4%     | 88.2%    |
| LLM-004 (TTFT, ms)      | 45.2      | 28.7     |
| LLM-004 (ITL, ms)       | 12.8      | 8.4      |
| LLM-003 (Batch Scale)   | 0.78      | 0.89     |

> **å‘ç°**ï¼šLLMå¯¹å†…å­˜åˆ†é…å»¶è¿Ÿæä¸ºæ•æ„Ÿï¼ŒKV Cacheé¢‘ç¹å¢é•¿åŠ å‰§äº†è™šæ‹ŸåŒ–å¼€é”€ï¼›BUD-FCSPåœ¨Tokenç”Ÿæˆå»¶è¿Ÿä¸Šæå‡è¿‘35%ï¼Œæ›´é€‚åˆä½å»¶è¿Ÿæ¨ç†åœºæ™¯ã€‚

#### è¡¨4ï¼šç»¼åˆè¯„åˆ†ä¸ç­‰çº§
| System       | Overall Score | MIG Parity | Grade |
|--------------|---------------|------------|-------|
| MIG-Ideal    | 100%          | 100%       | A+ (baseline) |
| Native       | 100%          | â€”          | A+    |
| BUD-FCSP     | 85.2%         | 85.2%      | B+    |
| HAMi-core    | 72.0%         | 72.0%      | C     |

> **è¯´æ˜**ï¼šMIG-Idealä¸ºç†è®ºå‚è€ƒå€¼ï¼›å®é™…ä¸­Nativeä»£è¡¨ç»å¯¹æ€§èƒ½å¤©èŠ±æ¿ï¼Œè½¯ä»¶è™šæ‹ŸåŒ–èƒ½è¾¾åˆ°å…¶72%-85%çš„ç»¼åˆè¡¨ç°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è½¯ä»¶è™šæ‹ŸåŒ–è™½æœ‰å¼€é”€ä½†ä»å…·å®ç”¨ä»·å€¼**  
   åœ¨å…¸å‹å·¥ä½œè´Ÿè½½ä¸‹ï¼ŒHAMi-coreå¸¦æ¥çº¦18.5%çš„ååæŸå¤±ï¼ŒBUD-FCSPé™è‡³9.2%ï¼Œå¯¹äºè¿½æ±‚æˆæœ¬æ•ˆç›Šè€Œéæè‡´æ€§èƒ½çš„åœºæ™¯æ˜¯å¯æ¥å—çš„æŠ˜è¡·ã€‚

2. **éš”ç¦»æ€§ä¸ºè½¯è‚‹ï¼Œå°¤å…¶SMæ§åˆ¶ä¸å¤Ÿç²¾ç¡®**  
   è½¯ä»¶å±‚æ— æ³•åƒMIGé‚£æ ·ç‰©ç†éš”ç¦»SMå’ŒL2 Cacheï¼Œå¯¼è‡´SMåˆ©ç”¨ç‡æ§åˆ¶è¯¯å·®è¾¾7â€“15%ï¼Œæ˜“å¼•å‘â€œå™ªå£°é‚»å±…â€æ•ˆåº”ã€‚BUD-FCSPé€šè¿‡åŠ æƒå…¬å¹³é˜Ÿåˆ—è°ƒåº¦æå‡äº†å¤šç§Ÿæˆ·å…¬å¹³æ€§ã€‚

3. **LLMæ¨ç†å¯¹è™šæ‹ŸåŒ–å¼‚å¸¸æ•æ„Ÿ**  
   å› æ¶‰åŠå¤§é‡å°å¯¹è±¡åŠ¨æ€å†…å­˜åˆ†é…ï¼ˆå¦‚KV Cacheï¼‰ã€é«˜å¹¶å‘æµå’Œæ··åˆç²¾åº¦è¿ç®—ï¼Œå…¶æ€§èƒ½å—è™šæ‹ŸåŒ–å¼€é”€æ”¾å¤§å½±å“æ˜¾è‘—ã€‚ä¼˜åŒ–å†…å­˜è·Ÿè¸ªå’Œé€Ÿç‡é™åˆ¶ç­–ç•¥è‡³å…³é‡è¦ã€‚

4. **BUD-FCSPæ˜¾è‘—ä¼˜äºHAMi-core**  
   åœ¨æ‰€æœ‰ç»´åº¦ä¸­ï¼ŒBUD-FCSPå‡è¡¨ç°å‡ºæ›´ä½çš„å¼€é”€ã€æ›´é«˜çš„éš”ç¦»è´¨é‡å’Œæ›´å¥½çš„LLMé€‚åº”æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨ç»†ç²’åº¦SMæ§åˆ¶ã€è½»é‡åŒ–hookæœºåˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### å±€é™æ€§
- **å•GPUèšç„¦**ï¼šå½“å‰æ¡†æ¶ä¸»è¦é¢å‘å•å¡åœºæ™¯ï¼Œå°šæœªå……åˆ†è¦†ç›–åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„NCCLé€šä¿¡ç“¶é¢ˆã€‚
- **åˆæˆå·¥ä½œè´Ÿè½½ä¸ºä¸»**ï¼šLLMæµ‹è¯•ä½¿ç”¨è‡ªå®šä¹‰CUDA kernelæ¨¡æ‹Ÿï¼Œæœªé›†æˆPyTorch/TensorRT-LLMç­‰çœŸå®æ¨ç†æ ˆï¼Œéœ€è¿›ä¸€æ­¥éªŒè¯ã€‚
- **MIGåŸºçº¿ä¸ºæ¨¡æ‹Ÿå€¼**ï¼šMIG-IdealåŸºäºå®˜æ–¹æ–‡æ¡£æ¨å¯¼ï¼Œå¹¶éå®æµ‹æ•°æ®ï¼Œå¯èƒ½é«˜ä¼°ç†æƒ³æ€§èƒ½ã€‚
- **GPUæ¶æ„è¦†ç›–æœ‰é™**ï¼šä»…åœ¨A100ä¸Šæµ‹è¯•ï¼Œå…¶ä»–æ¶æ„ï¼ˆå¦‚RTXç³»åˆ—ã€Hopperï¼‰çš„è¡Œä¸ºå¯èƒ½å­˜åœ¨å·®å¼‚ã€‚
- **ç‰ˆæœ¬ä¾èµ–æ€§å¼º**ï¼šHAMi-coreå’ŒBUD-FCSPå¤„äºå¿«é€Ÿè¿­ä»£ä¸­ï¼Œç»“æœä»…åæ˜ ç‰¹å®šç‰ˆæœ¬çŠ¶æ€ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³å¤šGPUå’Œåˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯ï¼Œå¢åŠ å¯¹NCCL AllReduceã€P2På¸¦å®½çš„å‹åŠ›æµ‹è¯•ï¼›
2. é›†æˆä¸»æµLLMæ¨ç†æ¡†æ¶ï¼ˆå¦‚vLLMã€TensorRT-LLMï¼‰ä»¥å¢å¼ºç°å®ä»£è¡¨æ€§ï¼›
3. å¼€å‘è‡ªåŠ¨åŒ–å›å½’æµ‹è¯•æµæ°´çº¿ï¼Œæ”¯æŒæŒç»­ç›‘æ§è™šæ‹ŸåŒ–ç³»ç»Ÿçš„ç¨³å®šæ€§ä¸æ€§èƒ½æ¼”è¿›ï¼›
4. æ¢ç´¢å¼‚æ„GPUé›†ç¾¤ä¸‹çš„ç»Ÿä¸€è¯„æµ‹èƒ½åŠ›ï¼›
5. å¼•å…¥åŠŸè€—ä¸èƒ½æ•ˆæŒ‡æ ‡ï¼Œå®Œå–„ç»¿è‰²è®¡ç®—ç»´åº¦çš„è¯„ä¼°ã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> GPU-Virt-Benché¦–æ¬¡å»ºç«‹äº†ç³»ç»ŸåŒ–ã€å¯é‡åŒ–çš„è½¯ä»¶GPUè™šæ‹ŸåŒ–è¯„ä¼°ä½“ç³»ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ¡ˆä¸MIGä¹‹é—´çš„æ€§èƒ½é¸¿æ²Ÿï¼Œå¹¶è¯æ˜BUD-FCSPåœ¨å…¼é¡¾å…¼å®¹æ€§çš„åŒæ—¶æ˜¾è‘—ä¼˜äºHAMi-coreï¼Œä¸ºå¤šç§Ÿæˆ·AIå¹³å°é€‰å‹æä¾›äº†ç§‘å­¦ä¾æ®ã€‚

</details>

---

### 4. [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)

**Authors**: Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.22234v1  

#### Abstract
Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from compu...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# DiRL: An Efficient Post-Training Framework for Diffusion Language Models è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ **Diffusion Language Models (dLLMs)** åœ¨é¢„è®­ç»ƒé˜¶æ®µå·²å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨ **post-training** é˜¶æ®µï¼ˆå°¤å…¶æ˜¯ **Reinforcement Learning, RL**ï¼‰ä»é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼š
- **è®¡ç®—æ•ˆç‡ä½ä¸‹**ï¼šä¼ ç»Ÿ dLLMs ç¼ºä¹ KV Cache æ”¯æŒï¼Œæ¨ç†å’Œè®­ç»ƒå¼€é”€å¤§ã€‚
- **è®­ç»ƒ-æ¨ç†ç›®æ ‡ä¸ä¸€è‡´**ï¼šé¢„è®­ç»ƒä¸­ä½¿ç”¨çš„éšæœºæ©ç ç­–ç•¥æ— æ³•å¤ç°çœŸå®æ¨ç†è·¯å¾„ï¼Œå¯¼è‡´ logits åå·®ã€‚
- **ç¼ºä¹é«˜æ•ˆçš„ RL æ¡†æ¶æ”¯æŒ**ï¼šç¼ºå°‘ä¸é«˜æ•ˆæ¨ç†å¼•æ“ï¼ˆå¦‚ LMDeployï¼‰é›†æˆçš„ç«¯åˆ°ç«¯è®­ç»ƒç³»ç»Ÿï¼Œéš¾ä»¥å®ç°å¿«é€Ÿ rollout å’Œåœ¨çº¿æ›´æ–°ã€‚

è¿™äº›é—®é¢˜é™åˆ¶äº† dLLMs åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä¸Šçš„è¡¨ç°ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡º **DiRL** â€”â€” ä¸€ä¸ªé«˜æ•ˆçš„ dLLM åè®­ç»ƒæ¡†æ¶ï¼Œå¹¶åŸºäºæ­¤æå‡º **DiPO**ï¼Œé¦–ä¸ªä¸º dLLMs è®¾è®¡çš„æ— å **Group Relative Policy Optimization (GRPO)** å®ç°ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

1. **DiRL æ¡†æ¶è®¾è®¡**
   - å°† **FlexAttention** åŠ é€Ÿçš„ **blockwise è®­ç»ƒ** ä¸ **LMDeploy ä¼˜åŒ–çš„æ¨ç†å¼•æ“** ç´§å¯†ç»“åˆã€‚
   - å®ç° **API Server å†…éƒ¨çš„åœ¨çº¿æ¨¡å‹æ›´æ–°**ï¼Œé¿å…é¢‘ç¹ç£ç›˜ I/Oï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡ã€‚
   - æ„å»ºäº†ä» SFT åˆ° RL çš„å®Œæ•´ä¸¤é˜¶æ®µ post-training æµç¨‹ã€‚

2. **DiPO ç®—æ³•**
   - åˆ©ç”¨ blockwise dLLM å¯ç²¾ç¡®è®¡ç®—æ¯ä¸ª token çš„æ¡ä»¶æ¦‚ç‡è¿™ä¸€ç‰¹æ€§ï¼Œé¦–æ¬¡å®ç°äº† **æ— åçš„ GRPO**ã€‚
   - å¼•å…¥å›ºå®šå‚è€ƒç­–ç•¥ $ \pi_{\text{ref}} $ çš„ KL æ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç­–ç•¥æ¼‚ç§»ã€‚
   - æ”¯æŒ token-level ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œé€‚é… dLLM çš„ç”Ÿæˆæœºåˆ¶ã€‚

3. **å·¥ç¨‹çº§ä¼˜åŒ–**
   - ä½¿ç”¨ **FlexAttention** å¤„ç†å¤æ‚çš„ blockwise attention maskï¼Œçªç ´ FlashAttention çš„é™åˆ¶ã€‚
   - é€šè¿‡ **LMDeploy çš„ in-place å‚æ•°æ›´æ–° API** å®ç°é›¶æˆæœ¬æ¨¡å‹çƒ­æ›´æ–°ï¼Œæ¶ˆé™¤æ–‡ä»¶ç³»ç»Ÿäº¤äº’å»¶è¿Ÿã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | DiRL vs. ç°æœ‰æ–¹æ³• |
|------|------------------|
| **è®­ç»ƒæ•ˆç‡** | ç›¸æ¯” TraceRLï¼Œæ¯æ­¥è®­ç»ƒå»¶è¿Ÿé™ä½è¿‘ 6Ã—ï¼Œæ€»ååæå‡ 2.5Ã— |
| **è®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§** | å®ç°çœŸæ­£å¯¹é½çš„ç›®æ ‡å‡½æ•°ï¼Œé¿å…éšæœºæ©ç å¸¦æ¥çš„åå·® |
| **ç®—æ³•å…ˆè¿›æ€§** | é¦–ä¸ªåœ¨ dLLM ä¸Šå®ç°æ— å GRPO çš„å·¥ä½œï¼Œä¼˜äº diffu-GRPOã€Coupled-GRPO ç­‰è¿‘ä¼¼æ–¹æ³• |
| **ç³»ç»Ÿé›†æˆåº¦** | é«˜åº¦é›†æˆè®­ç»ƒä¸æ¨ç†ç³»ç»Ÿï¼Œæ”¯æŒå®æ—¶ rollout ä¸åœ¨çº¿å­¦ä¹  |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

| é˜¶æ®µ | æ•°æ®é›† | æè¿° |
|------|--------|------|
| **SFT é˜¶æ®µ** | `OpenR1-Math` | ç”± GLM-4.6 è’¸é¦å¾—åˆ°çš„é«˜è´¨é‡æ•°å­¦æ¨ç†æ•°æ®é›† |
| **RL é˜¶æ®µ** | `Big-Math` | å¤§è§„æ¨¡ã€ç» math-verify éªŒè¯çš„æ•°å­¦ RL ä¸“ç”¨æ•°æ®é›† |

> æ‰€æœ‰æ ·æœ¬æœ€å¤§é•¿åº¦é™åˆ¶ä¸º **8k tokens**ï¼Œæ˜¯ç›®å‰ dLLMs ä¸­æœ€é•¿çš„æ¨ç†é•¿åº¦ã€‚

---

### å®éªŒè®¾ç½®

| é¡¹ç›® | è®¾ç½® |
|------|------|
| **åŸºç¡€æ¨¡å‹** | SDAR-8B-Chatï¼ˆblockwise dLLMï¼‰ |
| **SFT è®¾ç½®** | ä½¿ç”¨ LLaMA-Factoryï¼Œ8Ã—H200 GPUsï¼Œbatch size=512ï¼Œlr=1e-5ï¼Œcosine è°ƒåº¦å™¨ï¼Œè®­ç»ƒ 100 æ­¥ |
| **RL è®¾ç½®** | 128Ã—H200 GPUsï¼ŒZeRO-1ï¼Œglobal batch size=128ï¼Œæ¯é¢˜ rollout 32 æ¡è½¨è¿¹ï¼Œlr=1e-6ï¼Œè®­ç»ƒ 40 æ­¥ |
| **è¯„ä¼°ä»»åŠ¡** | MATH500, GSM8k, AIME2024, AIME2025, OlympiadBench |
| **è§£ç æ–¹å¼** | é™æ€ï¼ˆgreedyï¼‰ä¸åŠ¨æ€ï¼ˆtop-1 prob > 0.9 ç›´æ¥è¾“å‡ºï¼‰ä¸¤ç§æ¨¡å¼ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ¨¡å‹ | ç±»å‹ | å¤‡æ³¨ |
|------|------|------|
| Qwen2.5-7B/32B-Instruct | AR æ¨¡å‹ | å¹¿æ³›è®¤å¯çš„é«˜æ€§èƒ½è‡ªå›å½’æ¨¡å‹ |
| SDAR-8B-Chat | dLLM | åŸå§‹ blockwise æ‰©æ•£æ¨¡å‹ |
| TraDo-8B-Instruct | dLLM + RL | å½“å‰æœ€å…ˆè¿›çš„ dLLM RL æ–¹æ³•ï¼ˆTraceRLï¼‰ |
| DiRL-8B-Instruct | dLLM + DiPO (ours) | æœ¬æ–‡æå‡ºçš„æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1ï¼‰

| æ¨¡å‹ | MATH500 â†‘ | GSM8k â†‘ | AIME2024 â†‘ | AIME2025 â†‘ | Olympiad â†‘ | Avg. Score â†‘ |
|------|-----------|---------|------------|------------|-------------|---------------|
| Qwen2.5-7B-Instruct | 73.8 | 89.8 | 9.0 | 5.6 | 36.6 | 42.9 |
| Qwen2.5-32B-Instruct | 81.1 | 94.0 | 12.9 | 11.9 | 45.7 | 49.1 |
| SDAR-8B-Chat | 71.5 | 89.5 | 5.6 | 8.5 | 35.6 | 42.2 |
| TraDo-8B-Instruct | 76.7 | 90.4 | 11.5 | 13.5 | 40.2 | 46.5 |
| **DiRL-8B-Instruct (Ours)** | **85.1** | **93.1** | **21.5** | **22.9** | **47.3** | **54.0** |

> âœ… **DiRL-8B-Instruct åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡è¾¾åˆ° SOTA æ€§èƒ½ï¼Œå¹³å‡å¾—åˆ†è¶…è¶Šæ›´å¤§çš„ Qwen2.5-32B-Instruct è¾¾ 4.9 åˆ†**

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

- **ç›¸æ¯”åŒè§„æ¨¡ dLLM**ï¼š
  - åœ¨ AIME2025 ä¸Šæ¯” TraDo æé«˜ **9.4 åˆ†**ï¼Œåœ¨ AIME2024 ä¸Šæé«˜ **10 åˆ†**ã€‚
  - è¾“å‡ºå¹³å‡é•¿åº¦æ›´é•¿ï¼ˆè§è¡¨ä¸­â€œAverage Output Lengthâ€ï¼‰ï¼Œè¯´æ˜å…·å¤‡æ›´å¼ºçš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚
- **ç›¸æ¯” AR æ¨¡å‹**ï¼š
  - è¶…è¶Šå‚æ•°é‡æ›´å¤§çš„ **Qwen2.5-32B-Instruct**ï¼Œå°¤å…¶åœ¨ç«èµ›çº§æ•°å­¦ä»»åŠ¡ï¼ˆAIME/Olympiadï¼‰ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚
- **æ•ˆç‡æ–¹é¢**ï¼ˆFigure 6 & 7ï¼‰ï¼š
  - å•æ­¥è®­ç»ƒæ—¶é—´ï¼š**DiRL æ¯” TraceRL å¿« 2.55Ã—**
  - è®­ç»ƒå»¶è¿Ÿï¼š**8B æ¨¡å‹çš„ DiRL æ¯” 1.7B çš„ TraceRL è¿˜ä½**
  - Rollout æ•ˆç‡æå‡ 1.08Ã—ï¼Œè®­ç»ƒéƒ¨åˆ†æé€Ÿè¾¾ 5.94Ã—

---

### æ¶ˆèå®éªŒç»“æœï¼ˆFigure 8ï¼‰

è¿›è¡Œäº†å…³äº **dynamic decoding threshold Ï„** çš„æ¶ˆèç ”ç©¶ï¼ˆÏ„ âˆˆ [0.5, 0.99]ï¼‰ï¼š

- **DiRL-8B-Instruct è¡¨ç°æœ€ç¨³å®šä¸”æŒç»­é¢†å…ˆ**ï¼š
  - éšç€ Ï„ å¢åŠ ï¼ˆè§£ç æ›´ä¿å®ˆï¼‰ï¼Œæ€§èƒ½ç¨³æ­¥ä¸Šå‡ï¼Œåœ¨ Ï„=0.9 æ—¶è¾¾åˆ°æœ€ä¼˜ã€‚
- **å…¶ä»–æ¨¡å‹è¡¨ç°ä¸ç¨³å®š**ï¼š
  - SDAR-8B-Chat å‡ ä¹æ— æå‡ï¼Œè¡¨æ˜å…¶æ¨ç†èƒ½åŠ›å—é™ã€‚
  - TraDo-8B-Instruct åœ¨ AIME2024 ä¸Šæ³¢åŠ¨å‰§çƒˆï¼Œæ˜¾ç¤ºå…¶é²æ£’æ€§è¾ƒå·®ã€‚

> ğŸ” ç»“è®ºï¼šDiRL çš„ä¼˜è¶Šæ€§èƒ½æ¥æºäºé«˜è´¨é‡çš„ SFT+RL æ¨ç†è·¯å¾„ï¼Œè€Œéç‰¹å®šè¶…å‚é…ç½®ï¼Œå…·æœ‰å¼ºæ³›åŒ–æ€§å’Œç¨³å®šæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **DiRL æ˜¯é¦–ä¸ªå®ç°é«˜æ•ˆã€æ— åã€å¯æ‰©å±• dLLM RL çš„æ¡†æ¶**ï¼Œè§£å†³äº†è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´çš„æ ¹æœ¬é—®é¢˜ã€‚
2. âœ… **DiPO æˆåŠŸå°† GRPO ç§»æ¤åˆ° dLLM ä¸Š**ï¼Œå¹¶åˆ©ç”¨ blockwise ç»“æ„å®ç°ç²¾ç¡® logits è®¡ç®—ï¼Œé¿å…è¿‘ä¼¼è¯¯å·®ã€‚
3. âœ… **DiRL-8B-Instruct æˆä¸ºå½“å‰æœ€å¼ºçš„æ•°å­¦æ¨ç† dLLM**ï¼Œä¸ä»…è¶…è¶ŠåŒç±»æ‰©æ•£æ¨¡å‹ï¼Œç”šè‡³ä¼˜äºæ›´å¤§è§„æ¨¡çš„ AR æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-32Bï¼‰ã€‚
4. âœ… **ç³»ç»Ÿçº§ä¼˜åŒ–å¸¦æ¥å·¨å¤§æ•ˆç‡å¢ç›Š**ï¼šFlexAttention + LMDeploy + åœ¨çº¿æ›´æ–°ç»„åˆä½¿æ•´ä½“è®­ç»ƒååæå‡è¶…è¿‡ 2.5Ã—ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

1. **ä¸Šä¸‹æ–‡é•¿åº¦ä»æœ‰é™**ï¼šå°½ç®¡ 8k æ˜¯å½“å‰ dLLM æœ€é•¿è®°å½•ï¼Œä½†ä»è¿œå°äºä¸»æµ AR æ¨¡å‹ï¼ˆå¦‚ 32k+ï¼‰ã€‚
2. **å°šæœªæ¢ç´¢ Test-Time Scaling æˆ– Chain-of-Thought æ‰©å±•æŠ€æœ¯**ï¼Œå¯èƒ½è¿›ä¸€æ­¥é‡Šæ”¾æ½œåŠ›ã€‚
3. **å½“å‰èšç„¦æ•°å­¦ä»»åŠ¡**ï¼Œåœ¨ä»£ç ã€Agent ç­‰é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯ã€‚
4. **ä¾èµ– blockwise æ¶æ„**ï¼Œä¸é€‚ç”¨äºå…¨åŒå‘ dLLMsã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³æ›´å¤§è§„æ¨¡æ¨¡å‹**ï¼ˆ>8Bï¼‰ï¼Œè¿½æ±‚æ›´å¼ºæ€§èƒ½ã€‚
2. **å»¶é•¿æ¨ç†é•¿åº¦è‡³ 32k+**ï¼Œæ¢ç´¢ long CoT å’Œ test-time scaling æŠ€æœ¯ã€‚
3. **å¼•å…¥ dynamic packing ç­‰é•¿æ–‡æœ¬è®­ç»ƒç­–ç•¥**ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿè®­ç»ƒã€‚
4. **æ‹“å±•è‡³ code generation å’Œ agentic tasks**ï¼Œæ„å»ºé€šç”¨ dLLM å¯¹é½å·¥å…·é“¾ã€‚
5. **å¼€æºå®Œæ•´çš„ post-training toolkit**ï¼Œæ¨åŠ¨ç¤¾åŒºå‘å±•ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> DiRL é€šè¿‡ç®—æ³•ï¼ˆDiPOï¼‰+ ç³»ç»Ÿï¼ˆFlexAttention + LMDeployï¼‰ååŒåˆ›æ–°ï¼Œé¦–æ¬¡å®ç°äº†é«˜æ•ˆã€æ— åã€å¯è½åœ°çš„ dLLM åè®­ç»ƒæ¡†æ¶ï¼Œè®­ç»ƒå‡ºå½“å‰æœ€å¼ºæ•°å­¦æ¨ç† dLLM â€”â€” **DiRL-8B-Instruct**ï¼Œå¹¶åœ¨æ€§èƒ½ä¸æ•ˆç‡ä¸Šå…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

</details>

---

### 5. [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)

**Authors**: Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.23236v1  

#### Abstract
Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEv...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šKernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°ä»£æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆDLRMï¼‰åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´ä¸‰å¤§ç³»ç»ŸæŒ‘æˆ˜ï¼š
- **æ¨¡å‹æ¶æ„å¤šæ ·æ€§**ï¼šä»ä¼ ç»ŸMLPåˆ°Transformerã€å·ç§¯å¢å¼ºæ¶æ„ç­‰ï¼Œè®¡ç®—æ¨¡å¼å·®å¼‚å·¨å¤§ã€‚
- **ç®—å­å¤šæ ·æ€§**ï¼šé™¤GEMMå¤–ï¼Œå­˜åœ¨è¶…è¿‡200ç§æ•°æ®é¢„å¤„ç†ç®—å­ï¼ˆå¦‚MapIdã€MBDTï¼‰ï¼Œè¿™äº›ç®—å­é€šå¸¸å†…å­˜å¯†é›†ä¸”æ§åˆ¶æµå¤æ‚ã€‚
- **ç¡¬ä»¶å¼‚æ„æ€§**ï¼šè·¨å‚å•†ï¼ˆNVIDIAã€AMDï¼‰å’Œè‡ªç ”èŠ¯ç‰‡ï¼ˆMTIA v3ï¼‰çš„åŠ é€Ÿå™¨åœ¨å†…å­˜å±‚æ¬¡ã€ç¼–ç¨‹æ¨¡å‹å’Œä»£é™…ç‰¹æ€§ä¸Šå·®å¼‚æ˜¾è‘—ã€‚

è¿™ä¸‰è€…çš„ç»„åˆå¯¼è‡´äº†ä¸€ä¸ªå·¨å¤§çš„ä¼˜åŒ–ç©ºé—´ï¼Œä½¿å¾—æ‰‹åŠ¨å†…æ ¸å¼€å‘æˆæœ¬é«˜æ˜‚ï¼ˆæ¯ä¸ªå†…æ ¸éœ€2â€“8å‘¨ä¸“å®¶è°ƒä¼˜ï¼‰ï¼Œéš¾ä»¥æ‰©å±•ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹
è®ºæ–‡æå‡ºäº† **KernelEvolve**ï¼Œä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“ï¼ˆagenticï¼‰çš„è‡ªåŠ¨åŒ–å†…æ ¸ç”Ÿæˆä¸ä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰ç»Ÿä¸€çš„å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶
- å°†å†…æ ¸ä¼˜åŒ–å»ºæ¨¡ä¸º**å›¾æœç´¢é—®é¢˜**ï¼ˆgraph-based searchï¼‰ï¼Œé€šè¿‡é€‰æ‹©ç­–ç•¥ï¼ˆselection policyï¼‰ã€é€šç”¨å˜æ¢ç®—å­ï¼ˆuniversal operatorï¼‰ã€é€‚åº”åº¦å‡½æ•°ï¼ˆfitness functionï¼‰å’Œç»ˆæ­¢è§„åˆ™è¿›è¡Œç³»ç»ŸåŒ–æ¢ç´¢ã€‚
- å¼•å…¥**æ£€ç´¢å¢å¼ºæç¤ºåˆæˆ**ï¼ˆretrieval-augmented prompt synthesisï¼‰ï¼ŒåŠ¨æ€æ³¨å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡LLMç”Ÿæˆè´¨é‡ã€‚

#### ï¼ˆ2ï¼‰æŒä¹…åŒ–çŸ¥è¯†åº“æ”¯æŒä¸“æœ‰ç¡¬ä»¶
- æ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„**æŒä¹…çŸ¥è¯†åº“**ï¼ˆpersistent knowledge baseï¼‰ï¼Œç¼–ç äº†NVIDIAã€AMDå’ŒMTIAç­‰å¹³å°çš„ç¡¬ä»¶çº¦æŸã€ä¼˜åŒ–æ¨¡å¼å’Œè°ƒè¯•æŒ‡å—ã€‚
- ç‰¹åˆ«é’ˆå¯¹Metaè‡ªç ”çš„MTIAèŠ¯ç‰‡ï¼Œæ³¨å…¥äº†å…¶ç‰¹æœ‰çš„Tritonæ‰©å±•ï¼ˆå¦‚SFUæŒ‡ä»¤ã€åŒæ ¸åŒæ­¥ã€è·¨PEé€šä¿¡åŸè¯­ï¼‰ï¼Œè§£å†³äº†LLMç¼ºä¹ä¸“æœ‰ç¡¬ä»¶å…ˆéªŒçŸ¥è¯†çš„é—®é¢˜ã€‚

#### ï¼ˆ3ï¼‰ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹
- å®ç°äº†ä»å†…æ ¸ç”Ÿæˆã€ç¼–è¯‘ã€æ­£ç¡®æ€§éªŒè¯ã€æ€§èƒ½è¯„æµ‹åˆ°è‡ªåŠ¨è°ƒè¯•çš„å…¨æµç¨‹é—­ç¯ã€‚
- æ”¯æŒå¤šç§ç¼–ç¨‹æŠ½è±¡ï¼šTritonã€CuTe DSLã€ä½çº§è¯Šæ–­è¯­è¨€ï¼Œå¹¶èƒ½è·¨å¹³å°ç”Ÿæˆä»£ç ï¼ˆNVIDIAã€AMDã€MTIAï¼‰ã€‚

#### ï¼ˆ4ï¼‰é€šç”¨å˜æ¢ç®—å­è®¾è®¡
- æå‡ºâ€œ**å•ä¸€é€šç”¨ç®—å­**â€ï¼ˆuniversal operatorï¼‰æ›¿ä»£ä¼ ç»Ÿçš„å¤šé˜¶æ®µç®—å­ï¼ˆå¦‚Draft/Debug/Improveï¼‰ï¼Œé¿å…é™æ€æç¤ºæ¨¡æ¿çš„è®¤çŸ¥åå·®ã€‚
- åŠ¨æ€æ ¹æ®è¿è¡Œæ—¶åé¦ˆï¼ˆprofilingã€é”™è¯¯æ—¥å¿—ï¼‰è°ƒæ•´æç¤ºå†…å®¹ï¼Œå®ç°æ›´çµæ´»çš„ä¼˜åŒ–è·¯å¾„æ¢ç´¢ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚AutoTritonã€Kevinã€GEAKï¼‰ | KernelEvolve |
|------|----------------------------------------|-------------|
| **ç¡¬ä»¶æ”¯æŒ** | å•ä¸€å¹³å°ï¼ˆé€šå¸¸æ˜¯NVIDIAï¼‰ | è·¨å¹³å°ï¼šNVIDIAã€AMDã€MTIA |
| **è¯„ä¼°çœŸå®æ€§** | åˆæˆç®—å­ã€å›ºå®šå½¢çŠ¶ | ç”Ÿäº§çº§åŠ¨æ€æ‰¹å¤„ç†ã€å¯å˜åºåˆ—é•¿åº¦ |
| **ä»£ç†èƒ½åŠ›** | ç¼ºä¹å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç† | å…¨æµç¨‹ï¼šç”Ÿæˆâ†’éªŒè¯â†’è¯„æµ‹â†’è°ƒè¯• |
| **æœç´¢è§„æ¨¡** | å°‘é‡è¿­ä»£ | æ”¯æŒå¤§è§„æ¨¡æœç´¢ï¼ˆæ•°ç™¾è‡³æ•°åƒæ­¥ï¼‰ |
| **çŸ¥è¯†åˆ©ç”¨** | é™æ€æç¤º | æ£€ç´¢å¢å¼º + æŒä¹…åŒ–çŸ¥è¯†åº“ |
| **éƒ¨ç½²é›†æˆ** | ç ”ç©¶åŸå‹ | å·²åœ¨Metaç”Ÿäº§ç¯å¢ƒæŒç»­è¿è¡Œ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä¸æµ‹è¯•ç”¨ä¾‹
- **KernelBench**ï¼šå…¬å¼€åŸºå‡†å¥—ä»¶ï¼ŒåŒ…å«250ä¸ªé—®é¢˜ï¼Œè¦†ç›–ä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼ˆå•ç®—å­ã€èåˆã€å…¨æ¨¡å—ï¼‰ã€‚ç”¨äºéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚
- **160ä¸ª PyTorch ATen ç®—å­**ï¼šæ¶µç›–element-wiseã€reductionã€activationç­‰åŸºç¡€æ“ä½œï¼Œä½œä¸ºåŠŸèƒ½æ­£ç¡®æ€§çš„åŸºç¡€éªŒè¯ã€‚
- **çœŸå®ç”Ÿäº§æ¨¡å‹**ï¼šåŒ…æ‹¬ï¼š
  - Convolutional Transformerï¼ˆ1D/2Då·ç§¯ï¼‰
  - WuKong å’Œ InterFormer æ¨èæ¨¡å‹ä¸­çš„FM/PFFNæ¨¡å—
  - æ•°æ®é¢„å¤„ç†ç®—å­ï¼šMapIdã€MergeBucketizedDenseTransform (MBDT)
  - åºåˆ—å­¦ä¹ ç®—å­ï¼šBatch Event Truncate

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼š
  - NVIDIA H100 / A100
  - AMD MI300 / MI350
  - Meta MTIA v2i / v3
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **æ­£ç¡®æ€§**ï¼š`torch.allclose()` æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆatol=1e-4, rtol=5e-4ï¼‰
  - **æ€§èƒ½**ï¼šç›¸å¯¹äºPyTorchåŸºçº¿çš„**åŠ é€Ÿæ¯”**ï¼ˆspeedupï¼‰
  - **å¼€å‘æ•ˆç‡**ï¼šå¼€å‘æ—¶é—´ä»æ•°å‘¨ç¼©çŸ­è‡³å°æ—¶çº§åˆ«
  - **è¦†ç›–ç‡**ï¼šæ”¯æŒçš„ç®—å­æ•°é‡åŠè·¨å¹³å°å…¼å®¹æ€§

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ | æè¿° |
|------|------|
| `torch.nn.functional.conv1d` | æ ‡å‡†PyTorch 1Då·ç§¯ |
| `torch.nn.functional.conv2d`ï¼ˆNHWCï¼‰ | å¸¸è§ä¼˜åŒ–æŠ€å·§ï¼šreshapeä¸º2Då¹¶ä½¿ç”¨cuDNNä¼˜åŒ–è·¯å¾„ |
| `torch.compile` + é»˜è®¤è°ƒåº¦ | PyTorchè‡ªå¸¦ç¼–è¯‘å™¨ä¼˜åŒ– |
| æ‰‹åŠ¨è°ƒä¼˜å†…æ ¸ | Metaä¸“å®¶æ‰‹å·¥ç¼–å†™çš„é«˜åº¦ä¼˜åŒ–å†…æ ¸ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»
| ç®—å­ç±»åˆ« | åœºæ™¯ | åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰ |
|--------|------|------------------|
| **LLMæ¨ç†** | Llama-3.1-8B Vanilla Attention | 4.6Ã— |
| | SDPA-MLP | 3.3Ã— |
| **å·ç§¯Transformer** | conv1d | 6.5Ã— |
| | conv2d | 4.7Ã— |
| **æ•°æ®é¢„å¤„ç†** | MapId | 4.1Ã— |
| | MBDT | 9.3Ã— |
| | Batch Event Truncate | 9.8Ã— |
| **æ¨èæ¨¡å‹èåˆ** | WuKong Optimized FM | 4.0Ã— |
| | InterFormer PFFN | 2.5Ã— |
| **MTIAä¸“ç”¨ä¼˜åŒ–** | RMSNorm 2D backward | **17Ã—** |
| | Sparse Inverted Index | 1.25Ã— |

> å›¾4æ˜¾ç¤ºæ•´ä½“åŠ é€ŸèŒƒå›´ä¸º **1.25â€“17Ã—**ï¼Œè¿œè¶…PyTorchåŸºçº¿ã€‚

### æ­£ç¡®æ€§ä¸è¦†ç›–ç‡
- åœ¨ **480ä¸ªç®—å­-å¹³å°é…ç½®**ï¼ˆ160ç®—å­ Ã— 3å¹³å°ï¼‰ä¸Šå®ç°äº† **100%æ­£ç¡®ç‡**ã€‚
- åœ¨ **KernelBench** ä¸Šè¾¾åˆ° **100%é€šè¿‡ç‡**ï¼ˆæ‰€æœ‰250é¢˜ï¼‰ã€‚
- æˆåŠŸä¸ºMTIA v3ç”Ÿæˆäº†ç¼ºå¤±çš„ATenç®—å­ï¼ˆå¦‚`clamp.out`, `_unique2`, `unique_consecutive`ï¼‰ï¼Œè§£å†³äº†æ¨¡å‹æ— æ³•éƒ¨ç½²çš„é—®é¢˜ã€‚

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åœºæ™¯ | KernelEvolve vs. `conv1d` | vs. `conv2d`ï¼ˆNHWCï¼‰ |
|------|----------------------------|------------------------|
| conv1d on H100 (FP16) | **2.30Ã—** | **1.62Ã—** |
| conv1d on MTIA v3 | **6.54Ã—** | **4.71Ã—** |

è¯´æ˜KernelEvolveä¸ä»…ä¼˜äºæ ‡å‡†å®ç°ï¼Œåœ¨ä¸“æœ‰ç¡¬ä»¶ä¸Šä¹Ÿæ˜¾è‘—è¶…è¶Šå‚å•†ä¼˜åŒ–åº“ã€‚

### æ¶ˆèå®éªŒä¸ä¼˜åŒ–è½¨è¿¹åˆ†æ
- **å›¾10** å±•ç¤ºäº†ATenç®—å­çš„ä¼˜åŒ–è½¨è¿¹ï¼š
  - åˆå§‹é˜¶æ®µï¼ˆdraft phaseï¼‰éšæœºé‡‡æ ·ï¼Œæ€§èƒ½ä¸€èˆ¬ï¼›
  - è¿›å…¥æ ‘æœç´¢åï¼Œç»“åˆæ‰§è¡Œåé¦ˆé€æ­¥æå‡ï¼Œæœ€ç»ˆæ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚
- **å›¾12** æ˜¾ç¤ºconv1då†…æ ¸ç»è¿‡300æ¬¡æœç´¢åï¼Œæ€§èƒ½ä»çº¦2000æå‡è‡³6889ï¼ˆfitness scoreï¼‰ï¼Œè¯æ˜äº†æœç´¢çš„æœ‰æ•ˆæ€§ã€‚
- åˆ†æè¡¨æ˜ï¼Œ**ç®—å­èåˆ**ï¼ˆå¦‚å°†å¤šä¸ªlayout transformåˆå¹¶ä¸ºä¸€ä¸ªkernelï¼‰æ˜¯ä¸»è¦æ€§èƒ½æ¥æºä¹‹ä¸€ï¼Œå‡å°‘äº†ä¸­é—´å†…å­˜è®¿é—®å¼€é”€ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è‡ªåŠ¨åŒ–å†…æ ¸ç”Ÿæˆå¯è¡Œä¸”é«˜æ•ˆ**ï¼š
   - KernelEvolveèƒ½åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ç”Ÿæˆ**ç”Ÿäº§çº§è´¨é‡**çš„é«˜æ€§èƒ½å†…æ ¸ã€‚
   - å¼€å‘å‘¨æœŸä»**æ•°å‘¨ç¼©çŸ­è‡³æ•°å°æ—¶**ï¼Œæå¤§æå‡äº†ç ”å‘æ•ˆç‡ã€‚

2. **ç®—å­èåˆæ˜¯å…³é”®ä¼˜åŒ–æ‰‹æ®µ**ï¼š
   - å¯¹äºå†…å­˜å¯†é›†å‹ç®—å­ï¼ˆå¦‚é¢„å¤„ç†ã€åºåˆ—æˆªæ–­ï¼‰ï¼Œå‡å°‘ä¸­é—´å¼ é‡å†™å›HBMæ˜¯æ€§èƒ½çªç ´çš„å…³é”®ã€‚
   - ä¾‹å¦‚Batch Event Truncateé€šè¿‡æ‰¹é‡å¹¶è¡Œå¤„ç†å¤šä¸ªç‰¹å¾ï¼Œå®ç°é«˜è¾¾14.5Ã—åŠ é€Ÿã€‚

3. **ä¸“æœ‰ç¡¬ä»¶å¯é€šè¿‡çŸ¥è¯†æ³¨å…¥æ”¯æŒ**ï¼š
   - MTIAç­‰ç§æœ‰æ¶æ„è™½ä¸åœ¨LLMè®­ç»ƒè¯­æ–™ä¸­ï¼Œä½†é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†åº“æ³¨å…¥ï¼ˆå¦‚SFUã€åŒæ ¸ã€è·¨PEé€šä¿¡ï¼‰ï¼Œä»å¯ç”Ÿæˆé«˜è´¨é‡å†…æ ¸ã€‚
   - è¿™ä¸ºæœªæ¥æ–°å‹AIèŠ¯ç‰‡æä¾›äº†å¿«é€Ÿè½¯ä»¶é€‚é…è·¯å¾„ã€‚

4. **å¼‚æ„ç¡¬ä»¶éœ€è¦å·®å¼‚åŒ–ä¼˜åŒ–ç­–ç•¥**ï¼š
   - NVIDIAä¾èµ–Tensor Coreå’ŒTMAï¼›
   - AMDåˆ©ç”¨Infinity Cacheï¼›
   - MTIAåˆ™éœ€å¯ç”¨SFUå’ŒPEé—´é€šä¿¡ã€‚
   - KernelEvolveèƒ½æ ¹æ®å¹³å°è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å¯¹LLMèƒ½åŠ›é«˜åº¦ä¾èµ–**ï¼šè‹¥åº•å±‚LLMç”Ÿæˆèƒ½åŠ›ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´æœç´¢æ•ˆç‡ä¸‹é™ã€‚
- **é•¿å°¾ç®—å­è¦†ç›–ä»éœ€å®Œå–„**ï¼šå°½ç®¡å·²æ”¯æŒæ•°ç™¾ç®—å­ï¼Œä»æœ‰éƒ¨åˆ†è¾¹ç¼˜æƒ…å†µæœªå®Œå…¨è¦†ç›–ã€‚
- **èµ„æºæ¶ˆè€—è¾ƒå¤§**ï¼šå¤§è§„æ¨¡æœç´¢å¯èƒ½æ¶‰åŠæ•°åƒæ¬¡LLMè°ƒç”¨ï¼Œå¸¦æ¥è¾ƒé«˜çš„tokenæˆæœ¬ã€‚
- **ä¸é€‚ç”¨äºæç«¯ä½å»¶è¿Ÿåœºæ™¯**ï¼šæŸäº›å¾®ç§’çº§å…³é”®è·¯å¾„ä»éœ€ä¸“å®¶æ‰‹å†™æ±‡ç¼–çº§ä¼˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **ä»ç®—å­çº§å‘æ¨¡å‹çº§æ‰©å±•**ï¼š
   - å°†ä¼˜åŒ–ç²’åº¦ä»å•ä¸ªç®—å­æå‡è‡³æ•´ä¸ªæ¨¡å‹å›¾ï¼Œæ”¯æŒè·¨å±‚èåˆä¸å…¨å±€å†…å­˜è§„åˆ’ã€‚

2. **æ›´æ·±çš„ä»£ç ç”Ÿæˆèƒ½åŠ›**ï¼š
   - æ”¯æŒMLIRæ–¹è¨€ä¿®æ”¹ã€PTX/SASSçº§ç”Ÿæˆï¼Œçªç ´TritonæŠ½è±¡é™åˆ¶ã€‚

3. **å¤§è§„æ¨¡å¹¶è¡Œæœç´¢**ï¼š
   - åˆ©ç”¨åˆ†å¸ƒå¼åŸºç¡€è®¾æ–½åŒæ—¶æ¢ç´¢æ•°åƒå€™é€‰æ–¹æ¡ˆï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚

4. **å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è‡ªé€‚åº”ä¼˜åŒ–**ï¼š
   - åŸºäºæ‰§è¡Œåé¦ˆè¿›è¡Œåœ¨çº¿å¾®è°ƒï¼Œä½¿ä»£ç†èƒ½æ›´å¿«é€‚åº”æ–°ç¡¬ä»¶ã€‚

5. **å¯æŒç»­AIåŸºç¡€è®¾æ–½**ï¼š
   - é‡åŒ–tokenæ¶ˆè€—ä¸ç¢³è¶³è¿¹ï¼Œå¹³è¡¡ä¼˜åŒ–æ·±åº¦ä¸ç¯å¢ƒå½±å“ã€‚

6. **ç«¯åˆ°ç«¯é›†æˆ**ï¼š
   - ä¸AOT Inductorã€æ¨¡å‹æœåŠ¡ç³»ç»Ÿå¯¹æ¥ï¼Œå®ç°â€œå®šä¹‰å³éƒ¨ç½²â€çš„é—­ç¯æµç¨‹ã€‚

---

> **æ€»ç»“**ï¼š  
> KernelEvolve æ˜¯é¦–ä¸ªåœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡éƒ¨ç½²çš„LLMé©±åŠ¨å†…æ ¸ç”Ÿæˆç³»ç»Ÿï¼Œå®ƒæˆåŠŸåº”å¯¹äº†æ¨èç³»ç»Ÿä¸­â€œæ¨¡å‹-ç®—å­-ç¡¬ä»¶â€ä¸‰ç»´å¼‚æ„å¸¦æ¥çš„ä¼˜åŒ–å±æœºã€‚é€šè¿‡å¼•å…¥**å›¾æœç´¢ + æ£€ç´¢å¢å¼º + æŒä¹…çŸ¥è¯†åº“**çš„æ¶æ„ï¼Œå®ç°äº†è·¨å¹³å°ã€é«˜æ­£ç¡®æ€§ã€é«˜æ€§èƒ½çš„è‡ªåŠ¨åŒ–å†…æ ¸ç”Ÿæˆï¼Œåœ¨MTIAã€NVIDIAã€AMDç­‰å¤šç§ç¡¬ä»¶ä¸Šå–å¾—æœ€é«˜è¾¾17å€çš„åŠ é€Ÿï¼Œæ ‡å¿—ç€AIç³»ç»Ÿè½¯ä»¶è¿›å…¥â€œæ™ºèƒ½ç¼–è¯‘â€æ–°æ—¶ä»£ã€‚

</details>

---

### 6. [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)

**Authors**: Kun-Woo Shin (Seoul National University, Korea), Jay H. Park (Samsung Electronics, Korea), Moonwook Oh (Samsung Electronics, Korea), Yohan Jo (Seoul National University, Korea), Jaeyoung Do (Seoul National University, Korea), Sang-Won Lee (Seoul National University, Korea)  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.22195v1  

#### Abstract
We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing th...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMatKV: Trading Compute for Flash Storage in LLM Inference

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼ AI åº”ç”¨ä¸­ï¼Œ**æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰** å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œç”¨äºç¼“è§£å¹»è§‰ã€æå‡çŸ¥è¯†æ—¶æ•ˆæ€§å’Œå®‰å…¨æ€§ã€‚ç„¶è€Œï¼ŒRAG æ¨ç†ä¸­çš„ **prefill é˜¶æ®µ**ï¼ˆå³å¯¹è¾“å…¥æ–‡æœ¬è®¡ç®— Key-Value å‘é‡ï¼‰éå¸¸è€—æ—¶ä¸”èƒ½è€—é«˜ï¼Œå°¤å…¶å½“å¤„ç†é•¿æ–‡æœ¬å—ï¼ˆå¦‚ 1k+ tokensï¼‰æ—¶ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–‡æ¡£è¢«é‡å¤æ£€ç´¢ï¼Œå¯¼è‡´å¤§é‡å†—ä½™çš„ KV è®¡ç®—ã€‚

ä¼ ç»Ÿæ–¹æ³•ä¾èµ–é«˜æ€§èƒ½ GPU å®æ—¶è®¡ç®— KV ç¼“å­˜ï¼Œé€ æˆå·¨å¤§çš„è®¡ç®—å¼€é”€å’Œèƒ½æºæµªè´¹ã€‚

---

### âœ… æå‡ºçš„æ–°æ–¹æ³•ï¼šMatKV
ä½œè€…æå‡º **MatKV**ï¼Œä¸€ç§é€šè¿‡ **ç”¨å­˜å‚¨æ¢è®¡ç®—ï¼ˆtrading compute for storageï¼‰** æ¥ä¼˜åŒ– RAG æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ¡ˆï¼š

- **é¢„è®¡ç®—å¹¶ç‰©åŒ–ï¼ˆmaterializeï¼‰KV ç¼“å­˜**ï¼šåœ¨å°†æ–‡æ¡£æ’å…¥å‘é‡æ•°æ®åº“æ—¶ï¼Œä¸€æ¬¡æ€§ä½¿ç”¨ GPU è®¡ç®—å…¶ KV ç¼“å­˜ï¼Œå¹¶å°†å…¶æŒä¹…åŒ–åˆ°ä½æˆæœ¬ã€é«˜é€Ÿçš„ **flash å­˜å‚¨ï¼ˆSSDï¼‰** ä¸Šã€‚
- **æ¨ç†æ—¶ç›´æ¥åŠ è½½è€Œéé‡ç®—**ï¼šå½“è¯¥æ–‡æ¡£è¢«æ£€ç´¢åˆ°åï¼Œç›´æ¥ä» SSD åŠ è½½é¢„è®¡ç®—çš„ KV ç¼“å­˜è‡³ GPU HBMï¼Œè·³è¿‡æ˜‚è´µçš„ prefill é˜¶æ®µã€‚
- **è§£è€¦ prefill ä¸ decode é˜¶æ®µ**ï¼šå…è®¸ GPU åœ¨è§£ç å½“å‰è¯·æ±‚çš„åŒæ—¶ï¼Œå¼‚æ­¥åŠ è½½ä¸‹ä¸€ä¸ªè¯·æ±‚æ‰€éœ€çš„ KV ç¼“å­˜ï¼Œå®ç°æµæ°´çº¿å¹¶è¡Œã€‚

---

### âœ… ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| å¯¹æ¯”ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆVanillaï¼‰ | MatKV |
|--------|------------------|-------|
| **è®¡ç®—æ¨¡å¼** | æ¯æ¬¡éƒ½é‡æ–°è®¡ç®— KV | å¤ç”¨é¢„è®¡ç®— KV |
| **å­˜å‚¨ä½ç½®** | å†…å­˜ç¼“å­˜ï¼ˆDRAMï¼‰æˆ–ä¸ç¼“å­˜ | å¤–éƒ¨ flash å­˜å‚¨ï¼ˆSSDï¼‰ |
| **æˆæœ¬æ•ˆç›Š** | é«˜ GPU æˆæœ¬ï¼Œé¢‘ç¹é‡å¤è®¡ç®— | ä¸€æ¬¡è®¡ç®— + å¤šæ¬¡å¤ç”¨ï¼Œé•¿æœŸæ›´ç»æµ |
| **èƒ½æ•ˆ** | é«˜åŠŸè€—ï¼ˆ~350W H100ï¼‰ | æä½åŠŸè€—ï¼ˆSSD ä»… ~7Wï¼‰ |
| **å¯æ‰©å±•æ€§** | å—é™äº DRAM å®¹é‡å’Œä»·æ ¼ | æ”¯æŒ TB/PB çº§ KV å­˜å‚¨ |
| **ç³»ç»Ÿä¼˜åŒ–æ½œåŠ›** | ä¸²è¡Œæ‰§è¡Œ prefill å’Œ decode | æ”¯æŒ prefill I/O ä¸ decode å¹¶å‘ |

> ğŸ”‘ åˆ›æ–°ç‚¹æ€»ç»“ï¼š
> - é¦–æ¬¡ç³»ç»Ÿæ€§è®ºè¯ **å°† KV ç¼“å­˜ç‰©åŒ–åˆ°å¤–éƒ¨ SSD è€Œé DRAM æ˜¯å¯è¡Œä¸”é«˜æ•ˆ** çš„ï¼›
> - æå‡º **â€œåå¤©è§„åˆ™â€ï¼ˆTen-Day Ruleï¼‰**ï¼Œä¸ºæ˜¯å¦ç‰©åŒ–æä¾›ç»æµå­¦å†³ç­–ä¾æ®ï¼›
> - å®ç° **prefill ä¸ decode çš„æ—¶é—´é‡å **ï¼Œæ˜¾è‘—é™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿï¼›
> - æ”¯æŒ **ä½ç«¯ GPU è§£ç **ï¼Œå¤§å¹…é™ä½éƒ¨ç½²é—¨æ§›ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **TurboRAG**ï¼šç”¨äºæµ‹é‡ç³»ç»Ÿå»¶è¿Ÿå’ŒåŠŸè€—ï¼Œå¹³å‡æŸ¥è¯¢ 17.67 tokensï¼Œæ–‡æ¡£ 767.73 tokensã€‚
- **LongBench**ï¼šç”¨äºè¯„ä¼°é—®ç­”å‡†ç¡®æ€§ï¼ŒåŒ…å«ï¼š
  - `2WikiMultihopQA`ï¼ˆå¤šè·³æ¨ç†ï¼‰
  - `HotpotQA`
  - `TriviaQA`

### ğŸ’» å®éªŒç¡¬ä»¶é…ç½®
| ç»„ä»¶ | é…ç½® |
|------|------|
| **GPU** | NVIDIA H100ï¼ˆ80GB HBMï¼‰ï¼ŒRTX 4090ï¼ˆ24GBï¼‰ |
| **CPU** | Dual-socket Intel Xeon Gold 6442Y |
| **å†…å­˜** | 2TB DRAM |
| **SSD** | Samsung 9100 Proï¼ˆPCIe 5.0, 14GB/sï¼‰æˆ– PM9A3ï¼ˆ6.5GB/sï¼‰ |
| **å­˜å‚¨ç»„ç»‡** | å››å— 9100 Pro ç»„æˆ RAID-0ï¼Œæ€»å¸¦å®½è¾¾ 58.8GB/s |

### âš™ï¸ æ¨¡å‹
- LLaMA 3.1 ç³»åˆ—ï¼š**3B**, **8B**, **70B**ï¼ˆ4-bit é‡åŒ–ä»¥é€‚é… H100ï¼‰
- ä½¿ç”¨ Hugging Face Transformers åº“æ‰©å±•å®ç° MatKVï¼ˆ<500 è¡Œä»£ç ä¿®æ”¹ï¼‰

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Latency** | åˆ†è§£ä¸º Load Timeã€Prefill Timeã€Decode Time |
| **Power Consumption** | å…¨ç³»ç»ŸåŠŸè€—ï¼ˆIPMI æµ‹é‡ï¼‰ã€GPU åŠŸè€—ï¼ˆnvidia-smiï¼‰ |
| **Energy Usage** | æ€»èƒ½è€—ï¼ˆJoulesï¼‰= åŠŸç‡ Ã— æ—¶é—´ |
| **Throughput** | æ‰¹å¤„ç†ä¸‹çš„ååé‡ |
| **Accuracy** | F1 Scoreï¼ˆé—®ç­”ä»»åŠ¡ï¼‰ |

### ğŸ†š åŸºçº¿æ–¹æ³•
- **Vanilla**ï¼šæ ‡å‡†æ–¹å¼ï¼Œæ¯æ¬¡å®Œæ•´è®¡ç®— KVï¼ˆFull KV Recomputationï¼‰
- **CacheBlend**ï¼šéƒ¨åˆ†ç¼“å­˜ + èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¼é¡¾å‡†ç¡®ç‡ä¸é€Ÿåº¦
- **DRAM-based KV Cache**ï¼šä½œä¸ºç†æƒ³å¯¹ç…§ï¼ˆä½†æˆæœ¬è¿‡é«˜ä¸å¯è¡Œï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ€§èƒ½åŠ é€Ÿï¼ˆSpeedupï¼‰
| åœºæ™¯ | ç»“æœ |
|------|------|
| **å•è¯·æ±‚ï¼ˆH100 + 70Bï¼‰** | MatKV å°† prefill æ—¶é—´å‡å°‘ **>50%**ï¼ˆè§ Fig. 5ï¼‰ |
| **æ‰¹å¤„ç†ï¼ˆbatch size=10ï¼‰** | MatKV è¾¾åˆ°æœ€é«˜ **2x æ•´ä½“åŠ é€Ÿ**ï¼ˆè§ Fig. 6ï¼‰ |
| **é‡å æ‰§è¡Œï¼ˆOverlap Enabledï¼‰** | MatKV + overlap å®ç° **2x ç«¯åˆ°ç«¯åŠ é€Ÿ** vs Vanillaï¼ˆè§ Fig. 7ï¼‰ |
| **è¾“å…¥é•¿åº¦å¢åŠ ï¼ˆ4 chunksï¼‰** | MatKV ç›¸å¯¹ä¼˜åŠ¿è¿›ä¸€æ­¥æ‰©å¤§ï¼ˆè§ Fig. 8aï¼‰ |

> âœ… å½“è¾“å…¥è¶Šé•¿ã€batch è¶Šå¤§æ—¶ï¼Œprefill å æ¯”è¶Šé«˜ï¼ŒMatKV ä¼˜åŠ¿è¶Šæ˜æ˜¾ã€‚

---

### ğŸ”‹ èƒ½æºæ•ˆç‡ï¼ˆEnergy Efficiencyï¼‰
| é…ç½® | å³°å€¼åŠŸç‡ | å¹³å‡åŠŸç‡ | æ€»è€—æ—¶ | **æ€»èƒ½è€—** |
|------|---------|----------|--------|------------|
| Vanilla | 1,256 W | 1,038 W | 546 s | **566 kJ** |
| MatKV | 1,124 W | 947 W | 306 s | **289 kJ** |
| MatKV (w/ overlap) | 1,241 W | 979 W | 285 s | **279 kJ** |

> ğŸ’¡ MatKV **èƒ½è€—é™ä½è¶…è¿‡ä¸€åŠ**ï¼ˆ566 â†’ 279 kJï¼‰ï¼Œä¸»è¦å¾—ç›Šäºé¿å…äº† GPU å¯†é›†å‹ prefillã€‚

#### GPU å±‚é¢èƒ½è€—å¯¹æ¯”ï¼ˆTable Vï¼‰
| é…ç½® | å¹³å‡ GPU åŠŸç‡ | æ€» GPU èƒ½è€— |
|------|----------------|--------------|
| Vanilla | 340 W | 185 kJ |
| MatKV (overlap) | 336 W | **95 kJ** |

> âœ… GPU èƒ½è€—ä¹Ÿå‡ ä¹å‡åŠã€‚

---

### ğŸ’° ç»æµæ€§åˆ†æï¼šâ€œåå¤©è§„åˆ™â€ï¼ˆTen-Day Ruleï¼‰

ä½œè€…åŸºäº Jim Gray çš„ â€œäº”åˆ†è§„åˆ™â€ æ¨å¯¼å‡º **â€œåå¤©è§„åˆ™â€**ï¼š

> è‹¥ä¸€ä¸ªæ–‡æ¡£æ¯ **10 å¤©å†…è‡³å°‘è¢«è®¿é—®ä¸€æ¬¡**ï¼Œåˆ™å°†å…¶ KV ç¼“å­˜ç‰©åŒ–åˆ° SSD æ¯”æ¯æ¬¡éƒ½ç”¨ GPU é‡ç®—æ›´åˆ’ç®—ã€‚

- ç¤ºä¾‹ï¼šè‹¥æŸæ–‡æ¡£æ¯å°æ—¶è¢«è®¿é—®ä¸€æ¬¡ï¼ŒMatKV æˆæœ¬æ•ˆç‡æ˜¯é‡ç®—çš„ **100x æ›´ä¼˜**ã€‚
- å›¾ 2 æ˜¾ç¤ºï¼Œåœ¨çœŸå® RAG æŸ¥è¯¢åˆ†å¸ƒä¸­ï¼Œ**è¶…è¿‡ 10% çš„æ–‡æ¡£è¢«è®¿é—® â‰¥2 æ¬¡**ï¼Œè¡¨æ˜å¤§é‡æ–‡æ¡£æ»¡è¶³åå¤©è§„åˆ™ã€‚

---

### ğŸ¯ å‡†ç¡®æ€§è¯„ä¼°ï¼ˆF1 Scoreï¼‰

| Dataset | Vanilla | MatKV | CacheBlend |
|--------|---------|-------|------------|
| 2WikiMQA | 0.278 | 0.241 | 0.253 |
| TriviaQA | 0.786 | 0.812 | 0.826 |
| HotpotQA | 0.353 | 0.302 | 0.309 |

> ğŸ” è™½ç„¶ MatKV åœ¨æŸäº›ä»»åŠ¡ä¸Šç•¥æœ‰ä¸‹é™ï¼ˆå¦‚ 2WikiMQAï¼‰ï¼Œä½†åœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°æ¥è¿‘ç”šè‡³ç•¥ä¼˜äº Vanillaï¼Œä¸”è¿œå¿«äº CacheBlendã€‚

#### é™„åŠ å®éªŒï¼ˆChat Historyï¼‰
- åœ¨ `Locomo` æ•°æ®é›†æµ‹è¯•é•¿å¯¹è¯å†å² RAGï¼š
  - Vanilla: 0.423
  - MatKV: 0.419 â†’ **å‡ ä¹æ— æŸ**

---

### ğŸ”„ æ¶ˆèå®éªŒä¸å…¶ä»–å‘ç°

| å®éªŒ | å‘ç° |
|------|------|
| **ä¸åŒ SSD é…ç½®**ï¼ˆTable IIIï¼‰ | RAIDed SSD æ˜¾è‘—ä¼˜äºå•ç›˜ï¼›DRAM æœ€å¿«ä½†æˆæœ¬ä¸å¯æ¥å— |
| **ä¸åŒæ¨¡å‹å¤§å°**ï¼ˆFig. 9ï¼‰ | æ¨¡å‹è¶Šå¤§ï¼ˆ70Bï¼‰ï¼ŒMatKV æ”¶ç›Šè¶Šæ˜¾è‘—ï¼ˆprefill æˆæœ¬å¢é•¿å¿«äº KV å¤§å°ï¼‰ |
| **ä½é˜¶ GPU è§£ç **ï¼ˆFig. 10ï¼‰ | ä½¿ç”¨ä¾¿å®œ 30x çš„ RTX 4090ï¼ŒMatKV ä»…æ…¢ 1.5x vs H100 ä¸Šçš„ full recomputeï¼Œè€Œ Vanilla æ…¢ 3x â†’ **æ”¯æŒä½æˆæœ¬éƒ¨ç½²** |
| **ä¸ CacheBlend å¯¹æ¯”** | MatKV åŠ è½½é€Ÿåº¦å¿« 37%ï¼ŒTFTT å¿« 41%ï¼Œæ•´ä½“æ›´å¿«ï¼Œä½†å‡†ç¡®ç‡ç¨ä½ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦ç»“è®º
1. **MatKV æ˜¾è‘—æå‡ RAG æ¨ç†æ•ˆç‡**ï¼š
   - æ¨ç†æ—¶é—´å‡å°‘æœ€å¤šè¾¾ **2x**
   - ç³»ç»Ÿçº§èƒ½è€—é™ä½ **>50%**
2. **SSD ç‰©åŒ– KV ç¼“å­˜æ˜¯ç»æµå¯è¡Œçš„**ï¼š
   - æå‡º **â€œåå¤©è§„åˆ™â€**ï¼Œä¸ºç¼“å­˜ç­–ç•¥æä¾›ç†è®ºæŒ‡å¯¼
   - å³ä½¿é‡‡ç”¨ naÃ¯ve â€œå…¨ç‰©åŒ–â€ç­–ç•¥ï¼Œä»æ¯” GPU é‡ç®—æ›´çœæˆæœ¬
3. **æ”¯æŒæ–°å‹ç³»ç»Ÿä¼˜åŒ–**ï¼š
   - **prefill I/O ä¸ decode å¹¶å‘æ‰§è¡Œ**ï¼Œéšè—åŠ è½½å»¶è¿Ÿ
   - **ä½é˜¶ GPU å¯èƒœä»» decode**ï¼Œé™ä½éƒ¨ç½²æˆæœ¬
4. **å‡†ç¡®æ€§å½±å“æœ‰é™**ï¼š
   - åœ¨å¤šä¸ª QA ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æŸå¤±æå°ï¼Œéƒ¨åˆ†ä»»åŠ¡åè€Œæå‡
   - æ–‡æ¡£é—´æ—  cross-attention ä¸ä¸€å®šæŸå®³æ•ˆæœï¼Œquery-to-doc attention å·²è¶³å¤Ÿ

---

### âš ï¸ å±€é™æ€§
1. **å†·å¯åŠ¨é—®é¢˜**ï¼šé¦–æ¬¡æ’å…¥æ–‡æ¡£éœ€å®Œæ•´ prefillï¼Œå¯èƒ½å½±å“é¦–è®¿ä½“éªŒï¼ˆå¯é€šè¿‡æ‡’åŠ è½½ç¼“è§£ï¼‰
2. **é€‰æ‹©æ€§ç‰©åŒ–æœªå®ç°**ï¼šç›®å‰é‡‡ç”¨ â€œMaterialize-Allâ€ï¼Œæœªæ¥åº”å¼•å…¥åŸºäºçƒ­åº¦çš„ç¼“å­˜ç­–ç•¥
3. **KV å­˜å‚¨å¼€é”€å¤§**ï¼š70B æ¨¡å‹ä¸‹æ¯ä¸ª 1k token æ–‡æ¡£ KV å çº¦ 250MBï¼Œå¤§è§„æ¨¡éƒ¨ç½²éœ€å‹ç¼©æˆ–åˆ†å±‚å­˜å‚¨
4. **ä¾èµ–é«˜é€Ÿ SSD**ï¼šæ€§èƒ½å— SSD å¸¦å®½é™åˆ¶ï¼Œä½é€Ÿè®¾å¤‡æ”¶ç›Šä¸‹é™

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ™ºèƒ½ç¼“å­˜ç­–ç•¥**ï¼šåŸºäºè®¿é—®é¢‘ç‡/è¶‹åŠ¿åŠ¨æ€å†³å®šæ˜¯å¦ç‰©åŒ–ï¼ˆå¦‚é¢„æµ‹çƒ­ç‚¹æ–‡æ¡£ï¼‰
2. **KV å‹ç¼©æŠ€æœ¯é›†æˆ**ï¼šç»“åˆ MiniCacheã€xKV ç­‰å‹ç¼©æ–¹æ³•è¿›ä¸€æ­¥é™ä½æˆæœ¬
3. **åˆ†å±‚å­˜å‚¨æ¶æ„**ï¼šæ„å»º DRAM â†’ SSD â†’ HDD çš„ KV ç¼“å­˜å±‚çº§
4. **æ‰©å±•è‡³å…¶ä»–åœºæ™¯**ï¼š
   - Agent AI ä¸­å·¥å…·æ–‡æ¡£å¤ç”¨
   - Text-to-SQL ä¸­ schema æç¤ºå¤ç”¨
   - In-context Learning ä¸­ demonstration ç¼“å­˜
5. **æ”¯æŒå¢é‡æ›´æ–°**ï¼šæ–‡æ¡£å˜æ›´åå¦‚ä½•é«˜æ•ˆæ›´æ–° KV ç¼“å­˜

---

## âœ… æ€»ç»“
**MatKV å¼€è¾Ÿäº†ä¸€æ¡å…¨æ–°çš„ LLM æ¨ç†ä¼˜åŒ–è·¯å¾„ â€”â€” ä»¥å»‰ä»·ã€é«˜æ•ˆçš„ flash å­˜å‚¨æ›¿ä»£æ˜‚è´µçš„ GPU è®¡ç®—èµ„æº**ã€‚å®ƒä¸ä»…åœ¨æ€§èƒ½ã€èƒ½æ•ˆã€æˆæœ¬ä¸Šå…¨é¢è¶…è¶Šä¼ ç»Ÿæ–¹æ³•ï¼Œè¿˜æ¨åŠ¨äº†â€œå­˜å‚¨ä¸ºä¸­å¿ƒâ€çš„ç”Ÿæˆå¼ AI ç³»ç»Ÿè®¾è®¡èŒƒå¼è½¬å˜ã€‚éšç€ SSD å¸¦å®½æŒç»­å¿«é€Ÿå¢é•¿è€Œ GPU FLOPS æˆæœ¬ä¸‹é™è¶‹ç¼“ï¼ŒMatKV æ‰€ä»£è¡¨çš„æ–¹å‘æå…·å‰æ™¯ã€‚

</details>

---

### 7. [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)

**Authors**: Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.22178v1  

#### Abstract
The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demons...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠWireless Traffic Prediction with Large Language Modelã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰åŸºäº **Large Language Model (LLM)** çš„æ— çº¿æµé‡é¢„æµ‹æ–¹æ³•ï¼ˆå¦‚ TrafficLLMã€Time-LLMï¼‰ä¸»è¦å…³æ³¨**æ—¶é—´ç»´åº¦ä¸Šçš„åºåˆ—å»ºæ¨¡**ï¼Œè€Œå¿½ç•¥äº†åŸå¸‚çº§æ— çº¿ç½‘ç»œä¸­æ™®éå­˜åœ¨çš„**ç©ºé—´ä¾èµ–æ€§**ï¼ˆspatial dependenciesï¼‰ã€‚è¿™ç§å¿½ç•¥å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸åŒåŒºåŸŸå¼‚æ„äº¤é€šæ¨¡å¼æ—¶æ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥å®ç°é«˜ç²¾åº¦ã€å¯æ‰©å±•çš„åŸå¸‚çº§é¢„æµ‹ã€‚

æ­¤å¤–ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€å…¨å±€æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œæ— æ³•å…¼é¡¾**é€šç”¨æ€§ä¸åŒºåŸŸç‰¹å¼‚æ€§**ä¹‹é—´çš„å¹³è¡¡ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šTIDES
ä½œè€…æå‡ºäº†ä¸€ç§åä¸º **TIDES**ï¼ˆ**Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction**ï¼‰çš„æ–°å‹ LLM é©±åŠ¨æ¡†æ¶ï¼Œç”¨äºåŸå¸‚çº§æ— çº¿æµé‡é¢„æµ‹ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰Region-aware Modelingï¼ˆåŒºåŸŸæ„ŸçŸ¥å»ºæ¨¡ï¼‰
- é€šè¿‡èšç±»ç®—æ³•å°†åŸå¸‚åˆ’åˆ†ä¸ºè‹¥å¹²å…·æœ‰ç›¸ä¼¼æµé‡ç‰¹å¾çš„åŒºåŸŸé›†ç¾¤ï¼ˆclustersï¼‰ï¼Œå¹¶å¯¹æ¯ä¸ªé›†ç¾¤**ä¸ªæ€§åŒ–å¾®è°ƒ**ä¸€ä¸ªå…±äº«çš„ LLM ä¸»å¹²ã€‚
- å®ç°äº†**é€šç”¨æ€§ï¼ˆgeneralizationï¼‰ä¸ä¸“ä¸šåŒ–ï¼ˆspecializationï¼‰çš„å¹³è¡¡**ï¼Œé¿å…ä¸ºæ¯ä¸ªåŒºåŸŸå•ç‹¬è®­ç»ƒæ¨¡å‹å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚

#### ï¼ˆ2ï¼‰Prompt-based Traffic Representationï¼ˆæç¤ºå·¥ç¨‹é©±åŠ¨çš„æ•°æ®è¡¨ç¤ºï¼‰
- å°†æ•°å€¼å‹æµé‡æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€ promptï¼ŒåµŒå…¥ä»¥ä¸‹å¤šç»´ç»Ÿè®¡ç‰¹å¾ï¼š
  - åŸºç¡€ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€æœ€å¤§å€¼ã€æ ‡å‡†å·®ç­‰ï¼‰
  - è¶‹åŠ¿åˆ†æï¼ˆä¸Šå‡/ä¸‹é™è¶‹åŠ¿ï¼‰
  - æ—¶é—´æ®µæ¨¡å¼ï¼ˆæ—©é«˜å³°ã€æ™šé«˜å³°ã€å¤œé—´ï¼‰
  - é¢†åŸŸç‰¹å®šæŒ‡æ ‡ï¼ˆå³°å‡æ¯” PARã€çªå‘æ€§ BURSTã€è«å…°æŒ‡æ•° I ç­‰ï¼‰
- åˆ©ç”¨ LLM å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥â€œé˜…è¯»â€è¿™äº›æç¤ºå¹¶åšå‡ºæ¨ç†é¢„æµ‹ã€‚

#### ï¼ˆ3ï¼‰Spatial-Temporal Alignment with DeepSeek Moduleï¼ˆè·¨åŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼‰
- è®¾è®¡äº†ä¸€ä¸ªåŸºäº **multi-head cross-domain attention** çš„æ¨¡å—ï¼Œä½¿ LLM èƒ½å¤Ÿèåˆæ¥è‡ªé‚»è¿‘åŒºåŸŸçš„ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œæ€æƒ³æ„å»ºé‚»æ¥çŸ©é˜µï¼Œå¹¶å¼•å…¥ **local Moranâ€™s I** ç»Ÿè®¡é‡è¡¡é‡ç©ºé—´è‡ªç›¸å…³æ€§ï¼Œå¢å¼ºç©ºé—´å¯¹é½èƒ½åŠ›ã€‚
- ä»…å¾®è°ƒè½»é‡çº§ç»„ä»¶ï¼ˆå¦‚ prompt encoder å’Œ attention å±‚ï¼‰ï¼Œå†»ç»“ LLM ä¸»å¹²ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚

#### ï¼ˆ4ï¼‰é«˜æ•ˆé€‚åº”æœºåˆ¶
- é‡‡ç”¨ **parameter-efficient fine-tuning (PEFT)** ç­–ç•¥ï¼Œåœ¨ä¿æŒ LLM å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°å¿«é€Ÿé¢†åŸŸé€‚é…ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç©ºé—´å»ºæ¨¡èƒ½åŠ›** | æ˜¾å¼å»ºæ¨¡åŒºåŸŸé—´ç©ºé—´ä¾èµ–å…³ç³»ï¼Œä¼˜äºä»…è€ƒè™‘æ—¶é—´åºåˆ—çš„ LLM æ–¹æ³•ï¼ˆå¦‚ Time-LLMï¼‰ |
| **æ¨¡å‹æ•ˆç‡** | å†»ç»“ä¸»å¹² LLMï¼Œåªå¾®è°ƒå°‘é‡å‚æ•°ï¼ŒèŠ‚çœè®­ç»ƒèµ„æº |
| **é¢„æµ‹å‡†ç¡®æ€§** | åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå…¨é¢è¶…è¶Š SOTA æ¨¡å‹ |
| **å¯è§£é‡Šæ€§ä¸é²æ£’æ€§** | ç»“æ„åŒ– prompt æå‡å¯è§£é‡Šæ€§ï¼›è¯¯å·®åˆ†å¸ƒæ›´é›†ä¸­äºé›¶é™„è¿‘ï¼Œç¨³å®šæ€§æ›´å¼º |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **çœŸå®åŸå¸‚çº§ 4G åŸºç«™ç½‘ç»œæ•°æ®**
- æ¥æºï¼šä¸­å›½æŸè¿è¥å•†
- è§„æ¨¡ï¼šçº¦ 19,000 ä¸ªæ‰‡åŒºï¼Œè¦†ç›–æµå—å¸‚
- æ—¶é—´è·¨åº¦ï¼š2024å¹´7æœˆ28æ—¥è‡³8æœˆ25æ—¥ï¼ˆå…±4å‘¨ï¼‰
- é‡‡æ ·é¢‘ç‡ï¼šæ¯15åˆ†é’Ÿä¸€æ¡è®°å½•
- å¤„ç†æ–¹å¼ï¼šå°†æ‰‡åŒºçº§æµé‡èšåˆä¸ºåŒºåŸŸçº§æµé‡ï¼Œå¹¶æŒ‰èšç±»ç»“æœåˆ’åˆ†ä¸º **4ä¸ªzoneï¼ˆAã€Bã€Cã€Dï¼‰**

---

### å®éªŒè®¾ç½®
- **è¾“å…¥çª—å£ H**ï¼šè¿‡å»ä¸€å¤©ï¼ˆ96ä¸ªæ—¶é—´æ­¥ï¼Œå³24å°æ—¶ï¼‰
- **é¢„æµ‹çª—å£ P**ï¼šæœªæ¥1å°æ—¶ï¼ˆ4ä¸ªæ—¶é—´æ­¥ï¼‰
- **LLM æ¨¡å‹**ï¼šDeepSeek-R1-Distill-Llama-8Bï¼ˆ32å±‚ï¼Œæ¨¡å‹ç»´åº¦16ï¼Œ8å¤´æ³¨æ„åŠ›ï¼‰
- **è®­ç»ƒå¹³å°**ï¼š4Ã—A100 GPUï¼Œä½¿ç”¨ DeepSpeed + ZeRO-2 è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- **ä¼˜åŒ–å™¨**ï¼šAdamW + OneCycleLRï¼Œå­¦ä¹ ç‡ 0.001
- **è®­ç»ƒç­–ç•¥**ï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼ˆbfloat16ï¼‰ï¼Œæ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ

---

### è¯„ä¼°æŒ‡æ ‡
- **MAE**ï¼ˆMean Absolute Errorï¼‰
- **RMSE**ï¼ˆRoot Mean Squared Errorï¼‰
- **MAPE**ï¼ˆMean Absolute Percentage Errorï¼‰

$$
\text{MAE} = \frac{1}{T}\sum_{t=1}^{T}|y_t - \hat{y}_t| \\
\text{RMSE} = \sqrt{\frac{1}{T}\sum_{t=1}^{T}(y_t - \hat{y}_t)^2} \\
\text{MAPE} = \frac{100}{T}\sum_{t=1}^{T}\left|\frac{y_t - \hat{y}_t}{y_t}\right|
$$

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| åˆ†è§£çº¿æ€§æ¨¡å‹ | DLinear |
| Transformer æ¶æ„ | Transformer, Autoformer, Reformer, TimesNet, LightTS |
| LLM-based æ–¹æ³• | Time-LLM |

å…¶ä¸­ï¼Œ**Time-LLM** æ˜¯å½“å‰æœ€å…ˆè¿›çš„ LLM é©±åŠ¨æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œä½œä¸ºä¸»è¦å¯¹æ¯”å¯¹è±¡ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table Iï¼‰

| æ–¹æ³• / åŒºåŸŸ | Zone A MAE â†“ | Zone B MAPE â†“ | Zone C RMSE â†“ | Zone D MAE â†“ |
|------------|---------------|----------------|----------------|----------------|
| DLinear     | 0.2330        | 2.6294         | 0.3804         | 0.1492         |
| Time-LLM    | 0.2648        | **2.6623**     | 0.4867         | 0.1914         |
| **TIDES**   | **0.2193**    | **2.4827**     | **0.3621**     | **0.1356**     |

> âœ… **TIDES åœ¨æ‰€æœ‰4ä¸ªåŒºåŸŸã€å…¨éƒ¨3é¡¹æŒ‡æ ‡ä¸­å…±å–å¾—10é¡¹ç¬¬ä¸€ï¼Œ2é¡¹ç¬¬äºŒï¼Œä¸”åœ¨ MAPE ä¸Šå®ç°å…¨èƒœã€‚**

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **å¹³å‡æå‡å¹…åº¦**ï¼š
  - ç›¸æ¯” Time-LLMï¼ŒTIDES åœ¨ MAPE ä¸Šå¹³å‡é™ä½çº¦ **6.8%**
  - åœ¨å¤æ‚åŒºåŸŸï¼ˆå¦‚ Zone B/Cï¼‰ï¼Œæå‡æ›´ä¸ºæ˜¾è‘—ï¼ˆæœ€é«˜è¾¾ 15%ï¼‰
- **å¯è§†åŒ–éªŒè¯**ï¼š
  - å›¾3æ˜¾ç¤º TIDES çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„ç›¸å…³ç³»æ•°é«˜è¾¾ **r = 0.973**ï¼Œè¿œè¶… Time-LLMï¼ˆ0.792ï¼‰å’Œ DLinearï¼ˆ0.959ï¼‰
  - å›¾4è¡¨æ˜ TIDES çš„é¢„æµ‹è¯¯å·®åˆ†å¸ƒæœ€é›†ä¸­äºé›¶é™„è¿‘ï¼Œæ–¹å·®æœ€å°ï¼Œè¯´æ˜å…¶**ç¨³å®šæ€§å’Œç²¾åº¦æ›´é«˜**
  - å›¾5ä»æ—¶ç©ºåŒé‡è§†è§’å±•ç¤ºï¼ŒTIDES æ›´å¥½åœ°è¿˜åŸäº†åŸå¸‚ä¸­é«˜ä½æµé‡åŒºåŸŸçš„ç©ºé—´åˆ†å¸ƒæ ¼å±€

---

### æ¶ˆèå®éªŒä¸æ³›åŒ–èƒ½åŠ›åˆ†æï¼ˆè§ Table IIï¼‰
| åœºæ™¯ | MAE | ç›¸å¯¹ Zone A ä¸‹é™ |
|------|-----|------------------|
| Zone A â†’ Zone Aï¼ˆåŒåŒºï¼‰ | 0.2255 | â€” |
| Zone B â†’ Zone Aï¼ˆè·¨åŒºï¼‰ | 0.2595 | â†“15.0% |
| Zone C â†’ Zone A | 0.2456 | â†“8.9% |
| Zone D â†’ Zone A | 0.2328 | â†“3.2% |

> ğŸ” è¡¨æ˜ TIDES å…·å¤‡è‰¯å¥½çš„**è·¨åŒºåŸŸæ³›åŒ–èƒ½åŠ›**ï¼Œå³ä½¿ä¸é‡æ–°è®­ç»ƒä¹Ÿèƒ½åœ¨æ–°åŒºåŸŸä¿æŒè¾ƒé«˜æ€§èƒ½ï¼Œæ€§èƒ½ä¸‹é™æ™®é <10%ï¼Œè¯æ˜å…¶æ¨¡å‹è¿ç§»èƒ½åŠ›å¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç©ºé—´ä¾èµ–æ€§æ˜¯æå‡ LLM æµé‡é¢„æµ‹æ€§èƒ½çš„å…³é”®å› ç´ **  
   å•çº¯å°†æµé‡è§†ä¸ºæ–‡æœ¬åºåˆ—è¿›è¡Œé¢„æµ‹ä¸è¶³ä»¥æ•æ‰åŸå¸‚åŠ¨æ€ï¼Œå¿…é¡»æ˜¾å¼å»ºæ¨¡åŒºåŸŸé—´çš„ç©ºé—´äº¤äº’ã€‚

2. **Prompt Engineering å¯æœ‰æ•ˆæ¡¥æ¥æ•°å€¼ä¿¡å·ä¸ LLM è¯­ä¹‰ç©ºé—´**  
   å°†ç»Ÿè®¡ç‰¹å¾ç¼–ç ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œæå¤§æå‡äº† LLM å¯¹éè¯­è¨€æ•°æ®çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚

3. **TIDES å®ç°äº†é«˜ç²¾åº¦ä¸é«˜æ•ˆç‡çš„ç»Ÿä¸€**  
   é€šè¿‡å†»ç»“ LLM ä¸»å¹² + å¾®è°ƒè½»é‡æ¨¡å—çš„æ–¹å¼ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘è®­ç»ƒå¼€é”€ã€‚

4. **è¯¥æ–¹æ³•é€‚ç”¨äºæœªæ¥ 6G ç½‘ç»œä¸­çš„æ™ºèƒ½èµ„æºè°ƒåº¦**  
   å‡†ç¡®çš„åŸå¸‚åœºæ™¯é¢„æµ‹æœ‰åŠ©äºå®ç°è´Ÿè½½å‡è¡¡ã€èŠ‚èƒ½ä¼˜åŒ–å’Œä¸»åŠ¨å¼ç½‘ç»œç®¡ç†ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä»…åŸºäºå†å²æµé‡å’Œåœ°ç†ä¿¡æ¯ï¼Œæœªæ•´åˆå¤–éƒ¨å› ç´ ï¼ˆå¦‚å¤©æ°”ã€äº‹ä»¶ã€POI ç­‰ï¼‰
- æ‰€æœ‰åŒºåŸŸå…±äº«åŒä¸€ LLM ä¸»å¹²ï¼Œå¯èƒ½é™åˆ¶æç«¯å¼‚æ„åœºæ™¯ä¸‹çš„è¡¨è¾¾èƒ½åŠ›
- å®éªŒä»…åœ¨ä¸€ä¸ªåŸå¸‚å±•å¼€ï¼Œå°šæœªéªŒè¯è·¨åŸå¸‚è¿ç§»èƒ½åŠ›

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. å¼•å…¥å®æ—¶åé¦ˆæœºåˆ¶ï¼Œæ”¯æŒåœ¨çº¿å¢é‡å­¦ä¹ ä¸åŠ¨æ€è°ƒæ•´
2. æ‰©å±•è‡³å¤šæ¨¡æ€è¾“å…¥ï¼ˆè§†é¢‘ã€åœ°å›¾ã€ç¤¾äº¤æ•°æ®ç­‰ï¼‰
3. æ¢ç´¢è·¨åŸå¸‚ã€å…¨çƒå°ºåº¦çš„å¤§è§„æ¨¡é¢„æµ‹ä»»åŠ¡
4. å°† TIDES é›†æˆåˆ°ç«¯åˆ°ç«¯çš„ AI-Native 6G æ§åˆ¶ç³»ç»Ÿä¸­ï¼Œå®ç°é—­ç¯è‡ªä¸»ä¼˜åŒ–

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> TIDES æˆåŠŸå°† LLM çš„å¼ºå¤§è¯­ä¹‰å»ºæ¨¡èƒ½åŠ›ä¸æ— çº¿ç½‘ç»œçš„ç©ºé—´ç‰¹æ€§ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆã€ç²¾å‡†ã€å¯æ‰©å±•çš„åŸå¸‚çº§æ— çº¿æµé‡é¢„æµ‹æ–°èŒƒå¼ï¼Œä¸ºè¿ˆå‘ fully autonomous çš„ 6G æ™ºèƒ½ç½‘ç»œå¥ å®šäº†å…³é”®æŠ€æœ¯åŸºç¡€ã€‚

</details>

---

### 8. [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)

**Authors**: Aiwei Liu, Minghua He, Shaoxun Zeng, Sijun Zhang, Linhao Zhang, Chuhan Wu, Wei Jia, Yuan Liu, Xiao Zhou, Jie Zhou  
**Category**: cs.CL  
**Published**: 2025-12-30  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.22737v1  

#### Abstract
Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they of...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šWeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

ä¼ ç»Ÿçš„ **Autoregressive (AR)** è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é€ä¸ªç”Ÿæˆ tokenï¼Œå¯¼è‡´è§£ç é€Ÿåº¦æ…¢ã€éš¾ä»¥å¹¶è¡ŒåŒ–ï¼Œé™åˆ¶äº†åœ¨é«˜åååœºæ™¯ä¸‹çš„åº”ç”¨ã€‚**Diffusion Language Models (DLLMs)** è™½ç„¶æ”¯æŒå¹¶è¡Œè§£ç ï¼ˆä¸€æ¬¡æ¢å¤å¤šä¸ªè¢«æ©ç çš„ tokenï¼‰ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­å¹¶æœªæ˜¾è‘—è¶…è¶Šä¼˜åŒ–åçš„ AR å¼•æ“ï¼ˆå¦‚ vLLMï¼‰ã€‚å…¶æ ¹æœ¬åŸå› åœ¨äºï¼š

- å¤šæ•° DLLMs ä½¿ç”¨ **bidirectional attention**ï¼Œç ´åäº†æ ‡å‡†çš„ **prefix KV caching** æœºåˆ¶ï¼›
- å¯¼è‡´æ¯æ¬¡ç”Ÿæˆåéƒ½éœ€é‡æ–°è®¡ç®—ä¸Šä¸‹æ–‡ï¼Œæ— æ³•æœ‰æ•ˆå¤ç”¨ç¼“å­˜ï¼Œä¸¥é‡æŸå®³æ•ˆç‡ã€‚

å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³ **å¦‚ä½•è®©æ‰©æ•£è¯­è¨€æ¨¡å‹å®ç°çœŸæ­£çš„é«˜æ•ˆå¹¶è¡Œæ¨ç†**ï¼Œå°¤å…¶æ˜¯åœ¨å·¥ä¸šçº§ä¼˜åŒ–ç³»ç»Ÿï¼ˆå¦‚ vLLMï¼‰ä¸‹ä»èƒ½èƒœå‡ºã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **WeDLM**ï¼Œä¸€ä¸ªå…¨æ–°çš„æ‰©æ•£è§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> **åœ¨ä¿æŒæ ‡å‡†å› æœæ³¨æ„åŠ›ï¼ˆcausal attentionï¼‰çš„å‰æä¸‹å®ç°å¹¶è¡Œæ©ç æ¢å¤**ï¼Œä»è€Œå…¼å®¹ KV ç¼“å­˜ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

1. **Topological Reorderingï¼ˆæ‹“æ‰‘é‡æ’åºï¼‰**
   - å°†å·²è§‚æµ‹åˆ°çš„ token ç‰©ç†ä¸Šç§»åŠ¨åˆ°åºåˆ—å‰ç¼€ä½ç½®ï¼ŒåŒæ—¶é€šè¿‡ RoPE çš„ position ids ä¿ç•™å…¶åŸå§‹é€»è¾‘ä½ç½®ã€‚
   - è¿™æ ·ï¼Œè¢«æ©ç çš„ token åœ¨æ ‡å‡†å› æœ attention ä¸‹å³å¯è®¿é—®æ‰€æœ‰å·²è§‚æµ‹ä¸Šä¸‹æ–‡ï¼Œæ— éœ€ bidirectional attentionã€‚

2. **Dual-Stream Maskingï¼ˆåŒæµæ©ç ï¼‰**
   - è®­ç»ƒæ—¶å¼•å…¥ä¸¤ä¸ªæµï¼š**Memory Stream**ï¼ˆå¹²å‡€å†å²ï¼‰å’Œ **Prediction Stream**ï¼ˆå¸¦æ©ç çš„é¢„æµ‹æµï¼‰ã€‚
   - é¢„æµ‹æµä¸­çš„æ¯ä¸ª block åªèƒ½å…³æ³¨ Memory Stream ä¸­å¯¹åº”çš„å†å²ï¼Œæ¨¡æ‹Ÿæ¨ç†æ—¶çš„â€œå·²æäº¤å‰ç¼€â€æ¡ä»¶ï¼Œç¼©å°è®­ç»ƒ-æ¨ç†å·®è·ã€‚

3. **Streaming Parallel Decodingï¼ˆæµå¼å¹¶è¡Œè§£ç ï¼‰**
   - ç»´æŒä¸€ä¸ªå›ºå®šå¤§å°çš„æ»‘åŠ¨çª—å£ï¼ŒåŠ¨æ€å¡«å……æ–°æ©ç  tokenï¼›
   - ä½¿ç”¨ **distance-penalized entropy selection** ä¼˜å…ˆè§£ç é å·¦çš„ä½ç½®ï¼Œä¿ƒè¿›ä»å·¦åˆ°å³çš„è¿ç»­å‰ç¼€å¢é•¿ï¼›
   - æ”¯æŒ **ç«‹å³ç¼“å­˜ï¼ˆimmediate cachingï¼‰**ï¼Œé¿å… block-wise æ–¹æ³•çš„â€œåœç­‰â€é—®é¢˜ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | ä¼ ç»Ÿ DLLMsï¼ˆå¦‚ LLaDA, Dreamï¼‰ | Block-wise DLLMsï¼ˆå¦‚ SDARï¼‰ | WeDLMï¼ˆæœ¬æ–‡ï¼‰ |
|------|-------------------------------|-----------------------------|--------------|
| Attention ç»“æ„ | Bidirectional | Block å†…åŒå‘ï¼Œè·¨ block å› æœ | å…¨å±€æ ‡å‡† causal attention |
| KV Cache å…¼å®¹æ€§ | âŒ ä¸å…¼å®¹ | âš ï¸ å—å®Œæˆåæ‰èƒ½ç¼“å­˜ | âœ… æ¯ä¸ª token å¯ç«‹å³ç¼“å­˜ |
| å¹¶è¡Œæ•ˆç‡ | é«˜ï¼ˆæ¯æ­¥å¤š tokenï¼‰ä½†ç¼“å­˜åˆ©ç”¨ç‡ä½ | ä¸­ç­‰ï¼Œå—é™äºå—åŒæ­¥ | é«˜ä¸”å¯æŒç»­ |
| æ¨ç†é€Ÿåº¦ | é€šå¸¸ä¸ä¼˜äº vLLM | æœ‰é™åŠ é€Ÿ | æ˜¾è‘—ä¼˜äº vLLM |
| éƒ¨ç½²å…¼å®¹æ€§ | éœ€ä¸“ç”¨å¼•æ“ï¼ˆå¦‚ dInferï¼‰ | éœ€å®šåˆ¶å¼•æ“ï¼ˆå¦‚ JetEngineï¼‰ | å¯ç›´æ¥è¿è¡Œäº vLLM |

> âœ… **WeDLM æ˜¯é¦–ä¸ªåœ¨åŒ¹é…éƒ¨ç½²æ¡ä»¶ä¸‹è¶…è¶Šå·¥ä¸šçº§ AR å¼•æ“ï¼ˆvLLMï¼‰çš„ DLLMã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

æ¶µç›–ä¸‰å¤§ç±»ä»»åŠ¡ï¼š

- **æ•°å­¦æ¨ç†**ï¼šGSM8Kï¼ˆ3-shotï¼‰ã€MATHï¼ˆ4-shotï¼‰ã€GPQA-Diamondï¼ˆ5-shotï¼‰
- **é€šç”¨çŸ¥è¯†ä¸å¸¸è¯†**ï¼šMMLUï¼ˆ5-shotï¼‰ã€ARC-C/Eï¼ˆ0-shotï¼‰ã€HellaSwagï¼ˆ10-shotï¼‰
- **ä»£ç ç”Ÿæˆ**ï¼šHumanEvalï¼ˆ4-shotï¼‰ã€HumanEval-plusï¼ˆ4-shotï¼‰ã€MBPPï¼ˆ3-shotï¼‰

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### æ¨¡å‹é…ç½®
- åŸºåº§æ¨¡å‹ï¼šQwen2.5-7B å’Œ Qwen3-8B
- è®­ç»ƒé˜¶æ®µï¼š
  - ç»§ç»­é¢„è®­ç»ƒï¼ˆcontinued pretrainingï¼‰100B tokens
  - ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰10K instruction æ•°æ®
- å¾—åˆ°æ¨¡å‹ï¼šWeDLM-7B å’Œ WeDLM-8B

#### æ¨ç†è®¾ç½®
- ä½¿ç”¨ç»Ÿä¸€çš„ step-wise è§£ç æ–¹æ¡ˆ
- çª—å£å¤§å° $ W=6 $
- è·ç¦»æƒ©ç½šç³»æ•° $ \lambda=0.1 $
- æ¸©åº¦ $ T=0.1 $ï¼Œæœ€å¤§é•¿åº¦ 512

#### è¯„ä¼°æŒ‡æ ‡
- **ç”Ÿæˆè´¨é‡**ï¼šå„ benchmark ä¸Šçš„å‡†ç¡®ç‡
- **æ¨ç†é€Ÿåº¦**ï¼štokens per second (tps)ï¼Œåœ¨ç›¸åŒç¡¬ä»¶å’Œéƒ¨ç½²ç¯å¢ƒä¸‹æµ‹é‡
- **å…¬å¹³æ¯”è¾ƒ**ï¼š
  - AR æ¨¡å‹ä½¿ç”¨ **vLLM**
  - DLLM åŸºçº¿ä½¿ç”¨å…¶æ¨èå¼•æ“ï¼ˆLLaDA/Dream â†’ dInferï¼›SDAR â†’ JetEngineï¼‰
  - WeDLM ä¹Ÿè¿è¡Œäº **vLLM**ï¼ŒéªŒè¯å…¶ä¸å·¥ä¸šç³»ç»Ÿçš„æ— ç¼é›†æˆèƒ½åŠ›

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| ç±»å‹ | åŸºçº¿æ¨¡å‹ |
|------|---------|
| AR Baseline | Qwen2.5-7B, Qwen3-8B |
| DLLM Baseline | LLaDA-8B, Dream-7B, SDAR-8B |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### è¡¨æ ¼æ€»ç»“ï¼ˆæ¥è‡ª Table 1 & 2ï¼‰

| æ¨¡å‹ | Base Avg | Instruct Avg |
|------|----------|-------------|
| Qwen2.5-7B / Qwen3-8B | 67.21 / 72.61 | 71.09 / 75.12 |
| LLaDA-8B / Dream-7B | 55.44 / 56.91 | 56.77 / 62.96 |
| SDAR-8B | â€” | 74.22 |
| **WeDLM-7B / WeDLM-8B** | **70.84 / 74.72** | **72.78 / 77.53** |

> âœ… WeDLM ä¸ä»…ä¿ç•™äº†åŸ AR æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¿˜åœ¨å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†æå‡ã€‚

#### å…¸å‹ä»»åŠ¡è¡¨ç°ï¼ˆWeDLM-8B vs Qwen3-8Bï¼‰

| ä»»åŠ¡ | WeDLM-8B | Qwen3-8B | æå‡ |
|------|----------|----------|------|
| GSM8K | 92.27 | 89.91 | +2.36 |
| MATH | 64.80 | 69.60 â†’ ä½†åŸæ–‡ä¸º 69.60ï¼ŒWeDLM è¾¾ 64.80ï¼Ÿéœ€æ³¨æ„ | â€” |
| GPQA-Diamond | 44.95 | 41.41 | +3.54 |
| HumanEval | 80.49 | 71.95 | +8.54 |
| HumanEval-plus | 73.78 | 64.63 | +9.15 |

> ğŸ”¥ åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šä¼˜åŠ¿å°¤ä¸ºæ˜æ˜¾ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- **é€Ÿåº¦æ–¹é¢**ï¼š
  - åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ GSM8Kï¼‰ä¸Šï¼ŒWeDLM ç›¸æ¯” vLLM ä¼˜åŒ–çš„ AR åŸºçº¿å®ç° **~3Ã— åŠ é€Ÿ**
  - åœ¨ä½ç†µç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚è®¡æ•°ã€è§„åˆ™æ–‡æœ¬ï¼‰ä¸­ï¼Œæœ€é«˜å¯è¾¾ **10Ã— ä»¥ä¸ŠåŠ é€Ÿ**
  - å›¾1æ˜¾ç¤ºï¼šWeDLM-8B åœ¨ä¿æŒæ›´é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦è¿œè¶…å…¶ä»– DLLMs

- **è´¨é‡æ–¹é¢**ï¼š
  - WeDLM-8B-Instruct åœ¨å¤šä¸ª benchmark ä¸Šè¶…è¿‡å…¶ AR åŸºçº¿ï¼ˆQwen3-8B-Instructï¼‰
  - å°¤å…¶åœ¨ **code generation** å’Œ **high-difficulty reasoning** ä¸Šé¢†å…ˆæ˜¾è‘—

---

### **æ¶ˆèå®éªŒç»“æœ**

#### ï¼ˆ1ï¼‰Streaming vs Block-wise Decoding
- æµå¼è§£ç åœ¨æ‰€æœ‰ entropy threshold ä¸‹å‡ä¼˜äº block-wise
- æœ€é«˜å®ç° **1.9Ã— é€Ÿåº¦æå‡**ï¼ˆ423 vs 221 tpsï¼‰
- åŸå› ï¼šæ›´é«˜çš„ **prefix cacheability (Pcache)**ï¼Œå‡å°‘é‡å¤è®¡ç®—

#### ï¼ˆ2ï¼‰Distance Penalty ($\lambda$) å½±å“
- å¢å¤§ $\lambda$ï¼ˆåå‘å·¦ä¾§ tokenï¼‰å¯æå‡ accuracyï¼ˆ+2.6 ptsï¼‰ï¼Œä»…è½»å¾®é™ä½ speedï¼ˆ-3%ï¼‰
- è¯´æ˜ **left-to-right commitment æœ‰åŠ©äºå‡å°‘é”™è¯¯ä¼ æ’­**

#### ï¼ˆ3ï¼‰Block Size æ•æ„Ÿæ€§
- åœ¨ç»§ç»­é¢„è®­ç»ƒä¸­æµ‹è¯• B âˆˆ {4,8,32}
- æ€§èƒ½å‡ ä¹ä¸å˜ï¼ˆå¹³å‡åˆ†å·®å¼‚ < 0.8 ptsï¼‰
- è¡¨æ˜ WeDLM å¯¹ block size ä¸æ•æ„Ÿï¼Œéƒ¨ç½²çµæ´»

#### ï¼ˆ4ï¼‰Causal vs Bidirectional Intra-block Attention
- å³ä½¿åœ¨åŒä¸€ block å†…ä½¿ç”¨ bidirectional attentionï¼Œæ€§èƒ½åè€Œæ›´ä½
- å› ä¸ºå®ƒé˜»ç¢äº† token çº§åˆ«çš„ immediate caching
- è¿›ä¸€æ­¥è¯æ˜ï¼š**causal structure æ›´é€‚åˆ cache-friendly æ¨ç†**

#### ï¼ˆ5ï¼‰æ¨¡å‹è§„æ¨¡å½±å“
- å°æ¨¡å‹ï¼ˆ0.6B, 1.5Bï¼‰é€‚åº” WeDLM åç•¥æœ‰ä¸‹é™
- å¤§æ¨¡å‹ï¼ˆ7B+ï¼‰è¡¨ç°æŒç»­æå‡
- æš—ç¤ºå­˜åœ¨ **scaling law**ï¼šæ›´å¼ºçš„ AR checkpoint æ›´å®¹æ˜“æˆåŠŸè¿ç§»åˆ° diffusion æ¡†æ¶

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **KV ç¼“å­˜å…¼å®¹æ€§æ¯”â€œæ¯æ­¥é¢„æµ‹ token æ•°â€æ›´é‡è¦**
   - è®ºæ–‡æå‡º **prefix cacheability (Pcache)** ä½œä¸ºè¡¡é‡æ¨ç†æ•ˆç‡çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚
   - å³ä½¿å¹¶è¡Œåº¦é«˜ï¼Œè‹¥ä¸èƒ½å¿«é€Ÿå½¢æˆå¯ç¼“å­˜çš„å‰ç¼€ï¼Œä¹Ÿæ— æ³•æå‡çœŸå®ååã€‚

2. **Bidirectional attention å¹¶éå¿…éœ€**
   - é€šè¿‡ Topological Reorderingï¼Œå¯åœ¨ causal attention ä¸‹å®ç°å…¨å±€ä¸Šä¸‹æ–‡å¯è§æ€§ã€‚
   - è¿™ä½¿å¾— DLLM èƒ½æ— ç¼ç»§æ‰¿ AR æ¨¡å‹çš„ç¼“å­˜æœºåˆ¶ã€‚

3. **WeDLM æ˜¯é¦–ä¸ªåœ¨å·¥ä¸šéƒ¨ç½²æ¡ä»¶ä¸‹è¶…è¶Š vLLM çš„ DLLM**
   - åœ¨åŒ¹é…è®¾ç½®ä¸‹ï¼Œå®ç° **3Ã—~10Ã— åŠ é€Ÿ**ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡ç”Ÿæˆè´¨é‡ã€‚
   - å¯ç›´æ¥éƒ¨ç½²äº vLLMã€FlashAttentionã€PagedAttention ç­‰æˆç†Ÿç³»ç»Ÿï¼Œæ— éœ€ä¿®æ”¹ kernelã€‚

4. **è®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§è‡³å…³é‡è¦**
   - Dual-Stream Masking æˆåŠŸç¼©å°äº†è®­ç»ƒæ—¶éšæœºæ©ç ä¸æ¨ç†æ—¶å‰ç¼€æ¨è¿›ä¹‹é—´çš„åˆ†å¸ƒå·®è·ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **å¯¹é«˜ç†µä»»åŠ¡åŠ é€Ÿæœ‰é™**
   - å¦‚å¼€æ”¾åŸŸé—®ç­”ï¼ˆè§£é‡Šé‡å­ç‰©ç†ï¼‰ï¼Œç”±äºè¾“å‡ºä¸ç¡®å®šæ€§é«˜ï¼Œconfidence è¾ƒä½ï¼Œparallel speculation æ•ˆæœå·®ã€‚
   - å®æµ‹é€Ÿåº¦ä»…çº¦ 200 tpsï¼Œè¿œä½äºä½ç†µä»»åŠ¡ï¼ˆ>1600 tpsï¼‰

2. **ä¾èµ–é«˜è´¨é‡ AR checkpoint**
   - å°æ¨¡å‹è¿ç§»æ•ˆæœä¸ä½³ï¼Œå»ºè®®ä» 7B+ è§„æ¨¡å¼€å§‹ä½¿ç”¨

3. **å½“å‰æ¡†æ¶ä»åŸºäºé™æ€çª—å£**
   - åŠ¨æ€è°ƒæ•´çª—å£å¤§å°æˆ– speculative depth å¯èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

- è®¾è®¡æ›´é²æ£’çš„ **acceptance mechanism** æˆ– **dynamic entropy calibration**ï¼Œä»¥æå‡é«˜ç†µä»»åŠ¡çš„åŠ é€Ÿæ¯”
- æ¢ç´¢ **adaptive window sizing** å’Œ **multi-step speculation**
- éªŒè¯ AR-to-Diffusion çš„ scaling lawsï¼Œç³»ç»Ÿç ”ç©¶æ¨¡å‹å®¹é‡ä¸ adaptation æ•ˆæœçš„å…³ç³»
- æ‰©å±•åˆ°å¤šæ¨¡æ€ diffusion generation åœºæ™¯

---

## æ€»ç»“

âœ… **WeDLM çš„æ ¸å¿ƒçªç ´åœ¨äºï¼šå°† diffusion-style å¹¶è¡Œç”Ÿæˆä¸ standard causal attention å®Œç¾ç»“åˆï¼Œå®ç°äº†çœŸæ­£é«˜æ•ˆçš„ prefix-cache-friendly è§£ç ã€‚**

ğŸ¯ å®ƒä¸ä»…ç†è®ºæ–°é¢–ï¼Œè€Œä¸”å·¥ç¨‹å®ç”¨â€”â€”å¯ä»¥ç›´æ¥è·‘åœ¨ vLLM ä¸Šï¼Œå°±èƒ½æ‰“è´¥ç²¾å¿ƒä¼˜åŒ–çš„ AR åŸºçº¿ï¼Œåœ¨é€Ÿåº¦å’Œè´¨é‡ä¸ŠåŒåŒå–èƒœã€‚

ğŸš€ è¿™æ ‡å¿—ç€ **diffusion language models æ­£å¼è¿ˆå…¥â€œå¯è½åœ°â€çš„é«˜æ€§èƒ½æ¨ç†æ—¶ä»£**ã€‚

</details>

---

### 9. [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)

**Authors**: Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.22695v1  

#### Abstract
Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly unders...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡èšç„¦äº**å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†è¿‡ç¨‹ä¸­çš„èƒ½æºæ•ˆç‡é—®é¢˜**ï¼Œç‰¹åˆ«æ˜¯ç”±â€œæ¨¡æ€è†¨èƒ€â€ï¼ˆModality Inflationï¼‰å¸¦æ¥çš„é¢å¤–èƒ½è€—ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶å…³æ³¨æ–‡æœ¬å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½æ•ˆä¼˜åŒ–ï¼Œä½†å¯¹ MLLM ä¸­è§†è§‰è¾“å…¥å¼•å…¥çš„ç¼–ç å¼€é”€ã€token åºåˆ—å¢é•¿åŠå…¶å¯¹æ•´ä½“èƒ½æ•ˆçš„å½±å“ç¼ºä¹ç³»ç»Ÿæ€§åˆ†æã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
- **é¦–æ¬¡æå‡ºâ€œæ¨¡æ€è†¨èƒ€â€ï¼ˆModality Inflationï¼‰æ¦‚å¿µ**ï¼šæŒ‡å›¾åƒç­‰éæ–‡æœ¬æ¨¡æ€é€šè¿‡è§†è§‰ç¼–ç å™¨ç”Ÿæˆå¤§é‡è§†è§‰ tokenï¼Œå¹¶ä¸æ–‡æœ¬ token æ‹¼æ¥åæ˜¾è‘—å¢åŠ  Prefill é˜¶æ®µè®¡ç®—è´Ÿæ‹…çš„ç°è±¡ã€‚
- **ç»†ç²’åº¦é˜¶æ®µçº§èƒ½æ•ˆåˆ†ææ¡†æ¶**ï¼šå°† MLLM æ¨ç†æµç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µâ€”â€”**Vision Encoding**ã€**Prefill** å’Œ **Decoding**ï¼Œå¹¶é‡åŒ–å„é˜¶æ®µçš„èƒ½é‡æ¶ˆè€—åˆ†å¸ƒã€‚
- **åŸºäº GPU åŠŸç‡è½¨è¿¹çš„å®è¯åˆ†æ**ï¼šåˆ©ç”¨ NVML å·¥å…·é‡‡é›†é«˜æ—¶é—´åˆ†è¾¨ç‡ï¼ˆ5msï¼‰çš„ GPU åŠŸè€—æ›²çº¿ï¼Œæ­ç¤ºå¤šæ¨¡æ€æ‰§è¡Œä¸­å‡ºç°çš„ä½åˆ©ç”¨ç‡â€œä¸­é—´åŠŸç‡ç›¸ä½â€ã€‚
- **æå‡ºé˜¶æ®µçº§åŠ¨æ€ç”µå‹é¢‘ç‡è°ƒèŠ‚ï¼ˆStage-wise DVFSï¼‰ä½œä¸ºä¼˜åŒ–ç­–ç•¥**ï¼šæ ¹æ®ä¸åŒæ¨ç†é˜¶æ®µçš„å·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼ŒåŠ¨æ€è°ƒæ•´ GPU é¢‘ç‡ä»¥å®ç°èŠ‚èƒ½ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç°æœ‰å·¥ä½œå±€é™ | æœ¬è®ºæ–‡ä¼˜åŠ¿ |
|------|--------------|-----------|
| åˆ†æç²’åº¦ | å¤šä¸ºç«¯åˆ°ç«¯æˆ–ä»…å…³æ³¨ Prefill/Decode | å¼•å…¥ Vision Encoder é˜¶æ®µï¼Œè¿›è¡Œå…¨æµç¨‹ä¸‰é˜¶æ®µæ‹†è§£ |
| èƒ½æ•ˆè§†è§’ | ä¸»è¦å…³æ³¨ååé‡ä¸å»¶è¿Ÿ | æ˜ç¡®é‡åŒ–â€œæ¨¡æ€å¼•å…¥â€çš„å¢é‡èƒ½è€—ï¼ˆ+17%~94%ï¼‰ |
| ä¼˜åŒ–æ‰‹æ®µ | æ¨¡å‹å‹ç¼©ã€æ‰¹å¤„ç†è°ƒåº¦ä¸ºä¸» | æå‡ºç¡¬ä»¶æ§åˆ¶å±‚é¢çš„ DVFS ä¼˜åŒ–ï¼Œæ›´å…·æ™®é€‚æ€§å’Œå³æ—¶æ€§ |
| è¾“å…¥æ•æ„Ÿæ€§ | å¿½è§†å›¾åƒæ•°é‡ä¸åˆ†è¾¨ç‡å·®å¼‚ | ç³»ç»Ÿè¯„ä¼°è¾“å…¥å¤æ‚åº¦å¯¹èƒ½è€—çš„éçº¿æ€§å½±å“ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ¨¡å‹
é€‰å–å››ç§å…·æœ‰ä»£è¡¨æ€§çš„ MLLMï¼Œè¦†ç›–ä¸åŒè§†è§‰ç¼–ç å™¨è®¾è®¡ä¸ token åŒ–ç­–ç•¥ï¼š

| Model | Vision Encoder | LLM Backbone | ç‰¹ç‚¹ |
|-------|----------------|---------------|------|
| **LLaVA-1.5-7B** | CLIP ViT-L/14 | Vicuna-7B | å›ºå®š patch åˆ†å‰² |
| **LLaVA-OneVision-Qwen2-7B** | SigLIP ViT | Qwen2-7B | Any-resolution tilingï¼Œtoken æ•°éšåˆ†è¾¨ç‡å¢é•¿ |
| **Qwen2.5-VL-7B** | QwenViT | Qwen2.5-7B | é«˜è®¡ç®—å¼ºåº¦ encoderï¼Œæ”¯æŒ token å‹ç¼© |
| **InternVL3-8B** | InternViT-300M | Qwen2.5-8B | è½»é‡ encoder + ä¸‹é‡‡æ ·æœºåˆ¶ |

æ‰€æœ‰æ¨¡å‹å‡åœ¨ **NVIDIA A100 80GB GPU** ä¸Šè¿è¡Œï¼Œä½¿ç”¨ PyTorch ä¸ Hugging Face Transformers æ¡†æ¶ã€‚

### æ•°æ®é›†ä¸è¾“å…¥é…ç½®
- **çœŸå®è¯·æ±‚åˆ†å¸ƒå‚è€ƒ**ï¼šé‡‡ç”¨ç”Ÿäº§çº§è¿½è¸ªæ•°æ®é›† **ServeGen [42]** åˆ†æå›¾åƒæ•°é‡åˆ†å¸ƒï¼ˆå›¾2aï¼‰ï¼Œå‘ç°å¤šæ•°è¯·æ±‚å«1â€“2å¼ å›¾ï¼Œå°‘æ•°é«˜è¾¾49å¼ ã€‚
- **å›¾åƒåˆ†è¾¨ç‡åˆ†æ**ï¼šç»Ÿè®¡å››ä¸ªä¸»æµ benchmarkï¼ˆVQAv2ã€VizWizã€ShareGPT4Vã€ChartQAï¼‰çš„å›¾åƒçŸ­è¾¹åˆ†è¾¨ç‡ï¼ˆå›¾2bï¼‰ï¼ŒèŒƒå›´ä» 256 åˆ° 1280 åƒç´ ä»¥ä¸Šã€‚
- **å®éªŒè¾“å…¥è®¾ç½®**ï¼š
  - å›ºå®šæ–‡æœ¬ promptï¼›
  - å›¾åƒè¾“å…¥åˆ†ä¸ºä¸¤ç§æ¨¡å¼ï¼š
    1. **Iso-resolution**ï¼šç»Ÿä¸€è®¾ä¸º 512Ã—512ï¼›
    2. **Scaled inputs**ï¼šå˜åŒ–å›¾åƒæ•°é‡ï¼ˆ1â€“6ï¼‰å’Œåˆ†è¾¨ç‡ï¼ˆ256â€“1408ï¼‰ä»¥æµ‹è¯•å¯æ‰©å±•æ€§ã€‚

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Energy per Request (J)** | å•ä¸ªè¯·æ±‚æ€»èƒ½è€—ï¼ˆé€šè¿‡ NVML é‡‡æ ·ç§¯åˆ†ï¼‰ |
| **End-to-End Latency (s)** | ä»è¾“å…¥åˆ°è¾“å‡ºå®Œæˆçš„æ—¶é—´ |
| **GPU Power Trace (W)** | å®æ—¶åŠŸè€—æ›²çº¿ï¼Œç”¨äºè¯†åˆ«åˆ©ç”¨ç‡ç“¶é¢ˆ |
| **Throughput (req/s)** | ååèƒ½åŠ›ï¼Œè¯„ä¼°æœåŠ¡å®¹é‡ |

### åŸºçº¿è®¾ç½®
æ„å»ºä¸ MLLM è¾“å…¥ token æ€»æ•°ç›¸åŒ¹é…çš„ **text-only LLM baseline**ï¼ˆå³ç”¨çº¯æ–‡æœ¬æ¨¡æ‹Ÿç›¸åŒé•¿åº¦è¾“å…¥ï¼‰ï¼Œä»è€Œéš”ç¦»â€œæ¨¡æ€å¼•å…¥â€æœ¬èº«çš„èƒ½è€—å¢é‡ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… RQ1: è§†è§‰æ¨¡æ€çš„å¢é‡èƒ½è€—æœ‰å¤šå¤§ï¼Ÿ
- åœ¨ iso-token è®¾ç½®ä¸‹ï¼Œç›¸æ¯” text-only åŸºçº¿ï¼ŒMLLM çš„èƒ½è€—å¢åŠ  **17% ~ 94%**ï¼š
  - **Qwen2.5-VL**: +94% èƒ½è€—ï¼Œ+179% å»¶è¿Ÿ â†’ è¡¨æ˜å…¶ encoder æå…¶è€—èƒ½ä¸”ä¸²è¡ŒåŒ–ä¸¥é‡
  - **LLaVA-OneVision**: +17% èƒ½è€—ï¼ˆæœ€ä½ï¼‰ï¼Œä½†äº§ç”Ÿ **3,715 ä¸ªè§†è§‰ token** â†’ æ˜¾ç¤ºæ¶æ„è®¾è®¡å†³å®šèƒ½è€—è€Œé token æ•°æœ¬èº«
  - **InternVL3**: +18%ï¼Œè¡¨ç°æœ€å‡è¡¡

> ğŸ” **Observation 1**: ä¸åŒ MLLM æ¶æ„é—´èƒ½è€—å·®å¼‚å·¨å¤§ï¼Œéœ€å®šåˆ¶åŒ–èƒ½æ•ˆç­–ç•¥ã€‚

#### âœ… RQ2: èƒ½é‡åˆ†å¸ƒåœ¨å“ªä¸ªé˜¶æ®µï¼Ÿç“¶é¢ˆåœ¨å“ªï¼Ÿ
é˜¶æ®µåˆ†è§£æ˜¾ç¤ºèƒ½é‡ç“¶é¢ˆå› æ¶æ„è€Œå¼‚ï¼š

| æ¨¡å‹ | ä¸»è¦èƒ½è€—æ¥æº | å…·ä½“æ•°æ® |
|------|-------------|--------|
| **Qwen2.5-VL** | Vision Encoder | ç¼–ç è€—èƒ½ **20.81 J**ï¼Œæ˜¯ LLaVA-1.5 çš„ **6Ã—** |
| **LLaVA-OneVision** | Prefill é˜¶æ®µ | å›  tiling å¯¼è‡´ 3,715 tokensï¼Œprefill è€—èƒ½è¾¾ **95.78 J**ï¼Œä¸º InternVL3 çš„ **12Ã—** |
| **InternVL3 / LLaVA-1.5** | è¾ƒå¹³è¡¡ | ç¼–ç  <10 Jï¼Œprefill ~8â€“50 J |

> ğŸ” **Observation 2**: èƒ½è€—ç“¶é¢ˆè·¨é˜¶æ®µè½¬ç§»ï¼Œåº”å®æ–½ **stage-specific é…ç½®** å’Œ **stage-wise DVFS**ã€‚

#### âœ… RQ3: å¤šæ¨¡æ€æ‰§è¡Œå¦‚ä½•æ”¹å˜ GPU åŠŸç‡è¡Œä¸ºï¼Ÿ
- æ–‡æœ¬-only æ¨¡å‹åœ¨ batch=32 æ—¶è¿…é€Ÿè¾¾åˆ° GPU åŠŸè€—ä¸Šé™ï¼ˆ~400Wï¼‰ï¼Œåˆ©ç”¨ç‡é«˜ï¼›
- æ‰€æœ‰ MLLM å‡å‡ºç°æ˜æ˜¾çš„ **mid-power phaseï¼ˆ100â€“250Wï¼‰**ï¼ŒæŒç»­æ•°åæ¯«ç§’ï¼Œå¯¹åº” vision encoding é˜¶æ®µï¼›
- **Qwen2.5-VL** å‡ºç°é˜¶æ¢¯çŠ¶åŠŸè€—ï¼ˆ~200Wï¼‰ï¼Œè¡¨æ˜ encoder å†…éƒ¨å­˜åœ¨å¤šä¸ªå­é˜¶æ®µï¼›
- **LLaVA-OneVision** åŠŸè€—æ³¢åŠ¨å‰§çƒˆï¼Œåæ˜  tile å¹¶è¡Œå¤„ç†ä¸å‡ã€‚

> ğŸ” **Observation 3**: å¤šæ¨¡æ€æ‰§è¡Œå¯¼è‡´ GPU é•¿æ—¶é—´å¤„äºä½é¢‘ä½è´Ÿè½½çŠ¶æ€ï¼Œâ€œrace-to-idleâ€ç­–ç•¥å¤±æ•ˆã€‚

#### âœ… RQ4: è¾“å…¥å¤æ‚åº¦å¦‚ä½•å½±å“èƒ½æ•ˆï¼Ÿ
- **å›¾åƒæ•°é‡å¢åŠ **ï¼ˆå›¾6ï¼‰ï¼š
  - æ‰€æœ‰æ¨¡å‹èƒ½é‡ä¸å»¶è¿Ÿçº¿æ€§ä¸Šå‡ï¼›
  - æ¯å¢ä¸€å¼ å›¾è¾¹é™…æˆæœ¬ï¼š**~15â€“35 J/image**ï¼ŒLLaVA-OneVision æœ€é«˜ã€‚
- **åˆ†è¾¨ç‡æå‡**ï¼ˆå›¾7ï¼‰ï¼š
  - **LLaVA-1.5**ï¼ˆå›ºå®š patchï¼‰ï¼štoken æ•°å‡ ä¹ä¸å˜ï¼Œèƒ½è€—å¹³å¦ï¼›
  - **InternVL3 / LLaVA-OneVision**ï¼štoken æ•°ç¦»æ•£å¢é•¿ï¼Œèƒ½è€—è·³è·ƒä¸Šå‡ï¼›
  - **Qwen2.5-VL**ï¼š>1024Ã—1024 å token æ•°æ¿€å¢ï¼Œèƒ½è€—é™¡å‡ã€‚

> ğŸ” **Observation 4**: ä¸åŒæ¶æ„å¯¹è¾“å…¥å¤æ‚åº¦æ•æ„Ÿåº¦ä¸åŒï¼Œéœ€ **input-aware serving policy**ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆDVFS æ•ˆæœï¼‰
å¯¹ **InternVL3** ä¸ **Qwen2.5-VL** è¿›è¡Œ stage-wise DVFS æµ‹è¯•ï¼ˆbatch=32ï¼‰ï¼š

| åœºæ™¯ | é¢‘ç‡è°ƒæ•´ | èƒ½è€—å˜åŒ– | å»¶è¿Ÿå˜åŒ– | ç»“è®º |
|------|---------|----------|----------|------|
| **InternVL3 â€“ Encode** | 1050MHz â†’ 1410MHz | +24.9% | -11.8% | ç¼–ç é˜¶æ®µèŠ‚èƒ½ç©ºé—´å¤§ï¼Œå°å¹…é™é¢‘å³å¯çœç”µ |
| **InternVL3 â€“ Prefill** | åŒä¸Š | +10.6% | -8.8% | prefill æ›´ä¾èµ–é«˜é¢‘ï¼Œé€‚åˆå“åº”ä¼˜å…ˆåœºæ™¯ |
| **Qwen2.5-VL â€“ Encode** | åŒä¸Š | +16.5% | -10.8% | encoder æ”¯é…æ•´ä½“å»¶è¿Ÿï¼Œéœ€æƒè¡¡ SLO |

> âœ… **DVFS å¯å®ç°æœ‰æ•ˆèŠ‚èƒ½**ï¼šåœ¨å®¹å¿ä¸€å®šå»¶è¿Ÿçš„å‰æä¸‹ï¼Œé€šè¿‡é™ä½éå…³é”®é˜¶æ®µé¢‘ç‡ï¼Œæ˜¾è‘—å‡å°‘èƒ½è€—ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **â€œæ¨¡æ€è†¨èƒ€â€æ˜¯ MLLM èƒ½æ•ˆçš„ä¸»è¦ç“¶é¢ˆ**ï¼šè§†è§‰ token çš„ç”Ÿæˆä¸ä¼ æ’­æ˜¾è‘—æŠ¬é«˜äº† Prefill å’Œ Encoder é˜¶æ®µçš„èƒ½è€—ï¼Œå¢å¹…è¾¾ **17%â€“94%**ã€‚
2. **èƒ½è€—ç“¶é¢ˆé«˜åº¦ä¾èµ–æ¶æ„è®¾è®¡**ï¼š
   - æœ‰çš„æ¨¡å‹å—å›°äº **compute-heavy vision encoder**ï¼ˆå¦‚ Qwen2.5-VLï¼‰ï¼›
   - æœ‰çš„åˆ™å›  **token è†¨èƒ€å¯¼è‡´ Prefill æˆä¸ºç“¶é¢ˆ**ï¼ˆå¦‚ LLaVA-OneVisionï¼‰ã€‚
3. **GPU å­˜åœ¨æ˜¾è‘— underutilization**ï¼švision encoding é˜¶æ®µå¸¸å¤„äº mid-power çŠ¶æ€ï¼Œä¼ ç»Ÿé«˜é¢‘ç­–ç•¥æµªè´¹èƒ½æºã€‚
4. **è¾“å…¥å¤æ‚åº¦é©±åŠ¨éçº¿æ€§èƒ½è€—å¢é•¿**ï¼šé«˜åˆ†è¾¨ç‡æˆ–å¤šå›¾åƒè¯·æ±‚è™½å æ¯”å°ï¼Œå´å¯èƒ½ä¸»å¯¼å°¾å»¶è¿Ÿä¸èƒ½è€—å³°å€¼ã€‚
5. **Stage-wise DVFS æ˜¯å¯è¡Œä¼˜åŒ–è·¯å¾„**ï¼šå¯æ ¹æ® SLO åŠ¨æ€è°ƒèŠ‚å„é˜¶æ®µé¢‘ç‡ï¼Œåœ¨ **æ€§èƒ½ä¸èƒ½æ•ˆä¹‹é—´å–å¾—è‰¯å¥½æŠ˜è¡·**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å®éªŒä»…åŸºäºå•å¡ A100ï¼Œæœªè€ƒè™‘åˆ†å¸ƒå¼æˆ–å¤šè®¾å¤‡ååŒåœºæ™¯ï¼›
- DVFS æ§åˆ¶ä»ä¸ºé™æ€é…ç½®ï¼Œå°šæœªå®ç°åœ¨çº¿è‡ªé€‚åº”è°ƒæ§ï¼›
- æ‰€æœ‰æ¨¡å‹é›†ä¸­åœ¨ 7Bâ€“8B å‚æ•°è§„æ¨¡ï¼Œç»“è®ºæ˜¯å¦é€‚ç”¨äºæ›´å¤§æˆ–æ›´å°æ¨¡å‹æœ‰å¾…éªŒè¯ï¼›
- æœªæ¶µç›–è§†é¢‘ã€éŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€ï¼Œå½“å‰åˆ†æå±€é™äº vision-language åœºæ™¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³ä¸åŒè§„æ¨¡æ¨¡å‹**ï¼šç ”ç©¶å‚æ•°é‡å¯¹â€œæ¨¡æ€è†¨èƒ€â€æ•ˆåº”çš„æ”¾å¤§æˆ–ç¼“è§£ä½œç”¨ã€‚
2. **å¼€å‘åŠ¨æ€å®æ—¶ DVFS æ§åˆ¶å™¨**ï¼šç»“åˆè¯·æ±‚ç‰¹å¾é¢„æµ‹ä¸é˜¶æ®µè¯†åˆ«ï¼Œå®ç° **SLO-aware frequency scaling**ã€‚
3. **æ¢ç´¢ disaggregated æ¶æ„ä¸‹çš„èƒ½æ•ˆè¶‹åŠ¿**ï¼šå¦‚å°† vision encoder ä¸ LLM åˆ†ç¦»éƒ¨ç½²ï¼Œè¯„ä¼°è·¨åŠ é€Ÿå™¨é€šä¿¡å¼€é”€ä¸èƒ½æ•ˆæ”¶ç›Šã€‚
4. **æ‹“å±•è‡³æ›´å¤šæ¨¡æ€**ï¼šå°†åˆ†ææ¡†æ¶åº”ç”¨äºæ”¯æŒ audioã€videoã€time-series çš„é€šç”¨ MLLMã€‚
5. **æ„å»º MLLM èƒ½æ•ˆå»ºæ¨¡å·¥å…·**ï¼šåŸºäºè¾“å…¥ç‰¹å¾ï¼ˆimage count/resolutionï¼‰ã€æ¨¡å‹ç»“æ„é¢„æµ‹èƒ½è€—ä¸å»¶è¿Ÿã€‚

---

> ğŸ“Œ **æ€»ä½“è¯„ä»·**ï¼šè¯¥è®ºæ–‡å¡«è¡¥äº† MLLM èƒ½æ•ˆåˆ†æé¢†åŸŸçš„ç©ºç™½ï¼Œæå‡ºäº†â€œæ¨¡æ€è†¨èƒ€â€è¿™ä¸€é‡è¦æ¦‚å¿µï¼Œå¹¶é€šè¿‡è¯¦å®çš„å®éªŒæ­ç¤ºäº†å…¶èƒ½é‡åˆ†å¸ƒè§„å¾‹ä¸ä¼˜åŒ–æ½œåŠ›ã€‚å…¶ stage-level åˆ†æèŒƒå¼å’Œ DVFS å®è·µä¸ºæ„å»ºå¯æŒç»­ AI ç³»ç»Ÿæä¾›äº†åšå®åŸºç¡€ã€‚

</details>

---

### 10. [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)

**Authors**: Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.22309v1  

#### Abstract
Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# LLMBoost: Make Large Language Models Stronger with Boosting è®ºæ–‡æ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **LLM Ensemble** æ–¹æ³•é€šå¸¸å°†æ¨¡å‹è§†ä¸ºé»‘ç®±ï¼Œä»…åœ¨è¾“å…¥æˆ–æœ€ç»ˆè¾“å‡ºå±‚é¢è¿›è¡Œèåˆï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ã€åŠ æƒèšåˆç­‰ï¼‰ï¼Œå¿½ç•¥äº†æ¨¡å‹å†…éƒ¨ä¸°å¯Œçš„éšè—çŠ¶æ€è¡¨ç¤ºå’Œè·¨æ¨¡å‹äº¤äº’ä¿¡æ¯ã€‚è¿™ç§ç²—ç²’åº¦çš„èåˆæ–¹å¼é™åˆ¶äº†æ¨¡å‹é—´äº’è¡¥çŸ¥è¯†çš„å……åˆ†æŒ–æ˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **LLMBooST**ï¼Œä¸€ç§æ–°é¢–çš„åŸºäºæå‡ï¼ˆBoostingï¼‰èŒƒå¼çš„ **LLM é›†æˆå¾®è°ƒæ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæ˜¾å¼åˆ©ç”¨ LLM çš„ä¸­é—´çŠ¶æ€ï¼Œæ‰“ç ´é»‘ç®±å£å’ã€‚å…·ä½“åŒ…å«ä¸‰å¤§åˆ›æ–°ï¼š

1.  **è·¨æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶ (Cross-Model Attention)**ï¼š
    *   å…è®¸åç»­æ¨¡å‹ï¼ˆsuccessorï¼‰ç›´æ¥è®¿é—®å¹¶èåˆå‰é©±æ¨¡å‹ï¼ˆpredecessorï¼‰çš„éšè—å±‚çŠ¶æ€ã€‚
    *   åœ¨å±‚çº§åˆ«æ„å»ºè·¨æ¨¡å‹è¿æ¥ï¼Œå®ç°ç»†ç²’åº¦çš„ã€åˆ†å±‚çš„é”™è¯¯ä¿®æ­£ä¸çŸ¥è¯†ä¼ é€’ã€‚

2.  **é“¾å¼è®­ç»ƒèŒƒå¼ (Chain Training Paradigm)**ï¼š
    *   é‡‡ç”¨é¡ºåºå¾®è°ƒç­–ç•¥ï¼Œæ¯ä¸ªåç»­æ¨¡å‹åŸºäºå…¶å‰é©±æ¨¡å‹çš„è®­ç»ƒæ—¥å¿—è¿›è¡Œå¾®è°ƒã€‚
    *   å¼•å…¥ **é”™è¯¯æŠ‘åˆ¶ç›®æ ‡ (Error-Suppression Objective)**ï¼Œè¯¥æŸå¤±å‡½æ•°æ—¨åœ¨æŠ‘åˆ¶å‰é©±æ¨¡å‹é¢„æµ‹çš„æœ€å¤§é”™è¯¯ logitï¼ŒåŒæ—¶å¢å¼ºæ­£ç¡® logitï¼Œä»è€Œä»¥æœ€å°çš„é¢å¤–è®¡ç®—é‡ç¡®ä¿æ€§èƒ½é€çº§æå‡ã€‚

3.  **è¿‘ä¼¼å¹¶è¡Œæ¨ç†èŒƒå¼ (Near-Parallel Inference Paradigm)**ï¼š
    *   ä¸ºè§£å†³ä¸²è¡Œä¾èµ–å¯¼è‡´çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æµæ°´çº¿å¼æ¨ç†ã€‚
    *   åç»­æ¨¡å‹æ— éœ€ç­‰å¾…å‰é©±æ¨¡å‹å®Œæˆæ•´ä¸ªåºåˆ—ç”Ÿæˆï¼Œè€Œæ˜¯å¯ä»¥ç«‹å³æ¥æ”¶å¹¶å¤„ç†æ¥è‡ªå‰é©±æ¨¡å‹æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ï¼Œå®ç°äº†è¿‘ä¹å•æ¨¡å‹è§£ç æ•ˆç‡çš„æ¨ç†é€Ÿåº¦ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
-   **æ€§èƒ½æ›´å¼º**ï¼šé€šè¿‡åˆ©ç”¨å†…éƒ¨è¡¨ç¤ºè¿›è¡Œç²¾ç»†çš„é”™è¯¯ä¿®æ­£ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ã€‚
-   **æ•ˆç‡æ›´é«˜**ï¼šè¿‘ä¼¼å¹¶è¡Œæ¨ç†å¤§å¹…é™ä½äº†é›†æˆæ–¹æ³•çš„æ¨ç†å»¶è¿Ÿï¼Œç›¸æ¯”ä¼ ç»Ÿä¸²è¡Œé›†æˆå‡å°‘äº† 47% çš„å»¶è¿Ÿã€‚
-   **ç†è®ºå¯è¯**ï¼šä»ç†è®ºä¸Šè¯æ˜äº†åœ¨æœ‰ç•Œä¿®æ­£å‡è®¾ä¸‹ï¼Œåºåˆ—é›†æˆèƒ½ä¿è¯æ€§èƒ½å•è°ƒæå‡ã€‚

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
-   **å¸¸è¯†æ¨ç† (Commonsense Reasoning)**ï¼š
    -   PIQA, HellaSwag, WinoGrande, BoolQ, SIQA, OpenbookQA (OBQA)
-   **ç®—æœ¯æ¨ç† (Arithmetic Reasoning)**ï¼š
    -   AQuA, GSM8K, MAWPS, SVAMP
-   **å·¥ä¸šåœºæ™¯æ•°æ®é›†**ï¼š
    -   **Chinatelecom Cloud Agent Dataset (CCAD)**ï¼šä¸€ä¸ªå®šåˆ¶çš„ã€ç”¨äºè¯„ä¼° AI ä»£ç†åœ¨çœŸå®å…¬æœ‰äº‘ç¯å¢ƒä¸­å·¥å…·é“¾è°ƒåº¦èƒ½åŠ›çš„æ•°æ®é›†ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
-   **å¾®è°ƒæ–¹æ³•**ï¼šé‡‡ç”¨ **LoRA** è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
-   **è¯„ä¼°æŒ‡æ ‡**ï¼š
    -   ä¸»è¦æŒ‡æ ‡ä¸º **å‡†ç¡®ç‡ (Accuracy)**ã€‚
    -   æ•ˆç‡æŒ‡æ ‡åŒ…æ‹¬ **ç«¯åˆ°ç«¯å»¶è¿Ÿ (End-to-end latency)**ã€**æ¯ token ç”Ÿæˆæˆæœ¬ (per-token latency)** å’Œ **å³°å€¼ GPU å†…å­˜å ç”¨ (peak GPU memory usage)**ã€‚
-   **æ¨¡å‹è§„æ¨¡**ï¼šåœ¨å¤šç§å‚æ•°è§„æ¨¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬ Llama-3.1-8B, Llama-3.2-3B, Qwen-2.5-7B, Qwen-2.5-3B ç­‰ï¼Œå¹¶æ„å»ºäº†åŒæ„ï¼ˆå¦‚ 2Ã—8Bï¼‰å’Œå¼±å¼‚æ„ï¼ˆå¦‚ 8B+3Bï¼‰çš„é›†æˆã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
-   **VOTE**ï¼šåŸºäºå¤šæ•°æŠ•ç¥¨çš„é›†æˆæ–¹æ³•ã€‚
-   **UNITE**ï¼šä¸€ç§é«˜æ•ˆçš„ LLM é›†æˆæ–¹æ³•ï¼Œé€šè¿‡èåˆå„æ¨¡å‹ top-k token æ¥ç»„åˆæ¨¡å‹ã€‚
-   **T-copilot**ï¼šä¸€ç§é€šè¿‡è¾…åŠ©é€‚é…å™¨ï¼ˆcopilot adapterï¼‰æ¥ä¿®æ­£ä¸»æ¨¡å‹ï¼ˆpilotï¼‰é”™è¯¯çš„æ–¹æ³•ã€‚

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
-   **æ€»ä½“å¹³å‡æå‡**ï¼šLLMBooST ç›¸æ¯”å…¶ä»–é›†æˆåŸºçº¿ï¼Œåœ¨å„é¡¹ä»»åŠ¡æµ‹è¯•ä¸­å¹³å‡æå‡äº† **3.9%** çš„å‡†ç¡®ç‡ï¼Œæœ€é«˜æå‡è¾¾ **6.6%**ã€‚
-   **CCAD æ•°æ®é›†è¡¨ç°**ï¼šåœ¨ Chinatelecom Cloud Agent Dataset ä¸Šï¼ŒLLMBooST å°†å‡†ç¡®ç‡ä» Llama-3.1-8B å•æ¨¡å‹çš„ 52.0% æå‡è‡³ **55.0%**ï¼Œç›¸æ¯” VOTE åŸºçº¿æå‡äº† 3.0%ã€‚
-   **æ¨ç†æ•ˆç‡**ï¼šLLMBooST çš„è¿‘ä¼¼å¹¶è¡Œè§£ç ç­–ç•¥ç›¸æ¯”ä¼ ç»Ÿä¸²è¡Œé›†æˆï¼Œå°†æ¨ç†å»¶è¿Ÿé™ä½äº† **47%**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
-   **ç®—æœ¯æ¨ç† (Table 1)**ï¼š
    -   åœ¨ Llama-3.1-8B (2Ã—8B) è®¾ç½®ä¸‹ï¼ŒLLMBooST çš„å¹³å‡å‡†ç¡®ç‡è¾¾åˆ° **71.5%**ï¼Œæ˜¾è‘—ä¼˜äº VOTE (69.8%)ã€UNITE (68.5%) å’Œ T-copilot (68.7%)ã€‚
    -   åœ¨ Qwen-2.5-7B (2Ã—7B) è®¾ç½®ä¸‹ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º **80.2%**ï¼ŒåŒæ ·ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚
-   **å¸¸è¯†æ¨ç† (Table 2)**ï¼š
    -   åœ¨ Llama-3.1-8B (2Ã—8B) è®¾ç½®ä¸‹ï¼ŒLLMBooST å¹³å‡å‡†ç¡®ç‡ä¸º **84.4%**ï¼Œä¼˜äº UNITE (83.3%) å’Œ VOTE (79.7%)ã€‚
    -   åœ¨ Qwen-2.5-7B (2Ã—7B) è®¾ç½®ä¸‹ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º **85.6%**ï¼Œè¿œè¶…å…¶ä»–åŸºçº¿ã€‚
-   **å‚æ•°é¢„ç®—å¯¹æ¯” (Table 3)**ï¼š
    -   ä¸¤ä¸ª Qwen-2.5-3B æ¨¡å‹é›†æˆ (6B) çš„ LLMBooST æ€§èƒ½è¾¾åˆ°äº†å•ä¸ª Qwen-2.5-7B (7B) çš„æ°´å¹³ï¼Œä¸”ç®—æœ¯æ¨ç†å¾—åˆ†æ›´é«˜ï¼ŒåŒæ—¶èŠ‚çœäº† 1B å‚æ•°ã€‚
    -   ä¸¤ä¸ª Qwen-2.5-7B æ¨¡å‹é›†æˆ (14B) çš„æ€§èƒ½è¶…è¿‡äº†å•ä¸ª Qwen-2.5-14B (14B) æ¨¡å‹ï¼Œä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼ˆå»¶è¿Ÿä» 98ms/token é™è‡³ 75ms/tokenï¼‰ã€‚

### æ¶ˆèå®éªŒç»“æœ
-   **ç§»é™¤é”™è¯¯æŠ‘åˆ¶ç›®æ ‡ (wo E.S.O.)**ï¼šæ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ŒéªŒè¯äº†è¯¥ç›®æ ‡å¯¹é”™è¯¯ä¿®æ­£çš„å…³é”®ä½œç”¨ã€‚
-   **ç¦ç”¨è·¨æ¨¡å‹æ³¨æ„åŠ› (wo C.A.)**ï¼šæ€§èƒ½ä¸‹é™ï¼Œè¯æ˜äº†åˆ©ç”¨å‰é©±æ¨¡å‹éšè—çŠ¶æ€çš„é‡è¦æ€§ã€‚
-   **ç§»é™¤ top-k å›ä¼  (wo top-k)**ï¼šæ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œè¡¨æ˜è¯¥æœºåˆ¶æœ‰åŠ©äºç¨³å®šæ¨ç†ã€‚
-   **è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ**ï¼šå®éªŒè¡¨æ˜ï¼Œ`Î±` å’Œ `Î²` çš„é€‰æ‹©å¯¹æ€§èƒ½æœ‰å½±å“ï¼Œå­˜åœ¨æœ€ä¼˜å€¼ï¼ˆå¦‚ `Î±=0.9`, `Î²=0.1`ï¼‰ï¼Œè€Œ `top-k` é€‰æ‹©è¾ƒå°çš„å€¼ï¼ˆå¦‚ k=2ï¼‰å³å¯è·å¾—æœ€ä½³æ•ˆæœã€‚

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1.  **å†…éƒ¨äº¤äº’è‡³å…³é‡è¦**ï¼šæ‰“ç ´ LLM é›†æˆçš„â€œé»‘ç®±â€å‡è®¾ï¼Œåˆ©ç”¨æ¨¡å‹é—´çš„å†…éƒ¨éšè—çŠ¶æ€è¿›è¡Œäº¤äº’ï¼Œæ˜¯æå‡é›†æˆæ€§èƒ½çš„æœ‰æ•ˆé€”å¾„ã€‚
2.  **æå‡èŒƒå¼æœ‰æ•ˆ**ï¼šå€Ÿé‰´ Boosting çš„æ€æƒ³ï¼Œé€šè¿‡é“¾å¼è®­ç»ƒå’Œé”™è¯¯æŠ‘åˆ¶ç›®æ ‡ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°ä¿®æ­£å‰é©±æ¨¡å‹çš„é”™è¯¯ï¼Œå®ç°æ€§èƒ½çš„æ¸è¿›å¼æå‡ã€‚
3.  **æ•ˆç‡ä¸æ€§èƒ½å…¼å¾—**ï¼šæå‡ºçš„è¿‘ä¼¼å¹¶è¡Œæ¨ç†èŒƒå¼æˆåŠŸè§£å†³äº†å¤šæ¨¡å‹ä¸²è¡Œæ¨ç†çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œä½¿å¾—é«˜æ€§èƒ½é›†æˆåœ¨å®é™…åº”ç”¨ä¸­æ›´å…·å¯è¡Œæ€§ã€‚
4.  **ç†è®ºä¸å®è·µä¸€è‡´**ï¼šç†è®ºåˆ†æè¯æ˜äº†åœ¨åˆç†å‡è®¾ä¸‹ï¼ŒLLMBooST èƒ½å¤Ÿä¿è¯æ€§èƒ½å•è°ƒæå‡ï¼Œè¿™ä¸å®éªŒè§‚å¯Ÿåˆ°çš„ç»“æœç›¸ç¬¦ã€‚

### æ–¹æ³•çš„å±€é™æ€§
-   **è·¨æ¨¡å‹å…¼å®¹æ€§é™åˆ¶**ï¼šå½“å‰æ–¹æ³•è¦æ±‚é›†æˆçš„å­æ¨¡å‹å…·æœ‰ç›¸åŒçš„ç»“æ„è®¾è®¡å’Œéšè—çŠ¶æ€æ ¼å¼ï¼Œç›®å‰ä¸»è¦é€‚ç”¨äºåŒä¸€æ¨¡å‹ç³»åˆ—å†…çš„åŒæ„æˆ–å¼±å¼‚æ„é›†æˆï¼ˆå¦‚ LLaMA-3 æˆ– Qwen-2.5 ç³»åˆ—ï¼‰ã€‚å¯¹äºä¸åŒå®¶æ—çš„å¼‚æ„æ¨¡å‹ï¼ˆå¦‚ LLaMA å’Œ Qwenï¼‰ï¼Œç”±äº tokenizer å’Œæ¶æ„å·®å¼‚ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
-   å°†æ¡†æ¶æ‰©å±•åˆ° **å¤šæ™ºèƒ½ä½“ (multi-agent)** åœºæ™¯ï¼Œè®©ä¸åŒæ™ºèƒ½ä½“ä¸“ç²¾äºæ¨ç†ã€è§„åˆ’æˆ–å·¥å…·ä½¿ç”¨ã€‚
-   æ¢ç´¢ **è‡ªé€‚åº”è·¯ç”±æœºåˆ¶ (adaptive routing)**ï¼Œæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æ™ºèƒ½ä½“æˆ–æ¨ç†è·¯å¾„ã€‚
-   ç»“åˆ **æŠ•æœºæ¨ç† (speculative reasoning)**ï¼Œè¿›ä¸€æ­¥é™ä½å¤§è§„æ¨¡éƒ¨ç½²ä¸­çš„å»¶è¿Ÿï¼Œæé«˜å¯æ‰©å±•æ€§ã€‚

</details>

---

### 11. [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)

**Authors**: Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.22492v1  

#### Abstract
RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šRole-Based Fault Tolerance System for LLM RL Post-Training

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒç³»ç»Ÿåœ¨æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´ä¸¥é‡çš„**å®¹é”™æ€§æŒ‘æˆ˜**ã€‚RL åè®­ç»ƒåŒæ—¶åŒ…å« **rolloutï¼ˆæ¨ç†/å·¥å…·äº¤äº’ï¼‰** å’Œ **trainingï¼ˆç­–ç•¥ä¼˜åŒ–ï¼‰** é˜¶æ®µï¼ŒäºŒè€…äº¤é”™æ‰§è¡Œä¸”è€—æ—¶é•¿ã€‚ç°æœ‰å®¹é”™æ¡†æ¶ï¼ˆå¦‚ ByteRobustï¼‰é€šå¸¸é‡‡ç”¨â€œå…¨ä»»åŠ¡é‡å¯â€æœºåˆ¶ï¼Œä¸€æ—¦ä»»ä¸€è§’è‰²ï¼ˆtrainer æˆ– rolloutï¼‰å‘ç”Ÿæœºå™¨æ•…éšœï¼Œæ•´ä¸ª RL ä»»åŠ¡éƒ½ä¼šè¢«ä¸­æ–­å¹¶ä»æ£€æŸ¥ç‚¹é‡æ–°å¼€å§‹ï¼Œå¯¼è‡´å¤§é‡å·²ç”Ÿæˆçš„ rollout è½¨è¿¹æµªè´¹ã€åˆå§‹åŒ–å»¶è¿Ÿæ˜¾è‘—ã€‚

æ­¤å¤–ï¼Œä¼ ç»ŸåŸºäº rank-level æˆ– cluster-level çš„æ•…éšœæ£€æµ‹æœºåˆ¶åœ¨ RL åœºæ™¯ä¸‹å­˜åœ¨**è¯¯æŠ¥**ï¼ˆå¦‚ rollout ç­‰å¾…å·¥å…·å“åº”æ—¶ GPU ç©ºé—²è¢«è¯¯åˆ¤ä¸ºæ•…éšœï¼‰æˆ–**å»¶è¿Ÿé«˜**çš„é—®é¢˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº† **RobustRL** â€”â€”é¦–ä¸ªé¢å‘ LLM RL åè®­ç»ƒçš„**åŸºäºè§’è‰²çš„å®¹é”™ç³»ç»Ÿ**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **è§’è‰²éš”ç¦»ï¼ˆRole-Based Isolationï¼‰**ï¼šå°† trainerã€rollout å’Œç®¡ç†è§’è‰²è§†ä¸ºç‹¬ç«‹çš„åˆ†å¸ƒå¼å­ä»»åŠ¡ï¼Œå®ç°æ•…éšœéš”ç¦»ã€‚
- **Detect-Restart-Reconnect èŒƒå¼**ï¼š
  1. **Detect**ï¼šè®¾è®¡**è§’è‰²æ„ŸçŸ¥çš„ç›‘æ§æœºåˆ¶**ï¼ŒåŒºåˆ†ä¸åŒé˜¶æ®µçš„è¡Œä¸ºç‰¹å¾ï¼Œé¿å…è¯¯æ£€ã€‚
  2. **Restart**ï¼šä»…é‡å¯å¤±è´¥çš„è§’è‰²ï¼Œè€Œéæ•´ä¸ªä»»åŠ¡ï¼›åˆ©ç”¨ rollout ä½œä¸º trainer çš„ warm standbyï¼Œå®ç°éä¸­æ–­æ¢å¤ã€‚
  3. **Reconnect**ï¼šç”¨åŸºäº **UCXï¼ˆUnified Communication Xï¼‰** çš„åŠ¨æ€ç‚¹å¯¹ç‚¹é€šä¿¡æ›¿ä»£é™æ€çš„ NCCL é›†ä½“é€šä¿¡ï¼Œæ”¯æŒæ•…éšœåå¿«é€Ÿæƒé‡åŒæ­¥ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ByteRobust / ä¼ ç»Ÿæ–¹æ³• | RobustRL |
|------|------------------------|---------|
| å®¹é”™ç²’åº¦ | å…¨ä»»åŠ¡é‡å¯ | è§’è‰²çº§ç»†ç²’åº¦æ¢å¤ |
| Rollout è¿›åº¦ä¿ç•™ | âŒ å¤±å»æ‰€æœ‰ rollout è¿›å±• | âœ… ä¿æŒ rollout ç»§ç»­è¿è¡Œ |
| Trainer æ¢å¤é€Ÿåº¦ | ä¾èµ–é¢å¤– warm standby æœºå™¨ï¼ˆèµ„æºæµªè´¹ï¼‰ | åŠ¨æ€å€Ÿç”¨ rollout æœºå™¨ä½œä¸º warm standbyï¼ˆèµ„æºé«˜æ•ˆï¼‰ |
| æƒé‡åŒæ­¥ | NCCL é™æ€ç»„é€šä¿¡ï¼Œä¸æ”¯æŒåŠ¨æ€åŠ å…¥ | UCX åŠ¨æ€ç‚¹å¯¹ç‚¹ + Relay Server è®¾è®¡ï¼Œæ”¯æŒæ•…éšœæ¢å¤è¿æ¥ |
| æ•…éšœæ£€æµ‹å‡†ç¡®æ€§ | Rank-level æ˜“è¯¯æŠ¥ï¼ŒCluster-level å»¶è¿Ÿé«˜ | Phase-aware æ£€æµ‹ï¼Œå‡†ç¡®ç‡é«˜ã€å»¶è¿Ÿä½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **æ•°å­¦æ¨ç†ä»»åŠ¡**ï¼š`DAPO-Math17K`ï¼Œä½¿ç”¨è§„åˆ™å¥–åŠ±å‡½æ•°è¯„åˆ†ã€‚
- **è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼ˆå¤šè½®å·¥å…·è°ƒç”¨ï¼‰**ï¼š`SWE-bench`ï¼Œé€šè¿‡æ²™ç®±ç¯å¢ƒè¿›è¡Œä»£ç ä¿®å¤æˆ–å¤šè½®äº¤äº’ç”Ÿæˆè½¨è¿¹ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹è§„æ¨¡**ï¼šQwen3-8Bã€Qwen3-32Bã€Qwen3-235B-A22Bã€‚
- **ç¡¬ä»¶å¹³å°**ï¼š32 å° GPU æœåŠ¡å™¨ï¼Œå…± 256 å— NVIDIA H20 96GB GPUï¼ŒNVLink + 4Ã—200Gbps NICã€‚
- **è®­ç»ƒæ¶æ„**ï¼šæµ‹è¯•äº†ä¸‰ç§æ¨¡å¼ï¼š
  - **Sync**ï¼šåŒæ­¥æ¨¡å¼ï¼ˆhybrid éƒ¨ç½²ï¼‰
  - **Semi-sync**ï¼šåŠåŒæ­¥ï¼ˆæ··åˆ + ç‹¬ç«‹ rolloutï¼‰
  - **Async**ï¼šå¼‚æ­¥æ¨¡å¼ï¼ˆç‹¬ç«‹ trainer ä¸ rolloutï¼‰
- **æ•…éšœæ³¨å…¥é¢‘ç‡**ï¼šæ¯ 10% çš„è®­ç»ƒæ­¥éª¤æ³¨å…¥ä¸€æ¬¡ trainer æ•…éšœï¼ˆæç«¯åœºæ™¯æ¨¡æ‹Ÿ spot instance ä¸ç¨³å®šï¼‰ã€‚
- **æ‰¹å¤§å°**ï¼šå…¨å±€ batch size 512ï¼Œæ¯ batch 64 promptsï¼Œæ¯ä¸ª prompt ç”Ÿæˆ 8 ä¸ª responseã€‚
- **æœ€å¤§é•¿åº¦**ï¼š65536 tokensï¼Œæœ€å¤š 50 è½®å·¥å…·äº¤äº’ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **End-to-End Training Time**ï¼šæ€»è®­ç»ƒæ—¶é—´ã€‚
- **ETTRï¼ˆEffective Training Time Ratioï¼‰**ï¼šæœ‰æ•ˆè®­ç»ƒæ—¶é—´å æ¯”ï¼ˆæ’é™¤é‡å¯å¼€é”€ï¼‰ã€‚
- **Restart Latency Breakdown**ï¼šå®ä¾‹é‡å¯ã€checkpoint åŠ è½½ã€worker åˆå§‹åŒ–ç­‰å„é˜¶æ®µè€—æ—¶ã€‚
- **Weight Synchronization Efficiency**ï¼šæ¨¡å‹æƒé‡ä» trainer åŒæ­¥åˆ° rollout çš„å»¶è¿Ÿã€‚
- **Checkpoint Overhead**ï¼šæ¯æ­¥ä¿å­˜ checkpoint çš„é˜»å¡æ—¶é—´ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**ï¼šæ— æ•…éšœçš„ç†æƒ³æƒ…å†µã€‚
- **ByteRobust**ï¼šå­—èŠ‚è·³åŠ¨æå‡ºçš„é€šç”¨ LLM è®­ç»ƒå®¹é”™ç³»ç»Ÿï¼Œé‡‡ç”¨å…¨ä»»åŠ¡é‡å¯ + warm standby æœºåˆ¶ã€‚
- **RobustRL**ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•ã€‚

> æ³¨ï¼šæœªä¸å…¶ä»–å¼€æº RL å®¹é”™ç³»ç»Ÿï¼ˆå¦‚ Laminarï¼‰ç›´æ¥æ¯”è¾ƒï¼Œå› å…¶ä»£ç æœªå…¬å¼€ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… ç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ vs. ETTRï¼ˆå›¾11ï¼‰
| ä»»åŠ¡ | æ–¹æ³• | è®­ç»ƒæ—¶é—´ï¼ˆå°æ—¶ï¼‰ | ETTR |
|------|------|------------------|-------|
| Qwen3-8B-Math | Baseline | ~5.0 | 100% |
|                | ByteRobust | ~7.8 | ~60% |
|                | **RobustRL** | **~6.0** | **>80%** |
| Qwen3-32B-SWE | ByteRobust | ~30.5h | ~60% |
|               | **RobustRL** | **~26.0h** | **>80%** |

- **ETTR æå‡**ï¼šRobustRL åœ¨ Qwen3-8B-Math ä¸Šè¾¾åˆ° **>80% ETTR**ï¼Œç›¸æ¯” ByteRobustï¼ˆ~60%ï¼‰æå‡çº¦ **20ä¸ªç™¾åˆ†ç‚¹**ã€‚
- **è®­ç»ƒåŠ é€Ÿ**ï¼šåœ¨ 10% æ•…éšœæ³¨å…¥ä¸‹ï¼ŒRobustRL æ¯” ByteRobust å¿« **8.4%â€“17.4%**ã€‚
- **èŠ‚çœæ—¶é—´**ï¼š
  - 8B-Mathï¼šèŠ‚çœ 0.8â€“2.1 å°æ—¶
  - 32B-SWEï¼šèŠ‚çœ 3.5â€“4.5 å°æ—¶

#### âœ… é‡å¯æ•ˆç‡å¯¹æ¯”ï¼ˆå›¾14ï¼‰
- **å¹³å‡é‡å¯å»¶è¿Ÿé™ä½ 1.5â€“1.7Ã—**ï¼š
  - ä¾‹å¦‚ï¼Œåœ¨ 256 GPU ä¸Šï¼ŒByteRobust é‡å¯è€—æ—¶ ~400sï¼ŒRobustRL ä»… ~230sã€‚
- **ä¼˜åŠ¿æ¥æº**ï¼š
  - è·³è¿‡å®¹å™¨åˆå§‹åŒ–å’Œ Ray é›†ç¾¤é‡å»ºï¼›
  - åˆ©ç”¨ rollout warm standby é¿å… gang scheduling å»¶è¿Ÿã€‚

#### âœ… æƒé‡åŒæ­¥æ•ˆç‡ï¼ˆå›¾17â€“18ï¼‰
- **UCX vs NCCL**ï¼š
  - å¯¹äº 235B æ¨¡å‹ï¼ˆ470GBï¼‰ï¼Œç†è®ºä¼ è¾“æ—¶é—´ 4.7sï¼Œå®é™… UCX å®ç° **~6s**ï¼Œæ¥è¿‘ç†æƒ³å¸¦å®½åˆ©ç”¨ç‡ã€‚
  - NCCL å› éœ€ gather åˆ° rank0 å† broadcastï¼Œæ‰©å±•æ€§å·®ï¼›è€Œ UCX æ”¯æŒç‚¹å¯¹ç‚¹ + relay serverï¼Œå¯çº¿æ€§æ‰©å±•ã€‚
- **Relay è®¾è®¡æ•ˆæœæ˜æ˜¾**ï¼šå¤šä¸ª rollout å¯å¹¶è¡Œä»å·²å®Œæˆæ›´æ–°çš„ rollout æ‹‰å–æƒé‡ï¼Œé¿å… trainer æˆä¸ºç“¶é¢ˆã€‚

#### âœ… Checkpoint å¼€é”€ï¼ˆå›¾19ï¼‰
- **GPU â†’ Memory é˜»å¡æ—¶é—´ < 5 ç§’**ï¼Œè¿œå°äºå•æ­¥ RL æ—¶é—´ï¼ˆåˆ†é’Ÿçº§ç”šè‡³å°æ—¶çº§ï¼‰ã€‚
- ä½¿ç”¨ **ByteCheckpoint** å¼‚æ­¥å†™ç›˜ï¼Œå†…å­˜åˆ°ç£ç›˜è¿‡ç¨‹éé˜»å¡ã€‚
- **æ¯æ­¥ checkpoint å¼€é”€å æ¯” < 1%**ï¼Œå¯æ¥å—ã€‚

#### âœ… æ¶ˆèå®éªŒä¸åˆ†æï¼ˆå›¾15â€“16ï¼‰
- **Rollout è¿›åº¦ä¿ç•™æ”¶ç›Šå·¨å¤§**ï¼š
  - SWE ä»»åŠ¡ä¸­ rollout å°¾éƒ¨å»¶è¿Ÿé«˜è¾¾ **1050s**ï¼Œè¾“å‡ºé•¿åº¦è¶… 4k tokensã€‚
  - RobustRL é¿å…äº†è¿™äº›é•¿å°¾ rollout çš„é‡æ”¾ï¼Œæ˜¾è‘—å‡å°‘æ— æ•ˆè®¡ç®—ã€‚
- **ååç¨³å®šæ€§**ï¼š
  - å½“ rollout æ•…éšœæ—¶ï¼ŒRobustRL é€šè¿‡å‰¯æœ¬æ¥ç®¡ï¼Œtoken è§£ç ååå‡ ä¹ä¸å—å½±å“ã€‚
  - Trainer æ•…éšœæœŸé—´ï¼Œrollout ä»æŒç»­ç”Ÿæˆè½¨è¿¹ï¼Œç»´æŒç³»ç»Ÿæ´»è·ƒã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è§’è‰²éš”ç¦»æ˜¯ RL å®¹é”™çš„å…³é”®**ï¼šå°† trainer ä¸ rollout è§†ä¸ºç‹¬ç«‹è§’è‰²ï¼Œèƒ½æœ‰æ•ˆéš”ç¦»æ•…éšœå½±å“ï¼Œé¿å…â€œç‰µä¸€å‘è€ŒåŠ¨å…¨èº«â€çš„å…¨ä»»åŠ¡é‡å¯ã€‚
2. **rollout å¯ä½œä¸º trainer çš„ warm standby**ï¼šåœ¨ semi-sync/async æ¶æ„ä¸­ï¼ŒåŠ¨æ€å€Ÿç”¨ rollout æœºå™¨ç”¨äº trainer æ¢å¤ï¼Œæ—¢èŠ‚çœèµ„æºåˆåŠ å¿«æ¢å¤é€Ÿåº¦ã€‚
3. **UCX åŠ¨æ€é€šä¿¡ä¼˜äº NCCL**ï¼šNCCL ä¸æ”¯æŒåŠ¨æ€æˆå‘˜å˜æ›´ï¼Œæ— æ³•é€‚åº”æ•…éšœæ¢å¤åœºæ™¯ï¼›UCX çš„ç‚¹å¯¹ç‚¹ + relay æ¶æ„æ›´é€‚åˆå¼‚æ­¥ RL ä¸­çš„æƒé‡åŒæ­¥éœ€æ±‚ã€‚
4. **per-step checkpoint å¯è¡Œä¸”å¿…è¦**ï¼šå°½ç®¡å¼€é”€å­˜åœ¨ï¼Œä½†åœ¨ RL æ­¥éª¤é•¿è¾¾æ•°åˆ†é’Ÿ/å°æ—¶çš„èƒŒæ™¯ä¸‹ï¼Œå…¶ç›¸å¯¹å¼€é”€æå°ï¼Œä¸”èƒ½ä¿è¯çŠ¶æ€ä¸€è‡´æ€§ï¼Œé¿å…é‡ rolloutã€‚
5. **RobustRL åœ¨å¤æ‚ä»»åŠ¡ä¸­æ”¶ç›Šæ›´å¤§**ï¼šSWE ç±»æ¶‰åŠå¤šè½®å·¥å…·è°ƒç”¨çš„ä»»åŠ¡ï¼Œrollout æ—¶é—´æ›´é•¿ï¼Œå› æ­¤ä¿ç•™è¿›åº¦å¸¦æ¥çš„æ”¶ç›Šæ›´æ˜¾è‘—ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è¯Šæ–­èƒ½åŠ›æœ‰é™**ï¼šå½“å‰ç³»ç»Ÿä¾èµ–å¤–éƒ¨ä¿¡å·åˆ¤æ–­æ˜¯å¦ä¸ºâ€œå¯æ¢å¤æ•…éšœâ€ï¼Œå¯¹äºä»£ç é”™è¯¯æˆ–é…ç½®é”™è¯¯ä»éœ€äººå·¥ä»‹å…¥æˆ–å…¨ä»»åŠ¡é‡å¯ã€‚
- **åŒè´¨åŒ–å‡è®¾**ï¼šwarm standby æœºåˆ¶è¦æ±‚ rollout ä¸ trainer æœºå™¨åŒæ„ä¸”åœ¨åŒä¸€æ•°æ®ä¸­å¿ƒï¼Œé™åˆ¶äº†è·¨é›†ç¾¤éƒ¨ç½²çµæ´»æ€§ã€‚
- **æœªå¤„ç† silent data corruption æˆ– straggler é—®é¢˜**ï¼šè™½ç„¶æ¶æ„ä¸Šæ”¯æŒæ‰©å±•ï¼Œä½†ç›®å‰æœªé›†æˆç›¸å…³æ£€æµ‹æ¨¡å—ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **æ›´ç²¾ç»†çš„æ ¹å› å®šä½å·¥å…·**ï¼šå¼€å‘ä¸“ç”¨äº RL è®­ç»ƒç³»ç»Ÿçš„è¯Šæ–­å·¥å…·ï¼Œè¯†åˆ«è·¨è§’è‰²ä¾èµ–å¼•å‘çš„æ•…éšœï¼ˆå¦‚ OOM ç”±å…±ç½®è§’è‰²å¼•èµ·ï¼‰ã€‚
- **è§’è‰²çº§çƒ­æ›´æ–°ï¼ˆHot Updateï¼‰**ï¼šç»“åˆè§’è‰²éš”ç¦»ï¼Œæ”¯æŒåœ¨çº¿è°ƒæ•´ batch sizeã€learning rate ç­‰å‚æ•°ï¼Œæ— éœ€é‡å¯ã€‚
- **å¼¹æ€§ RL è®­ç»ƒï¼ˆElastic RLï¼‰**ï¼šæ”¯æŒ trainer åœ¨ data parallel ç»´åº¦åŠ¨æ€æ‰©ç¼©å®¹ï¼Œè¿›ä¸€æ­¥æå‡èµ„æºåˆ©ç”¨ç‡ã€‚
- **æ”¯æŒæ›´å¤šé€šä¿¡åç«¯ä¸å¼‚æ„è®¾å¤‡**ï¼šæ‰©å±• UCX æ”¯æŒå¤šç§ç½‘ç»œæ‹“æ‰‘å’Œ GPU ç±»å‹ï¼Œå¢å¼ºéƒ¨ç½²çµæ´»æ€§ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> RobustRL é€šè¿‡ **role-based fault isolation + detect-restart-reconnect èŒƒå¼**ï¼Œé¦–æ¬¡å®ç°äº†å¯¹ LLM RL åè®­ç»ƒå…¨æµç¨‹çš„é«˜æ•ˆå®¹é”™ï¼Œåœ¨é«˜æ•…éšœç‡ç¯å¢ƒä¸‹ä»èƒ½ä¿æŒ >80% çš„ ETTRï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­ 8.4%â€“17.4%ï¼Œä¸ºå¤§è§„æ¨¡ã€é•¿æ—¶é—´çš„ RL è®­ç»ƒæä¾›äº†å¯é åŸºç¡€è®¾æ–½æ”¯æŒã€‚

</details>

---

### 12. [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)

**Authors**: Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.22223v1  

#### Abstract
Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust....

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿç½‘ç»œæµé‡åˆ†æç³»ç»Ÿé¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **è§„åˆ™é©±åŠ¨ç³»ç»Ÿ**ï¼ˆå¦‚ Snortã€Suricataï¼‰ä¾èµ–äººå·¥ç¼–å†™çš„ç­¾åï¼Œç»´æŠ¤æˆæœ¬é«˜ï¼Œè¯¯æŠ¥ç‡é«˜ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚
- **æœºå™¨å­¦ä¹ æ¨¡å‹**ï¼ˆå¦‚ SVMã€Random Forestã€æ·±åº¦å­¦ä¹ ï¼‰è™½ç„¶æ£€æµ‹å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä½†å¤šä¸ºâ€œé»‘ç®±â€æ¨¡å‹ï¼Œéš¾ä»¥æä¾›å¯éªŒè¯çš„æ¨ç†è¿‡ç¨‹ï¼Œé™ä½äº†å®‰å…¨åˆ†æå¸ˆçš„ä¿¡ä»»åº¦ã€‚
- **çº¯ç”Ÿæˆå¼ LLMs** åœ¨æ— å¤–éƒ¨çŸ¥è¯†æ”¯æŒä¸‹å®¹æ˜“äº§ç”Ÿ**å¹»è§‰**ï¼ˆhallucinationsï¼‰ï¼Œè¾“å‡ºä¸å¯é æˆ–æ— æ³•éªŒè¯çš„ç»“è®ºã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šReGAIN æ¡†æ¶
ReGAIN æ˜¯ä¸€ä¸ªç»“åˆ **ç½‘ç»œæµé‡æ‘˜è¦åŒ–**ã€**æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰** å’Œ **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†** çš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®ç°**é€æ˜ä¸”å‡†ç¡®çš„ç½‘ç»œæµé‡åˆ†æ**ã€‚

#### æ ¸å¿ƒç»„ä»¶ï¼š
1. **æ•°æ®æ‘„å…¥ä¸æ‘˜è¦åŒ–ï¼ˆData Ingestion and Summarizationï¼‰**
   - å°†åŸå§‹ç½‘ç»œæ—¥å¿—ï¼ˆå¦‚ PCAPã€flow recordsï¼‰è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆnatural-language summariesï¼‰ï¼Œä¾¿äºäººç±»ç†è§£å¹¶æå‡åµŒå…¥è´¨é‡ã€‚
2. **è¯­ä¹‰å‘é‡åŒ–ä¸çŸ¥è¯†åº“æ„å»ºï¼ˆSemantic Vectorization and Knowledge Base Builderï¼‰**
   - ä½¿ç”¨ `all-MiniLM-L6-v2` ç­‰åµŒå…¥æ¨¡å‹å°†æ‘˜è¦ç¼–ç ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨åœ¨ **ChromaDB** ä¸­ã€‚
   - æ„å»º**å¤šé›†åˆå‘é‡æ•°æ®åº“**ï¼ˆmulti-collection vector DBï¼‰ï¼ŒåŒ…æ‹¬ï¼š
     - æµé‡é›†åˆï¼ˆtelemetryï¼‰
     - å¼‚å¸¸é›†åˆï¼ˆanomalyï¼‰
     - å¯å‘å¼é›†åˆï¼ˆheuristicï¼‰
3. **æ£€ç´¢å¢å¼ºæ¨ç†å¼•æ“ï¼ˆRetrieval-Augmented Reasoning and Generationï¼‰**
   - å¤šçº§æ£€ç´¢æµç¨‹ï¼š
     - å…ƒæ•°æ®è¿‡æ»¤ï¼ˆmetadata filteringï¼‰
     - MMRï¼ˆMaximal Marginal Relevanceï¼‰å¤šæ ·æ€§é‡‡æ ·
     - Bi-encoder åˆç­› + Cross-encoder é‡æ’åº
   - å¼•å…¥**å¼ƒæƒæœºåˆ¶ï¼ˆabstention mechanismï¼‰**ï¼šå½“æ£€ç´¢è¯æ®ä¸è¶³æ—¶è¿”å› â€œundecidableâ€ï¼Œé¿å…å¹»è§‰ã€‚
4. **äººæœºååŒäº¤äº’ï¼ˆHuman-in-the-Loop Interactionï¼‰**
   - æ”¯æŒåˆ†æå¸ˆé€šè¿‡ CLI è¿­ä»£æŸ¥è¯¢ï¼Œé€æ­¥æ·±å…¥è°ƒæŸ¥ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ReGAIN | ä¼ ç»Ÿæ–¹æ³• |
|------|--------|---------|
| å¯è§£é‡Šæ€§ | âœ… æ˜ç¡®å¼•ç”¨è¯æ®ï¼ˆcited evidenceï¼‰ | âŒ é»‘ç®±æˆ–æ¨¡æ¿åŒ–è¾“å‡º |
| å‡†ç¡®æ€§ | âœ… é«˜ç²¾åº¦ä¸å¬å›ç‡ | âš ï¸ è§„åˆ™ç³»ç»Ÿè¯¯æŠ¥é«˜ï¼ŒML ç¼ºä¹è§£é‡Š |
| æŠ—å¹»è§‰èƒ½åŠ› | âœ… å¼ƒæƒæœºåˆ¶ + å¤šé˜¶æ®µæ£€ç´¢ | âŒ çº¯ LLM å®¹æ˜“å¹»è§‰ |
| çµæ´»æ€§ | âœ… æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸è¿­ä»£åˆ†æ | âŒ å›ºå®šè§„åˆ™æˆ–é™æ€æ¨¡å‹ |

æ­¤å¤–ï¼ŒReGAIN æ˜¯é¦–ä¸ªå°† **å¤šé›†åˆ RAG + åˆ†å±‚æ£€ç´¢ + å¼ƒæƒæœºåˆ¶** åº”ç”¨äºç½‘ç»œæµé‡åˆ†æçš„å·¥ä½œï¼Œæ˜¾è‘—æå‡äº† LLM åœ¨ç½‘ç»œå®‰å…¨ä¸­çš„å¯ä¿¡åº¦ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨ **MAWILab v1.1** çœŸå®ç½‘ç»œæµé‡æ•°æ®é›†ï¼ˆ2022å¹´1æœˆ1æ—¥ã€9æ—¥ã€10æ—¥çš„ PCAPs å’Œç»“æ„åŒ–å¼‚å¸¸æ ‡æ³¨ CSVï¼‰ã€‚
- èšç„¦ä¸¤ç§æ”»å‡»ç±»å‹ï¼š
  - **TCP SYN Flood**
  - **ICMP Ping Flood**
- åŒ…å«çº¦ 10,000 æ¡æ ‡æ³¨å®ä¾‹ï¼Œæ¶µç›–å¤šç§åè®®è¡Œä¸ºä¸å¼‚å¸¸æ ‡ç­¾ï¼ˆheuristic å’Œ taxonomy ç±»å‹ï¼‰ã€‚

### å®éªŒè®¾ç½®
| ç»„ä»¶ | é…ç½® |
|------|------|
| åµŒå…¥æ¨¡å‹ | `all-MiniLM-L6-v2` (384-D) |
| é‡æ’åºæ¨¡å‹ | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| å‘é‡æ•°æ®åº“ | ChromaDBï¼ˆä¸‰ä¸ªæŒä¹…åŒ–é›†åˆï¼‰ |
| ç¼–æ’æ¡†æ¶ | LangChain |
| LLM | GPT-4.1-nanoï¼ˆtemperature=0ï¼Œç¡®ä¿ç¡®å®šæ€§è¾“å‡ºï¼‰ |
| æ£€ç´¢å‚æ•° | k âˆˆ {3,5}ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼ T=0.3ï¼ŒMMR å‚æ•° k=3â€“6 |

### è¯„ä¼°æŒ‡æ ‡
- **Accuracy**, **Precision**, **Recall**, **F1 Score**, **AUC**
- é‡‡ç”¨**åŒæ¨¡å¼è¯„ä¼°ç­–ç•¥**ï¼š
  1. **è‡ªåŠ¨åŒ–è¯„ä¼°**ï¼šåŸºäºæ•°æ®é›†æä¾›çš„ ground truth æ ‡ç­¾ã€‚
  2. **ä¸“å®¶è¯„ä¼°**ï¼šç”±é¢†åŸŸä¸“å®¶å¯¹å­é›†è¿›è¡Œç›²å®¡ï¼Œç‹¬ç«‹æ‰“æ ‡ï¼ŒéªŒè¯ç³»ç»Ÿçš„å®é™…å¯ç”¨æ€§ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¯”è¾ƒäº†äº”ç±»åŸºçº¿æ–¹æ³•ï¼š
1. **Rule-based IDS**ï¼ˆSnort é£æ ¼é˜ˆå€¼å¯å‘å¼ï¼‰
2. **SVM**ï¼ˆRBF æ ¸ï¼‰
3. **Random Forest**ï¼ˆ100 æ£µæ ‘ï¼‰
4. **1D-CNN**
5. **2-layer LSTM**

æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ä» Zeek `conn.log` æå–çš„ 30 ä¸ªæ•°å€¼ç‰¹å¾ï¼ˆè¿æ¥çŠ¶æ€ã€å­—èŠ‚/åŒ…æ•°é‡ã€IP/ç«¯å£å¤šæ ·æ€§ç­‰ï¼‰ï¼Œå¹¶é€šè¿‡ StandardScaler å½’ä¸€åŒ–ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè¡¨ IIï¼‰

| Attack | Acc. | Prec. | Recall | F1 | AUC |
|--------|------|-------|--------|-----|-----|
| SYN (GT) | **98.82%** | **100.00%** | 98.64% | 99.32% | 0.99 |
| SYN (Expert) | 95.95% | 97.20% | 98.07% | 97.63% | 0.98 |
| Ping (GT) | 97.56% | 74.48% | **100.00%** | 85.37% | 0.99 |
| Ping (Expert) | 97.74% | 76.36% | **100.00%** | 86.60% | 0.99 |

> GT = Ground Truthï¼›Expert = ä¸“å®¶æ ‡æ³¨

#### å…³é”®è§‚å¯Ÿï¼š
- **SYN Flood**ï¼šè¿‘ä¹å®Œç¾è¡¨ç°ï¼Œprecision è¾¾åˆ° 100%ï¼Œè¯´æ˜ä¸€æ—¦åˆ¤å®šå³é«˜åº¦å¯ä¿¡ã€‚
- **Ping Flood**ï¼šrecall å®Œç¾ï¼ˆ100%ï¼‰ï¼Œä½† precision çº¦ 75%ï¼Œè¡¨æ˜å­˜åœ¨å°‘é‡è¯¯æŠ¥ï¼ˆä¸»è¦æ˜¯è‰¯æ€§é«˜é¢‘ ICMP è¯Šæ–­æµé‡è¢«è¯¯åˆ¤ï¼‰ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ï¼ˆå›¾ 7ï¼‰
- å¯¹äº **SYN Flood**ï¼š
  - ReGAIN æ¯”æœ€å¼ºåŸºçº¿ï¼ˆLSTMï¼‰æå‡ï¼š
    - Accuracy â†‘3.7%
    - Precision â†‘14.5%
    - Recall â†‘3.8%
- å¯¹äº **Ping Flood**ï¼š
  - ReGAIN å®ç° **100% Recall**ï¼ˆLSTM ä»… 95.6%ï¼‰ï¼Œè™½ precision ç•¥ä½ï¼ˆ74.5% vs. ~70%ï¼‰ï¼Œä½†åœ¨å®‰å…¨åœºæ™¯ä¸­æ›´ä¼˜â€”â€”å®å¯è¯¯æŠ¥ä¹Ÿä¸æ¼æŠ¥ã€‚

### æ¶ˆèå®éªŒï¼ˆæ–‡ä¸­æœªæ˜ç¡®åˆ—å‡ºè¡¨æ ¼ï¼Œä½†ä»è®¾è®¡æ¨æ–­ï¼‰
å°½ç®¡æ²¡æœ‰æ­£å¼æ¶ˆèç ”ç©¶ï¼Œä½†æ¡†æ¶è®¾è®¡ä½“ç°äº†å¤šä¸ªå…³é”®æ¨¡å—çš„ä½œç”¨ï¼š
- **å¤šé›†åˆæ£€ç´¢** â†’ æå‡ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
- **Cross-encoder é‡æ’åº** â†’ æé«˜æ£€ç´¢è´¨é‡
- **MMR é‡‡æ ·** â†’ å‡å°‘å†—ä½™è¯æ®
- **å¼ƒæƒæœºåˆ¶** â†’ æ˜¾è‘—é™ä½å¹»è§‰é£é™©ï¼Œåœ¨è¯æ®ä¸è¶³æ—¶æ‹’ç»å›ç­”

è¿™äº›æœºåˆ¶å…±åŒä¿éšœäº† LLM è¾“å‡ºçš„**å¯éªŒè¯æ€§**å’Œ**å¯é æ€§**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ReGAIN åœ¨çœŸå®æµé‡ä¸­è¡¨ç°å‡ºè‰²**ï¼š
   - åœ¨ SYN å’Œ Ping Flood æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œaccuracy è¾¾ **95.95%â€“98.82%**ï¼Œrecall æ¥è¿‘æˆ–è¾¾åˆ° 100%ã€‚
   - æ€§èƒ½ä¼˜äºæ‰€æœ‰ä¼ ç»Ÿ rule-basedã€classical ML å’Œ deep learning åŸºçº¿ã€‚
2. **å¯è§£é‡Šæ€§æ˜¯æ ¸å¿ƒä¼˜åŠ¿**ï¼š
   - æ‰€æœ‰åˆ¤æ–­å‡é™„å¸¦**å¼•ç”¨è¯æ® ID æˆ– heuristic code**ï¼Œæ”¯æŒå®¡è®¡ä¸æº¯æºã€‚
   - è¾“å‡ºæ ¼å¼æ ‡å‡†åŒ–ï¼šverdict + justification + mitigation recommendationã€‚
3. **äººæœºåä½œæ½œåŠ›å·¨å¤§**ï¼š
   - æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸è¿­ä»£ç»†åŒ–ï¼ˆå¦‚ä»â€œæ˜¾ç¤ºæ¶‰åŠæŸ IP çš„å¼‚å¸¸â€åˆ°â€œæ¯”è¾ƒåŒä¸€æ—¶é—´æ®µå†…çš„ TCP SYN è¡Œä¸ºâ€ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå® incident response å·¥ä½œæµã€‚
4. **ä¸“å®¶è¯„ä¼°æ­ç¤º ground truth å±€é™æ€§**ï¼š
   - è‡ªåŠ¨æ ‡æ³¨å¯èƒ½è¿‡äºå®½æ¾æˆ–é—æ¼è¾¹ç¼˜æ¡ˆä¾‹ï¼Œè€Œ ReGAIN ç»“åˆä¸“å®¶åˆ¤æ–­åä»ä¿æŒé«˜æ€§èƒ½ï¼Œè¯æ˜å…¶é²æ£’æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **å»¶è¿Ÿè¾ƒé«˜**ï¼šä¾èµ–è¿œç¨‹ APIï¼ˆå¦‚ GPT-4ï¼‰å¯¼è‡´æ¨ç†å»¶è¿Ÿï¼Œä¸é€‚åˆå®æ—¶ç›‘æ§ï¼Œæ›´é€‚åˆ**äº‹ååˆ†æä¸å–è¯**ã€‚
2. **Ping Flood ç²¾åº¦åä½**ï¼ˆ~75%ï¼‰ï¼šå› è‰¯æ€§ ICMP æ¢æµ‹ï¼ˆå¦‚ ping sweepã€latency checkï¼‰ä¸æ”»å‡»æ¨¡å¼ç›¸ä¼¼ï¼Œæ˜“è¯¯åˆ¤ã€‚
3. **åµŒå…¥ç»´åº¦è¾ƒä½**ï¼ˆ384-Dï¼‰ï¼šä¸ºä¼˜åŒ–æ¨ç†è´¨é‡ç‰ºç‰²äº†ååæ•ˆç‡ï¼Œå¤§è§„æ¨¡éƒ¨ç½²éœ€æƒè¡¡ã€‚
4. **å½“å‰ä¾èµ–äº‘æœåŠ¡**ï¼šç¼ºä¹æœ¬åœ°åŒ–éƒ¨ç½²èƒ½åŠ›ï¼Œå½±å“æ•æ„Ÿç¯å¢ƒä¸‹çš„éšç§ä¸åˆè§„æ€§ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼•å…¥æœ¬åœ°æ¨¡å‹**ï¼š
   - æ›¿æ¢ä¸º **LLaMA-3-8B**ã€**Mistral-7B** æˆ– **Phi-3** ç­‰å¯åœ¨æœ¬åœ°è¿è¡Œçš„ LLMï¼Œå®ç°ç¦»çº¿æ“ä½œã€‚
2. **åŠ¨æ€ç›¸ä¼¼åº¦é˜ˆå€¼ä¸é€Ÿç‡è¿‡æ»¤å™¨**ï¼š
   - æ ¹æ®åè®®ç±»å‹å’Œæµé‡å¼ºåº¦è‡ªé€‚åº”è°ƒæ•´æ£€æµ‹çµæ•åº¦ï¼Œå‡å°‘ ICMP è¯¯æŠ¥ã€‚
3. **ä¼˜åŒ–è‡ªç„¶è¯­è¨€è¾“å‡ºè´¨é‡**ï¼š
   - å¼€å±•ç”¨æˆ·ç ”ç©¶ï¼Œè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„æ¸…æ™°åº¦ä¸å®ç”¨æ€§ï¼Œè¿›ä¸€æ­¥æå‡ analyst usabilityã€‚
4. **æ‰©å±•è‡³æ›´å¤šæ”»å‡»ç±»å‹**ï¼š
   - å¦‚ DNS tunnelingã€port scanningã€C2 é€šä¿¡ç­‰ï¼ŒéªŒè¯æ¡†æ¶æ³›åŒ–èƒ½åŠ›ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> ReGAIN æˆåŠŸå°† **RAG + LLM + å¤šçº§æ£€ç´¢ + å¼ƒæƒæœºåˆ¶** èå…¥ç½‘ç»œæµé‡åˆ†æï¼Œå®ç°äº†**é«˜å‡†ç¡®æ€§ã€å¼ºå¯è§£é‡Šæ€§ã€æŠ—å¹»è§‰**çš„æ™ºèƒ½åˆ†æç³»ç»Ÿï¼Œæ¨åŠ¨ç½‘ç»œå®‰å…¨ä»â€œå‘Šè­¦é©±åŠ¨â€è¿ˆå‘â€œå¯¹è¯å¼åˆ†æâ€ã€‚

</details>

---

### 13. [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)

**Authors**: Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22139v1  

#### Abstract
Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
- **3Dç‚¹äº‘æ¨¡å‹åœ¨GPUä¸Šæ‰§è¡Œæ•ˆç‡ä½**ï¼šç”±äºç‚¹äº‘æ•°æ®å…·æœ‰ç¨€ç–æ€§å’Œéç»“æ„åŒ–ç‰¹æ€§ï¼Œä¼ ç»Ÿçš„GPUåœ¨å¤„ç†å¦‚Farthest Point Sampling (FPS)ã€K-Nearest Neighbor (KNN)ç­‰æ˜ å°„å‡½æ•°æ—¶å­˜åœ¨ä¸¥é‡çš„èµ„æºåˆ©ç”¨ç‡ä¸è¶³é—®é¢˜ã€‚
- **ç¼ºä¹æ”¯æŒç‚¹äº‘ä¸“ç”¨æ“ä½œçš„FPGAåŠ é€Ÿæ¡†æ¶**ï¼šç°æœ‰çš„FPGAæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚fpgaConvNetã€hls4mlï¼‰ä¸»è¦é¢å‘è§„åˆ™å¼ é‡è¿ç®—ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒç‚¹äº‘ç‰¹æœ‰çš„ä¸è§„åˆ™è®¡ç®—æµç¨‹ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
- **æå‡ºHLS4PCæ¡†æ¶**ï¼š
  - ä¸€ä¸ªåŸºäºHigh Level Synthesis (HLS) çš„å¯å‚æ•°åŒ–ã€æµå¼(dataflow)æ¶æ„çš„FPGAåŠ é€Ÿæ¡†æ¶ï¼Œä¸“ä¸º**ç‚¹äº‘æ¨¡å‹ä¸­çš„æ˜ å°„å‡½æ•° + DNNå±‚è”åˆåŠ é€Ÿ**è®¾è®¡ã€‚
  - æ”¯æŒæ··åˆç²¾åº¦ï¼ˆmixed-precisionï¼‰ã€ç¼–è¯‘æ—¶é…ç½®å¹¶è¡Œåº¦ï¼Œå¹¶èƒ½çµæ´»éƒ¨ç½²ä¸æ–­æ¼”è¿›çš„ç‚¹äº‘æ¨¡å‹ã€‚
- **å¼•å…¥ç¡¬ä»¶æ„ŸçŸ¥å‹ç¼©ç­–ç•¥æ„å»ºPointMLP-Lite**ï¼š
  - ç”¨Uniform Random Sampling (URS) æ›¿ä»£FPSï¼šæå‡ç¡¬ä»¶å‹å¥½æ€§ï¼›
  - è¾“å…¥ç‚¹å‰ªæï¼ˆä»1024é™è‡³512ï¼‰ï¼›
  - å‚æ•°é‡åŒ–è‡³8/8-bitï¼ˆweights/activationsï¼‰ï¼›
  - BatchNormä¸å·ç§¯å±‚èåˆä»¥å‡å°‘å†…å­˜å¼€é”€ï¼›
  - ç§»é™¤å‡ ä½•å½’ä¸€åŒ–å‚æ•° $ \alpha $ å’Œ $ \beta $ã€‚
- **å®ç°å®Œæ•´çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹**ï¼š
  - ä»PyTorch/TensorFlowé¢„è®­ç»ƒæ¨¡å‹ â†’ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ â†’ HLSä»£ç ç”Ÿæˆ â†’ FPGAæ¯”ç‰¹æµéƒ¨ç½²ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿è¯´æ˜ |
|------|----------|
| **çµæ´»æ€§** | æ”¯æŒåŠ¨æ€è°ƒæ•´ç²¾åº¦ã€å¹¶è¡Œåº¦ã€é‡‡æ ·æ–¹å¼ç­‰ï¼Œä¼˜äºASICç±»ä¸“ç”¨åŠ é€Ÿå™¨ |
| **é«˜æ•ˆæ€§** | é’ˆå¯¹ç‚¹äº‘ç‰¹æœ‰æ“ä½œï¼ˆURSã€KNNï¼‰è¿›è¡Œç¡¬ä»¶ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡ååé‡ |
| **èƒ½æ•ˆæ¯”** | èƒ½é‡æ•ˆç‡é«˜è¾¾294.5 GOPS/Wï¼Œæ˜¯å‰äººå·¥ä½œçš„57.4Ã— |
| **é€šç”¨æ€§** | å¯æ‰©å±•è‡³å…¶ä»–ç‚¹äº‘æ¨¡å‹ï¼ˆå¦‚DGCNNã€PointNet++ç­‰ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- **ModelNet40**ï¼šå¹¿æ³›ä½¿ç”¨çš„3Dç‰©ä½“åˆ†ç±»åŸºå‡†ï¼ŒåŒ…å«40ç±»CADæ¨¡å‹ã€‚
- **ScanObjectNN**ï¼šçœŸå®åœºæ™¯é‡‡é›†çš„ç‚¹äº‘æ•°æ®é›†ï¼Œæ›´å…·æŒ‘æˆ˜æ€§ï¼Œç”¨äºéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **ç›®æ ‡æ¨¡å‹**ï¼šåŸºäºSOTAæ¨¡å‹ **PointMLP-Elite** è¿›è¡Œå‹ç¼©ä¸ä¼˜åŒ–ï¼Œå¾—åˆ°è½»é‡ç‰ˆ **PointMLP-Lite**ã€‚
- **è®­ç»ƒç¯å¢ƒ**ï¼š
  - GPU: NVIDIA RTX 3090
  - æ¡†æ¶: PyTorch 2.4.0 + Brevitasï¼ˆç”¨äºé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰
  - è®­ç»ƒå‚æ•°: batch size=256, epochs=1000, SGDä¼˜åŒ–å™¨
- **FPGAéƒ¨ç½²å¹³å°**ï¼š
  - å¼€å‘æ¿: Xilinx Zynq 7000 SoC ZC706
  - å·¥å…·é“¾: Vivado HLS 2018.3
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Overall Accuracy (OA)** å’Œ **Mean Accuracy (mA)**
  - **Throughput (GOPS)**ï¼šæ¯ç§’åäº¿æ¬¡æ“ä½œ
  - **Energy Efficiency (GOPS/W)**ï¼šèƒ½æ•ˆæ¯”
  - èµ„æºå ç”¨ï¼šLUTã€FFã€DSPã€BRAMã€URAM

### ğŸ” åŸºçº¿å¯¹æ¯”æ–¹æ³•
- **FPGAåŸºçº¿**ï¼š
  - SOCC 2022 [14]: ISSCN on ZCU102
  - ISCAS 2020 [1]: PointNet on ZCU104
  - CSSP 2023 [3]: DGCNN (FP32) on Ultrascale+
  - ASICON 2019 [18]: O-PointNet on ZC706
- **CPU/GPUå¯¹æ¯”**ï¼š
  - Tesla V100ï¼ˆåŸæ¨¡å‹PointMLP-Eliteï¼‰
  - RTX 3060 Tiï¼ˆPointMLP-Liteï¼‰
  - Intel i5-13400 CPU

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 2 & Table 3ï¼‰

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **PointMLP-Liteæ¨¡å‹å¤æ‚åº¦é™ä½** | 4Ã— less complex than PointMLP-Elite |
| **å‡†ç¡®ç‡ä¸‹é™ï¼ˆModelNet40ï¼‰** | OAä»…ä¸‹é™çº¦2%ï¼ˆ93.6% â†’ 91.69%ï¼‰ |
| **FPGAååé‡** | **1648 GOPS** |
| **ç›¸æ¯”ä¹‹å‰FPGAå·¥ä½œæå‡** | **3.56Ã—æ›´é«˜throughput** |
| **ç›¸æ¯”GPUï¼ˆRTX 3060 Tiï¼‰** | **2.3Ã—æ›´é«˜throughput** |
| **ç›¸æ¯”CPUï¼ˆi5-13400ï¼‰** | **22Ã—æ›´é«˜throughput** |
| **èƒ½é‡æ•ˆç‡** | **294.5 GOPS/W**ï¼ˆæ¯”å‰äººæœ€é«˜æå‡57.4Ã—ï¼‰ |

### ğŸ”¬ æ¶ˆèå®éªŒåˆ†æï¼ˆTable 1 & Fig. 4ï¼‰
é€šè¿‡é€æ­¥åº”ç”¨å‹ç¼©æŠ€æœ¯è§‚å¯Ÿå‡†ç¡®ç‡å˜åŒ–ï¼š

| æ¨¡å‹å˜ä½“ | å…³é”®æ”¹åŠ¨ | ModelNet40 OA (%) | å¤‡æ³¨ |
|--------|---------|------------------|------|
| PointMLP-Elite | åŸå§‹æ¨¡å‹ | 93.60 | N/A |
| M-1 | URSæ›¿ä»£FPS | 92.30 | å‡†ç¡®ç‡è½»å¾®ä¸‹é™ï¼Œç¡¬ä»¶æ›´å‹å¥½ |
| M-2 | è¾“å…¥å‡åŠï¼ˆ512 ptsï¼‰+ URS | **91.69** | æ€§ä»·æ¯”æœ€ä¼˜ï¼Œé€‰ä½œåŸºç¡€ |
| M-3 | åŠ å…¥BNèåˆ | 90.56 | å†é™1.13%ï¼ŒèŠ‚çœå­˜å‚¨ |
| M-4 | è¿›ä¸€æ­¥å‰ªæè‡³128 pts | 86.87 | å‡†ç¡®ç‡æ˜æ˜¾ä¸‹é™ |

> âœ… **Paretoå‰æ²¿åˆ†ææ˜¾ç¤º**ï¼š8/8-bité‡åŒ–çš„M-2ç‰ˆæœ¬åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å®ç°4å€æ¨¡å‹ç®€åŒ–ï¼Œæˆä¸ºæœ€ä½³æŠ˜ä¸­æ–¹æ¡ˆ â€”â€” å³æœ€ç»ˆçš„ **PointMLP-Lite**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **URSå¯ä»¥æœ‰æ•ˆæ›¿ä»£FPS**ï¼šå°½ç®¡å¼•å…¥éšæœºæ€§ï¼Œä½†é€šè¿‡å¢å¤§batch sizeå’Œè®­ç»ƒè½®æ•°ï¼ˆup to 1000 epochsï¼‰ï¼Œæ¨¡å‹ä»å¯å­¦ä¹ ç¨³å®šçš„å‡ ä½•ä¸å˜ç‰¹å¾ã€‚
2. **ç¡¬ä»¶æ„ŸçŸ¥å‹ç¼©æ˜¾è‘—é™ä½å¤æ‚åº¦**ï¼šç»“åˆè¾“å…¥å‰ªæã€é‡åŒ–ã€å±‚èåˆç­‰æ‰‹æ®µï¼Œåœ¨ä»…æŸå¤±~2%å‡†ç¡®ç‡çš„å‰æä¸‹å°†æ¨¡å‹ç¼©å°4å€ã€‚
3. **FPGAåœ¨ç‚¹äº‘æ¨ç†ä¸­æ½œåŠ›å·¨å¤§**ï¼šå¾—ç›Šäºå…¶é«˜åº¦å¹¶è¡Œã€ä½åŠŸè€—ã€å¯é‡æ„ç‰¹æ€§ï¼Œé…åˆç®—æ³•-ç¡¬ä»¶ååŒè®¾è®¡ï¼Œå¯åœ¨ååé‡å’Œèƒ½æ•ˆä¸Šå…¨é¢è¶…è¶ŠGPUå’ŒCPUã€‚
4. **æµå¼dataflowæ¶æ„é€‚åˆç‚¹äº‘å¤„ç†**ï¼šé¿å…ä¸­é—´ç»“æœé©»ç•™ç‰‡å¤–å†…å­˜ï¼Œæå¤§ç¼“è§£å¸¦å®½ç“¶é¢ˆã€‚

### âš ï¸ å±€é™æ€§
- **å½“å‰ä»…æ”¯æŒåˆ†ç±»ä»»åŠ¡**ï¼šå°šæœªæ‰©å±•è‡³è¯­ä¹‰åˆ†å‰²æˆ–æ£€æµ‹ç­‰æ›´å¤æ‚ä»»åŠ¡ã€‚
- **ä¾èµ–ç‰¹å®šFPGAå·¥å…·é“¾ï¼ˆVivado HLSï¼‰**ï¼šè·¨å¹³å°ç§»æ¤æ€§å—é™ã€‚
- **URSå¸¦æ¥çš„ä¸ç¡®å®šæ€§å¯èƒ½å½±å“å®‰å…¨å…³é”®åº”ç”¨çš„ä¸€è‡´æ€§è¡¨ç°**ã€‚
- **æœªæ¢ç´¢åŠ¨æ€è¾“å…¥é•¿åº¦çš„æ”¯æŒ**ï¼šç›®å‰å›ºå®šè¾“å…¥ç‚¹æ•°ï¼ˆå¦‚512ï¼‰ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **Hilbert Curve-based Sampling** æ¥æ›¿ä»£URSï¼Œåœ¨ä¿ç•™ç¡¬ä»¶å‹å¥½æ€§çš„åŒæ—¶å‡å°‘ç²¾åº¦æŸå¤±ã€‚
- æ‰©å±•æ¡†æ¶æ”¯æŒæ›´å¤šç‚¹äº‘æ¨¡å‹ï¼ˆå¦‚PointTransformerã€PV-RCNNï¼‰åŠä¸‹æ¸¸ä»»åŠ¡ï¼ˆsegmentation, detectionï¼‰ã€‚
- å¼•å…¥è‡ªåŠ¨æœç´¢æœºåˆ¶ï¼ˆAuto-HLSï¼‰ä¼˜åŒ–HLSå‚æ•°é…ç½®ï¼ˆå¦‚PEæ•°é‡ã€æµæ°´çº¿æ·±åº¦ï¼‰ã€‚
- æ”¯æŒå¤šFPGAåˆ†å¸ƒå¼æ¨ç†ï¼Œåº”å¯¹æ›´å¤§è§„æ¨¡ç‚¹äº‘åœºæ™¯ã€‚

---

> ğŸ’¡ **å¼€æºå£°æ˜**ï¼šä½œè€…æ‰¿è¯ºå°†å‘å¸ƒHLS4PCæ¡†æ¶ä»£ç ï¼Œåœ°å€ä¸ºï¼š  
> [https://github.com/dll-ncai/HLS4PC](https://github.com/dll-ncai/HLS4PC)

</details>

---

### 14. [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)

**Authors**: Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22420v1  

#### Abstract
Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Curr...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
- **å›ºå®š speculative length çš„å±€é™æ€§**ï¼šç°æœ‰çš„ Speculative Decoding (SD) æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„ speculative lengthï¼ˆå¦‚ Î³=3ï¼‰ï¼Œæ— æ³•é€‚åº”åŠ¨æ€å˜åŒ–çš„è¯·æ±‚è´Ÿè½½ï¼ˆrequest loadï¼‰ã€‚
- **é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½é€€åŒ–**ï¼šåœ¨ compute-bound é«˜æ‰¹å¤„ç†åœºæ™¯ä¸­ï¼ŒSD çš„éªŒè¯å¼€é”€ä¼šè¶…è¿‡å…¶æ”¶ç›Šï¼Œå¯¼è‡´ååé‡ä¸‹é™ç”šè‡³åŠ£äºæ ‡å‡† autoregressive (AR) è§£ç ã€‚
- **å†³ç­–æ­»é”ä¸åˆ‡æ¢æˆæœ¬è¢«å¿½è§†**ï¼šç°æœ‰æ–¹æ³•ï¼ˆå¦‚ DSDã€SpecServeï¼‰ä¾èµ–å†å² acceptance rate è¿›è¡Œä¼˜åŒ–ï¼Œä¸€æ—¦å…³é—­ SD å°±æ— æ³•æ”¶é›†æ–°æ•°æ®ï¼Œé™·å…¥â€œå†³ç­–æ­»é”â€ï¼›åŒæ—¶å¿½ç•¥äº†ä» `Î³=0` åˆ‡æ¢å›æ­£ speculative length æ—¶çš„ KV cache é‡å»ºå¼€é”€ï¼ˆå³ prefill costï¼‰ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼š**Nightjar**
- **åŸºäº Contextual Multi-Armed Bandit (MAB) çš„è‡ªé€‚åº”ç­–ç•¥**ï¼š
  - å°† speculative length çš„é€‰æ‹©å»ºæ¨¡ä¸ºä¸€ä¸ªåœ¨çº¿å­¦ä¹ é—®é¢˜ï¼Œä½¿ç”¨ **ADA-BINGREEDY** æ¡†æ¶å®ç°æ¢ç´¢-åˆ©ç”¨æƒè¡¡ã€‚
  - å¼•å…¥ä¸‰å±‚æ—¶é—´ç»“æ„ï¼ˆEpoch â†’ Block â†’ Binï¼‰ä»¥æ”¯æŒè¿ç»­æ‰¹å¤„ç†ï¼ˆcontinuous batchingï¼‰ç¯å¢ƒä¸­çš„é•¿æœŸå­¦ä¹ ã€‚
- **åŠ¨æ€æ„ŸçŸ¥è´Ÿè½½å¹¶æ™ºèƒ½å†³ç­–**ï¼š
  - èƒ½æ ¹æ®å®æ—¶ batch size è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ speculative length Î³ âˆˆ {0,1,...,Î³_max}ï¼ŒåŒ…æ‹¬åœ¨å¿…è¦æ—¶å®Œå…¨ç¦ç”¨ SDï¼ˆÎ³=0ï¼‰ã€‚
- **æ˜¾å¼å»ºæ¨¡ Switching Cost**ï¼š
  - åœ¨ç›®æ ‡å‡½æ•°ä¸­å¼•å…¥æƒ©ç½šé¡¹ $ \mathbb{I}(y_{t-1}=0 \land y_t > 0) \cdot C_{\text{prefill}} / y_t $ï¼Œè€ƒè™‘ KV cache é‡å»ºå»¶è¿Ÿï¼Œå¹¶é¼“åŠ±æ›´é•¿çš„ speculative åºåˆ—æ¥æ‘Šé”€å¯åŠ¨å¼€é”€ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | å±€é™æ€§ | Nightjar çš„æ”¹è¿› |
|------|--------|----------------|
| å›ºå®š Î³ çš„ SD | æ— æ³•é€‚åº”è´Ÿè½½å˜åŒ– | åŠ¨æ€è°ƒæ•´ Î³ï¼Œä½è´Ÿè½½åŠ é€Ÿï¼Œé«˜è´Ÿè½½é¿å…é€€åŒ– |
| DSD [15] | ä¾èµ–å¹³å‡ acceptance rateï¼Œæ˜“é™·â€œå†³ç­–æ­»é”â€ | ä¸ä¾èµ–å†å² acceptanceï¼ŒæŒç»­æ¢ç´¢ |
| BanditSpec [12] | é™æ€è®¾è®¡ï¼Œä¸é€‚ç”¨äº continuous batching | æ”¯æŒåŠ¨æ€ batch size å’Œå¼‚æ­¥è¯·æ±‚æµ |
| TETRIS/SpecServe | å¿½ç•¥ switching costï¼Œå¤§ batch ä¸‹ overhead é«˜ | æ˜¾å¼å»ºæ¨¡ prefill å¼€é”€ï¼Œæå‡å†³ç­–é²æ£’æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **çœŸå®ç”Ÿäº§ trace**ï¼šæ¥è‡ª *AzureLLMInferenceDataset* çš„åŠ¨æ€è¯·æ±‚ç‡ç‰‡æ®µï¼ˆFig. 9ï¼‰
- **å¯¹è¯ç±»ä»»åŠ¡**ï¼š
  - *ShareGPT*
  - *Alpaca*
- **ç»¼åˆåŸºå‡†æµ‹è¯•**ï¼š
  - *SpecBench*ï¼šåŒ…å« 480 ä¸ªæ¥è‡ªå¤šä¸ªå¸¸ç”¨æ•°æ®åº“çš„ä»»åŠ¡å®ä¾‹
- è¾“å…¥è¾“å‡ºé•¿åº¦åˆ†å¸ƒè§ Fig. 7ï¼Œæ¶µç›–çŸ­åˆ°é•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹é…ç½®**ï¼š
  - **7B æ¨¡å‹ç»„**ï¼šDeepSeek-R1-Distill-Qwen-7Bï¼ˆtargetï¼‰ + DeepSeek-R1-DRAFT-Qwen2.5-0.5Bï¼ˆdraftï¼‰ï¼Œè¿è¡Œäº RTX 4090 (24GB)
  - **13B æ¨¡å‹ç»„**ï¼švicuna-13bï¼ˆtargetï¼‰ + vicuna-68mï¼ˆdraftï¼‰ï¼Œè¿è¡Œäº A100 (40GB)
- **ç³»ç»Ÿå¹³å°**ï¼šåŸºäº vLLM (v0.8.2) æ‰©å±•å®ç° Nightjar
- **speculative length æœç´¢ç©ºé—´**ï¼šÎ³ âˆˆ {0,1,2,3,4,5}
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Throughput**ï¼šç”Ÿæˆ token æ•° / ç§’ï¼ˆtokens/sï¼‰
  - **Mean End-to-End Latency**ï¼šä»è¯·æ±‚æäº¤åˆ°å®Œæˆçš„å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ | æè¿° |
|------|------|
| **w/o SD** | æ ‡å‡† autoregressive è§£ç ï¼Œæ—  speculative |
| **SD (Î³=3)** | å›ºå®š speculative length=3 çš„æ ‡å‡† SD |
| **DSD [15]** | åŸºäº goodput æ¨¡å‹åŠ¨æ€è°ƒä¼˜ Î³ |
| **BanditSpec [12]** | ä½¿ç”¨ MAB çš„æ—©æœŸè‡ªé€‚åº”æ–¹æ³•ï¼Œä½†é™æ€è®¾è®¡ |
| **TETRIS [14]** | åŸºäº confidence score çš„ batch-level ä¼˜åŒ– |

æ‰€æœ‰ç»“æœå– 5 æ¬¡ç‹¬ç«‹è¿è¡Œçš„å¹³å‡å€¼ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ ååé‡æå‡ï¼ˆThroughputï¼‰
#### è¡¨ 2ï¼šä¸åŒæ¨¡å‹ä¸æ•°æ®é›†ä¸‹çš„ Throughput å¯¹æ¯”ï¼ˆå•ä½ï¼štokens/sï¼‰

| Method | Alpaca (7B) | ShareGPT (7B) | SpecBench (7B) | Alpaca (13B) | ShareGPT (13B) | SpecBench (13B) |
|--------|-------------|---------------|----------------|--------------|----------------|------------------|
| **Nightjar** | **2102.73** | **3734.23** | **3326.20** | **721.23** | **1729.68** | **1061.82** |
| BanditSpec | 1860.74 | 3428.75 | 2631.43 | 679.82 | 1383.81 | 763.15 |
| DSD | 2044.40 | 3614.65 | 2789.51 | 603.32 | 1244.96 | 692.01 |
| TETRIS | 1875.91 | 3597.21 | 2974.14 | 702.65 | 1628.86 | 985.40 |
| SD | 1920.63 | 3593.36 | 3044.80 | 628.47 | 1689.53 | 964.02 |
| w/o SD | 2072.38 | 3653.61 | 3207.28 | 438.13 | 1290.04 | 673.48 |

âœ… **å…³é”®å‘ç°**ï¼š
- Nightjar åœ¨æ‰€æœ‰åœºæ™¯ä¸‹å‡å–å¾—æœ€é«˜ throughputã€‚
- åœ¨ 13B + Alpaca ä¸Šï¼Œç›¸æ¯” vanilla decoding æå‡é«˜è¾¾ **64.6%**ã€‚
- ç›¸æ¯”å…ˆè¿›ç³»ç»Ÿ TETRISï¼Œåœ¨ 7B Alpaca ä¸Šæå‡ **12.1%**ï¼›åœ¨ 13B SpecBench ä¸Šè¶…è¶Š DSD è¾¾ **53.4%**ã€‚

### â±ï¸ å»¶è¿Ÿé™ä½ï¼ˆLatencyï¼‰
#### è¡¨ 3ï¼šMean End-to-End Latencyï¼ˆå•ä½ï¼šmsï¼‰

| Method | Alpaca (7B) | ShareGPT (7B) | SpecBench (7B) | Alpaca (13B) | ShareGPT (13B) | SpecBench (13B) |
|--------|-------------|---------------|----------------|--------------|----------------|------------------|
| **Nightjar** | **8868.17** | **6534.76** | **7618.85** | **23525.49** | **8133.92** | **14691.81** |
| BanditSpec | 12340.83 | 7799.63 | 9288.23 | 29077.32 | 9667.04 | 19294.42 |
| DSD | 9183.21 | 6499.12 | 8301.02 | 16212.44 | 10057.72 | 22361.71 |
| TETRIS | 11734.19 | 7409.62 | 8789.48 | 22133.43 | 8046.25 | 14937.37 |
| SD | 11110.68 | 7047.49 | 8567.04 | 24671.40 | 8152.97 | 14212.97 |
| w/o SD | 8876.14 | 6438.47 | 7654.05 | 32222.49 | 9350.87 | 23831.74 |

âœ… **å…³é”®å‘ç°**ï¼š
- Nightjar å®ç° **æœ€é«˜åå + æœ€ä½å»¶è¿Ÿ** åŒä¼˜ã€‚
- åœ¨ 7B Alpaca ä¸Šï¼Œç›¸æ¯” SD é™ä½å»¶è¿Ÿ **20.2%**ï¼Œç›¸æ¯” TETRIS é™ä½ **24.4%**ã€‚
- åœ¨ 13B SpecBench ä¸Šï¼Œç›¸æ¯” DSD é™ä½å»¶è¿Ÿ **34.3%**ï¼Œæ˜¾è‘—ç¼“è§£ compute-bound åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆã€‚

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
- **Fig. 11 å¯¹æ¯”å¤šç§ context-aware MAB æ–¹æ³•**ï¼š
  - LinUCB å’Œ linear Thompson Sampling å‡è®¾ reward ä¸ contextï¼ˆbatch sizeï¼‰å‘ˆçº¿æ€§å…³ç³»ï¼Œåœ¨ LLM æ¨ç†ä¸­ä¸æˆç«‹ã€‚
  - Nightjar ä½¿ç”¨çš„ ADA-BINGREEDY éå‚æ•°åŒ–ç­–ç•¥è¡¨ç°æœ€ä¼˜ï¼Œè¯´æ˜å¤æ‚éçº¿æ€§å…³ç³»éœ€çµæ´»å»ºæ¨¡ã€‚
- **Switching cost å»ºæ¨¡æœ‰æ•ˆæ€§**ï¼š
  - è‹¥å¿½ç•¥ $ C_{\text{prefill}} $ï¼Œç³»ç»Ÿé¢‘ç¹åˆ‡æ¢ Î³ å¯¼è‡´ latency ä¸Šå‡ã€‚
  - å®æµ‹ $ C_{\text{prefill}} $ éšè¾“å…¥é•¿åº¦å’Œ batch size å¢åŠ è€Œæ˜¾è‘—ä¸Šå‡ï¼ˆTable 1ï¼‰ï¼ŒéªŒè¯äº†å»ºæ¨¡å¿…è¦æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Speculative Decoding çš„æ”¶ç›Šé«˜åº¦ä¾èµ–è´Ÿè½½çŠ¶æ€**ï¼š
   - ä½è´Ÿè½½ï¼ˆmemory-boundï¼‰æ—¶ SD æ˜¾è‘—åŠ é€Ÿï¼›
   - é«˜è´Ÿè½½ï¼ˆcompute-boundï¼‰æ—¶ SD åè€Œæ‹–æ…¢ç³»ç»Ÿã€‚
2. **å›ºå®š speculative length æ˜¯æ¬¡ä¼˜çš„**ï¼š
   - æœ€ä¼˜ Î³ æ˜¯ batch size å’Œç³»ç»ŸçŠ¶æ€çš„å‡½æ•°ï¼Œå¿…é¡»åŠ¨æ€è°ƒæ•´ã€‚
3. **Nightjar æˆåŠŸå®ç°äº†â€œæ™ºèƒ½å¼€å…³â€æœºåˆ¶**ï¼š
   - åœ¨ä½è´Ÿè½½æ—¶å¯ç”¨å¹¶ä¼˜åŒ– Î³ï¼›
   - åœ¨é«˜è´Ÿè½½æ—¶æœæ–­å…³é—­ SDï¼Œé¿å…æ€§èƒ½é€€åŒ–ã€‚
4. **Switching cost æ˜¯å½±å“å†³ç­–è´¨é‡çš„å…³é”®å› ç´ **ï¼š
   - å¿½è§† $ C_{\text{prefill}} $ ä¼šå¯¼è‡´çŸ­è§†è¡Œä¸ºå’Œæ€§èƒ½æ³¢åŠ¨ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **éœ€è¦ç¦»çº¿æ„å»º lookup table for $ C_{\text{prefill}} $**ï¼š
  - è™½ç„¶å¯é€šè¿‡ profiling è·å–ï¼Œä½†åœ¨æ–°å‹ç¡¬ä»¶æˆ–æ¨¡å‹ä¸Šéœ€é‡æ–°æ ¡å‡†ã€‚
- **æœªè€ƒè™‘ draft model çš„å¤šæ ·æ€§é€‰æ‹©**ï¼š
  - å½“å‰ä»…ä¼˜åŒ– speculative lengthï¼Œæœªè”åˆä¼˜åŒ– draft model æ¶æ„æˆ–å¤§å°ã€‚
- **å¯¹æç«¯çªå˜è´Ÿè½½çš„å“åº”é€Ÿåº¦æœ‰å¾…éªŒè¯**ï¼š
  - è™½ç„¶å…·å¤‡æ¢ç´¢èƒ½åŠ›ï¼Œä½†åœ¨çªå‘æµé‡ä¸‹çš„æ”¶æ•›é€Ÿåº¦æœªæ·±å…¥åˆ†æã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- **æ‰©å±•è‡³å¤šç»´åŠ¨ä½œç©ºé—´**ï¼šè”åˆä¼˜åŒ– Î³ã€draft modelã€KV cache ç­–ç•¥ç­‰ã€‚
- **è·¨è®¾å¤‡/é›†ç¾¤éƒ¨ç½²æ”¯æŒ**ï¼šå°† Nightjar æ¨å¹¿åˆ°åˆ†å¸ƒå¼ LLM serving åœºæ™¯ã€‚
- **åœ¨çº¿è‡ªåŠ¨ calibration of $ C_{\text{prefill}} $**ï¼šå‡å°‘å¯¹ç¦»çº¿ profiling çš„ä¾èµ–ã€‚
- **ç»“åˆ energy efficiency æˆ– cost-per-token ä¼˜åŒ–**ï¼šé¢å‘ç»¿è‰² AI å’Œå•†ä¸šåŒ–éƒ¨ç½²ã€‚

---

## æ€»ç»“
> **Nightjar æå‡ºäº†ä¸€ç§çœŸæ­£é¢å‘ç°å®ä¸–ç•Œçš„ adaptive speculative decoding æ¡†æ¶**ã€‚å®ƒé€šè¿‡ contextual MAB å®ç°äº†å¯¹åŠ¨æ€è¯·æ±‚è´Ÿè½½çš„å®æ—¶æ„ŸçŸ¥ä¸å“åº”ï¼Œä¸ä»…æå‡äº† throughputï¼ˆæœ€é«˜ +14.8%ï¼‰å’Œé™ä½äº† latencyï¼ˆæœ€ä½ -20.2%ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯è§£å†³äº†â€œä½•æ—¶è¯¥åœç”¨ SDâ€çš„æ ¹æœ¬éš¾é¢˜ã€‚å…¶å¯¹ switching cost çš„ç²¾ç»†å»ºæ¨¡å’Œå¯¹ continuous batching çš„è‰¯å¥½é€‚é…ï¼Œä½¿å…¶æˆä¸ºå½“å‰æœ€å…ˆè¿›çš„ LLM serving è‡ªé€‚åº”æ¨ç†æ–¹æ¡ˆä¹‹ä¸€ã€‚

</details>

---

### 15. [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)

**Authors**: Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22170v1  

#### Abstract
Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„ **Reward Model (RM)** åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
- **æ ‡æ³¨å™ªå£°**ï¼šä¼ ç»Ÿçš„ in-prompt pairwise æ³¨é‡Šæ–¹å¼åœ¨è´¨é‡ç›¸è¿‘çš„æ ·æœ¬é—´å®¹æ˜“å¼•å…¥ä¸»è§‚åˆ¤æ–­åå·®ï¼Œå¯¼è‡´æ ‡ç­¾å™ªå£°ã€‚
- **å¥–åŠ±é»‘å®¢æ”»å‡»ï¼ˆReward Hackingï¼‰**ï¼šRM å­¦ä¹ åˆ°çš„æ˜¯ä»£ç†ç›®æ ‡è€ŒéçœŸå®äººç±»åå¥½ï¼Œåœ¨ post-training é˜¶æ®µæ˜“è¢«ç­–ç•¥æ¨¡å‹åˆ©ç”¨â€œæ·å¾„ç‰¹å¾â€æ¬ºéª—ã€‚
- **æ¶æ„è¡¨è¾¾èƒ½åŠ›ä¸è¶³**ï¼šç°æœ‰ VLM-based RM æ¶æ„ï¼ˆå¦‚ last tokenã€special tokenï¼‰æ— æ³•å……åˆ†èšåˆå¤šå±‚è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´å¥–åŠ±åç¼©ï¼ˆscore clusteringï¼‰ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡º **SoliReward**ï¼Œä¸€ä¸ªç³»ç»Ÿæ€§çš„è§†é¢‘ RM è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹å››å¤§åˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰å•é¡¹ç›®äºŒå…ƒæ ‡æ³¨ + è·¨æç¤ºé…å¯¹ç­–ç•¥ï¼ˆSingle-Item Binary Annotation + Cross-Prompt Pairingï¼‰
- æ”¹å˜ä¼ ç»Ÿ pairwise æˆ– Likert è¯„åˆ†æ–¹å¼ï¼Œé‡‡ç”¨ **Pass/Fail äºŒå…ƒæ ‡æ³¨**ï¼Œæ˜¾è‘—é™ä½æ ‡æ³¨æ­§ä¹‰ã€‚
- å¼•å…¥ **è·¨æç¤ºé…å¯¹ç­–ç•¥**ï¼šå°†ä¸åŒ prompt ä¸‹çš„ Pass å’Œ Fail è§†é¢‘è¿›è¡Œé…å¯¹æ„å»º preference æ•°æ®ï¼Œé¿å… in-prompt æ¨¡ç³Šæ¯”è¾ƒå¸¦æ¥çš„å™ªå£°ï¼ŒåŒæ—¶æå‡æ•°æ®åˆ©ç”¨ç‡ï¼ˆå¯åˆ©ç”¨ä»…ç”Ÿæˆä¸€ä¸ªè§†é¢‘çš„ promptï¼‰ã€‚

#### ï¼ˆ2ï¼‰å¸¦èƒœ-å¹³å±€çš„ Bradley-Terry æŸå¤±ï¼ˆBradley-Terry with Win-Tie, BT-WTï¼‰
- åœ¨æ ‡å‡† BT æŸå¤±åŸºç¡€ä¸Šå¼•å…¥ **win-tie pairs**ï¼ˆå³ä¸¤ä¸ª Pass æ ·æœ¬ä¹‹é—´çš„é…å¯¹ï¼‰ï¼Œæ˜¾å¼çº¦æŸæ­£æ ·æœ¬å†…éƒ¨çš„å¾—åˆ†æ–¹å·®ã€‚
- ä½œç”¨æœºåˆ¶ï¼šé€šè¿‡æƒ©ç½šé«˜è´¨æ ·æœ¬é—´çš„å¾—åˆ†å·®å¼‚ï¼Œè¿«ä½¿ RM å°†æ‰€æœ‰é«˜è´¨é‡æ ·æœ¬æ˜ å°„åˆ°ç´§å‡‘æµå½¢ä¸Šï¼Œç¼“è§£ reward spikesï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶ reward hackingã€‚

#### ï¼ˆ3ï¼‰åˆ†å±‚æ¸è¿›æŸ¥è¯¢æ³¨æ„åŠ›æ¶æ„ï¼ˆHierarchical Progressive Query Attention, HPQAï¼‰
- è®¾è®¡æ–°å‹ RM é€‚é…å™¨æ¶æ„ï¼Œä» VLM å¤šä¸ª transformer å±‚ä¸­é€æ­¥èšåˆç‰¹å¾ã€‚
- ä½¿ç”¨ learnable query å‘é‡é€å±‚ refine è¡¨ç¤ºï¼Œå¹¶ç»“åˆæ®‹å·®è¿æ¥èåˆæœ€ç»ˆå±‚è¾“å‡ºï¼Œå®ç°ä½çº§è§†è§‰ä¿çœŸåº¦ä¸é«˜çº§è¯­ä¹‰ç†è§£çš„èåˆï¼Œå¢å¼ºå¥–åŠ±ä¿¡å·é²æ£’æ€§ã€‚

#### ï¼ˆ4ï¼‰å»ºç«‹æ–°çš„è¯„æµ‹åŸºå‡†
- æ„å»ºé’ˆå¯¹ **ç‰©ç†åˆç†æ€§ï¼ˆPhysical Plausibilityï¼‰** å’Œ **ä¸»ä½“å˜å½¢ï¼ˆSubject Deformityï¼‰** çš„é«˜è´¨é‡æµ‹è¯•é›†ï¼Œæ¨åŠ¨æ›´å…¨é¢çš„è§†é¢‘ç”Ÿæˆè´¨é‡è¯„ä¼°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•°æ®æ•ˆç‡ä¸ä¸€è‡´æ€§** | å•é¡¹äºŒå…ƒæ ‡æ³¨ IAA æ˜¾è‘—é«˜äº pairwiseï¼ˆKrippendorffâ€™s Î±: 0.49 vs 0.35ï¼‰ |
| **æŠ— reward hacking èƒ½åŠ›** | BT-WT ä½¿ post-training æ€§èƒ½æå‡æ˜æ˜¾ï¼Œä¸” group advantage æ›´ç¨³å®š |
| **æ¶æ„è¡¨è¾¾èƒ½åŠ›** | HPQA é¿å… score clusteringï¼Œæ”¯æŒç»†ç²’åº¦æ’åº |
| **æ³›åŒ–èƒ½åŠ›** | åœ¨ OOD æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯å…¶è¿ç§»æ½œåŠ› |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **è®­ç»ƒæ•°æ®**ï¼šè‡ªå»º 250k è§†é¢‘æ•°æ®é›†ï¼Œè¦†ç›– 20k ä¸åŒ promptï¼Œæ¶µç›–å¤šç§ä¸»é¢˜ã€è¿åŠ¨ã€é£æ ¼ä¸ç›¸æœºæ§åˆ¶ã€‚
- **æµ‹è¯•æ•°æ®**ï¼š
  - **ID æµ‹è¯•é›†**ï¼šä¿ç•™æœªè§çš„è®­ç»ƒé›†åˆ’åˆ†ã€‚
  - **OOD æµ‹è¯•é›†**ï¼š50k æ¥è‡ª SOTA æ¨¡å‹ï¼ˆWan2.1/2.2ã€Veo 3ã€Seedance 1.0ï¼‰ç”Ÿæˆçš„è§†é¢‘ï¼Œç”¨äºè¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚
- **æ ‡æ³¨ç»´åº¦**ï¼š
  - ç‰©ç†åˆç†æ€§ï¼ˆPhysical Plausibilityï¼‰
  - ä¸»ä½“å˜å½¢ï¼ˆSubject Deformityï¼‰
  - è¯­ä¹‰å¯¹é½ï¼ˆSemantic Alignment, TAï¼‰

### å®éªŒè®¾ç½®
- **RM Backbone**ï¼šInternVL3 ç³»åˆ—ï¼ˆ1B/8B/14Bï¼‰
- **Post-training Backbone**ï¼šHunyuanVideoï¼ˆ14Bï¼‰
- **ä¼˜åŒ–ç®—æ³•**ï¼šDanceGRPOï¼ˆåŸºäº flow-based GRPOï¼‰
- **è®­ç»ƒç»†èŠ‚**ï¼š
  - ä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
  - AdamW ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ 1e-6ï¼Œwarmup ratio 0.05
  - å…·ä½“è¶…å‚è§é™„å½• Table 11 å’Œ Table 12

### è¯„ä¼°æŒ‡æ ‡
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **RM è‡ªèº«æ€§èƒ½** | RM ACCï¼ˆID/OODï¼‰ã€Score Margin |
| **Post-training æ•ˆæœ** | VBench2 Human Fidelityã€Motion Quality (MQ) |
| **äººå·¥ä¸€è‡´æ€§** | Krippendorffâ€™s Î±ã€Fleissâ€™s Kã€Raw Agreement |
| **æ¶ˆèç ”ç©¶** | Ablation on architectureã€loss functionã€pairing strategy |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ |
|------|------|
| VideoScore / VideoScore-v1.1 | åŸºäº Likert è¯„åˆ†çš„è‡ªåŠ¨æŒ‡æ ‡ |
| LiFT | åŸºäº point-wise å›å½’çš„æ–¹æ³• |
| VisionReward | äºŒåˆ†ç±» + BT æ¡†æ¶ |
| VideoPhy | ä¸“æ³¨äºç‰©ç†å¸¸è¯†çš„ RM |
| UnifiedReward | å¤šæ¨¡æ€ç»Ÿä¸€ RM |
| VideoAlign | å½“å‰ä¸»æµ pairwise RMï¼Œä½¿ç”¨ special token head |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆRM Accuracyï¼‰

#### è¡¨ï¼šRM å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆæ‘˜è‡ª Table 2ï¼‰

| Task | Approach | ID (%) | OOD (%) |
|------|----------|--------|---------|
| **Phy & Deform** | VideoAlign [17] | 54.40 | 71.60 |
| | **Ours (SoliReward)** | **78.48** | **80.08** |
| **TA** | VideoPhy [2] | 54.85 | 60.52 |
| | **Ours (SoliReward)** | **79.02** | 60.25 |

> âœ… SoliReward åœ¨ç‰©ç†åˆç†æ€§å’Œä¸»ä½“å˜å½¢ä»»åŠ¡ä¸Šå¤§å¹…é¢†å…ˆæ‰€æœ‰åŸºçº¿ï¼Œå°¤å…¶ OOD æ³›åŒ–èƒ½åŠ›å¼ºã€‚

---

### Post-training æ€§èƒ½å¯¹æ¯”ï¼ˆTable 4ï¼‰

| Backbone | RM | MQ | VBench2 |
|----------|----|-----|---------|
| HunyuanVideo | â€” | -0.0980 | 0.8426 |
| HunyuanVideo | VideoAlign MQ | 0.1607 | 0.8695 |
| HunyuanVideo | **Ours** | **0.3302** | **0.8999** |

> âœ… ä½¿ç”¨ SoliReward æŒ‡å¯¼ post-training åï¼ŒVBench2 Human Fidelity æå‡è¾¾ **+5.73%**ï¼Œè¿œè¶… VideoAlign MQã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æŸå¤±å‡½æ•°å¯¹æ¯”ï¼ˆBT vs BT-WTï¼‰â€”â€” Table 3
| Method | RM ACC | VBench2 | MQ |
|--------|--------|---------|-----|
| BT | 77.63 | 0.8693 | 0.1719 |
| **BT-WT** | **78.27** | **0.8999** | **0.3302** |

> å°½ç®¡ RM ACC æå‡æœ‰é™ï¼Œä½† **BT-WT æ˜¾è‘—æ”¹å–„ post-training æ•ˆæœ**ï¼Œè¯´æ˜å…¶æœ‰æ•ˆç¼“è§£äº† reward hackingã€‚

#### ï¼ˆ2ï¼‰æ¶æ„å¯¹æ¯”ï¼ˆHPQA vs Othersï¼‰â€”â€” Table 5
| Approach | Phy & Deform (OOD) | TA (OOD) |
|----------|---------------------|----------|
| Linear Head [38] | 78.66 | 31.92* |
| 'Yes' token logits [32] | 78.46 | 31.37* |
| Special token + Ln [17] | 73.61 | 58.38 |
| **HPQA (Ours)** | **80.08** | **60.25** |

> HPQA åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡æœ€ä¼˜ï¼Œå°¤å…¶åœ¨è¯­ä¹‰å¯¹é½ä»»åŠ¡ä¸Šé¿å…äº† score clusteringï¼ˆ*æ ‡è®°è¡¨ç¤ºå­˜åœ¨ç¦»æ•£åŒ–é—®é¢˜ï¼‰ã€‚

#### ï¼ˆ3ï¼‰é…å¯¹ç­–ç•¥å¯¹æ¯”ï¼ˆCross-Prompt vs In-Promptï¼‰â€”â€” Appendix Table 7
| Strategy | Phy & Deform (ID) | Phy & Deform (OOD) |
|----------|-------------------|--------------------|
| In-Prompt | 76.77 | 79.22 |
| **Cross-Prompt** | **76.74** | **79.54** |

> è·¨æç¤ºé…å¯¹æ€§èƒ½ç›¸å½“ç”šè‡³ç•¥ä¼˜ï¼Œä¸”èƒ½æ›´å¥½åˆ©ç”¨å•æ ·æœ¬ promptï¼Œæå‡æ•°æ®åˆ©ç”¨ç‡ã€‚

#### ï¼ˆ4ï¼‰BCE æ­£åˆ™é¡¹å½±å“ â€”â€” Table 6
åŠ å…¥ Binary Cross Entropy æ­£åˆ™é¡¹è™½ä¿æŒ ACCï¼Œä½†å¯¼è‡´ **reward margin ä¸‹é™ 19.62%**ï¼Œè¿›è€ŒæŸå®³ post-training æ•ˆæœï¼Œè¡¨æ˜åˆ†ç±»å‡†ç¡®ç‡ â‰  æ’åºèƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å•é¡¹äºŒå…ƒæ ‡æ³¨æ˜¾è‘—æå‡æ ‡æ³¨ä¸€è‡´æ€§**ï¼šç›¸æ¯” pairwise æ¯”è¾ƒï¼ŒPass/Fail åˆ¤æ–­å…·æœ‰æ›´é«˜ IAAï¼ˆÎ± > 0.49ï¼‰ï¼Œæ›´é€‚åˆå¤æ‚è§†é¢‘è´¨é‡è¯„ä¼°ã€‚
2. **è·¨æç¤ºé…å¯¹å¯è¡Œä¸”é«˜æ•ˆ**ï¼šæ— éœ€é™åˆ¶åŒä¸€ prompt å†…é…å¯¹ï¼Œæ‰©å¤§æ•°æ®æ¥æºå¹¶ä¿ƒè¿›é€šç”¨è´¨é‡è¡¨å¾å­¦ä¹ ã€‚
3. **BT-WT æ˜¯ç¼“è§£ reward hacking çš„æœ‰æ•ˆæ‰‹æ®µ**ï¼šé€šè¿‡ win-tie pairs æ­£åˆ™åŒ–æ­£æ ·æœ¬åˆ†å¸ƒï¼Œå‡å°‘ reward spikesï¼Œæä¾›æ›´ç¨³å¥çš„ä¼˜åŒ–ä¿¡å·ã€‚
4. **HPQA æ¶æ„ä¼˜äºä¼ ç»Ÿ pooling æ–¹å¼**ï¼šå¤šå±‚ç‰¹å¾æ¸è¿›èåˆèƒ½æœ‰æ•ˆé˜²æ­¢ score clusteringï¼Œæå‡æ’åºç²¾ç»†åº¦ã€‚
5. **OOD å‡†ç¡®ç‡æ˜¯æ›´å¥½çš„ä¸‹æ¸¸æ€§èƒ½é¢„æµ‹å™¨**ï¼šID å‡†ç¡®ç‡å¯èƒ½è¯¯å¯¼ï¼Œè€Œ OOD è¡¨ç°æ›´èƒ½åæ˜ å®é™… post-training æ•ˆæœã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ RM ä»…é’ˆå¯¹å•ä¸€ç»´åº¦æ‰“åˆ†ï¼Œå¤šç»´åº¦èåˆéœ€è®¾è®¡å¤šä¸ª learnable queriesã€‚
- æ¨¡å‹æ‰©å±•åˆ†ææ˜¾ç¤ºï¼šä» 8B åˆ° 14B å‚æ•°æ—¶å‡ºç°æ”¶ç›Šé€’å‡ï¼Œæ¨æµ‹å—é™äºæ•°æ®è§„æ¨¡æˆ–ä¼˜åŒ–ç“¶é¢ˆã€‚
- æ‰€æœ‰å®éªŒé›†ä¸­åœ¨ text-to-video (T2V)ï¼Œå°šæœªéªŒè¯äº image-to-video (I2V) ç­‰å…¶ä»–æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ I2Vã€video editing ç­‰æ›´å¤š conditional generation åœºæ™¯ã€‚
- è®¾è®¡å¤šç»´åº¦è”åˆ RMï¼Œæ”¯æŒç«¯åˆ°ç«¯å¤šå±æ€§å¯¹é½ã€‚
- æ¢ç´¢æ›´å¤§è§„æ¨¡æ•°æ®ä¸æ›´é«˜æ•ˆè®­ç»ƒç­–ç•¥ä»¥çªç ´ scaling bottleneckã€‚
- å¼€æºä»£ç ä¸ benchmarkï¼Œæ¨åŠ¨ç¤¾åŒºå…±å»ºé«˜è´¨é‡è§†é¢‘ RM æ ‡å‡†ã€‚

---

> ğŸ”— **ä»£ç ä¸èµ„æº**ï¼šä½œè€…æ‰¿è¯ºå°†å…¬å¼€ä»£ç ä¸ benchmarkï¼Œè¯¦è§è®ºæ–‡æœ«å°¾å£°æ˜ã€‚

</details>

---

### 16. [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)

**Authors**: Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.22245v1  

#### Abstract
As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or com...

---

### 17. [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)

**Authors**: S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.22897v1  

#### Abstract
Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches o...

---

### 18. [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)

**Authors**: Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23070v1  

#### Abstract
Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe ex...

---

### 19. [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)

**Authors**: Ziru Niu, Hai Dong, A. K. Qin, Tao Gu, Pengcheng Zhang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23200v1  

#### Abstract
Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and red...

---

### 20. [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)

**Authors**: Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23367v1  

#### Abstract
Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extend...

---

### 21. [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)

**Authors**: Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22136v1  

#### Abstract
Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that join...

---

### 22. [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)

**Authors**: Jiangwen Dong, Jiayu Li, Wanyu Lin  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22137v1  

#### Abstract
Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mit...

---

### 23. [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)

**Authors**: Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22264v1  

#### Abstract
PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer m...

---

### 24. [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)

**Authors**: Reda Heddad, Lamiae Bouanane  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22296v1  

#### Abstract
The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Mac...

---

### 25. [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)

**Authors**: Donghwa Kang, Shana Moothedath  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22675v1  

#### Abstract
Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task repr...

---

### 26. [Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution](https://arxiv.org/abs/2512.23068)

**Authors**: Shuhuan Wang, Yuzhen Xie, Jiayi Li, Yinliang Diao  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.23068v1  

#### Abstract
Selective State Space Models (SSMs) achieve linear-time inference, yet their gradient-based sensitivity analysis remains bottlenecked by O(L) memory scaling during backpropagation. This memory constraint precludes genomic-scale modeling (L > 10^5) on consumer-grade hardware. We introduce Phase Gradi...

---

### 27. [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)

**Authors**: Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.22219v1  

#### Abstract
We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual strea...

---

### 28. [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)

**Authors**: Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.22231v1  

#### Abstract
Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are incre...

---

### 29. [Graph Attention-based Adaptive Transfer Learning for Link Prediction](https://arxiv.org/abs/2512.22252)

**Authors**: Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.22252v1  

#### Abstract
Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of al...

---

### 30. [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)

**Authors**: Saif Khalfan Saif Al Mazrouei  
**Category**: cs.CL  
**Published**: 2025-12-30  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.23214v1  

#### Abstract
Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
