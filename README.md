# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-12 06:46:28 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)

**Authors**: Luca Moschella, Laura Manduchi, Ozan Sener  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2602.10238v1  

#### Abstract
The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve o...

---

### 2. [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622)

**Authors**: Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Yang Chen, Xiaotong Lin, Wuliang Huang, Ziyi Gao, Xing Fu, Yu Cheng, Weiqiang Wang  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2602.10622v1  

#### Abstract
Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention ma...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰ï¼Œ**decoder-only LLMs** è™½ç„¶å› å…¶è‡ªå›å½’ç‰¹æ€§è¢«å¹¿æ³›ç”¨äºç”¨æˆ·è¡Œä¸ºå»ºæ¨¡ï¼ˆå¦‚æ¨èã€è¥é”€ç­‰ï¼‰ï¼Œä½†åœ¨ä½œä¸º **user representation learning** çš„ç¼–ç å™¨æ—¶ï¼Œå…¶æ ‡å‡†çš„ **causal attention masking** é™åˆ¶äº†æ¨¡å‹å¯¹å…¨å±€ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å°è¯•å¼•å…¥ **bidirectional attention** æˆ– **hybrid masking**ï¼Œä½†ç¼ºä¹ç³»ç»Ÿæ€§æ¯”è¾ƒï¼Œä¸”ä»é¢„è®­ç»ƒçš„å› æœæ³¨æ„åŠ›å‘åŒå‘æ³¨æ„åŠ›è¿‡æ¸¡æ—¶å­˜åœ¨è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚

æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒ **attention masking** ç­–ç•¥åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å¯¹ç”¨æˆ·è¡¨å¾è´¨é‡çš„å½±å“ï¼Œå¹¶æ­ç¤ºäº†**è®­ç»ƒåŠ¨æ€è¿‡ç¨‹**ï¼ˆå°¤å…¶æ˜¯ä» causal åˆ° bidirectional çš„è¿‡æ¸¡ï¼‰æ˜¯å½±å“æœ€ç»ˆè¡¨ç°çš„å…³é”®å› ç´ ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº† **Gradient-Guided Soft Masking (GG-SM)**ï¼Œä¸€ç§åŸºäºæ¢¯åº¦å¼•å¯¼çš„è½¯æ©ç é¢„çƒ­æœºåˆ¶ï¼Œç”¨äºå¹³æ»‘åœ°å®ç°ä» causal åˆ° bidirectional attention çš„è¿‡æ¸¡ï¼š

- **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨è®­ç»ƒåˆæœŸï¼ˆwarm-up é˜¶æ®µï¼‰ï¼Œåˆ©ç”¨æŸå¤±å‡½æ•°å¯¹éšè—çŠ¶æ€çš„æ¢¯åº¦èŒƒæ•° $\|\nabla_{\mathbf{h}} \mathcal{L}\|$ æ¥åŠ¨æ€å†³å®šå“ªäº›â€œæœªæ¥çš„ tokenâ€åº”è¯¥è¢«æå‰å…³æ³¨â€”â€”å³æ¢¯åº¦è¶Šå¤§ï¼Œè¯´æ˜è¯¥ token å¯¹ä»»åŠ¡è¶Šé‡è¦ï¼Œå› æ­¤åº”èµ‹äºˆæ›´é«˜çš„å¯è§æ€§æƒé‡ã€‚
- åœ¨ warm-up ç»“æŸåï¼Œå›ºå®šè¿™äº›ç”±æ¢¯åº¦é©±åŠ¨çš„è½¯æ©ç ï¼Œå¹¶é€šè¿‡çº¿æ€§è°ƒåº¦å™¨é€æ­¥æ‰“å¼€å…¨éƒ¨æœªæ¥æ³¨æ„åŠ›ï¼Œæœ€ç»ˆåœ¨æ¨ç†é˜¶æ®µä½¿ç”¨å®Œå…¨çš„ **bidirectional attention**ã€‚

è¿™ç§æ–¹æ³•å°†æ³¨æ„åŠ›æœºåˆ¶è§†ä¸ºä¸€ä¸ªå¯æ¼”åŒ–çš„ç“¶é¢ˆï¼Œè€Œéé™æ€è®¾è®¡ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç¼ºé™· | GG-SM çš„ä¼˜åŠ¿ |
|------|------|--------------|
| **Causal Masking** | ä»…èƒ½æ•æ‰å•å‘ä¾èµ–ï¼Œæ— æ³•å»ºæ¨¡é•¿ç¨‹è¯­ä¹‰æ•´åˆ | æ”¯æŒå…¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œæå‡è¡¨ç¤ºå®Œæ•´æ€§ |
| **Hybrid Masking** | æ¶æ„å¤æ‚ï¼Œéœ€é¢å¤–æ¨¡å—ï¼ˆå¦‚ MLP æˆ– global queryï¼‰ï¼Œéš¾ä»¥ä¸é¢„è®­ç»ƒå¯¹é½ | ä¸å¼•å…¥é¢å¤–å‚æ•°ï¼Œä¿æŒä¸ decoder é¢„è®­ç»ƒå…¼å®¹ |
| **Scheduler-only Transition** | å›ºå®šè°ƒåº¦ç­–ç•¥å¿½ç•¥ä»»åŠ¡ä¿¡å·ï¼Œå¯èƒ½å¯¼è‡´ä¼˜åŒ–éœ‡è¡ | åˆ©ç”¨æ¢¯åº¦æä¾›æ•°æ®é©±åŠ¨çš„ä¼˜å…ˆçº§ä¿¡å·ï¼Œæ›´ç¨³å®šé«˜æ•ˆ |

âœ… **GG-SM çš„ä¼˜åŠ¿æ€»ç»“**ï¼š
- æ›´ç¨³å®šçš„è®­ç»ƒæ”¶æ•›ï¼›
- æ›´é«˜è´¨é‡çš„ bidirectional ç”¨æˆ·è¡¨å¾ï¼›
- å…¼å®¹ decoder-only LLM çš„é¢„è®­ç»ƒæ¨¡å¼ï¼›
- æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œæ˜“äºé›†æˆã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
åŸºäºçœŸå®ä¸–ç•Œ **Alipay** ç”¨æˆ·è¡Œä¸ºæ•°æ®æ„å»ºä¸¤ä¸ªè®­ç»ƒæ•°æ®é›†ï¼š

1. **Rule-based Behavioral Trajectories Dataset ($D_{\text{behavior}}$)**  
   - åŒ…å«ç”¨æˆ·è¿‡å» 90 å¤©çš„å¤šæ¨¡æ€äº¤äº’åºåˆ—ï¼š`PayBill`, `MiniProgram`, `SPM paths`, `App`, `Search`, `Tabular features`
   - æ­£æ ·æœ¬ä¸ºåç»­ä¸€ä¸ªæœˆå†…çš„èšåˆè¡Œä¸ºï¼Œç”¨äºé¢„æµ‹æœªæ¥è¡Œä¸ºã€‚

2. **LLM-Synthesized Query-Answer Alignments Dataset ($D_{\text{qa}}$)**  
   - ä½¿ç”¨ **Qwen-Max** è‡ªåŠ¨ç”Ÿæˆç”¨æˆ·ç†è§£ç›¸å…³çš„ QA å¯¹ã€‚
   - å¼•å…¥â€œå›°éš¾æ­£æ ·æœ¬æŒ–æ˜â€æœºåˆ¶ï¼šå…ˆç”Ÿæˆåˆå§‹ QA â†’ ç”¨å¼º embedding æ¨¡å‹è®¡ç®— alignment difficultyï¼ˆ$S_d = 1 - \text{Sim}(u\oplus q, a)$ï¼‰â†’ è¿‡æ»¤å‡ºé«˜éš¾åº¦æ ·æœ¬ â†’ æå–å…±æ€§è§„åˆ™ä¼˜åŒ– prompt â†’ æ‰©å±•ç”Ÿæˆé«˜è´¨é‡ hard positivesã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- **Backbone**: `Qwen2.5-0.5B-Instruct`ï¼ˆdecoder-only LLMï¼‰
- **Modality Encoders**: `gte-base-zh` åˆ†åˆ«å¤„ç†å„ç±»è¡Œä¸ºç‰¹å¾
- **Adapter**: LoRA (rank=64, Î±=64)
- **è¾“å…¥æ ¼å¼**ï¼šå„æ¨¡æ€ç”¨ç‰¹æ®Šæ ‡ç­¾åŒ…è£¹ï¼ˆå¦‚ `<bill>...</bill>`ï¼‰ï¼Œæœ«å°¾æ·»åŠ  `<USER>` token ä½œä¸º embedding æå–é”šç‚¹
- **è¾“å‡º embedding**ï¼šå– `<USER>` ä½ç½®çš„æœ€åä¸€å±‚ hidden state å¹¶è¿›è¡Œ L2 å½’ä¸€åŒ–

#### è®­ç»ƒç›®æ ‡
é‡‡ç”¨ **InfoNCE loss** è¿›è¡Œ contrastive learningï¼š
$$
\mathcal{L}_{\text{cl}} = -\log \frac{\exp(s(\mathbf{u}_i, \mathbf{a}_i)/\tau)}{\sum_j \exp(s(\mathbf{u}_i, \mathbf{a}_j)/\tau)}
$$
å¹¶å¼•å…¥ mask factor $m_{ij}$ é¿å… false negatives å¹²æ‰°ã€‚

#### è¯„ä¼°æ–¹å¼
- **ä¸‹æ¸¸ä»»åŠ¡**ï¼š9 ä¸ªå·¥ä¸šçº§ binary classification ä»»åŠ¡ï¼Œè¦†ç›–ä¸‰å¤§é¢†åŸŸï¼š
  - **User Prediction**ï¼šConcert Click, User Login, MAU Loss
  - **Behavior Preference**ï¼šPublic Transit, Consumption Power, Food, Movie
  - **Marketing Sensitivity**ï¼šAchievement, Physical Preference
- **è¯„ä¼°æŒ‡æ ‡**ï¼š**AUC**ï¼ˆArea Under the ROC Curveï¼‰
- **è®­ç»ƒèµ„æº**ï¼š64 Ã— A100-80GB GPUsï¼Œbatch size 2048ï¼ŒAdamW + cosine decay

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **General Embedding Models** | Qwen3-Embedding-0.6B, Llama-embed-nemotron-8B, KaLM-Embedding-Gemma3-12B |
| **Traditional User Models** | MSDP, One4all, CPC |
| **LLM-based User Models** | FOUND, InstructUE |
| **Attention Masking Variants** | Causal, Hybrid (mask/gq/mlp), Bidirectional (direct/scheduler/GG-SM) |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆå¹³å‡ AUCï¼‰

| æ–¹æ³• | Avg AUC |
|------|--------|
| **Qwen3-Embedding-0.6B** | 0.6720 |
| **Llama-embed-nemotron-8B** | 0.7357 |
| **KaLM-Embedding** | 0.7156 |
| **FOUND (SOTA LLM-based)** | 0.7690 |
| **InstructUE** | 0.7728 |
| **Ours (w/ GG-SM)** | **0.7745** âœ… |

> å°½ç®¡ä»…ä½¿ç”¨ **0.5B å‚æ•°é‡**çš„ backboneï¼ŒGG-SM æ˜¾è‘—ä¼˜äºå¤šä¸ªæ›´å¤§è§„æ¨¡çš„é€šç”¨ embedding æ¨¡å‹ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

| ç»´åº¦ | ç»“æœåˆ†æ |
|------|----------|
| **vs. Causal Baseline** | GG-SM æå‡æ˜æ˜¾ï¼ˆ0.6542 â†’ 0.7745ï¼‰ï¼Œè¯æ˜åŒå‘æ³¨æ„åŠ›å¯¹ç”¨æˆ·ç†è§£è‡³å…³é‡è¦ |
| **vs. Hybrid Methods** | Hybridmask: 0.7710, Hybridmlp: 0.7718 â†’ GG-SM ä»å°å¹…é¢†å…ˆï¼Œä¸”æ— é¢å¤–å‚æ•° |
| **vs. Scheduler-only** | Scheduler: 0.7733 â†’ GG-SM è¾¾åˆ° **0.7745**ï¼ŒéªŒè¯æ¢¯åº¦å¼•å¯¼çš„æœ‰æ•ˆæ€§ |
| **è·¨åŸŸç¨³å®šæ€§** | åœ¨æœ€éš¾çš„ **Marketing Sensitivity** ä¸Šä¹Ÿè¾¾åˆ°å³°å€¼ AUCï¼Œæ˜¾ç¤ºæ›´å¼ºæ³›åŒ–èƒ½åŠ› |

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| å˜ä½“ | Avg AUC | åˆ†æ |
|------|--------|------|
| Causal (Oracle) | 0.6542 | ä¸¥é‡å—é™äºå±€éƒ¨ä¸Šä¸‹æ–‡ |
| Bidirectional (Direct) | 0.7721 | æ¯” causal å¥½ï¼Œä½†ä¸å¦‚æ¸è¿›å¼å¼€æ”¾ |
| w/ Scheduler | 0.7733 | å›ºå®šè°ƒåº¦æœ‰æ•ˆï¼Œä½†æœªåˆ©ç”¨ä»»åŠ¡ä¿¡å· |
| **w/ GG-SM (Ours)** | **0.7745** | æ¢¯åº¦å¼•å¯¼å¸¦æ¥æœ€ç¨³å®šã€æœ€å¼ºçš„è¡¨ç° |

> å›¾ 3 æ˜¾ç¤ºï¼š**GG-SM æ”¶æ•›æ›´å¿«ã€æŸå¤±ä¸‹é™æ›´å¹³ç¨³**ï¼Œè€Œ scheduler å‡ºç°æ˜æ˜¾éœ‡è¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **attention masking è®¾è®¡ç›´æ¥å½±å“ç”¨æˆ·è¡¨å¾è´¨é‡**ï¼š
   - Bidirectional > Hybrid > Causalï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šä¸€è‡´æˆç«‹ã€‚
2. **è®­ç»ƒè·¯å¾„æ¯”æœ€ç»ˆç»“æ„æ›´é‡è¦**ï¼š
   - ç›´æ¥åˆ‡æ¢åˆ° bidirectional å®¹æ˜“å¯¼è‡´ä¸æ”¶æ•›ï¼›**æ¸è¿›å¼è¿‡æ¸¡ + æ¢¯åº¦å¼•å¯¼** æ˜¯å…³é”®ã€‚
3. **GG-SM å®ç°äº†å‚æ•°æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡**ï¼š
   - å³ä½¿ä½¿ç”¨å°æ¨¡å‹ï¼ˆ0.5Bï¼‰ï¼Œä¹Ÿèƒ½è¶…è¶Šå¤§æ¨¡å‹ï¼ˆ8B+ï¼‰ï¼Œè¯´æ˜**é¢†åŸŸé€‚é…ä¼˜äºå•çº¯æ‰©å‚**ã€‚
4. **decoder-only LLM å¯ä»¥å®‰å…¨ç”¨äº bidirectional encoding**ï¼š
   - åªè¦è®­ç»ƒè¿‡ç¨‹å¾—å½“ï¼Œå³ä½¿ç ´å autoregressive constraintï¼Œä»èƒ½è·å¾— superior è¡¨ç¤ºèƒ½åŠ›ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡åˆæˆæ•°æ®**ï¼šè™½ç„¶æå‡ºè®­ç»ƒ-free åˆæˆæ¡†æ¶ï¼Œä½†ä»éœ€å¼ºå¤§ LLMï¼ˆå¦‚ Qwen-Maxï¼‰ç”Ÿæˆå›°éš¾æ ·æœ¬ã€‚
- **è®¡ç®—å¼€é”€å¢åŠ **ï¼šæ¢¯åº¦è®¡ç®—å¢åŠ äº† warm-up é˜¶æ®µçš„å†…å­˜è´Ÿæ‹…ã€‚
- **åº”ç”¨åœºæ™¯å—é™äº offline encoding**ï¼šæ¨ç†æ—¶è™½å¯ç”¨ bidirectionalï¼Œä½†ä¸èƒ½ç”¨äºå®æ—¶ç”Ÿæˆåœºæ™¯ï¼ˆå› è¿åè‡ªå›å½’å‡è®¾ï¼‰ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ¢ç´¢æ›´é«˜æ•ˆçš„æ¢¯åº¦ä¼°è®¡æ–¹å¼ï¼ˆå¦‚ä½ç§©è¿‘ä¼¼ï¼‰ä»¥é™ä½ warm-up æˆæœ¬ï¼›
2. å°† GG-SM åº”ç”¨äºå…¶ä»–æ¨¡æ€ï¼ˆå¦‚è§†é¢‘ã€è¯­éŸ³ï¼‰çš„ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡ï¼›
3. ç ”ç©¶å¦‚ä½•åœ¨ä¿ç•™ autoregressive capability çš„åŒæ—¶ï¼Œå®ç°éƒ¨åˆ†åŒå‘æ„ŸçŸ¥ï¼ˆpartial bidirectionalityï¼‰ï¼›
4. æ¨åŠ¨ hybrid masking æˆä¸ºä¸»æµèŒƒå¼ï¼Œæ¢ç´¢æ›´å¤š user-centric attention æ§åˆ¶æœºåˆ¶ã€‚

---

> ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/JhCircle/Deepfind-GGSM](https://github.com/JhCircle/Deepfind-GGSM)

</details>

---

### 3. [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)

**Authors**: Yupu Gu, Rongzhe Wei, Andy Zhu, Pan Li  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2602.10965v1  

#### Abstract
Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šMoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
ç°æœ‰çš„ **Knowledge Editing (KE)** æ–¹æ³•ä¸»è¦é’ˆå¯¹ **dense Transformer æ¶æ„** è®¾è®¡ï¼Œè€Œç°ä»£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨ **Mixture-of-Experts (MoE)** æ¶æ„ä»¥å®ç°é«˜æ•ˆæ‰©å±•ã€‚ç›´æ¥å°† dense æ¨¡å‹çš„ç¼–è¾‘æ–¹æ³•åº”ç”¨äº MoE æ¨¡å‹ä¼šé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **è®¡ç®—æˆæœ¬é«˜æ˜‚**ï¼šéœ€æ›´æ–°æ‰€æœ‰ä¸“å®¶å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—é‡éšä¸“å®¶æ•°é‡çº¿æ€§å¢é•¿ï¼ˆå¦‚ 128Ã—ï¼‰ã€‚
2. **ä¸“å®¶è€¦åˆï¼ˆExpert Couplingï¼‰**ï¼šè¾“å‡ºæ˜¯å¤šä¸ªä¸“å®¶åŠ æƒç»„åˆçš„ç»“æœï¼Œå•ä¸ªä¸“å®¶ä¿®æ”¹å¯èƒ½è¢«ç¨€é‡Šæˆ–å¼•å‘å‰¯ä½œç”¨ã€‚
3. **è·¯ç”±åˆ†å¸ƒåç§»ï¼ˆRouting Distribution Shiftï¼‰**ï¼šå‚æ•°æ‰°åŠ¨ä¼šæ”¹å˜ä¸‹æ¸¸ MoE å±‚çš„è¾“å…¥ï¼Œå¯¼è‡´è·¯ç”±å™¨é€‰æ‹©ä¸åŒä¸“å®¶ï¼Œç ´åæ¨¡å‹ç¨³å®šæ€§ä¸ç¼–è¾‘å±€éƒ¨æ€§ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡º **MoEEdit**ï¼Œé¦–ä¸ªä¸“ä¸º MoE æ¶æ„è®¾è®¡çš„ã€**è·¯ç”±ç¨³å®šï¼ˆrouting-stableï¼‰çš„å‚æ•°ä¿®æ”¹å‹çŸ¥è¯†ç¼–è¾‘æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **Per-Expert Null-Space Projectionï¼ˆæ¯ä¸“å®¶é›¶ç©ºé—´æŠ•å½±ï¼‰**  
  é€šè¿‡å°†ä¸“å®¶å‚æ•°æ›´æ–°é™åˆ¶åœ¨ä¿ç•™é›†ï¼ˆpreservation setï¼‰ç‰¹å¾çš„æ­£äº¤è¡¥ç©ºé—´ä¸­ï¼Œç¡®ä¿æ›´æ–°ä¸ä¼šå½±å“ä¿ç•™é›†è¾“å‡ºï¼Œä»è€Œ**æŠ‘åˆ¶è·¯ç”±è¾“å…¥å˜åŒ–ï¼Œé˜²æ­¢è·¯ç”±æ¼‚ç§»**ã€‚

- **Randomized Block Coordinate Descent (BCD) Solverï¼ˆéšæœºå—åæ ‡ä¸‹é™æ±‚è§£å™¨ï¼‰**  
  å°†å…¨å±€ä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºæŒ‰ä¸“å®¶åˆ’åˆ†çš„å­é—®é¢˜ï¼Œé€ä¸ªæ›´æ–°ç›¸å…³ä¸“å®¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚è¯¥æ±‚è§£å™¨**æ—¶é—´å¤æ‚åº¦ä¸ä¸“å®¶éšè—ç»´åº¦æˆçº¿æ€§å…³ç³»ï¼Œè€Œéä¸“å®¶æ€»æ•°**ï¼Œå…·å¤‡è‰¯å¥½å¯æ‰©å±•æ€§ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
- âœ… é¦–æ¬¡ç³»ç»Ÿè§£å†³ MoE ç¼–è¾‘ä¸­çš„ **è·¯ç”±ç¨³å®šæ€§é—®é¢˜**ã€‚
- âœ… æ˜¾è‘—ä¼˜äºå°† dense æ¨¡å‹ç¼–è¾‘æ–¹æ³•ï¼ˆå¦‚ FTã€UnKEï¼‰ç›´æ¥è¿ç§»åˆ° MoE ä¸Šçš„è¡¨ç°ã€‚
- âœ… åœ¨ä¿æŒé«˜ **efficacy** å’Œ **generalization** çš„åŒæ—¶ï¼Œå®ç°å“è¶Šçš„ **specificity** ä¸ **routing stability**ã€‚
- âœ… è®¡ç®—ä¸å†…å­˜æ•ˆç‡è¿œè¶…é—­å¼æ±‚è§£æ–¹æ¡ˆï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ MoE æ¨¡å‹ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **COUNTERFACT**ï¼šç”¨äºå•è·³åäº‹å®çŸ¥è¯†ç¼–è¾‘ï¼Œè¯„ä¼°æ¨¡å‹å¯¹äº‹å®æ›´æ”¹çš„å“åº”èƒ½åŠ›ã€‚
- **zsREï¼ˆZero-Shot Relation Extractionï¼‰**ï¼šè¯„ä¼°æ¨¡å‹åœ¨é›¶æ ·æœ¬å…³ç³»æŠ½å–ä»»åŠ¡ä¸Šçš„ç¼–è¾‘æ•ˆæœä¸æ³›åŒ–èƒ½åŠ›ã€‚

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹**ï¼š
  - `Qwen3-30B-A3B`ï¼šæ¯å±‚ 128 ä¸ªä¸“å®¶ï¼ŒTop-8 æ¿€æ´»ã€‚
  - `GPT-OSS-20B`ï¼šæ¯å±‚ 32 ä¸ªä¸“å®¶ï¼ŒTop-4 æ¿€æ´»ã€‚
- **ç¼–è¾‘æ–¹å¼**ï¼šè¿›è¡Œ **1,000 æ¬¡è¿ç»­æ‰¹é‡ç¼–è¾‘ï¼ˆbatched editsï¼‰**ï¼Œæ¯æ¬¡ batch å¤§å°ä¸º 50ã€‚
- **ç¡¬ä»¶ä¸ç²¾åº¦**ï¼šä½¿ç”¨ NVIDIA H20 GPUï¼Œæƒé‡ä½¿ç”¨ BF16ï¼Œä¼˜åŒ–è¿‡ç¨‹ä½¿ç”¨ FP32ã€‚

### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Efficacy** | ç¼–è¾‘æç¤ºä¸‹ç›®æ ‡ç­”æ¡ˆç”Ÿæˆçš„æˆåŠŸç‡ |
| **Generalization** | åœ¨æ”¹å†™ï¼ˆparaphraseï¼‰ä¸Šä¸‹æ–‡ä¸­ä»èƒ½æ­£ç¡®å›ç­”çš„èƒ½åŠ› |
| **Specificity** | å¯¹æ— å…³æ§åˆ¶æ ·æœ¬ä¸äº§ç”Ÿå¹²æ‰°çš„ç¨‹åº¦ï¼ˆå±€éƒ¨æ€§ï¼‰ |
| **Utility** | ä¸‰é¡¹æŒ‡æ ‡çš„å¹³å‡å€¼ï¼Œç»¼åˆè¡¡é‡æ€§èƒ½ |
| **Routing Similarity (RS)** | ç¼–è¾‘å‰å Top-K è·¯ç”±ä¸“å®¶é›†åˆçš„ Jaccard ç›¸ä¼¼åº¦ï¼Œè¡¡é‡è·¯ç”±ç¨³å®šæ€§ |
| **KL Divergence** | è·¯ç”±åˆ†å¸ƒå˜åŒ–çš„é‡åŒ–æŒ‡æ ‡ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Fine-Tuning (FT)**ï¼šå…¨é‡å¾®è°ƒã€‚
- **FT-L**ï¼šå¸¦èŒƒæ•°çº¦æŸçš„å¾®è°ƒã€‚
- **AdaLoRA**ï¼šä½ç§©è‡ªé€‚åº”å¾®è°ƒã€‚
- **UnKE**ï¼šåŸºäºç»“æ„åŒ–ä¼˜åŒ–çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ã€‚
- æ‰€æœ‰ baseline å‡é€‚é…è‡³ MoE æ¶æ„å¹¶ä½œç”¨äºç›¸åº”æ¨¡å—ï¼ˆå¦‚ `mlp.experts.down_proj`ï¼‰ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰**

#### **åœ¨ Qwen3-30B-A3B ä¸Šçš„è¡¨ç°ï¼ˆCOUNTERFACTï¼‰**
| Method     | Efficacy â†‘ | Generalization â†‘ | Specificity â†‘ | Utility â†‘ |
|------------|-----------|------------------|---------------|-----------|
| UnKE       | 89.30     | 82.85            | 48.15         | 73.43     |
| **MoEEdit** | **99.30** | **94.10**        | **80.97**     | **91.46** |

> MoEEdit åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå…¨é¢é¢†å…ˆï¼Œå°¤å…¶åœ¨ **specificity** ä¸Šæå‡è¶…è¿‡ 32 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### **åœ¨ GPT-OSS-20B ä¸Šçš„è¡¨ç°ï¼ˆZsREï¼‰**
| Method     | Efficacy â†‘ | Generalization â†‘ | Specificity â†‘ | Utility â†‘ |
|------------|-----------|------------------|---------------|-----------|
| AdaLoRA    | 43.46     | 42.96            | 33.60         | 40.01     |
| **MoEEdit** | **81.68** | **68.44**        | **32.55**     | **60.89** |

> MoEEdit åœ¨ **efficacy** å’Œ **generalization** ä¸Šåˆ†åˆ«æå‡ **+38.22** å’Œ **+25.48**ï¼Œè¾¾åˆ°ç»å¯¹ä¼˜åŠ¿ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- MoEEdit åœ¨ä¸¤ä¸ªæ¨¡å‹å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾— **state-of-the-art æ€§èƒ½**ã€‚
- ç›¸æ¯” FT å’Œ UnKEï¼ŒMoEEdit æ˜¾è‘—æå‡äº† **ç¼–è¾‘æœ‰æ•ˆæ€§** ä¸ **æ³›åŒ–èƒ½åŠ›**ï¼ŒåŒæ—¶é¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚
- åœ¨ **routing stability** æ–¹é¢è¡¨ç°å°¤ä¸ºçªå‡ºï¼ˆè§ä¸‹è¡¨ï¼‰ã€‚

#### **è·¯ç”±ç¨³å®šæ€§å¯¹æ¯”ï¼ˆTable 2ï¼ŒQwen3-30B-A3Bï¼‰**
| Method     | Editing Set RS â†‘ (Lay. 31â€“40) | Preservation Set RS â†‘ (Lay. 31â€“40) |
|------------|-------------------------------|-------------------------------------|
| FT         | 29.98                         | 30.97                               |
| UnKE       | 44.80                         | 43.84                               |
| **MoEEdit** | **89.93**                     | **90.22**                           |

> MoEEdit çš„å¹³å‡ RS è¶…è¿‡ **88%**ï¼Œè¡¨æ˜å…¶å‡ ä¹å®Œå…¨ä¿ç•™äº†åŸå§‹è·¯ç”±è·¯å¾„ï¼›è€Œå…¶ä»–æ–¹æ³•ä»…ç»´æŒçº¦ 30%-45%ï¼Œè¯´æ˜å­˜åœ¨ä¸¥é‡è·¯ç”±æ¼‚ç§»ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **(1) æŠ•å½±æœºåˆ¶çš„ä½œç”¨ï¼ˆAblation on Projectionï¼‰**
| Method             | Editing Set RS (avg.) | Pres. Set RS (avg.) | Î”RS â†“ |
|--------------------|------------------------|-----------------------|--------|
| MoEEdit (w/ Proj)  | 88.24                  | 88.60                 | â€”      |
| MoEEdit (w/o Proj) | 73.43                  | 73.39                 | ~15    |

> ç§»é™¤é›¶ç©ºé—´æŠ•å½±åï¼Œ**RS ä¸‹é™çº¦ 15 ä¸ªç™¾åˆ†ç‚¹**ï¼ŒéªŒè¯äº†è¯¥è®¾è®¡å¯¹è·¯ç”±ç¨³å®šçš„å†³å®šæ€§ä½œç”¨ã€‚

#### **(2) BCD æ±‚è§£å™¨ vs. é—­å¼æ±‚è§£å™¨**
- **é—­å¼æ±‚è§£å™¨**ï¼šéœ€æ±‚è§£è§„æ¨¡ä¸º $(Nd_k)^2$ çš„çŸ©é˜µé€†ï¼Œæ—¶é—´å’Œå†…å­˜å¼€é”€å‘ˆè¿‘äºŒæ¬¡å¢é•¿ï¼Œåœ¨ $N > 60$ æ—¶ä¸å¯è¡Œã€‚
- **BCD æ±‚è§£å™¨**ï¼šè¿è¡Œæ—¶é—´åŸºæœ¬æ’å®šï¼Œ**å¯æ‰©å±•è‡³ 128 ä¸ªä¸“å®¶ä»¥ä¸Š**ï¼Œä¸”ä»…éœ€ 6â€“10 è½®å³å¯æ”¶æ•›ã€‚

#### **(3) BCD è¿­ä»£æ¬¡æ•°çš„å½±å“**
- å‰å‡ è½®å¿«é€Ÿæå‡ efficacy å’Œ generalizationï¼Œåç»­è¶‹äºé¥±å’Œã€‚
- **6â€“10 è½® BCD å·²è¶³å¤Ÿè·å¾—æœ€ä¼˜æ€§ä»·æ¯”**ï¼Œæ›´å¤šè¿­ä»£å¸¦æ¥è¾¹é™…æ”¶ç›Šé€’å‡ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **MoE æ¶æ„çš„çŸ¥è¯†ç¼–è¾‘å¿…é¡»è€ƒè™‘è·¯ç”±ç¨³å®šæ€§**ï¼šå³ä½¿å°å¹…åº¦å‚æ•°æ‰°åŠ¨ä¹Ÿå¯èƒ½å¼•å‘çº§è”å¼è·¯ç”±åç§»ï¼Œç ´åæ¨¡å‹è¡Œä¸ºä¸€è‡´æ€§ã€‚
2. **ä¸“å®¶æ„ŸçŸ¥ï¼ˆexpert-awareï¼‰è®¾è®¡è‡³å…³é‡è¦**ï¼šç›²ç›®å¤åˆ¶ dense æ¨¡å‹ç¼–è¾‘ç­–ç•¥æ— æ³•åº”å¯¹ MoE çš„ç¨€ç–æ€§ä¸æ¡ä»¶è®¡ç®—ç‰¹æ€§ã€‚
3. **MoEEdit å®ç°äº†é«˜æ•ˆã€ç¨³å®šã€ç²¾å‡†çš„ç¼–è¾‘**ï¼šåœ¨å¤šä¸ª benchmark ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ ‘ç«‹äº† MoE KE çš„æ–°æ ‡æ†ã€‚
4. **é›¶ç©ºé—´æŠ•å½± + BCD æ˜¯æœ‰æ•ˆç»„åˆ**ï¼šå‰è€…ä¿éšœç¨³å®šæ€§ï¼Œåè€…ä¿éšœå¯æ‰©å±•æ€§ï¼ŒäºŒè€…ååŒæ„å»ºäº†ä¸€ä¸ªå®ç”¨åŒ–çš„ MoE ç¼–è¾‘æ¡†æ¶ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- å½“å‰æ–¹æ³•èšç„¦äº **MLP ä¸“å®¶å†…éƒ¨çš„ down-projection æƒé‡**ï¼Œæœªæ¶‰åŠ attention æˆ– router æœ¬èº«çš„ç¼–è¾‘ã€‚
- ä¾èµ–äº **preservation set æ„å»ºé›¶ç©ºé—´**ï¼Œè‹¥è¯¥é›†åˆä¸è¶³å¯èƒ½å¯¼è‡´è¿‡åº¦å¹²æ‰°ã€‚
- å¯¹ extremely rare facts æˆ– deeplyè€¦åˆçš„çŸ¥è¯†ä»å¯èƒ½å­˜åœ¨ç¼–è¾‘å¤±è´¥é£é™©ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢ **router-aware ç¼–è¾‘æœºåˆ¶**ï¼Œå…è®¸åœ¨å¿…è¦æ—¶å®‰å…¨è°ƒæ•´è·¯ç”±é€»è¾‘ã€‚
- ç»“åˆ **parameter-preserving ä¸ parameter-modifying æ–¹æ³•**ï¼Œæ„å»ºæ··åˆç¼–è¾‘ç³»ç»Ÿã€‚
- å°† MoEEdit æ‰©å±•åˆ° **å¤šè·³æ¨ç†** å’Œ **å› æœçŸ¥è¯†å›¾è°±æ›´æ–°** åœºæ™¯ã€‚
- ç ”ç©¶ **æŒç»­ç¼–è¾‘ï¼ˆlifelong editingï¼‰ä¸‹çš„ç¨³å®šæ€§ç´¯ç§¯æ•ˆåº”**ã€‚

---

> ğŸ”— **ä»£ç åœ°å€**ï¼š[https://github.com/Terence-Gu/MoEEdit](https://github.com/Terence-Gu/MoEEdit)

</details>

---

### 4. [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)

**Authors**: Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo, Tianyi Wang, Yiheng Feng, Samuel Labi, Sikai Chen  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.10458v1  

#### Abstract
Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offeri...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFound-RL: foundation model-enhanced reinforcement learning for autonomous driving

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å¼ºåŒ–å­¦ä¹ ï¼ˆ**Reinforcement Learning, RL**ï¼‰åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼ˆ**autonomous driving, AD**ï¼‰ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š

- **æ ·æœ¬æ•ˆç‡ä½**ï¼ˆsample inefficiencyï¼‰ï¼šä¼ ç»Ÿ RL ä¾èµ–å¤§é‡è¯•é”™æ¢ç´¢ï¼Œè®­ç»ƒç¼“æ…¢ä¸”ä¸ç¨³å®šã€‚
- **è¯­ä¹‰å¯è§£é‡Šæ€§å·®**ï¼šç¼ºä¹å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„é«˜å±‚è¯­ä¹‰ç†è§£ï¼Œéš¾ä»¥å¤„ç†å®‰å…¨å…³é”®ä»»åŠ¡ã€‚

åŒæ—¶ï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ**Vision-Language Models, VLMs**ï¼‰å…·å¤‡å¼ºå¤§çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶é«˜è®¡ç®—å¼€é”€å¯¼è‡´éš¾ä»¥é›†æˆåˆ°é«˜é¢‘çš„ RL è®­ç»ƒå¾ªç¯ä¸­ï¼Œé€ æˆä¸¥é‡çš„**æ¨ç†å»¶è¿Ÿç“¶é¢ˆ**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡º **Found-RL** â€”â€”ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ã€èåˆåŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰ä¸ RL çš„ç»Ÿä¸€å¹³å°ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰å¼‚æ­¥æ‰¹é‡æ¨ç†æ¡†æ¶ï¼ˆAsynchronous Batch Inference Frameworkï¼‰
- å°† VLM çš„é‡è´Ÿè½½æ¨ç†è¿‡ç¨‹ä»ä»¿çœŸä¸»å¾ªç¯ä¸­è§£è€¦ã€‚
- é€šè¿‡å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œå°†å¤šä¸ªç¯å¢ƒçš„è§‚æµ‹æ‰“åŒ…æˆå¾®æ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œæ¨ç†ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿã€‚
- æ”¯æŒå®æ—¶æˆ–è¿‘å®æ—¶çš„ RL è®­ç»ƒï¼Œå³ä½¿ä½¿ç”¨è®¡ç®—å¯†é›†å‹ VLMã€‚

#### ï¼ˆ2ï¼‰VLM åŠ¨ä½œå¼•å¯¼æœºåˆ¶ï¼ˆVLM Action Guidanceï¼‰
- **Value-Margin Regularization (VMR)**ï¼šé¼“åŠ± critic å¯¹ VLM æ¨èåŠ¨ä½œèµ‹äºˆæ›´é«˜ Q å€¼ï¼Œæå‡ç­–ç•¥å‘ä¸“å®¶è¡Œä¸ºé æ‹¢ã€‚
- **Advantage-Weighted Action Guidance (AWAG)**ï¼šä»…å½“ VLM åŠ¨ä½œä¼°è®¡ä¼˜äºå½“å‰ç­–ç•¥æ—¶æ‰æ¨¡ä»¿ï¼Œå®ç°è‡ªé€‚åº”è¡Œä¸ºå…‹éš†ã€‚

#### ï¼ˆ3ï¼‰åŸºäº CLIP çš„å¥–åŠ±å¡‘å½¢ï¼ˆCLIP-based Reward Shapingï¼‰
- å¼•å…¥ **Conditional Contrastive Action Alignment**ï¼š
  - å°†è½¦é€Ÿå’Œå¯¼èˆªæŒ‡ä»¤ç¦»æ•£åŒ–ä½œä¸ºä¸Šä¸‹æ–‡æ¡ä»¶ã€‚
  - æ„å»ºå°è§„æ¨¡ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„åŠ¨ä½œé”šç‚¹é›†ï¼ˆaction anchorsï¼‰ï¼Œé¿å…å…¨å±€åˆ†ç±»çš„æ¦‚ç‡ç¨€é‡Šã€‚
  - åˆ©ç”¨ CLIP å›¾åƒ-æ–‡æœ¬åŒ¹é…å¾—åˆ†ï¼Œç”Ÿæˆå½’ä¸€åŒ–çš„ã€åŸºäºè¾¹ç•Œçš„å¥–åŠ± bonusã€‚
- æœ‰æ•ˆç¼“è§£ CLIP åœ¨åŠ¨æ€çŠ¶æ€æ„ŸçŸ¥ä¸Šçš„â€œåŠ¨æ€å¤±æ˜â€é—®é¢˜ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | Found-RL çš„ä¼˜åŠ¿ |
|------|----------------|
| **æ•ˆç‡** | å¼‚æ­¥æ‰¹å¤„ç†ä½¿ VLM åé¦ˆä¸é˜»å¡ä»¿çœŸï¼Œæ”¯æŒé«˜é¢‘ç‡è®­ç»ƒ |
| **æ€§èƒ½** | è½»é‡çº§ RL æ¨¡å‹ï¼ˆä»… 3.82M å‚æ•°ï¼‰è¾¾åˆ°ç”šè‡³è¶…è¶Šåäº¿å‚æ•° VLM çš„é©¾é©¶è¡¨ç° |
| **å®æ—¶æ€§** | æ¨ç†é€Ÿåº¦è¾¾ ~500 FPSï¼Œè¿œè¶… VLMï¼ˆé€šå¸¸ <1 FPSï¼‰ï¼Œé€‚åˆéƒ¨ç½² |
| **é€šç”¨æ€§** | æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§ VLM å’Œ RL ç®—æ³•ç»„åˆ |
| **å®‰å…¨æ€§** | æ˜¾è‘—å‡å°‘ç¢°æ’ä¸çº¢ç¯è¿è§„ï¼Œæå‡é©¾é©¶åˆè§„æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä¸ä»¿çœŸç¯å¢ƒ

- **ä»¿çœŸå¹³å°**ï¼šåŸºäº **CARLA** æ­å»ºå¤šåœºæ™¯äº¤é€šæ¨¡æ‹Ÿå™¨ã€‚
- **è®­ç»ƒåœ°å›¾**ï¼š
  - **Leaderboard benchmark**ï¼šTown01, Town03, Town04, Town06
  - **NoCrash benchmark**ï¼šTown01ï¼ˆè®­ç»ƒï¼‰ã€Town02ï¼ˆæµ‹è¯•ï¼‰
- **æ•°æ®æ¥æº**ï¼šæ”¶é›†çº¦ **137ä¸‡æ¡**çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œæ¥è‡ª Roach PPO å’Œ autopilot roaming ç­–ç•¥ã€‚

---

### ğŸ§ª å®éªŒè®¾ç½®

#### è§‚æµ‹ç©ºé—´ï¼ˆObservation Spaceï¼‰
| ç±»å‹ | è¾“å…¥å†…å®¹ |
|------|--------|
| **VLM-based agents** | BEV å›¾åƒ (192Ã—192Ã—3) + æ–‡æœ¬æç¤º |
| **RL-based agents** | BEV masks (96Ã—96Ã—15) + ç»“æ„åŒ–çŠ¶æ€å‘é‡ï¼ˆé€Ÿåº¦ã€æŒ‡ä»¤ã€äº¤é€šç¯ç­‰ï¼‰ |

#### åŠ¨ä½œç©ºé—´
- è¿ç»­æ§åˆ¶ï¼š`[throttle/brake, steer]`ï¼ˆRLï¼‰æˆ– `[throttle, steer, brake]`ï¼ˆVLMï¼‰

#### å¥–åŠ±å‡½æ•°
- é‡‡ç”¨ ROACH é£æ ¼å¥–åŠ±ï¼šé€Ÿåº¦è·Ÿè¸ªã€è·¯çº¿ä¿æŒã€èˆªå‘å¯¹é½ã€å¹³æ»‘æ€§æƒ©ç½šã€ç»ˆç«¯å¥–æƒ©ã€‚
- åŠ å…¥ **VLM-derived reward bonus**ï¼ˆæ¥è‡ª CLIP å¯¹é½å¾—åˆ†ï¼‰ã€‚

#### ç»ˆæ­¢æ¡ä»¶
- æ›´ä¸¥æ ¼çš„ç»ˆæ­¢é€»è¾‘ï¼šåç¦»è·¯å¾„ã€é—¯çº¢ç¯/åœè½¦æ ‡å¿—ã€ç¢°æ’å³ç»ˆæ­¢ï¼ˆæ›´è´´è¿‘çœŸå®å®‰å…¨è¦æ±‚ï¼‰ã€‚

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

åˆ†ä¸ºå››å¤§ç±»ï¼š

| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| **Comprehensive** | Return, Driving Score, Infraction Penalty |
| **Route** | Success Rate, Route Completion, Speed |
| **Energy** | Icellï¼ˆç”µæ± ç”µæµä¼°è®¡ï¼‰, Fuel Rate |
| **Safety** | Collisions (Ped./Veh.), Red Light Violations |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

#### ï¼ˆ1ï¼‰RL åŸºçº¿
- **TD3**, **SAC**, **DrQv2**

#### ï¼ˆ2ï¼‰VLM åŸºçº¿ï¼ˆç›´æ¥ä½œä¸ºç­–ç•¥ï¼‰
- **InternVL3-1B/2B**, **Qwen2.5-vl-3B/7B**, **Visual RWKV-0.1B**

#### ï¼ˆ3ï¼‰å…¶ä»– AD æ–¹æ³•
- **TransFuser**, **LAV**, **TCP**ï¼ˆRGB/LiDAR è¾“å…¥ï¼‰
- **BC**, **AWAC**, **CQL**, **IQL**ï¼ˆåŸºäº BEV çš„ IL / offline RLï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆLeaderboard Benchmarkï¼‰

| æ–¹æ³• | Driving Score â†‘ | Success Rate â†‘ | Red Light â†“ | Icell â†“ |
|------|------------------|----------------|------------|---------|
| **DrQv2-CLIP (Ours)** | **0.77** | **0.57** | **0.01** | 0.13 |
| DrQv2-VMR | 0.72 | 0.60 | 0.02 | 0.09 |
| Vanilla DrQv2 | 0.56 | 0.38 | 0.02 | 0.09 |
| Qwen2.5-vl-7B | 0.76 | 0.65 | 0.05 | 0.04 |
| InternVL3-1B | 0.63 | 0.49 | 0.07 | 0.04 |

> âœ… **Found-RL çš„ DrQv2-CLIP åœ¨é©¾é©¶å¾—åˆ†ä¸Šè¶…è¶Šæ‰€æœ‰ VLM åŸºçº¿ï¼Œå¹¶å®ç°æœ€ä½çº¢ç¯è¿è§„ç‡**

---

### ğŸ”„ NoCrash Benchmark æ€§èƒ½ï¼ˆè·¨åŸé•‡æ³›åŒ–ï¼‰

| æ–¹æ³• | Town01 Driving Score | Town02 Driving Score | Town02 Success Rate |
|------|------------------------|------------------------|-----------------------|
| **DrQv2-CLIP** | 0.75 | **0.76** | **0.71** |
| SAC-VMR | 0.71 | 0.61 | 0.47 |
| Vanilla SAC | 0.24 | 0.24 | 0.07 |

> âœ… **VLM å¢å¼ºæ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨æœªè§åŸé•‡ä¸­ SAC ä»å¤±è´¥æ¢å¤è‡³ç¨³å¥æ°´å¹³**

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰VLM æŒ‡å¯¼æœ‰æ•ˆæ€§ï¼ˆTable 1 & Fig. 6ï¼‰
- æ‰€æœ‰å¼•å…¥ VLM æŒ‡å¯¼çš„æ–¹æ³•å‡åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µåŠ é€Ÿæ”¶æ•›ã€‚
- **DrQv2-CLIP** è¾¾åˆ°æœ€é«˜æ¸è¿‘æ€§èƒ½ï¼Œæ–¹å·®æ›´ä½ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒæŒ‡å¯¼æœºåˆ¶æ¯”è¾ƒ
- **VMR**ï¼šåˆæœŸå¿«é€Ÿæå‡ï¼Œä½†éœ€è¡°å‡æ­£åˆ™ç³»æ•°é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **AWAG**ï¼šActor loss æ›´ç¨³å®šï¼Œé€‚åˆé•¿æœŸå­¦ä¹ ã€‚

#### ï¼ˆ3ï¼‰CLIP å¥–åŠ±æœ‰æ•ˆæ€§ï¼ˆFig. 10ï¼‰
- å¥–åŠ±å¯ç”¨æ€§æ¥è¿‘ 100%ï¼ŒéªŒè¯é«˜ååè®¾è®¡æˆåŠŸã€‚
- å¹³å‡ CLIP å¯¹é½ margin å¿«é€Ÿä¸Šå‡åè¶‹äºå¹³ç¨³ï¼Œè¡¨æ˜ç­–ç•¥å­¦ä¼šç»´æŒé«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **è¯­ä¹‰ç›‘ç£å¯æå¤§æå‡ RL æ•ˆç‡ä¸å®‰å…¨æ€§**  
   VLM æä¾›çš„é«˜å±‚è¯­ä¹‰åé¦ˆï¼ˆåŠ¨ä½œå»ºè®® + å¯†é›†å¥–åŠ±ï¼‰æ˜¾è‘—åŠ é€Ÿå­¦ä¹ ï¼Œå‡å°‘å±é™©è¡Œä¸ºã€‚

2. **è½»é‡ RL æ¨¡å‹å¯åª²ç¾å¤§è§„æ¨¡ VLM è¡¨ç°**  
   ä»… **3.82M å‚æ•°** çš„ RL æ¨¡å‹ï¼Œåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¿‡ **1Bâ€“7B å‚æ•° VLM**ï¼Œå®ç°â€œè’¸é¦å¼å¢å¼ºâ€ã€‚

3. **å¼‚æ­¥æ‰¹å¤„ç†æ˜¯å·¥ç¨‹å¯è¡Œçš„å…³é”®**  
   æˆåŠŸè§£å†³ VLM æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œä½¿é—­ç¯è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚

4. **å‹ç¼©è¾“å…¥ä»èƒ½ä¿æŒé«˜æ€§èƒ½**  
   ä½¿ç”¨ 96Ã—96 BEV masksï¼ˆä»…ä¸º VLM è¾“å…¥çš„ 25%ï¼‰å³å¯å–å¾—ä¼˜å¼‚ç»“æœï¼Œåˆ©äºè¾¹ç¼˜éƒ¨ç½²ã€‚

5. **è§„åˆ™éµå®ˆä¼˜äºæ¿€è¿›é©¾é©¶**  
   å°½ç®¡è¾“å…¥ç»´åº¦æ›´ä½ï¼ŒFound-RL åœ¨çº¢ç¯è¿è§„ç­‰é™æ€è§„åˆ™ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä½“ç°æ›´å¼ºçš„åˆè§„æ€§ã€‚

---

### âš ï¸ å±€é™æ€§

1. **è§†è§‰åˆ†è¾¨ç‡é™åˆ¶å½±å“åŠ¨æ€ç‰©ä½“å¤„ç†**  
   96Ã—96 è¾“å…¥å¯èƒ½å¯¼è‡´å¯¹è¿œå¤„è¡Œäººæˆ–é«˜é€Ÿè½¦è¾†è¯†åˆ«ä¸è¶³ï¼Œéƒ¨åˆ†è§£é‡Šäº†è¾ƒé«˜çš„è½¦è¾†ç¢°æ’ç‡ã€‚

2. **ä¾èµ–é«˜è´¨é‡ VLM å¾®è°ƒæ•°æ®**  
   å½“å‰æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡ä¸“å®¶è½¨è¿¹è¿›è¡Œ VLM å¾®è°ƒï¼Œè‹¥æ•°æ®åå·®å¤§å¯èƒ½ä¼ é€’é”™è¯¯å…ˆéªŒã€‚

3. **CLIP å¯¹è¿ç»­åŠ¨ä½œç¦»æ•£åŒ–æ•æ„Ÿ**  
   åŠ¨ä½œç©ºé—´åˆ’åˆ†ä¾èµ–äººå·¥è®¾å®šé˜ˆå€¼ï¼Œå¯èƒ½æŸå¤±ç»†ç²’åº¦æ§åˆ¶èƒ½åŠ›ã€‚

4. **åé¦ˆå»¶è¿Ÿé£é™©**  
   è‹¥æ‰©å±•è‡³æ›´å¤§æ¨¡å‹ï¼Œå¼‚æ­¥åé¦ˆå¯èƒ½å‡ºç°â€œè¿‡æœŸæŒ‡å¯¼â€ï¼Œå½±å“ç¨³å®šæ€§ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†å»¶è¿Ÿ**  
   æ¢ç´¢æ¨¡å‹é‡åŒ–ã€è’¸é¦ã€ç¡¬ä»¶åŠ é€Ÿä»¥æ”¯æŒæ›´å¤§è§„æ¨¡ VLM å®æ—¶åé¦ˆã€‚

2. **å¤š VLM ååŒæŒ‡å¯¼**  
   è®¾è®¡å¤šä¸“å®¶ç³»ç»Ÿï¼Œä¸åŒ VLM åˆ†åˆ«è´Ÿè´£å®‰å…¨ã€èˆ’é€‚ã€æ•ˆç‡ç­‰å­ç›®æ ‡ã€‚

3. **æ‰©å±•è‡³ CARLA Leaderboard 2.0**  
   åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å®˜æ–¹æ’è¡Œæ¦œä¸ŠéªŒè¯é²æ£’æ€§ã€‚

4. **ç«¯åˆ°ç«¯è½»é‡ VLM ç­–ç•¥**  
   æ¢ç´¢å°†å°å‹ VLM ç›´æ¥ä½œä¸º policy backboneï¼Œå…¼é¡¾è¯­ä¹‰ç†è§£ä¸å®æ—¶æ€§ã€‚

5. **çœŸå®ä¸–ç•Œè¿ç§»éªŒè¯**  
   æ¨åŠ¨æ–¹æ³•å‘å®è½¦å¹³å°è¿ç§»ï¼Œç»“åˆåŸŸè‡ªé€‚åº”æŠ€æœ¯ç¼©å°ä»¿çœŸå·®è·ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **Found-RL è¯æ˜äº†â€œå°æ¨¡å‹ + å¤§è„‘ï¼ˆVLMï¼‰â€çš„èŒƒå¼å¯è¡Œæ€§â€”â€”é€šè¿‡å¼‚æ­¥è¯­ä¹‰å¢å¼ºï¼Œè®©è½»é‡ RL åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å®ç°é«˜æ•ˆã€å®‰å…¨ã€è¿‘ä¼¼äººç±»æ°´å¹³çš„å†³ç­–èƒ½åŠ›ã€‚**

</details>

---

### 5. [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)

**Authors**: Youhe Jiang, Fangcheng Fu, Eiko Yoneki  
**Category**: cs.DC  
**Published**: 2026-02-12  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.10729v1  

#### Abstract
The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**BOUTE: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ç³»ç»Ÿé¢ä¸´é«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨æ»¡è¶³ä¸¥æ ¼å»¶è¿Ÿï¼ˆlatencyï¼‰å’Œå“åº”è´¨é‡ï¼ˆresponse qualityï¼‰è¦æ±‚çš„å‰æä¸‹ã€‚ç°æœ‰ç ”ç©¶é€šå¸¸ä»ä¸¤ä¸ªç‹¬ç«‹è§’åº¦ä¼˜åŒ–ï¼š
- **ç®—æ³•å±‚é¢**ï¼šé€šè¿‡ **Heterogeneous Model Routing**ï¼ˆå¼‚æ„æ¨¡å‹è·¯ç”±ï¼‰ï¼Œå°†ç®€å•æŸ¥è¯¢è·¯ç”±åˆ°å°æ¨¡å‹ï¼Œå¤æ‚æŸ¥è¯¢è·¯ç”±åˆ°å¤§æ¨¡å‹ã€‚
- **ç³»ç»Ÿå±‚é¢**ï¼šé€šè¿‡ **Heterogeneous Model Deployment**ï¼ˆå¼‚æ„æ¨¡å‹éƒ¨ç½²ï¼‰ï¼Œåˆ©ç”¨ä¸åŒç±»å‹çš„ GPUï¼ˆå¦‚ H100 å’Œ RTX 5090ï¼‰æ„å»ºæ›´å…·æ€§ä»·æ¯”çš„æœåŠ¡æ¶æ„ã€‚

ç„¶è€Œï¼Œè¿™ä¸¤è€…ä¹‹é—´å­˜åœ¨å¼ºè€¦åˆå…³ç³»ï¼š
- è·¯ç”±ç­–ç•¥å½±å“å„æ¨¡å‹çš„è´Ÿè½½åˆ†å¸ƒï¼Œè¿›è€Œå†³å®šæœ€ä¼˜éƒ¨ç½²ï¼›
- éƒ¨ç½²é…ç½®åˆå†³å®šäº†æ¯ä¸ªæ¨¡å‹çš„å®é™…å»¶è¿Ÿè¡¨ç°ï¼Œåè¿‡æ¥å½±å“è·¯ç”±å†³ç­–ã€‚

å› æ­¤ï¼Œ**å­¤ç«‹åœ°ä¼˜åŒ–è·¯ç”±æˆ–éƒ¨ç½²ä¼šå¯¼è‡´æ¬¡ä¼˜è§£**ã€‚æœ¬æ–‡æå‡ºåº”è¿›è¡Œ **ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡ï¼ˆalgorithm-system co-designï¼‰** æ¥å®ç°çœŸæ­£çš„æˆæœ¬æ•ˆç›Šæœ€å¤§åŒ–ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡ºäº† **BOUTE** â€”â€” ä¸€ä¸ªåŸºäº **Multi-Objective Bayesian Optimization (MOBO)** çš„è´¨é‡æ„ŸçŸ¥è°ƒåº¦ç³»ç»Ÿï¼Œç”¨äºè”åˆä¼˜åŒ–å¼‚æ„æ¨¡å‹è·¯ç”±ä¸éƒ¨ç½²ã€‚

#### ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼š

- **Contribution 1ï¼šæ­ç¤ºäº†å¼‚æ„æ¨¡å‹è·¯ç”±ä¸å¼‚æ„éƒ¨ç½²ä¹‹é—´çš„ååŒæ•ˆåº”**
  - å®éªŒè¡¨æ˜ï¼Œå°†èµ„æºéœ€æ±‚ä¸åŒçš„æ¨¡å‹åŒ¹é…åˆ°åˆé€‚çš„ GPU ç±»å‹ï¼ˆä¾‹å¦‚ï¼šå°æ¨¡å‹ç”¨ RTX 5090ï¼Œå¤§æ¨¡å‹ç”¨ H100ï¼‰å¯æ˜¾è‘—æå‡æ•´ä½“æ•ˆç‡ã€‚
  - è¿™ç§ç¡¬ä»¶-æ¨¡å‹é€‚é…åˆ›é€ äº†â€œåŒèµ¢â€å±€é¢ï¼šæ—¢é™ä½äº†å°æ¨¡å‹æœåŠ¡æˆæœ¬ï¼Œåˆèƒ½é‡Šæ”¾é¢„ç®—å¢å¼ºå¤§æ¨¡å‹æœåŠ¡èƒ½åŠ›ã€‚

- **Contribution 2ï¼šå½¢å¼åŒ–ä¸ºå¸¦çº¦æŸçš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥ MOBO æ¡†æ¶æ±‚è§£**
  - å°†è·¯ç”±é˜ˆå€¼ï¼ˆTï¼‰ã€GPU åˆ†é…çŸ©é˜µï¼ˆAï¼‰å’Œå¹³è¡Œç­–ç•¥ï¼ˆPï¼‰ä½œä¸ºè”åˆå†³ç­–å˜é‡ã€‚
  - ç›®æ ‡å‡½æ•°ä¸ºæœ€å°åŒ–ç³»ç»Ÿå»¶è¿Ÿ $L$ å¹¶æœ€å¤§åŒ–å“åº”è´¨é‡ $Q$ï¼ŒåŒæ—¶æ»¡è¶³é¢„ç®—å’Œèµ„æºé™åˆ¶ã€‚
  - è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼š
    - **Offline Preparation Phase**ï¼šé¢„å…ˆå»ºç«‹æ€§èƒ½æ•°æ®åº“ï¼Œæ¨¡æ‹Ÿå„ç§ `(deployment, workload)` ç»„åˆä¸‹çš„å»¶è¿Ÿã€‚
    - **Online Optimization Phase**ï¼šä½¿ç”¨ MOBO å¿«é€Ÿæœç´¢å¸•ç´¯æ‰˜æœ€ä¼˜è§£é›†ï¼ˆPareto-optimal solutionsï¼‰ã€‚

- **Contribution 3ï¼šå¼•å…¥å¤šç§ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†ä»¥åŠ é€Ÿæ”¶æ•›**
  - **Load-fraction encoding**ï¼šå°†è·¯ç”±é˜ˆå€¼é‡å‚æ•°åŒ–ä¸ºè´Ÿè½½æ¯”ä¾‹ï¼Œé¿å…å› è¯„åˆ†åˆ†å¸ƒä¸å‡å¯¼è‡´çš„ä¼˜åŒ–éœ‡è¡ã€‚
  - **Model-GPU preference encoding**ï¼šé€šè¿‡åå¥½æƒé‡æ ¸ï¼ˆPreferenceWeightedKernelï¼‰å¼•å¯¼æœç´¢åå‘å·²çŸ¥é«˜æ•ˆçš„æ¨¡å‹-GPU åŒ¹é…ç»„åˆã€‚
  - **Constrained qNEHVI acquisition function**ï¼šæ”¯æŒåœ¨ä¼˜åŒ–ä¸­ç›´æ¥å¤„ç†å»¶è¿Ÿå’Œè´¨é‡çš„ç¡¬æ€§çº¦æŸã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | å±€é™æ€§ | BOUTE çš„æ”¹è¿› |
|------|--------|-------------|
| **Standalone LLM serving** | å•ä¸€æ¨¡å‹æ— æ³•åŠ¨æ€é€‚åº”æŸ¥è¯¢éš¾åº¦ï¼Œè¦ä¹ˆé«˜æˆæœ¬ï¼Œè¦ä¹ˆä½è´¨é‡ | åŠ¨æ€è·¯ç”± + å¤šæ¨¡å‹åä½œ |
| **RouteLLM / HybridLLM** | åªå…³æ³¨è·¯ç”±é€»è¾‘ï¼Œé‡‡ç”¨å‡åŒ€èµ„æºé…ç½®ï¼Œå¿½ç•¥æ¨¡å‹å¯¹ GPU çš„åå¥½ | è”åˆä¼˜åŒ–éƒ¨ç½²ï¼Œå®ç°èµ„æºç²¾å‡†åˆ†é… |
| **ThunderServe / Helix** | ä»…å…³æ³¨å¼‚æ„éƒ¨ç½²ï¼Œæœªç»“åˆè·¯ç”±æœºåˆ¶ | å®ç°ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡ï¼Œå‘æŒ¥åŒé‡å¼‚æ„ä¼˜åŠ¿ |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼šé¦–æ¬¡å®ç°äº† **routing ä¸ deployment çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼Œè§£å†³äº†äºŒè€…é—´çš„å¾ªç¯ä¾èµ–éš¾é¢˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä¸å·¥ä½œè´Ÿè½½
- **GSM8K**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰
- **MTBench**ï¼šå¤šè½®å¯¹è¯åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç»¼åˆå¾—åˆ†ï¼ˆscoreï¼‰
- å·¥ä½œè´Ÿè½½ trace é‡‡æ ·è‡ªçœŸå®ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼ŒåŒ…å«è¾“å…¥/è¾“å‡ºé•¿åº¦åˆ†å¸ƒç­‰ç»Ÿè®¡ç‰¹å¾

### âš™ï¸ å®éªŒç¯å¢ƒ
- **GPU ç±»å‹**ï¼š
  - NVIDIA H100-80G ($2.64â€“3.07/h)
  - RTX PRO 6000-96G ($1.84â€“2.09/h)
  - RTX 5090-32G ($0.89/h)
  - RTX 4090-24G ($0.59/h)
- **äº’è”å¸¦å®½**ï¼š
  - H100ï¼šNVLinkï¼ˆ300 GB/sï¼‰
  - RTX ç³»åˆ—ï¼šPCIeï¼ˆ60 GB/sï¼‰
- **æ€»é¢„ç®—**ï¼šå›ºå®šä¸º $30/h çš„æˆæœ¬ä¸Šé™

### ğŸ§ª æ¨¡å‹ä¸ç»„ä»¶
- **æ¨¡å‹ç±»å‹**ï¼š
  - Llama3.1-8Bï¼ˆsmall modelï¼‰
  - Llama3.1-70Bï¼ˆlarge modelï¼‰
- **Router**ï¼šåŸºäº RouteLLM æ„å»ºï¼Œä½¿ç”¨ GPT-4-as-a-judge å’Œ MMLU æ•°æ®è®­ç»ƒ
- **Parallelism æ”¯æŒ**ï¼šData Parallelism (DP), Tensor Parallelism (TP), Pipeline Parallelism (PP)

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- **å»¶è¿ŸæŒ‡æ ‡**ï¼šAverage Latency, P95, P99, P100
- **ååé‡**ï¼šThroughput (req/s)
- **æœåŠ¡è´¨é‡**ï¼šAggregated Accuracy (GSM8K), Aggregated Score (MTBench)
- **æˆæœ¬æ•ˆç‡**ï¼šè¾¾åˆ°ç›¸åŒ QoS æ‰€éœ€çš„æ¯å°æ—¶è´¹ç”¨ï¼ˆ$/hï¼‰

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ | æè¿° |
|------|------|
| **Stand-Alone-Homo** | ä½¿ç”¨ vLLM åœ¨åŒæ„ H100 ä¸Šè¿è¡Œ Llama3.1-70B |
| **Stand-Alone-Hetero** | ä½¿ç”¨ vLLM åœ¨æ··åˆ GPU ä¸Šè¿è¡Œ Llama3.1-70B |
| **RouteLLM** | ä½¿ç”¨ RouteLLM è·¯ç”±ï¼Œåœ¨å„ç±» GPU ä¸Šå‡åŒ€åˆ†é…èµ„æº |
| **BOUTE-Homo** | BOUTE çš„å˜ä½“ï¼Œä»…åœ¨åŒæ„ GPU ä¸Šè¿è¡Œï¼ˆç”¨äºæ¶ˆèåˆ†æï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ€»ä½“æ€§èƒ½æå‡ï¼ˆEnd-to-End Evaluationï¼‰

#### åœ¨ GSM8K ä¸Šçš„è¡¨ç°ï¼ˆè´¨é‡è¦æ±‚ï¼š86â€“90ï¼‰
| å¯¹æ¯”é¡¹ | æœ€å¤§æå‡ | å¹³å‡æå‡ |
|-------|--------|---------|
| **ç›¸æ¯” Standalone æ–¹æ³•** | ååé‡ â†‘ **90%**ï¼ŒP95 â†“ **91%** | ååé‡ â†‘ **55%**ï¼ŒP95 â†“ **57%** |
| **ç›¸æ¯” RouteLLM** | ååé‡ â†‘ **71%**ï¼ŒP95 â†“ **66%** | ååé‡ â†‘ **52%**ï¼ŒP95 â†“ **50%** |
| **ç›¸æ¯” BOUTE-Homo** | ååé‡ â†‘ **16%**ï¼ŒP95 â†“ **15%** | ååé‡ â†‘ **14%**ï¼ŒP95 â†“ **14%** |

> ğŸ’¡ è¡¨æ˜ BOUTE æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨å¼‚æ„ç¯å¢ƒä¸‹ä¼˜åŠ¿æ˜æ˜¾ã€‚

#### åœ¨ MTBench ä¸Šçš„è¡¨ç°ï¼ˆè´¨é‡è¦æ±‚ï¼š8.1â€“8.5ï¼‰
| å¯¹æ¯”é¡¹ | æœ€å¤§æå‡ | å¹³å‡æå‡ |
|-------|--------|---------|
| **ç›¸æ¯” Standalone æ–¹æ³•** | æ€§èƒ½ â†‘ **157%** | æ€§èƒ½ â†‘ **115%** |
| **ç›¸æ¯” RouteLLM** | æ€§èƒ½ â†‘ **80%** | æ€§èƒ½ â†‘ **42%** |
| **ç›¸æ¯” BOUTE-Homo** | æ€§èƒ½ â†‘ **21%** | æ€§èƒ½ â†‘ **20%** |

> âœ… å¼‚æ„éƒ¨ç½²å¸¦æ¥çš„å¢ç›Šé«˜è¾¾ **20%+**ï¼ŒéªŒè¯äº†å…¶å¿…è¦æ€§ã€‚

---

### ğŸ’° æˆæœ¬æ•ˆç‡åˆ†æï¼ˆCost Efficiencyï¼‰

| æ–¹æ³• | è¾¾åˆ°ç‰¹å®š QoS æ‰€éœ€æˆæœ¬ï¼ˆå¹³å‡ï¼‰ |
|------|----------------------------|
| Stand-Alone-Homo | $61.83/h |
| RouteLLM | $53.55/h |
| BOUTE-Homo | $38.41/h |
| **BOUTE** | **$32.25/h** |

> âœ… BOUTE åœ¨ä¿æŒç›¸åŒå»¶è¿Ÿå’Œè´¨é‡ç›®æ ‡ä¸‹ï¼š
- ç›¸æ¯” standalone æ–¹æ³•èŠ‚çœ **çº¦ 48%** æˆæœ¬
- ç›¸æ¯” RouteLLM èŠ‚çœ **38%** æˆæœ¬
- ç›¸æ¯” BOUTE-Homo å†é™ **15%** æˆæœ¬

> ğŸ“Œ **ç»“è®º**ï¼šBOUTE å¯å®ç° **15%â€“61% çš„æˆæœ¬é™ä½**ï¼ˆå¹³å‡ 38%ï¼‰ï¼Œæ˜¯æå…·ç»æµæ•ˆç›Šçš„ LLM serving æ–¹æ¡ˆã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

| è®¾ç½® | è°ƒåº¦æ—¶é—´ï¼ˆ3 GPU typesï¼‰ | ç›¸å¯¹æ€§èƒ½ä¸‹é™ |
|------|------------------------|--------------|
| **å®Œæ•´ BOUTE** | **23.5 ç§’** | åŸºå‡† |
| BOUTE (w/o structural info) | 3.6 åˆ†é’Ÿï¼ˆâ†‘ 9Ã—ï¼‰ | æ˜æ˜¾åŠ£åŒ–ï¼Œå¸¸é™·å…¥å±€éƒ¨æœ€ä¼˜ |
| BOUTE (w/o offline prep) | ~12 åˆ†é’Ÿ | å› æ¯æ¬¡éœ€é‡æ–°ä»¿çœŸï¼Œå¼€é”€å·¨å¤§ |

> âœ… **å…³é”®å‘ç°**ï¼š
- ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚ load-fraction encodingã€preference kernelï¼‰æå¤§æå‡äº† **æ ·æœ¬æ•ˆç‡** å’Œ **æ”¶æ•›é€Ÿåº¦**ã€‚
- ç¦»çº¿å‡†å¤‡é˜¶æ®µï¼ˆoffline preparationï¼‰å¯¹äºå®æ—¶æ€§è‡³å…³é‡è¦ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å¼‚æ„æ¨¡å‹è·¯ç”±ä¸å¼‚æ„éƒ¨ç½²å¤©ç„¶äº’è¡¥**  
   ä¸åŒæ¨¡å‹å¯¹ GPU ç‰¹æ€§çš„åå¥½å·®å¼‚æ˜¾è‘—ï¼ˆå¦‚ Llama3.1-8B åœ¨ RTX 5090 ä¸Šå¿« 1.5Ã—ï¼Œè€Œ Llama3.1-70B åœ¨ H100 ä¸Šå¿« 2Ã—ï¼‰ã€‚åˆç†åŒ¹é…å¯å¤§å¹…æå‡æ€§ä»·æ¯”ã€‚

2. **å¿…é¡»è¿›è¡Œè”åˆä¼˜åŒ–è€Œéåˆ†æ­¥ä¼˜åŒ–**  
   è·¯ç”±ä¸éƒ¨ç½²ç›¸äº’ä¾èµ–ï¼Œå•ç‹¬ä¼˜åŒ–ä»»ä¸€æ–¹éƒ½ä¼šç ´åå…¨å±€æœ€ä¼˜æ€§ã€‚MOBO æ¡†æ¶æœ‰æ•ˆè§£å†³äº†è¿™ä¸€å¾ªç¯ä¾èµ–é—®é¢˜ã€‚

3. **ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†æ˜¾è‘—æå‡ä¼˜åŒ–æ•ˆç‡**  
   åŠ å…¥ load-fraction ç¼–ç ã€preference-aware kernel ç­‰è®¾è®¡åï¼ŒMOBO æ”¶æ•›æ›´å¿«ä¸”æ›´ç¨³å®šã€‚

4. **BOUTE å®ç°äº†å¸•ç´¯æ‰˜å‰æ²¿ä¸Šçš„æ”¯é…åœ°ä½**  
   åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­ï¼ŒBOUTE åœ¨å»¶è¿Ÿ-è´¨é‡å¹³é¢ä¸Šå‡è¡¨ç°å‡º **Pareto dominance**ï¼Œå³åœ¨åŒç­‰è´¨é‡ä¸‹å»¶è¿Ÿæ›´ä½ï¼Œæˆ–åœ¨åŒç­‰å»¶è¿Ÿä¸‹è´¨é‡æ›´é«˜ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ç¦»çº¿å»ºæ¨¡ä¾èµ–å‡†ç¡®çš„æ€§èƒ½é¢„æµ‹å™¨**  
   è‹¥ simulator è¯¯å·®è¾ƒå¤§ï¼ˆ>10%ï¼‰ï¼Œå¯èƒ½å¯¼è‡´éƒ¨ç½²é€‰æ‹©åå·®ã€‚è™½ç„¶æ–‡ä¸­æŠ¥å‘Šè¯¯å·®åœ¨ 2%-7%ï¼Œä½†ä»å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚

2. **æ‰©å±•æ€§å—é™äºå€™é€‰ç©ºé—´å¢é•¿**  
   å½“æ¨¡å‹ç§ç±»ã€GPU ç±»å‹æˆ–å¹¶è¡Œç­–ç•¥å¢å¤šæ—¶ï¼Œå€™é€‰éƒ¨ç½²æ•°é‡å‘ˆæŒ‡æ•°çº§ä¸Šå‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥å‰ªææˆ–è¿‘ä¼¼ã€‚

3. **å‡è®¾é™æ€å·¥ä½œè´Ÿè½½åˆ†å¸ƒ**  
   å½“å‰ä¼˜åŒ–åŸºäºç¨³æ€è´Ÿè½½å‡è®¾ï¼Œè‹¥æŸ¥è¯¢å¤æ‚åº¦åˆ†å¸ƒå‰§çƒˆå˜åŒ–ï¼ˆdriftï¼‰ï¼Œå¯èƒ½éœ€è¦å‘¨æœŸæ€§é‡ä¼˜åŒ–ã€‚

4. **æœªè€ƒè™‘å†·å¯åŠ¨ä¸å®¹é”™æœºåˆ¶**  
   å®é™…ç”Ÿäº§ç¯å¢ƒä¸­è¿˜éœ€å¤„ç†æ¨¡å‹åŠ è½½å»¶è¿Ÿã€èŠ‚ç‚¹æ•…éšœç­‰é—®é¢˜ï¼Œå½“å‰æ¡†æ¶å°šæœªæ¶µç›–ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **åœ¨çº¿è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶**  
   å¼•å…¥åé¦ˆæ§åˆ¶ loopï¼Œæ ¹æ®å®æ—¶ç›‘æ§æ•°æ®åŠ¨æ€è°ƒæ•´è·¯ç”±ä¸éƒ¨ç½²ç­–ç•¥ã€‚

2. **æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹ä¸ MoE æ¶æ„**  
   å°†æ¡†æ¶æ‰©å±•è‡³æ”¯æŒç¨€ç–æ¿€æ´»æ¨¡å‹ï¼ˆå¦‚ Mixtralã€DeepSeek-MoEï¼‰ã€‚

3. **è·¨åŒºåŸŸ/å¤šäº‘éƒ¨ç½²ä¼˜åŒ–**  
   ç»“åˆ SkyServe ç­‰æ€æƒ³ï¼Œå°† BOUTE æ¨å¹¿è‡³åˆ†å¸ƒå¼äº‘ç¯å¢ƒï¼Œè¿›ä¸€æ­¥é™ä½æˆæœ¬ã€‚

4. **ç»¿è‰²è®¡ç®—å¯¼å‘ä¼˜åŒ–**  
   å°†èƒ½è€—ï¼ˆenergy consumptionï¼‰çº³å…¥å¤šç›®æ ‡ä¼˜åŒ–ç›®æ ‡ï¼Œæ¨åŠ¨å¯æŒç»­ AI å‘å±•ã€‚

---

## âœ… æ€»ç»“

| ç»´åº¦ | å†…å®¹ |
|------|------|
| **æ ¸å¿ƒæ€æƒ³** | åˆ©ç”¨ **Multi-Objective Bayesian Optimization** å®ç° **Heterogeneous Model Routing + Heterogeneous Deployment** çš„è”åˆä¼˜åŒ– |
| **å…³é”®æŠ€æœ¯** | MOBO æ¡†æ¶ + Offline Performance Database + Structural Priorsï¼ˆload-fraction, preference kernelï¼‰ |
| **æ€§èƒ½ä¼˜åŠ¿** | ååé‡æœ€é«˜æå‡ **1.9Ã—**ï¼ŒP95 å»¶è¿Ÿæœ€å¤šé™ä½ **91%**ï¼Œæˆæœ¬å¹³å‡å‡å°‘ **38%** |
| **å®é™…æ„ä¹‰** | ä¸ºå¤§è§„æ¨¡ LLM å•†ä¸šåŒ–éƒ¨ç½²æä¾›äº†é«˜æ•ˆã€ä½æˆæœ¬ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äº **LLM serving democratization** |

> ğŸ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **BOUTE æ˜¯é¦–ä¸ªå®ç° LLM è·¯ç”±ä¸éƒ¨ç½²ååŒä¼˜åŒ–çš„ç³»ç»Ÿï¼Œé€šè¿‡ MOBO æ¡†æ¶åœ¨å¼‚æ„è½¯ç¡¬ä»¶ä¸Šå®ç°äº†æè‡´çš„æˆæœ¬æ•ˆç›Šï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½è¿œè¶…å½“å‰ SOTA æ–¹æ³•ã€‚**

</details>

---

### 6. [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)

**Authors**: Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, Yada Zhu  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.10210v1  

#### Abstract
Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly contin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHow Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs?

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ç”¨äºè¯„ä¼° **Retrieval-Augmented Generation (RAG)** å’Œ **Knowledge Graph-RAG (KG-RAG)** ç³»ç»Ÿçš„åŸºå‡†æ•°æ®é›†å­˜åœ¨ä¸¥é‡çš„ **pretraining contamination** é—®é¢˜ï¼Œå³æµ‹è¯•é—®é¢˜æ‰€ä¾èµ–çš„çŸ¥è¯†å¯èƒ½å·²è¢«åŒ…å«åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒæ•°æ®ä¸­ã€‚è¿™å¯¼è‡´æ¨¡å‹å¯ä»¥é€šè¿‡å‚æ•°åŒ–è®°å¿†ï¼ˆparametric recallï¼‰è€ŒéçœŸæ­£çš„æ£€ç´¢ä¸æ¨ç†æ¥å›ç­”é—®é¢˜ï¼Œä»è€Œé«˜ä¼°äº†å…¶å®é™…èƒ½åŠ›ã€‚

æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†éš¾ä»¥æœ‰æ•ˆåŒºåˆ†æ¨¡å‹åœ¨ **multi-hop reasoning**ã€**hybrid knowledge integration**ï¼ˆç»“åˆéç»“æ„åŒ–æ–‡æœ¬ä¸ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼‰ç­‰æ–¹é¢çš„çœŸå®è¡¨ç°ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šHYBRIDRAG-BENCH
ä½œè€…æå‡ºäº† **HYBRIDRAG-BENCH** â€”â€”ä¸€ä¸ªå…¨è‡ªåŠ¨çš„åŸºå‡†æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- ä»è¿‘æœŸï¼ˆpost-2023ï¼‰çš„ **arXiv ç§‘å­¦æ–‡çŒ®** ä¸­æå–çŸ¥è¯†ï¼Œç¡®ä¿çŸ¥è¯†æ—¶æ•ˆæ€§å¹¶æœ€å¤§ç¨‹åº¦å‡å°‘ä¸ä¸»æµLLMé¢„è®­ç»ƒæ•°æ®çš„é‡å ã€‚
- æ„å»º **æ··åˆçŸ¥è¯†ç¯å¢ƒï¼ˆhybrid knowledge environmentï¼‰**ï¼ŒåŒæ—¶åŒ…å«ï¼š
  - **éç»“æ„åŒ–æ–‡æœ¬ç‰‡æ®µï¼ˆtext chunksï¼‰**
  - **ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆknowledge graphï¼‰**
- è‡ªåŠ¨ç”ŸæˆåŸºäºæ˜¾å¼æ¨ç†è·¯å¾„ï¼ˆexplicit reasoning pathsï¼‰çš„é—®ç­”å¯¹ï¼Œå¹¶ç¡®ä¿ç­”æ¡ˆå¿…é¡»é€šè¿‡æ£€ç´¢å’Œå¤šè·³æ¨ç†æ‰èƒ½è·å¾—ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ä¼˜åŠ¿ç»´åº¦ | å…·ä½“è¯´æ˜ |
|---------|--------|
| **æŠ—æ±¡æŸ“æ€§ï¼ˆContamination-awareï¼‰** | ä½¿ç”¨è¿‘æœŸ arXiv æ–‡çŒ®ä½œä¸ºçŸ¥è¯†æºï¼Œé¿å…ä¸ä¸»æµLLMï¼ˆå¦‚Qwen, LLaMAï¼‰çš„é¢„è®­ç»ƒæˆªæ­¢æ—¶é—´é‡å ï¼Œç¡®ä¿ç­”æ¡ˆæ— æ³•é€šè¿‡è®°å¿†è·å¾—ã€‚ |
| **å¯æ§æ€§ä¸å¯æ‰©å±•æ€§ï¼ˆFlexible & Scalableï¼‰** | æ”¯æŒæŒ‰é¢†åŸŸï¼ˆdomainï¼‰ã€æ—¶é—´èŒƒå›´ï¼ˆtime frameï¼‰çµæ´»å®šåˆ¶ï¼Œä¾¿äºæŒç»­æ›´æ–°å’Œå¤ç°å®éªŒã€‚ |
| **è¯Šæ–­æ€§å¼ºï¼ˆDiagnosticï¼‰** | ç”Ÿæˆçš„é—®é¢˜è¦†ç›–å¤šç§æ¨ç†ç±»å‹ï¼ˆå•è·³ã€æ¡ä»¶ã€å¤šè·³ã€åäº‹å®ã€å¼€æ”¾åˆæˆï¼‰ï¼Œå¯ç”¨äºç»†ç²’åº¦åˆ†æä¸åŒæ–¹æ³•çš„èƒ½åŠ›å·®å¼‚ã€‚ |
| **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜** | æ•´ä¸ªæµç¨‹ï¼ˆçŸ¥è¯†æŠ½å– â†’ å›¾è°±æ„å»º â†’ QAç”Ÿæˆ â†’ è´¨é‡æ§åˆ¶ï¼‰é«˜åº¦è‡ªåŠ¨åŒ–ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **HYBRIDRAG-BENCH å®ä¾‹åŒ–ä¸‰ä¸ªé¢†åŸŸå­é›†**ï¼š
  - **Arxiv-AI**: äººå·¥æ™ºèƒ½ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰
  - **Arxiv-CY**: æ²»ç†ä¸æ”¿ç­–ï¼ˆgovernance and policyï¼‰
  - **Arxiv-BIO**: ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ˆbioinformaticsï¼‰
- æ¯ä¸ªå­é›†çº¦åŒ…å« **1,000 ä¸ª QA å¯¹**ï¼Œå‡æ¥è‡ª 2023 å¹´ä»¥åçš„ arXiv è®ºæ–‡ã€‚
- é—®é¢˜ç±»å‹åˆ†å¸ƒè§ä¸‹è¡¨ï¼š

| é—®é¢˜ç±»å‹ | Arxiv-AI | Arxiv-CY | Arxiv-BIO |
|--------|--------|--------|--------|
| å•è·³ï¼ˆSingle-hopï¼‰ | 29% | 25% | 25% |
| æ¡ä»¶å•è·³ï¼ˆw. Conditionï¼‰ | 16% | 13% | 19% |
| å¤šè·³ï¼ˆRegularï¼‰ | 19% | 15% | 19% |
| éš¾å¤šè·³ï¼ˆDifficultï¼‰ | 17% | 17% | 13% |
| åäº‹å®ï¼ˆCounterfactualï¼‰ | 13% | 18% | 6% |
| å¼€æ”¾å¼ï¼ˆOpen-endedï¼‰ | 5% | 12% | 18% |

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹è§„æ¨¡**ï¼šä½¿ç”¨å¤šä¸ªä¸»æµ LLMï¼ŒåŒ…æ‹¬ï¼š
  - `DeepSeek-V3.2` (685B)
  - `Qwen 2.5` (72B)
  - `LLaMA 3.3` (70B)
  - `LLaMA 3.1` (8B)
- **è¯„ä¼°æ–¹å¼**ï¼š
  - é‡‡ç”¨ **LLM-as-a-Judge** åè®®è¿›è¡Œè‡ªåŠ¨è¯„åˆ†ï¼Œåˆ¤æ–­ç”Ÿæˆç­”æ¡ˆæ˜¯å¦å¿ å®äºè¯æ®ä¸”æ­£ç¡®ã€‚
  - æŠ¥å‘Š **å‡†ç¡®ç‡ï¼ˆAccuracy %ï¼‰**ï¼Œæ¯ç»„å®éªŒè¿è¡Œ 5 æ¬¡å–å¹³å‡å€¼ä¸æ ‡å‡†å·®ã€‚
- **æ—¶é—´æ§åˆ¶**ï¼šæ‰€æœ‰æ–‡æ¡£æ”¶é›†æ—¶é—´å‡æ™šäºå„æ¨¡å‹å…¬å¼€çš„é¢„è®­ç»ƒæˆªæ­¢æ—¶é—´ï¼Œé˜²æ­¢è®°å¿†æ³„éœ²ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å…±æ¯”è¾ƒäº†å››ç±»æ–¹æ³•ï¼š

| ç±»åˆ« | æ–¹æ³•åˆ—è¡¨ |
|------|--------|
| **çº¯LLMæç¤ºï¼ˆLLM-onlyï¼‰** | IOï¼ˆç›´æ¥è¾“å‡ºï¼‰ã€Chain-of-Thought (CoT)ã€Self-Consistency (SC) |
| **æ–‡æœ¬RAGï¼ˆText-based RAGï¼‰** | Dense Retrieval + RAG |
| **ç®€å•å›¾å¢å¼ºï¼ˆNaive KG Augmentationï¼‰** | 1-hop KGï¼ˆæ³¨å…¥ä¸€è·³é‚»å±…ï¼‰ã€RAG + 1-hop KG |
| **å…ˆè¿›KG-RAGæ–¹æ³•ï¼ˆHybrid KG-RAGï¼‰** | CoK, RoG, ToG, ToG2.0, PoG, R2-KG, HippoRAG2.0, **EvoReasoner** |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ€»ä½“å‡†ç¡®ç‡ %ï¼Œä»¥ Qwen2.5-72B ä¸ºä¾‹ï¼‰

| æ–¹æ³• | Arxiv-AI | Arxiv-CY | Arxiv-BIO |
|------|----------|----------|----------|
| **IO** | 27.07 | 27.83 | 22.81 |
| **CoT** | 35.32 | 39.01 | 36.33 |
| **RAG** | 50.44 | 40.99 | 52.16 |
| **RAG + 1-hop KG** | 50.64 | 47.48 | 57.16 |
| **ToG2.0** | 58.61 | 59.05 | 60.36 |
| **EvoReasoner** | **71.30** | **66.89** | **73.31** |

> âœ… **è§‚å¯Ÿ**ï¼šå³ä½¿æ˜¯æœ€å¤§è§„æ¨¡çš„ LLMï¼ˆå¦‚ DeepSeek-V3.2ï¼‰ï¼Œåœ¨æ— å¤–éƒ¨æ£€ç´¢çš„æƒ…å†µä¸‹ä¹Ÿä»…èƒ½è¾¾åˆ° ~40% å‡†ç¡®ç‡ï¼Œè¡¨æ˜ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **RQ1ï¼šæ˜¯å¦å…·æœ‰æŒ‘æˆ˜æ€§ï¼Ÿ**
  - æ‰€æœ‰ LLM-only æ–¹æ³•å‡†ç¡®ç‡å‡ä½äº 40%ï¼Œå³ä½¿æ˜¯æœ€å¼ºçš„ DeepSeek-V3.2ã€‚
  - è¡¨æ˜ HYBRIDRAG-BENCH æˆåŠŸè§„é¿äº†å‚æ•°åŒ–è®°å¿†ï¼Œé—®é¢˜å¿…é¡»ä¾èµ–å¤–éƒ¨çŸ¥è¯†ã€‚

- **RQ2ï¼šæ˜¯å¦éœ€è¦å¤–éƒ¨æ£€ç´¢ï¼Ÿ**
  - å¼•å…¥ RAG åï¼Œå‡†ç¡®ç‡æå‡ **7â€“29ä¸ªç™¾åˆ†ç‚¹**ï¼Œè¯æ˜æ£€ç´¢è‡³å…³é‡è¦ã€‚
  - ä½†å•çº¯æ³¨å…¥ä¸€è·³KGèŠ‚ç‚¹ï¼ˆ1-hop KGï¼‰åè€Œä¼šé™ä½æ€§èƒ½ï¼Œå› å…¶å¼•å…¥å™ªå£°å¹²æ‰°æ¨¡å‹ã€‚

- **RQ3ï¼šç»“æ„åŒ–çŸ¥è¯†æ˜¯å¦æœ‰é¢å¤–ä»·å€¼ï¼Ÿ**
  - **Hybrid KG-RAG æ–¹æ³•æ™®éä¼˜äºçº¯æ–‡æœ¬ RAG**ã€‚
  - ä¾‹å¦‚ï¼Œåœ¨ Arxiv-BIO ä¸Šï¼Œ`EvoReasoner` æ¯”çº¯ RAG é«˜å‡º **21ä¸ªç™¾åˆ†ç‚¹**ã€‚
  - è¡¨æ˜ç»“æ„åŒ–å›¾è°±èƒ½æä¾›æ›´å¯é çš„å¤šè·³æ¨ç†è·¯å¾„ã€‚

- **RQ4ï¼šèƒ½å¦åŒºåˆ†ä¸åŒæ¨ç†ç­–ç•¥ï¼Ÿ**
  - ç»†ç²’åº¦åˆ†ææ˜¾ç¤ºï¼š
    - **å¤šè·³é—®é¢˜**ï¼š`ToG`, `ToG2.0`, `EvoReasoner` æ˜¾è‘—é¢†å…ˆï¼Œå› å…¶æ”¯æŒå›¾éå†ä¸è·¯å¾„ä¼˜åŒ–ã€‚
    - **åäº‹å®é—®é¢˜**ï¼š`CoT`, `SC`, `ToG2.0` è¡¨ç°ä¼˜å¼‚ï¼Œä½“ç°å…¶æ¨ç†ä¸ä¸ç¡®å®šæ€§å¤„ç†èƒ½åŠ›ï¼›è€Œ `1-hop KG` æ¥è¿‘é›¶åˆ†ã€‚
    - **å¼€æ”¾å¼é—®é¢˜**ï¼šä¾èµ–å¼ºæ–‡æœ¬æ£€ç´¢çš„æ–¹æ³•ï¼ˆå¦‚ RAGï¼‰è¡¨ç°æ›´å¥½ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **KGæ„å»ºè´¨é‡éªŒè¯**ï¼ˆTable 5ï¼‰ï¼š
  - ä½¿ç”¨ MINE åŸºå‡†æµ‹è¯•çŸ¥è¯†å›¾è°±çš„äº‹å®æ•è·ç‡ï¼ˆfact recovery rateï¼‰ã€‚
  - æœ¬æ–‡ä½¿ç”¨çš„ **EvoKG** æ–¹æ³•è¾¾åˆ° **71.36%**ï¼Œä¼˜äº OpenIE (29.36%)ã€GraphRAG (47.08%) å’Œ KGGen (66.46%)ã€‚
  - è¯´æ˜å›¾è°±æ„å»ºæœ¬èº«å¯é ï¼Œæ€§èƒ½ç“¶é¢ˆä¸åœ¨çŸ¥è¯†æå–é˜¶æ®µã€‚

- **è®¡ç®—æˆæœ¬åˆ†æ**ï¼š
  - KG æ„å»ºçš„ token æ¶ˆè€—ä¸æ–‡æ¡£é•¿åº¦å‘ˆè¿‘ä¼¼çº¿æ€§å…³ç³»ï¼ˆFigure 2ï¼‰ï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²ã€‚
  - æ”¯æŒå¹¶è¡Œå¤„ç†ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿå¯æ§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç°æœ‰åŸºå‡†æ˜“å—æ±¡æŸ“å½±å“**ï¼šè®¸å¤šçœ‹ä¼¼â€œé«˜æ€§èƒ½â€çš„ RAG ç»“æœå¯èƒ½æ˜¯ç”±äºæ¨¡å‹å·²è§è¿‡ç›¸å…³çŸ¥è¯†ï¼Œè€ŒéçœŸå®æ¨ç†èƒ½åŠ›ã€‚
2. **HYBRIDRAG-BENCH æˆåŠŸéš”ç¦»è®°å¿†ä¸æ¨ç†**ï¼šé€šè¿‡ä½¿ç”¨è¿‘æœŸç§‘å­¦æ–‡çŒ®ï¼Œç¡®ä¿æ¨¡å‹å¿…é¡»ä¾èµ–æ£€ç´¢æ‰èƒ½ä½œç­”ã€‚
3. **ç»“æ„åŒ–çŸ¥è¯†æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›**ï¼šç»“åˆçŸ¥è¯†å›¾è°±çš„ KG-RAG æ–¹æ³•åœ¨å¤šè·³ã€åäº‹å®ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜æ˜¾ä¼˜äºçº¯æ–‡æœ¬ RAGã€‚
4. **ä¸åŒæ–¹æ³•èƒ½åŠ›å·®å¼‚æ˜¾è‘—**ï¼šæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å„ç±»æ–¹æ³•çš„ä¼˜åŠ£ï¼Œå°¤å…¶åœ¨ç»†ç²’åº¦æ¨ç†è¡Œä¸ºä¸Šæä¾›è¯Šæ–­ä¿¡å·ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– LLM è¿›è¡ŒçŸ¥è¯†æŠ½å–ä¸ QA ç”Ÿæˆ**ï¼šè™½ç„¶è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ï¼Œä½†çŸ¥è¯†å›¾è°±è´¨é‡å’Œé—®é¢˜åˆç†æ€§ä»å—é™äºåº•å±‚ LLM çš„èƒ½åŠ›ã€‚
- **é¢†åŸŸä¾èµ–æ€§å¼º**ï¼šç›®å‰ä»…åŸºäº arXiv ç§‘å­¦æ–‡çŒ®ï¼Œå¯¹å¸¸è¯†æˆ–é€šç”¨çŸ¥è¯†è¦†ç›–æœ‰é™ã€‚
- **è¯„ä¼°ä¾èµ– LLM-as-a-Judge**ï¼šå°½ç®¡å¸¸ç”¨ï¼Œä½†ä»å¯èƒ½å­˜åœ¨è¯„åˆ†åå·®ï¼Œæœªæ¥å¯å¼•å…¥äººç±»è¯„ä¼°ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤šçŸ¥è¯†æºï¼ˆå¦‚æ–°é—»ã€ä¸“åˆ©ã€ä¼ä¸šæ–‡æ¡£ï¼‰ã€‚
- æ”¯æŒåŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œå®ç°æŒç»­å­¦ä¹ åœºæ™¯ä¸‹çš„è¯„ä¼°ã€‚
- æ¢ç´¢æ›´å¤æ‚çš„æ¨ç†æ¨¡å¼ï¼ˆå¦‚æ—¶åºæ¨ç†ã€å› æœæ¨æ–­ï¼‰ã€‚
- å°†æ¡†æ¶åº”ç”¨äºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯¾ç¨‹å­¦ä¹ ï¼ˆcurriculum learningï¼‰è®¾è®¡ã€‚

---

> ğŸ”— **ä»£ç ä¸æ•°æ®å¼€æº**ï¼š  
> ä½œè€…å·²å°†ä»£ç å’Œæ•°æ®é›†å…¬å¼€ï¼š[github.com/junhongmit/HybridRAG-Bench](https://github.com/junhongmit/HybridRAG-Bench)  
> ä¸ºåç»­ç ”ç©¶æä¾›äº†å¯å¤ç”¨çš„ç ”ç©¶åŸºç¡€è®¾æ–½ã€‚

</details>

---

### 7. [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743)

**Authors**: Vaisakh Shaj, Cameron Barker, Aidan Scannell, Andras Szecsenyi, Elliot J. Crowley, Amos Storkey  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.10743v1  

#### Abstract
State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by refram...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šKalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking

---

## 1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **State-Space Models**ï¼ˆå¦‚ Mambaï¼‰å’Œ **Gated Linear Attention**ï¼ˆGLAï¼‰è™½ç„¶å®ç°äº†çº¿æ€§å¤æ‚åº¦å’Œå¹¶è¡Œè®­ç»ƒï¼Œä½†åœ¨ä»¥ä¸‹æ–¹é¢å­˜åœ¨å±€é™ï¼š
- éšè—çŠ¶æ€æ›´æ–°æ˜¯**çº¿æ€§æˆ–ä»¿å°„**çš„ï¼Œè¡¨è¾¾èƒ½åŠ›å—é™ï¼›
- ç¼ºä¹å¯¹**çŠ¶æ€ä¸ç¡®å®šæ€§**çš„æ˜¾å¼å»ºæ¨¡ï¼›
- åœ¨éœ€è¦å¤æ‚æ¨ç†ã€é•¿ç¨‹ä¾èµ–å’Œé²æ£’çŠ¶æ€è¿½è¸ªçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚

æœ¬æ–‡æå‡ºå°†åºåˆ—å»ºæ¨¡é‡æ–°ç†è§£ä¸ºä¸€ä¸ª**è´å¶æ–¯æ»¤æ³¢é—®é¢˜**ï¼Œä»è€Œå¼•å…¥æ›´ä¸°å¯Œçš„éçº¿æ€§åŠ¨æ€å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æœºåˆ¶ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šKalman Linear Attention (KLA)

KLA æ˜¯ä¸€ç§æ–°çš„ **sequence mixer** å±‚ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†è¯­è¨€å»ºæ¨¡è§†ä¸ºä»â€œå™ªå£°è§‚æµ‹â€ä¸­è¿›è¡Œæ½œåœ¨çŠ¶æ€ä¼°è®¡çš„è¿‡ç¨‹ï¼Œé‡‡ç”¨å¡å°”æ›¼æ»¤æ³¢æ¡†æ¶å®ç°æ—¶é—´å¹¶è¡Œçš„æ¦‚ç‡æ¨æ–­ã€‚

#### åˆ›æ–°ç‚¹ï¼š
- **C1. å¡å°”æ›¼æ»¤æ³¢çš„ä¿¡æ¯å½¢å¼é‡å‚æ•°åŒ–ï¼ˆAssociative Reparameterisationï¼‰**
  - å°†æ ‡å‡†å¡å°”æ›¼æ»¤æ³¢è½¬æ¢ä¸º**ä¿¡æ¯å½¢å¼**ï¼ˆinformation formï¼‰ï¼Œè¯æ˜å…¶ç²¾åº¦æ›´æ–°æ˜¯ä¸€ä¸ª **Mobius å˜æ¢**ï¼ˆå³åˆ†å¼çº¿æ€§å˜æ¢ï¼‰ã€‚
  - è¯¥å˜æ¢å…·æœ‰**ç»“åˆå¾‹**ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ `associative scan` å®ç°**å¹¶è¡Œè®¡ç®—**ï¼Œè¾¾åˆ° $O(\log T)$ å¹¶è¡Œæ·±åº¦ï¼Œä¸ Mamba åŒçº§æ•ˆç‡ã€‚

- **C2. ä¸ç¡®å®šæ€§é©±åŠ¨çš„éçº¿æ€§é—¨æ§æœºåˆ¶**
  - æ›´æ–°ä¸­çš„â€œé—¨â€ç”±å†å²ç²¾åº¦ä¸å½“å‰è§‚æµ‹ç²¾åº¦çš„æ¯”ç‡å†³å®šï¼Œå½¢æˆ**å†å²ä¾èµ–ä¸”éçº¿æ€§**çš„é—¨æ§è¡Œä¸ºã€‚
  - è¶…è¶Šäº† GLA/Mamba ä¸­åŸºäºè¾“å…¥æ§åˆ¶çš„çº¿æ€§é—¨æ§ã€‚

- **C3. æ˜¾å¼çš„ä¿¡å¿µçŠ¶æ€ä¸ç¡®å®šæ€§è¾“å‡º**
  - KLA è¾“å‡ºä¸ä»…åŒ…æ‹¬åéªŒå‡å€¼ï¼ˆç”¨äºé¢„æµ‹ï¼‰ï¼Œè¿˜åŒ…æ‹¬**åéªŒæ–¹å·®/ç²¾åº¦**ï¼Œå¯ç”¨äºè§£é‡Šæ¨¡å‹ç½®ä¿¡åº¦ã€é€‰æ‹©æ€§è¿‡æ»¤ç­‰ã€‚

- **C4. Drop-in æ›¿ä»£æ¨¡å—è®¾è®¡**
  - KLA å¯ä½œä¸º Transformerã€SSM æˆ– GLA çš„ç›´æ¥æ›¿ä»£ç»„ä»¶ï¼Œé›†æˆåˆ°ç°ä»£è¯­è¨€æ¨¡å‹æ¶æ„ä¸­ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç‰¹æ€§ | Softmax Attention | SSM/GLA (e.g., Mamba) | KLA (æœ¬æ–‡) |
|------|-------------------|------------------------|------------|
| è¡¨è¾¾åŠ› | éçº¿æ€§ï¼ˆsoftmaxï¼‰ | çº¿æ€§/ä»¿å°„ | **åˆ†å¼çº¿æ€§ï¼ˆMobiusï¼‰** |
| è®­ç»ƒæ•ˆç‡ | $O(T^2)$ | $O(T)$ | $O(T)$ |
| æ¨ç†æ•ˆç‡ | $O(T)$ | $O(1)$ | $O(1)$ |
| æ˜¯å¦æ”¯æŒå¹¶è¡Œè®­ç»ƒ | âŒ | âœ… | âœ… |
| æ˜¾å¼çŠ¶æ€ä¸ç¡®å®šæ€§ | âŒ | âŒ | âœ… |

> âœ… **KLA åœ¨ä¿æŒ SSM æ•ˆç‡çš„åŒæ—¶ï¼Œè·å¾—äº†æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä¸ä»»åŠ¡

#### ï¼ˆ1ï¼‰MAD-Lab åˆæˆè¯­è¨€å»ºæ¨¡å¥—ä»¶ï¼ˆSynthetic Tasksï¼‰
ç”¨äºæµ‹è¯•ä¸åŒè¯­è¨€æŠ€èƒ½ï¼Œå…±å…­é¡¹ä»»åŠ¡ï¼š

| ä»»åŠ¡ | æµ‹è¯•èƒ½åŠ› |
|------|----------|
| **Compression** | åºåˆ—å‹ç¼©ä¸ä¿¡æ¯ç“¶é¢ˆ |
| **Memorization** | å›ºå®šæ˜ å°„è®°å¿† |
| **In-context Recall (MQAR)** | ä¸Šä¸‹æ–‡å†…é”®å€¼æ£€ç´¢ |
| **Noisy Recall** | å™ªå£°ç¯å¢ƒä¸‹çš„é€‰æ‹©æ€§æ³¨æ„ |
| **Fuzzy Recall** | å¤štokenç»„åˆé”®å€¼åŒ¹é… |
| **Selective Copy** | æœ‰åºå¤åˆ¶ä¸å¹²æ‰°æŠ‘åˆ¶ |

#### ï¼ˆ2ï¼‰é•¿ä¸Šä¸‹æ–‡å¤šæŸ¥è¯¢å…³è”å›å¿†ï¼ˆLong-Context MQARï¼‰
- åºåˆ—é•¿åº¦ï¼š**T = 2048**
- è¯è¡¨å¤§å°ï¼š**V = 256**
- æŒ‘æˆ˜é«˜å®¹é‡å­˜å‚¨ä¸è¿œè·ç¦»æ£€ç´¢èƒ½åŠ›

#### ï¼ˆ3ï¼‰A5 ç½®æ¢ç¾¤çŠ¶æ€è¿½è¸ªä»»åŠ¡ï¼ˆPermutation Compositionï¼‰
- è¾“å…¥æ˜¯ä¸€ä¸² A5 ç¾¤å…ƒç´ ï¼Œç›®æ ‡æ˜¯è®¡ç®—å®ƒä»¬çš„ä¹˜ç§¯ã€‚
- æœ¬è´¨æ˜¯**ä¸å¯å¹¶è¡ŒåŒ–çš„é¡ºåºçŠ¶æ€è·Ÿè¸ªé—®é¢˜**ï¼Œç”¨äºæ£€éªŒæ¨¡å‹æ˜¯å¦å…·å¤‡çœŸæ­£çš„â€œçŠ¶æ€æ¼”åŒ–â€èƒ½åŠ›ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **æ¨¡å‹ç»“æ„** | å•å±‚æˆ–åŒå±‚ blockï¼Œç»Ÿä¸€ä½¿ç”¨ fused-MLP æ¶æ„ï¼ˆç±»ä¼¼ Mambaï¼‰ |
| **å‚æ•°é‡æ§åˆ¶** | æ‰€æœ‰æ¨¡å‹é…ç½®ä¸ºç›¸åŒæœ‰æ•ˆçŠ¶æ€å¤§å°ï¼ˆå¦‚ 2048ï¼‰ä»¥å…¬å¹³æ¯”è¾ƒ |
| **è®­ç»ƒæ–¹å¼** | AdamW ä¼˜åŒ–å™¨ï¼Œå›ºå®šå­¦ä¹ ç‡ $1\times10^{-3}$ï¼Œæœ€å¤§ 750 è½® |
| **è¯„ä¼°æŒ‡æ ‡** | åˆ†ç±»å‡†ç¡®ç‡ï¼ˆAccuracy %ï¼‰ï¼Œåœ¨å¤šä¸ªéšæœºç§å­ä¸Šå–å¹³å‡ |
| **ç¡¬ä»¶å¹³å°** | NVIDIA A100 80GB GPUï¼Œä½¿ç”¨ Triton å†…æ ¸åŠ é€Ÿ scan æ“ä½œ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ | ç±»å‹ | ç®€ä»‹ |
|------|------|------|
| **Mamba** | Selective SSM | å½“å‰æœ€æµè¡Œçš„é«˜æ•ˆ SSM æ¶æ„ |
| **GLA** | Gated Linear Attention | ç»Ÿä¸€å¤šç§çº¿æ€§ RNN/SSM çš„é€šç”¨æ¡†æ¶ |
| **GDN** (Gated Delta Net) | Delta Rule + SSM | å¼ºè°ƒè”æƒ³è®°å¿†æœºåˆ¶ï¼Œæ“…é•¿ recall |
| **mLSTM** | æ”¹è¿›ç‰ˆ LSTM | å…·å¤‡ä¹˜æ³•é—¨æ§çš„ç°ä»£å¾ªç¯ç½‘ç»œ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ï¼ˆ1ï¼‰åˆæˆä»»åŠ¡æ€§èƒ½å¯¹æ¯”ï¼ˆMAD-Labï¼‰

| æ–¹æ³• | Compression | Memorization | Context Recall | Noisy Recall | Fuzzy Recall | Selective Copy |
|------|-------------|--------------|----------------|--------------|---------------|----------------|
| GDN | 65.53 | 99.93 | 99.94 | 99.95 | 37.04 | 90.30 |
| GLA | 49.45 | 99.99 | 73.60 | 85.24 | 18.22 | 82.41 |
| Mamba | 78.35 | 99.96 | 99.92 | 99.93 | 30.83 | 80.60 |
| mLSTM | 57.17 | 99.98 | 99.98 | 99.99 | 25.43 | 36.61 |
| **KLA (Ours)** | **85.03** | 98.87 | **99.95** | **99.93** | **45.70** | **90.67** |
| **KLA+** (probabilistic decode) | **88.87** | **99.94** | 99.94 | 99.95 | 43.32 | 91.45 |

> ğŸ” **å…³é”®å‘ç°**ï¼š
> - KLA åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Š**æŒå¹³æˆ–è¶…è¶Šæœ€å¼ºåŸºçº¿**ï¼›
> - åœ¨ **Noisy Recall** å’Œ **Selective Copy** ä¸Šä¼˜åŠ¿æ˜¾è‘— â†’ éªŒè¯äº†ä¸ç¡®å®šæ€§åŠ æƒè¿‡æ»¤çš„æœ‰æ•ˆæ€§ï¼›
> - ä½¿ç”¨**æ¦‚ç‡è§£ç **ï¼ˆmarginalizationï¼‰è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œå°¤å…¶åœ¨å™ªå£°ä»»åŠ¡ä¸­ã€‚

---

### ï¼ˆ2ï¼‰é•¿ä¸Šä¸‹æ–‡ MQAR æ€§èƒ½ï¼ˆT=2048, V=256ï¼‰

| æ¨¡å‹ç»´åº¦ | KLA | GDN | Mamba | GLA |
|---------|-----|-----|-------|-----|
| d=64 | ~80% | ~75% | ~70% | <10% |
| d=128 | ~90% | ~85% | ~80% | <10% |
| **d=256** | **>95%** | ~90% | ~85% | <10% |

> âœ… **KLA åœ¨æç«¯è®¾ç½®ä¸‹ä»æ¥è¿‘å®Œç¾è¡¨ç°ï¼Œè€Œ GLA å®Œå…¨å¤±è´¥**  
> ğŸ’¡ æ¨æµ‹åŸå› ï¼šKLA çš„ **Mobius ç²¾åº¦æ›´æ–°æœºåˆ¶**èƒ½è‡ªé€‚åº”åœ°æŠ‘åˆ¶é¥±å’ŒçŠ¶æ€ä¸‹çš„æ–°å™ªå£°è¾“å…¥ï¼Œå®ç°éšå¼å‹ç¼©ã€‚

---

### ï¼ˆ3ï¼‰A5 çŠ¶æ€è¿½è¸ªä»»åŠ¡ï¼ˆPermutation Compositionï¼‰

![å›¾1](#)  
> *è§£å†³ A5 ä»»åŠ¡æ‰€éœ€æœ€å°å±‚æ•° vs åºåˆ—é•¿åº¦*

| æ–¹æ³• | æ‰€éœ€å±‚æ•°è¶‹åŠ¿ |
|------|-------------|
| Transformer / Linear SSM | éšé•¿åº¦å¢é•¿è€Œå¢åŠ ï¼ˆæ— æ³•æ’å®šæ·±åº¦è§£å†³ï¼‰ |
| **KLA** | **ä»…éœ€ 1â€“2 å±‚å³å¯è§£å†³ä»»æ„é•¿åº¦ä»»åŠ¡** |

> ğŸ§  **æ„ä¹‰é‡å¤§**ï¼šè¡¨æ˜ KLA çš„æ›´æ–°æœºåˆ¶å±äºæ¯” TCâ° æ›´å¼ºçš„è®¡ç®—ç±»ï¼Œå…·å¤‡çœŸæ­£æ„ä¹‰ä¸Šçš„**é¡ºåºçŠ¶æ€è¿½è¸ªèƒ½åŠ›**ï¼Œçªç ´äº†ä¼ ç»Ÿçº¿æ€§ SSM çš„ç†è®ºé™åˆ¶ã€‚

---

### ï¼ˆ4ï¼‰æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰

#### A. OU Prior Discretisation æ¶ˆè
- ç§»é™¤ OU åŠ¨åŠ›å­¦ç¦»æ•£åŒ– â†’ å‡†ç¡®ç‡ä¸‹é™ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œå°¤å…¶åœ¨æ·±å±‚æ¨¡å‹ä¸­ã€‚
- è¯´æ˜**è¿ç»­æ—¶é—´å…ˆéªŒ + æ­£ç¡®ç¦»æ•£åŒ–**å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚

#### B. è¿‡ç¨‹å™ªå£°ï¼ˆProcess Noiseï¼‰æ¶ˆè
- å›ºå®šè¿‡ç¨‹å™ªå£° $p_t = 0$ï¼ˆé€€åŒ–ä¸ºç¡®å®šæ€§ç³»ç»Ÿï¼‰â†’ å¹³å‡å‡†ç¡®ç‡ä¸‹é™ **49.8ä¸ªç™¾åˆ†ç‚¹**ï¼
- æœ€ä¸¥é‡çš„æ˜¯ Memorizationï¼ˆâ†“93.6%ï¼‰å’Œ In-context Recallï¼ˆâ†“54.7%ï¼‰

> â— ç»“è®ºï¼š**è¿‡ç¨‹å™ªå£°ä¸æ˜¯å†—ä½™é¡¹ï¼Œè€Œæ˜¯ç»´æŒæ¨¡å‹è¡¨è¾¾åŠ›çš„å…³é”®ç»„æˆéƒ¨åˆ†**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **è´å¶æ–¯æ»¤æ³¢è§†è§’å¯æå‡åºåˆ—å»ºæ¨¡èƒ½åŠ›**  
   å°† token è§†ä¸ºâ€œå™ªå£°è§‚æµ‹â€è€Œéâ€œæ§åˆ¶ä¿¡å·â€ï¼Œè‡ªç„¶å¼•å‡ºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„çŠ¶æ€æ›´æ–°æœºåˆ¶ã€‚

2. âœ… **ä¿¡æ¯å½¢å¼ä¸‹çš„å¡å°”æ›¼æ›´æ–°å…·æœ‰ç»“åˆæ€§**  
   å…¶ç²¾åº¦é€’å½’æ„æˆ Mobius å˜æ¢ï¼Œå¯é€šè¿‡ `associative scan` å®ç°é«˜æ•ˆå¹¶è¡Œï¼Œæ‰“ç ´â€œè´å¶æ–¯=ä¸²è¡Œâ€çš„åˆ»æ¿å°è±¡ã€‚

3. âœ… **KLA å®ç°äº†æ•ˆç‡ä¸è¡¨è¾¾åŠ›çš„åŒèµ¢**  
   - ä¿ç•™äº† SSM çš„ $O(T)$ å¤æ‚åº¦å’Œå¹¶è¡Œè®­ç»ƒèƒ½åŠ›ï¼›
   - è·å¾—äº†è¶…è¶Šçº¿æ€§ç³»ç»Ÿçš„éçº¿æ€§è¡¨è¾¾åŠ›ï¼ˆåˆ†å¼çº¿æ€§ï¼‰ï¼›
   - æ˜¾å¼è¾“å‡ºä¸ç¡®å®šæ€§ï¼Œå¢å¼ºå¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚

4. âœ… **åœ¨å¤šé¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA**
   - åœ¨åˆæˆä»»åŠ¡ã€é•¿ä¸Šä¸‹æ–‡ recall å’Œ A5 çŠ¶æ€è¿½è¸ªä¸Šå…¨é¢ä¼˜äº Mambaã€GLAã€GDN ç­‰å…ˆè¿›æ–¹æ³•ã€‚

---

### å±€é™æ€§

- å½“å‰æœªå……åˆ†åˆ©ç”¨ posterior covariance è¿›è¡Œå®é™…åº”ç”¨ï¼ˆå¦‚ hallucination detectionã€OOD detectionï¼‰ï¼›
- å®éªŒé›†ä¸­åœ¨æµ…å±‚ã€å°è§„æ¨¡è®¾ç½®ï¼Œå°šæœªéªŒè¯åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„è¡¨ç°ï¼›
- Triton kernel å°šæœªå®Œå…¨èåˆæ•´ä¸ª blockï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´ï¼ˆå¦‚ fused kernelï¼‰ï¼›
- å¯¹ extremely long contextï¼ˆ>8kï¼‰çš„æ”¯æŒæœ‰å¾…è¿›ä¸€æ­¥æµ‹è¯•ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

- å°† KLA æ‰©å±•è‡³ **Probabilistic LLMs**ï¼Œæ¢ç´¢ä¸ç¡®å®šæ€§é‡åŒ–åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„ä»·å€¼ï¼›
- å¼€å‘æ›´é«˜æ•ˆçš„ **custom CUDA/Triton kernels**ï¼Œå®ç°ç«¯åˆ°ç«¯ fused sequence modelingï¼›
- æ¢ç´¢ KLA åœ¨ **RLã€world modelingã€partial observability** åœºæ™¯çš„åº”ç”¨ï¼›
- ç†è®ºåˆ†æ KLA çš„è®¡ç®—å¤æ‚åº¦è¾¹ç•ŒåŠå…¶ä¸ç”µè·¯å¤æ‚åº¦ç±»çš„å…³ç³»ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **KLA æˆåŠŸå°†ç»å…¸è´å¶æ–¯æ»¤æ³¢ä¸ç°ä»£é«˜æ•ˆåºåˆ—å»ºæ¨¡ç»“åˆï¼Œåœ¨ä¸ç‰ºç‰²é€Ÿåº¦çš„å‰æä¸‹ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§é©±åŠ¨çš„éçº¿æ€§æ›´æ–°ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨è¾¾åŠ›ä¸çŠ¶æ€è¿½è¸ªèƒ½åŠ›ï¼Œä¸ºä¸‹ä¸€ä»£é«˜æ•ˆè¯­è¨€æ¨¡å‹æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 8. [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)

**Authors**: Jie Jiang, Yangru Huang, Zeyu Wang, Changping Wang, Yuling Xiong, Jun Zhang, Huan Yu  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.10699v1  

#### Abstract
Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated dec...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šSpend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation**

---

## **1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
è¯¥è®ºæ–‡é’ˆå¯¹**ç”Ÿæˆå¼æ¨èç³»ç»Ÿ**ï¼ˆGenerative Recommendation, GRï¼‰ä¸­çš„ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜â€”â€”**æ¦‚ç‡-å¥–åŠ±é”™ä½**ï¼ˆprobability-reward misalignmentï¼‰ã€‚

åœ¨åŸºäºè‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆå¼æ¨èä¸­ï¼Œé€šå¸¸é‡‡ç”¨ likelihood-driven è§£ç ç­–ç•¥ï¼ˆå¦‚ beam searchï¼‰è¿›è¡Œå€™é€‰é‡‡æ ·ï¼Œè€Œè®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–éå¯å¯¼çš„ä¸šåŠ¡å¥–åŠ±ï¼ˆå¦‚ç‚¹å‡»ç‡ CTRï¼‰ã€‚è¿™ç§ä¸åŒ¹é…å¯¼è‡´ä¸¤ä¸ªç»“æ„æ€§ç¼ºé™·ï¼š

1. **æ¢ç´¢ä¸è¶³**ï¼ˆinsufficient explorationï¼‰  
   é«˜å¥–åŠ±ä½†ä½æ¦‚ç‡çš„ itemï¼ˆå¦‚é•¿å°¾å•†å“ï¼‰å› å…¶å‰ç¼€ token æ¦‚ç‡è¾ƒä½ï¼Œåœ¨æ—©æœŸè¢« beam search å‰ªæï¼Œæ— æ³•è¢«æœ‰æ•ˆé‡‡æ ·ã€‚

2. **ä¼˜åŠ¿å‹ç¼©**ï¼ˆadvantage compressionï¼‰  
   å…±äº«é«˜æ¦‚ç‡å‰ç¼€çš„å€™é€‰é¡¹å½¢æˆâ€œå…„å¼Ÿç»„â€ï¼ˆsibling groupsï¼‰ï¼Œå…¶å¥–åŠ±é«˜åº¦ç›¸å…³ï¼Œå¯¼è‡´ GRPO ç­‰ groupwise RL æ–¹æ³•è®¡ç®—å‡ºçš„ä¼˜åŠ¿å·®å¼‚æå°ï¼Œå‰Šå¼±äº†å­¦ä¹ ä¿¡å·ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ï¼šV-STAR**
ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡ºäº† **V-STAR**ï¼ˆ**Value-guided Sampling and Tree-structured Advantage Reinforcement**ï¼‰ï¼Œä¸€ä¸ªè‡ªæ¼”åŒ–çš„é‡‡æ ·-ä¼˜åŒ–é—­ç¯æ¡†æ¶ï¼ŒåŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼š

#### **(1) Value-Guided Efficient Decoding (VED)**
- **ç›®æ ‡**ï¼šæå‡é«˜ä»·å€¼è·¯å¾„çš„å¯è¾¾æ€§ï¼Œé¿å…è¿‡æ—©å‰ªæã€‚
- **æœºåˆ¶**ï¼š
  - æ„å»ºä¸€ä¸ªè½»é‡çº§çš„ **value model**ï¼Œé¢„æµ‹ä»å½“å‰ SID å‰ç¼€å‡ºå‘çš„æœŸæœ›æœªæ¥å›æŠ¥ã€‚
  - å¼•å…¥ **prefix priority score**ï¼š$ G(s) = V(s) + \lambda H_\pi(s) $ï¼Œç»“åˆ valueï¼ˆé¢„æœŸå¥–åŠ±ï¼‰å’Œ policy entropyï¼ˆä¸ç¡®å®šæ€§ï¼‰ã€‚
  - åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œä¼˜å…ˆæ‰©å±• **é«˜ä»·å€¼ä¸”é«˜ä¸ç¡®å®šæ€§** çš„å‰ç¼€èŠ‚ç‚¹ï¼ˆdecisive nodesï¼‰ï¼Œå®ç°**æœ‰å¯¼å‘çš„ç¨€ç–æœç´¢**ã€‚

#### **(2) Sibling-GRPO**
- **ç›®æ ‡**ï¼šç¼“è§£å…„å¼Ÿç»„å†…çš„ä¼˜åŠ¿å‹ç¼©ï¼Œå¢å¼ºå­¦ä¹ ä¿¡å·ã€‚
- **æœºåˆ¶**ï¼š
  - åˆ©ç”¨ç”Ÿæˆæ ‘çš„æ‹“æ‰‘ç»“æ„ï¼Œå°†å€™é€‰åˆ’åˆ†ä¸º **sibling groups**ï¼ˆå…±äº«åŒä¸€çˆ¶å‰ç¼€çš„å­èŠ‚ç‚¹ï¼‰ã€‚
  - åœ¨æ¯ä¸ª sibling group å†…éƒ¨è®¡ç®— **relative advantage**ï¼Œè€Œéåœ¨æ•´ä¸ªå€™é€‰é›†ä¸Šåšå…¨å±€å½’ä¸€åŒ–ã€‚
  - æ›´æ–°èšç„¦äº**åˆ†æ”¯å†³ç­–ç‚¹**ï¼Œå¼ºåŒ–å¯¹å…³é”® token é€‰æ‹©çš„æ¢¯åº¦ä¿¡å·ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | V-STAR |
|------|--------|--------|
| **æ¢ç´¢æ•ˆç‡** | è´ªå©ªå‰ªæï¼Œå¿½ç•¥ä½æ¦‚ç‡é«˜å¥–åŠ±è·¯å¾„ | ä»·å€¼å¼•å¯¼ï¼Œç²¾å‡†æ‰©å±•é«˜æ½œåŠ›åˆ†æ”¯ |
| **å­¦ä¹ ä¿¡å·å¼ºåº¦** | å…¨å±€å½’ä¸€åŒ–å¯¼è‡´ä¼˜åŠ¿å‹ç¼© | sibling-relative å½’ä¸€åŒ–ä¿ç•™å±€éƒ¨å¯¹æ¯” |
| **è®¡ç®—å¼€é”€** | å…¨æ ‘æœç´¢ï¼ˆå¦‚ MCTSï¼‰ä¸å¯è¡Œï¼›ç›²ç›®æ‰©å¤§ beam å®½åº¦ä½æ•ˆ | å›ºå®šé¢„ç®—ä¸‹çš„ç¨€ç–æ‰©å±•ï¼Œé«˜æ•ˆåˆ©ç”¨è®¡ç®—èµ„æº |
| **ç»“æ„æ„ŸçŸ¥** | å¿½è§†ç”Ÿæˆæ ‘çš„å±‚æ¬¡ç»“æ„ | æ˜¾å¼å»ºæ¨¡ prefix tree æ‹“æ‰‘ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **æ•°æ®é›†**
- **ç¦»çº¿æ•°æ®é›†**ï¼šAmazon Review æ•°æ®é›†çš„ä¸¤ä¸ªå­é›†
  - **Industrial Products**
  - **Office Products**
- **åœ¨çº¿æ•°æ®**ï¼šå¾®ä¿¡è§†é¢‘å·ï¼ˆWeChat Channelsï¼‰çœŸå®æµé‡ï¼Œè¿›è¡Œä¸ºæœŸ 5 å¤©çš„ A/B æµ‹è¯•ï¼ˆ5% æµé‡ï¼‰ã€‚

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹éª¨å¹²**ï¼šQwen2.5-1.5B
- **è®­ç»ƒæµç¨‹**ï¼š
  1. **SFT**ï¼ˆSupervised Fine-tuningï¼‰
  2. **RL å¾®è°ƒ**ï¼šä½¿ç”¨ GRPO æˆ– Sibling-GRPO
- **VED å‚æ•°**ï¼š
  - å€™é€‰æ•°ï¼š16
  - SID é•¿åº¦ï¼š3
  - åˆå§‹åŒ– beam å®½åº¦ï¼š8
  - æŠ˜æ‰£å› å­ Î³ï¼š0.99
  - æ­£åˆ™ç³»æ•° Î»ï¼š0.1

### **è¯„ä¼°æŒ‡æ ‡**
- **ç¦»çº¿æŒ‡æ ‡**ï¼š
  - **HR@K**ï¼ˆHit Rateï¼‰
  - **NDCG@K**ï¼ˆNormalized Discounted Cumulative Gainï¼‰
- **åœ¨çº¿æŒ‡æ ‡**ï¼š
  - **GMV**ï¼ˆGross Merchandise Volumeï¼‰
  - **GMV-Normal**ï¼ˆç‚¹å‡»/è½¬åŒ–ä¼˜åŒ–å¹¿å‘Šçš„ GMVï¼‰

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **ä¼ ç»Ÿåºåˆ—æ¨è** | GRU4Rec, Caser, SASRec |
| **ç”Ÿæˆå¼é SID** | HSTU, BIGRec |
| **SID ç”Ÿæˆå¼ï¼ˆæ—  RLï¼‰** | TIGER, LCRec, D3 |
| **SID + RL / DPO** | S-DPO, MiniOneRec |
| **è§£ç ç­–ç•¥å¯¹æ¯”** | Beam Search, Top-K Sampling |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **ç¦»çº¿æ€§èƒ½ï¼ˆTable 1ï¼‰**
åœ¨ Industrial å’Œ Office æ•°æ®é›†ä¸Šï¼ŒV-STAR å‡å–å¾— SOTA æ€§èƒ½ï¼š

| æ–¹æ³• | Industrial HR@3 | Industrial NDCG@10 | Office HR@3 | Office NDCG@10 |
|------|------------------|---------------------|------------|---------------|
| MiniOneRec (æœ€å¼ºåŸºçº¿) | 0.1143 | 0.1167 | 0.1217 | 0.1088 |
| **Ours (V-STAR)** | **0.1189** (+4.0%) | **0.1217** (+4.3%) | **0.1344** (+10.4%) | **0.1196** (+9.9%) |

> âœ… **ä¸€è‡´è¶…è¶Šæ‰€æœ‰åŸºçº¿**ï¼Œå°¤å…¶åœ¨ Office æ•°æ®é›†ä¸Šå¢ç›Šæ˜¾è‘—ã€‚

---

### **åœ¨çº¿ A/B æµ‹è¯•ï¼ˆSection 5.3ï¼‰**
åœ¨å¾®ä¿¡è§†é¢‘å·çœŸå®åœºæ™¯ä¸­ï¼š

| æŒ‡æ ‡ | ç›¸å¯¹æå‡ |
|------|----------|
| **GMV** | **+1.23%** |
| **GMV-Normal** | **+1.87%** |

> âœ… å•†ä¸šä»·å€¼æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨çœŸå®ä¸šåŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

---

### **æ¶ˆèå®éªŒ**

#### **(1) è§£ç ç­–ç•¥å¯¹æ¯”ï¼ˆTable 2ï¼‰**
| è§£ç ç­–ç•¥ | Industrial NDCG@10 | Office HR@10 |
|---------|--------------------|-------------|
| Beam Search | 0.1194 | 0.1684 |
| Top-K | 0.1090 | 0.1644 |
| **Ours (VED)** | **0.1217** | **0.1746** |

> âœ… VED åœ¨ä¿æŒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºçº¯éšæœºï¼ˆTop-Kï¼‰å’Œè´ªå©ªï¼ˆBeamï¼‰ç­–ç•¥ã€‚

#### **(2) æ‰©å±•è§„åˆ™æ¶ˆèï¼ˆTable 3ï¼‰**
| æ‰©å±•è§„åˆ™ | Industrial NDCG@10 |
|---------|--------------------|
| Value-only ($V_\phi$) | 0.1202 |
| Entropy-only ($H_\pi$) | 0.1198 |
| **Joint Score ($G(s)$)** | **0.1217** |

> âœ… **ä»·å€¼ä¸ä¸ç¡®å®šæ€§çš„è”åˆè¯„åˆ†**æ•ˆæœæœ€ä½³ï¼Œè¯æ˜ä¸¤è€…ååŒä½œç”¨ã€‚

#### **(3) è®­ç»ƒç›®æ ‡æ¶ˆèï¼ˆTable 4ï¼‰**
| è®­ç»ƒç›®æ ‡ | Industrial NDCG@10 |
|---------|--------------------|
| GRPO (å…¨å±€å½’ä¸€åŒ–) | 0.1189 |
| Sibling-GRPO | 0.1204 |
| **GRPO + Sibling-GRPO (è”åˆ)** | **0.1217** |

> âœ… Sibling-GRPO æ˜æ˜¾ä¼˜äºæ ‡å‡† GRPOï¼Œè”åˆä½¿ç”¨æ•ˆæœæœ€å¥½ã€‚

---

### **å…¶ä»–åˆ†æ**
- **å¤šæ ·æ€§ vs è´¨é‡**ï¼ˆTable 6ï¼‰ï¼š
  - VED çš„å€™é€‰é›† **å¤šæ ·æ€§æ›´é«˜**ï¼ˆDiv(C) æ›´å¤§ï¼‰ï¼ŒåŒæ—¶ **æœ€é«˜å¥–åŠ±é¡¹æ›´ä¼˜**ï¼ˆMaxReward æ›´é«˜ï¼‰ã€‚
- **è®¡ç®—æ•ˆç‡**ï¼ˆFigure 4ï¼‰ï¼š
  - åœ¨ç›¸åŒ token é¢„ç®—ä¸‹ï¼ŒVED çš„æ€§èƒ½å¢é•¿è¿œè¶…æ‰©å¤§ beam å®½åº¦ï¼Œ**è®¡ç®—æ•ˆç‡æ›´é«˜**ã€‚
- **æ¡ˆä¾‹ç ”ç©¶**ï¼ˆTable 5ï¼‰ï¼š
  - Beam Search å’Œ Top-K å‡æœªæ‰¾åˆ° GT itemï¼ˆACCUTECK Digital Scaleï¼‰ï¼Œè€Œ VED æˆåŠŸé€šè¿‡æ–°é¢–åˆ†æ”¯ `<a_20>` å‘ç°ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **æ¦‚ç‡é©±åŠ¨çš„è§£ç å­˜åœ¨ç»“æ„æ€§åè§**ï¼Œä¼šç³»ç»Ÿæ€§å¿½ç•¥é«˜å¥–åŠ±é•¿å°¾ itemã€‚
2. **ä»·å€¼å¼•å¯¼çš„ç¨€ç–æœç´¢**ï¼ˆVEDï¼‰èƒ½ä»¥æä½æˆæœ¬æå‡é«˜ä»·å€¼è·¯å¾„çš„å¯è¾¾æ€§ã€‚
3. **Sibling-GRPO æœ‰æ•ˆç¼“è§£äº†ä¼˜åŠ¿å‹ç¼©**ï¼Œä½¿ RL å­¦ä¹ ä¿¡å·åœ¨å…„å¼Ÿç»„å†…ä¾ç„¶å¼ºå¥ã€‚
4. **V-STAR æ„æˆè‡ªæ¼”åŒ–é—­ç¯**ï¼šæ›´å¥½çš„é‡‡æ · â†’ æ›´æ¸…æ™°çš„å­¦ä¹ ä¿¡å· â†’ æ›´å¥½çš„ç­–ç•¥ â†’ æ›´å¥½çš„é‡‡æ ·ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–é«˜è´¨é‡çš„ value model**ï¼šè‹¥ value ä¼°è®¡ä¸å‡†ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯æ‰©å±•ã€‚
- **ä»éœ€ç¦»çº¿ç´¢å¼•æ„å»º**ï¼šSID ç¼–ç ä¾èµ– RQ-VAE ç­‰ç¦»çº¿å¤„ç†ã€‚
- **æµ‹è¯•æ—¶é»˜è®¤å›é€€åˆ° beam search**ï¼šè™½ç„¶å¯éƒ¨ç½² VEDï¼Œä½†å¢åŠ äº†å»¶è¿Ÿã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- å°† VED éƒ¨ç½²è‡³çº¿ä¸Šæ¨ç†ï¼Œè¿›ä¸€æ­¥æå‡é•¿å°¾è¦†ç›–ã€‚
- æ¢ç´¢æ›´é«˜æ•ˆçš„ value model æ¶æ„ï¼ˆå¦‚å…±äº« backboneï¼‰ã€‚
- å°† Sibling-GRPO æ€æƒ³æ¨å¹¿è‡³å…¶ä»–ç»“æ„åŒ–ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ slate generationï¼‰ã€‚

---

> **æ€»ç»“**ï¼šV-STAR é€šè¿‡ **value-guided structured sampling** å’Œ **tree-aware credit assignment**ï¼Œå®ç°äº†åœ¨ä¸¥æ ¼å»¶è¿Ÿçº¦æŸä¸‹ï¼Œç”Ÿæˆå¼æ¨èç³»ç»Ÿçš„**é«˜æ•ˆæ¢ç´¢**ä¸**ç¨³å®šä¼˜åŒ–**ï¼Œä¸ºå·¥ä¸šçº§ GR ç³»ç»Ÿæä¾›äº†å®ç”¨ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

### 9. [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796)

**Authors**: Jie Jiang, Ke Cheng, Xin Xu, Mengyang Pang, Tianhao Lu, Jiaheng Li, Yue Liu, Yuan Wang, Jun Zhang, Huan Yu, Zhouchen Lin  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.10796v1  

#### Abstract
Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# PRISM: Parallel Residual Iterative Sequence Model è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ç”Ÿæˆå¼åºåˆ—å»ºæ¨¡é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§çš„çŸ›ç›¾ï¼š**Transformers** è™½ç„¶è¡¨è¾¾èƒ½åŠ›å¼ºï¼Œä½†å…¶ $O(N^2)$ çš„è®¡ç®—å¤æ‚åº¦åœ¨å¤„ç†è¶…é•¿åºåˆ—æ—¶æ•ˆç‡æä½ï¼›è€Œç°æœ‰çš„é«˜æ•ˆæ¨¡å‹ï¼ˆå¦‚ Linear Attention å’Œ SSMsï¼‰è™½ç„¶å…·å¤‡ $O(N)$ å¤æ‚åº¦ï¼Œä½†ç”±äºä¾èµ–å•æ­¥çº¿æ€§æ›´æ–°ï¼ˆOne-Shot Optimizationï¼‰ï¼Œå­˜åœ¨â€œ**çº¿æ€§ç“¶é¢ˆ**â€ï¼ˆLinearity Bottleneckï¼‰ï¼Œå³åªèƒ½è¿›è¡Œ Rank-1 çš„çŠ¶æ€æ›´æ–°ï¼Œéš¾ä»¥æ•æ‰é«˜é˜¶ç‰¹å¾ç›¸å…³æ€§å’Œè¿­ä»£ä¼˜åŒ–èƒ½åŠ›ã€‚

æ›´è¿›ä¸€æ­¥ï¼Œåƒ **Test-Time Training (TTT)** è¿™ç±»é€šè¿‡è¿è¡Œæ—¶å¤šæ­¥æ¢¯åº¦ä¸‹é™æå‡è¡¨è¾¾åŠ›çš„æ–¹æ³•ï¼Œè™½ç„¶èƒ½çªç ´çº¿æ€§é™åˆ¶ï¼Œå´å› æ¢¯åº¦ä¾èµ–äºéšè—çŠ¶æ€è€Œå¯¼è‡´**ä¸²è¡Œä¾èµ–**ï¼ˆSerial Dependencyï¼‰ï¼Œç ´åäº†ç¡¬ä»¶å¹¶è¡Œæ€§ï¼Œè®­ç»ƒååé‡æä½ã€‚

PRISM æ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€â€œ**è¡¨è¾¾åŠ›ä¸æ•ˆç‡ä¸å¯å…¼å¾—**â€çš„æ ¹æœ¬å¼ åŠ›è€Œæå‡ºã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

PRISM (**Parallel Residual Iterative Sequence Model**) æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **Amortized Residual Optimization** æ¡†æ¶ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> å°†æ˜¾å¼çš„ã€ä¸²è¡Œçš„å¤šæ­¥è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œâ€œæ‘Šé”€â€ï¼ˆamortizeï¼‰ä¸ºä¸€ä¸ªå¯å¹¶è¡Œè®¡ç®—çš„å‰é¦ˆç®—å­ã€‚

å…·ä½“åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰Write-Forget Decouplingï¼ˆå†™å…¥-é—å¿˜è§£è€¦ï¼‰
- **ç†è®ºä¾æ®**ï¼šé€šè¿‡è°±æ‰°åŠ¨åˆ†æè¯æ˜ï¼Œ**é—å¿˜æ“ä½œ**ï¼ˆAï¼‰å¯¹è¿‘ä¼¼è¯¯å·®é²æ£’ï¼ˆç´¯ç§¯è¯¯å·®ä¸º $O(\ln T)$ æˆ– $O(1)$ï¼‰ï¼Œè€Œ**å†™å…¥æ“ä½œ**ï¼ˆBï¼‰å¯¹è¯¯å·®æå…¶æ•æ„Ÿï¼ˆç´¯ç§¯è¯¯å·®ä¸º $O(T)$ï¼‰ã€‚
- **è®¾è®¡å†³ç­–**ï¼šå› æ­¤å°†é—å¿˜è·¯å¾„ä¿æŒä¸ºç®€å•çš„ Rank-1 çº¿æ€§è¡°å‡ï¼ˆä¿è¯ç¨³å®šæ€§å’Œå¹¶è¡Œæ€§ï¼‰ï¼Œè€Œå°†æ¨¡å‹è¡¨è¾¾åŠ›é›†ä¸­åœ¨éçº¿æ€§ã€é«˜ç§©çš„å†™å…¥è·¯å¾„ä¸Šã€‚

#### ï¼ˆ2ï¼‰Input-Anchored Loop Unrollingï¼ˆè¾“å…¥é”šå®šå¾ªç¯å±•å¼€ï¼‰
- ä¸ºäº†ç»•è¿‡æ˜¾å¼æ±‚è§£å™¨çš„ä¸²è¡Œä¾èµ–ï¼ŒPRISM å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µä»£ç†æœºåˆ¶ï¼š
  1. **çŸ­å·ç§¯é”šå®š**ï¼ˆShortConvï¼‰ï¼šç”¨å±€éƒ¨å†å²çª—å£çš„å·ç§¯è¾“å‡º $u \approx S_{t-1}k_t$ æ¥è¿‘ä¼¼å…¨å±€çŠ¶æ€äº¤äº’ï¼Œä½œä¸ºâ€œé”šç‚¹â€ã€‚
  2. **å­¦ä¹ å‹é¢„æµ‹å™¨**ï¼ˆLearned Predictorï¼‰ï¼šåŸºäºè¯¥é”šç‚¹ï¼Œå¹¶è¡Œé¢„æµ‹å¤šæ­¥è¿­ä»£ä¿®æ­£çš„æ–¹å‘å’Œå¢ç›Šï¼Œä»è€Œç›´æ¥ä¼°è®¡æœ€ç»ˆçš„é«˜ç§©æ³¨å…¥çŸ©é˜µ $B_t$ã€‚

è¿™ä½¿å¾—åŸæœ¬éœ€è¦ä¸²è¡Œæ‰§è¡Œçš„éçº¿æ€§ä¼˜åŒ–è½¨è¿¹è¢«â€œæŠ˜å â€è¿›ä¸€ä¸ªå¹¶è¡Œå¯è®¡ç®—çš„ç®—å­ä¸­ã€‚

#### ï¼ˆ3ï¼‰Rank-L Accumulationï¼ˆç§©-L ç§¯ç´¯ï¼‰
- é€šè¿‡ä¸€ä¸ªæœªå±•å¼€çš„æ®‹å·®å¾ªç¯ï¼ˆunrolled residual loopï¼‰ï¼Œé€æ­¥æ„å»ºä¸€ä¸ªç”± $L$ ä¸ªæ­£äº¤ Rank-1 åˆ†é‡ç»„æˆçš„é«˜ç§©æ›´æ–°çŸ©é˜µ $B = \sum_{l=1}^L \beta^{(l)} (\delta^{(l)} \otimes k^{(l)})$ã€‚
- ç†è®ºè¯æ˜è¯¥è®¾è®¡å®ç°äº† **Rank-L Accumulation**ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçº¿æ€§æ¨¡å‹çš„ Rank-1 ç“¶é¢ˆï¼Œç»“æ„ä¸Šé€¼è¿‘ç†æƒ³éçº¿æ€§å¢é‡è§„åˆ™çš„è§£ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | PRISM | TTT / TITANS | Linear Models (e.g., DeltaNet) |
|------|-------|---------------|-------------------------------|
| **è¡¨è¾¾åŠ›** | âœ… é«˜ï¼ˆæ¨¡æ‹Ÿå¤šæ­¥éçº¿æ€§ä¼˜åŒ–ï¼‰ | âœ… é«˜ï¼ˆçœŸå®å¤šæ­¥ä¼˜åŒ–ï¼‰ | âŒ ä½ï¼ˆå•æ­¥ Rank-1 æ›´æ–°ï¼‰ |
| **å¹¶è¡Œæ€§** | âœ… å®Œå…¨å¹¶è¡Œï¼ˆ$O(N)$ è®­ç»ƒï¼‰ | âŒ ä¸²è¡Œä¾èµ–ï¼ˆ$O(N)$ æ¨ç†å»¶è¿Ÿï¼‰ | âœ… å®Œå…¨å¹¶è¡Œ |
| **ååé‡** | â¬†ï¸ æé«˜ï¼ˆå®æµ‹ 174Ã— TTTï¼‰ | â¬‡ï¸ æä½ | â– ä¸­ç­‰ |
| **ç»“æ„åˆ›æ–°** | âœ… å°†ä¼˜åŒ–è½¨è¿¹â€œæ‘Šé”€â€ä¸ºé™æ€ç­–ç•¥ | âœ… æ˜¾å¼ä¼˜åŒ– | âŒ å¯å‘å¼æ›´æ–° |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
åœ¨å››ä¸ªä¸»æµæ¨èç³»ç»ŸåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡å¯¹**é«˜ç§©ç¨€ç–æ€§**ï¼ˆHigh-Rank Sparsityï¼‰è¦æ±‚æé«˜ï¼Œæ˜¯æ£€éªŒæ¨¡å‹è¡¨è¾¾åŠ›çš„ç†æƒ³å‹åŠ›æµ‹è¯•ï¼š
- **Amazon Books**
- **Amazon Movies**
- **Amazon Elecs**
- **Yelp**

æ•°æ®ç»Ÿè®¡è§ä¸‹è¡¨ï¼š

| Dataset | #Users | #Items | Avg. Length |
|--------|--------|--------|------------|
| Amazon_books | 31,103 | 339,960 | 106.72 |
| Amazon_movies | 9,429 | 58,636 | 121.22 |
| Amazon_elecs | 1,869 | 33,135 | 65.89 |
| Yelp | 17,233 | 126,829 | 93.17 |

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°æŒ‡æ ‡
- **æ¨èè´¨é‡**ï¼š`NDCG@K`, `Hit@K` ($K=200, 500$)ï¼Œä»¥åŠ `AUC`
- **æ•ˆç‡æŒ‡æ ‡**ï¼š**Training Throughput**ï¼ˆåƒ token/ç§’ï¼‰ï¼Œåœ¨å•å¼  NVIDIA H20 GPU ä¸Šæµ‹é‡

#### æ¨¡å‹è§„æ¨¡
ä¸»å®éªŒé‡‡ç”¨ **0.13B å‚æ•°æ¨¡å‹**ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

#### å¹¶è¡ŒåŒ–å®ç°
æ‰€æœ‰é€’å½’æ¨¡å‹å‡åŸºäº **Flash-Linear-Attention** å†…æ ¸å®ç°ï¼Œä»¥æœ€å¤§åŒ–ç¡¬ä»¶æ•ˆç‡ã€‚

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

åˆ†ä¸ºä¸‰ç±»ï¼š

#### ï¼ˆ1ï¼‰Transformer ä¸Šç•Œï¼ˆéçº¿æ€§å¼ºåŸºçº¿ï¼‰
- **SASRec**, **HSTU**ï¼šä»£è¡¨æ€§èƒ½ä¸Šé™ï¼Œä½†ä¸º $O(N^2)$

#### ï¼ˆ2ï¼‰çº¿æ€§é€’å½’æ¨¡å‹ï¼ˆå¯å‘å¼æ›´æ–°ï¼‰
- **SLA**, **GLA**, **GSA**, **MoM**, **Mamba-2**

#### ï¼ˆ3ï¼‰ä¼˜åŒ–å¯å‘æ¨¡å‹ï¼ˆOptimization-Inspiredï¼‰
- **Gated DeltaNet**, **TTT-Linear**, **ATLAS**, **TITANS**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ Amazon_books å’Œ Amazon_movies ä¸ºä¾‹ï¼‰

| Model | Amazon_books NDCG@200 | Amazon_movies NDCG@200 | AUC (Mean) | Throughput (K tok/s) |
|-------|--------------------------|--------------------------|------------|------------------------|
| SASRec (Transformer) | 0.0215 | 0.0244 | 0.8910 | ~3.8 @16K |
| HSTU (Transformer) | 0.0233 | 0.0293 | 0.7748 | ~3.8 @16K |
| TITANS (Optimization) | 0.0243 | 0.0278 | 0.9395 | ~0.34 |
| **PRISM (Ours)** | **0.0238** | **0.0289** | **0.9393** | **~57â€“61** |
| Mamba-2 | 0.0234 | 0.0276 | 0.9385 | ~60 |
| Gated DeltaNet | 0.0230 | 0.0253 | 0.9367 | ~60 |

> æ³¨ï¼šPRISM åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Š TITANS å’Œ TTTï¼Œä¸”è¿œè¶…å…¶ä»–çº¿æ€§æ¨¡å‹ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

- **è¡¨è¾¾åŠ›æ–¹é¢**ï¼š
  - PRISM æ€§èƒ½**åª²ç¾ç”šè‡³è¶…è¶Š TTT å’Œ TITANS**ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰ä¼ ç»Ÿçº¿æ€§æ¨¡å‹ï¼ˆå¦‚ SLA, GLAï¼‰ã€‚
  - åœ¨ Amazon Books ä¸Šï¼ŒPRISM çš„ AUC ä¸ SASRec å‡ ä¹æŒå¹³ï¼Œè¡¨æ˜å…¶**å‡ ä¹æ¶ˆé™¤äº†çº¿æ€§æ³¨æ„åŠ›çš„â€œè¡¨è¾¾ç¨â€**ã€‚

- **æ•ˆç‡æ–¹é¢**ï¼š
  - PRISM çš„è®­ç»ƒååé‡**é«˜è¾¾ 57â€“61K tokens/s**ï¼Œåœ¨æ•´ä¸ªåºåˆ—é•¿åº¦èŒƒå›´å†…ä¿æŒç¨³å®šã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**TTT ä»…ä¸º 0.34K tokens/s**ï¼Œ**PRISM å®ç°äº† 174Ã— çš„ååæå‡**ã€‚
  - Transformer++ åœ¨çŸ­åºåˆ—ä¸Šæœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨ 16K åºåˆ—ä¸Šå› äºŒæ¬¡å¤æ‚åº¦ä¸¥é‡é€€åŒ–ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

åœ¨ Amazon Elecs ä¸Šå¯¹ PRISM å„ç»„ä»¶è¿›è¡Œæ¶ˆèï¼š

| Model Variant | Hit@200 | AUC |
|--------------|---------|-----|
| **PRISM (Full)** | **0.1409** | **0.7134** |
| w/o Iterative Refinement (L=1) | 0.1155 | 0.6805 |
| w/o Non-Linearity | 0.1316 | 0.7047 |
| w/o ShortConv Anchor | 0.1406 | 0.7076 |
| w/o Gain Predictor | 0.1383 | 0.7098 |

#### ç»“è®ºï¼š
- **ç§»é™¤è¿­ä»£ç»†åŒ–ï¼ˆL=1ï¼‰å¯¼è‡´æœ€å¤§æ€§èƒ½ä¸‹é™**ï¼ŒéªŒè¯äº†â€œæ·±åº¦â€æ˜¯æ‰©å±•è®°å¿†å®¹é‡çš„å…³é”®ã€‚
- **ç§»é™¤éçº¿æ€§**æ˜æ˜¾é™ä½æ€§èƒ½ï¼Œè¯´æ˜ GELU ç­‰éçº¿æ€§æ¿€æ´»å¯¹å»ºæ¨¡å¤æ‚è½¨è¿¹è‡³å…³é‡è¦ã€‚
- **ç§»é™¤ ShortConv é”šç‚¹æˆ–å¢ç›Šé¢„æµ‹å™¨**ä¹Ÿå¯¼è‡´æ€§èƒ½ä¸‹æ»‘ï¼Œè¯´æ˜å±€éƒ¨ä¸Šä¸‹æ–‡é”šå®šå’Œè‡ªé€‚åº”æ­¥é•¿ä¸å¯æˆ–ç¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°

1. **â€œOne-Shot Bottleneckâ€ æ˜¯é«˜æ•ˆåºåˆ—æ¨¡å‹çš„æ ¸å¿ƒé™åˆ¶**ï¼šæ ‡å‡†çº¿æ€§æ›´æ–°æ— æ³•æ”¯æŒå¤šæ­¥è¿­ä»£ä¼˜åŒ–ï¼Œé™åˆ¶äº†è¡¨è¾¾åŠ›ã€‚
2. **Amortized Optimization æ˜¯å¯è¡Œè·¯å¾„**ï¼šæ— éœ€åœ¨æ¨ç†æ—¶æ‰§è¡ŒçœŸå®æ¢¯åº¦ä¸‹é™ï¼Œå³å¯é€šè¿‡å­¦ä¹ è¾“å…¥é”šå®šçš„ä»£ç†ç­–ç•¥æ¥â€œæ¨¡æ‹Ÿâ€ä¼˜åŒ–è½¨è¿¹ã€‚
3. **PRISM å®ç°äº†è¡¨è¾¾åŠ›ä¸æ•ˆç‡çš„ç»Ÿä¸€**ï¼š
   - è¡¨è¾¾åŠ›ä¸Šæ¥è¿‘ TTT/TITANSï¼›
   - æ•ˆç‡ä¸Šä¸æ ‡å‡†çº¿æ€§æ¨¡å‹ç›¸å½“ï¼Œ**ååé‡è¾¾ TTT çš„ 174Ã—**ï¼›
   - ç†è®ºä¸Šè¯æ˜å…¶å…·æœ‰ **Rank Accumulation** ç‰¹æ€§ï¼Œçªç ´ Rank-1 é™åˆ¶ã€‚
4. **Mechanistic Probing æ˜¾ç¤º PRISM å…·å¤‡éçº¿æ€§æ¨ç†èƒ½åŠ›**ï¼š
   - åœ¨é€»è¾‘ä»»åŠ¡ï¼ˆå¦‚ Parity, XORï¼‰ä¸Šï¼ŒPRISM æˆåŠŸè€Œ LA/MoM å¤±è´¥ï¼ˆ~50%ï¼Œéšæœºæ°´å¹³ï¼‰ï¼›
   - åœ¨ç»“æ„ä»»åŠ¡ï¼ˆå¦‚ Palindrome, Modulo Addï¼‰ä¸Šï¼ŒPRISM ç”šè‡³**è¶…è¿‡ Transformer**ï¼Œå¾—ç›Šäºå…¶å¯¹å±€éƒ¨ç»“æ„çš„å¼ºå½’çº³åç½®ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

1. **å±€éƒ¨è¿‘ä¼¼ç‰ºç‰²äº†å…¨å±€ååº”æ€§**ï¼š
   - ShortConv ä»…æ•è·å±€éƒ¨å†å²èƒ½é‡ï¼Œ**æ— æ³•æ„ŸçŸ¥é¥è¿œè¿‡å»çš„è®°å¿†çŠ¶æ€**ï¼ˆå¦‚â€œè¿™ä¸ªäº‹å®æ˜¯å¦å·²å­˜å‚¨ï¼Ÿâ€ï¼‰ã€‚
   - å› æ­¤ PRISM ä¸å…·å¤‡çœŸæ­£çš„â€œé•¿æœŸçº é”™â€èƒ½åŠ›ã€‚

2. **ä¸è§£å†³å®¹é‡å¢™é—®é¢˜**ï¼š
   - PRISM æå‡çš„æ˜¯**å†™å…¥è´¨é‡**ï¼ˆFidelityï¼‰ï¼Œè€Œé**çŠ¶æ€å®¹é‡**ï¼ˆCapacityï¼‰ã€‚
   - åœ¨å›ºå®šç»´åº¦ $d \times d$ çš„çŠ¶æ€ä¸‹ï¼Œä¿¡æ¯è¦†ç›–ä»æ˜¯ä¸å¯é¿å…çš„é—®é¢˜ã€‚

3. **ä¾èµ–é«˜è´¨é‡å±€éƒ¨ä¿¡å·**ï¼š
   - è‹¥ä»»åŠ¡çš„å…³é”®ä¿¡æ¯é«˜åº¦åˆ†æ•£æˆ–ä¾èµ–é•¿è·ç¦»ä¾èµ–ï¼ŒShortConv å¯èƒ½å¤±æ•ˆã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **Capacity Expansion + PRISM Writing**ï¼š
   - å°† PRISM ä½œä¸ºé«˜å¯†åº¦å†™å…¥ç®—å­ï¼Œä¸ **Mixture-of-Memories (MoM)** æˆ– **Gated Slot Attention (GSA)** ç­‰å®¹é‡æ‰©å±•æ¶æ„ç»“åˆã€‚

2. **Hybrid Architectures**ï¼š
   - æ„å»º **PRISM + Transformer** æ··åˆæ¨¡å‹ï¼šPRISM è´Ÿè´£æ—¥å¸¸é«˜æ•ˆæ›´æ–°ï¼ŒTransformer å±‚å®šæœŸä»‹å…¥çº æ­£â€œé•¿å°¾æ¼‚ç§»â€ã€‚

3. **åŠ¨æ€è°ƒæ•´ Refinement Depth $L$**ï¼š
   - æ ¹æ®è¾“å…¥å¤æ‚åº¦åŠ¨æ€é€‰æ‹©è¿­ä»£ç»†åŒ–å±‚æ•°ï¼Œå®ç°è®¡ç®—èµ„æºçš„è‡ªé€‚åº”åˆ†é…ã€‚

4. **æ¢ç´¢æ›´ä¼˜çš„ Anchor Mechanism**ï¼š
   - æ›¿ä»£ ShortConvï¼Œå°è¯•ä½¿ç”¨è½»é‡çº§æ³¨æ„åŠ›æˆ–ç¨€ç–æ£€ç´¢æ¥æ„å»ºæ›´å‡†ç¡®çš„çŠ¶æ€ä»£ç†ã€‚

---

> **ä»£ç å·²å¼€æº**ï¼š`github.com`ï¼ˆåŸæ–‡æåŠï¼Œå…·ä½“é“¾æ¥éœ€æŸ¥è¯ï¼‰

</details>

---

### 10. [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)

**Authors**: Cassandre Notton, Benjamin Stott, Philippe Schoeb, Anthony Walsh, Gr\'egoire Leboucher, Vincent Espitalier, Vassilis Apostolou, Louis-F\'elix Vigneux, Alexia Salavrakos, Jean Senellart  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.11092v1  

#### Abstract
Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠMerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learningã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰é¢†åŸŸå­˜åœ¨ä»¥ä¸‹å…³é”®æŒ‘æˆ˜ï¼š
- **è½¯ä»¶ç”Ÿæ€ç¢ç‰‡åŒ–**ï¼šä¸åŒæ¡†æ¶ï¼ˆå¦‚ Qiskitã€Cirqã€Percevalã€PennyLane ç­‰ï¼‰ä¸“æ³¨äºç‰¹å®šç¡¬ä»¶æˆ–èŒƒå¼ï¼ˆgate-based æˆ– photonicï¼‰ï¼Œå¯¼è‡´ç®—æ³•éš¾ä»¥è·¨å¹³å°å¤ç”¨ã€‚
- **ç¼ºä¹ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•**ï¼šå¤šæ•°ç ”ç©¶åŸºäºå­¤ç«‹çš„ç®—æ³•ææ¡ˆï¼Œä»£ç å¯å¤ç°æ€§ä½ï¼ˆphotonic QML ä¸­ä»…çº¦ 43% å¼€æºï¼‰ï¼Œä¸”å®éªŒè®¾ç½®ä¸ç»Ÿä¸€ï¼Œéš¾ä»¥å…¬å¹³æ¯”è¾ƒã€‚
- **ä»¿çœŸä¸ç¡¬ä»¶è„±èŠ‚**ï¼šè®¸å¤šæ¡†æ¶æ— æ³•åœ¨å¼ºæ¨¡æ‹Ÿï¼ˆstrong simulationï¼‰ä¸çœŸå® QPU æ‰§è¡Œä¹‹é—´æ— ç¼åˆ‡æ¢ï¼Œé™åˆ¶äº†ç®—æ³•-ç¡¬ä»¶ååŒè®¾è®¡ï¼ˆco-designï¼‰ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
ä½œè€…æå‡º **MerLin** â€”â€” ä¸€ä¸ªé¢å‘å…‰å­é‡å­æœºå™¨å­¦ä¹ ï¼ˆphotonic QMLï¼‰ä¸æ··åˆé‡å­-ç»å…¸æ¨¡å‹çš„å¼€æºæ¡†æ¶ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

#### ï¼ˆ1ï¼‰**ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒé›†æˆ**
- å°†ä¼˜åŒ–åçš„ **Strong Linear Optical Simulation (SLOS)** å¼•æ“åµŒå…¥ PyTorch å’Œ scikit-learn ç”Ÿæ€ã€‚
- æ”¯æŒ `torch.nn.Module` é£æ ¼çš„ `QuantumLayer`ï¼Œå®ç°è‡ªåŠ¨å¾®åˆ†ï¼ˆautogradï¼‰é©±åŠ¨çš„æ¢¯åº¦ä¼˜åŒ–ï¼Œé€‚ç”¨äº VQCã€kernel methodsã€reservoir computing ç­‰å¤šç§æ¶æ„ã€‚

#### ï¼ˆ2ï¼‰**ç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ï¼ˆHardware-aware Designï¼‰**
- æ”¯æŒé€šè¿‡ `MerlinProcessor` æ¥å£è¿æ¥ Quandela çš„çœŸå® photonic QPUï¼ˆå¦‚ Belenosã€Ascellaï¼‰ï¼Œå®ç°åœ¨äº‘ä¸Šè¿›è¡Œ shot-based inferenceã€‚
- æä¾› detector modelã€noise modelingã€microbatch æ§åˆ¶ç­‰æ¥å£ï¼Œè´´è¿‘å®é™…ç¡¬ä»¶çº¦æŸã€‚

#### ï¼ˆ3ï¼‰**è·¨æ¨¡æ€æ¡¥æ¥èƒ½åŠ›ï¼ˆCross-modal Interoperabilityï¼‰**
- å¼•å…¥ `QuantumBridge` æŠ½è±¡å±‚ï¼Œæ”¯æŒä» gate-based ç”µè·¯ï¼ˆå¦‚ qubit VQCï¼‰å‘ photonic ç”µè·¯è½¬æ¢ï¼ˆä¾‹å¦‚ dual-rail ç¼–ç æˆ– QLOQ æ–¹æ¡ˆï¼‰ï¼Œä¿ƒè¿› hybrid æ¶æ„æ¢ç´¢ã€‚

#### ï¼ˆ4ï¼‰**ä»¥å¤ç°ä¸ºå…ˆçš„åŸºå‡†å¹³å°ï¼ˆReproduction-first Benchmarkingï¼‰**
- æˆåŠŸå¤ç°äº† **18 ç¯‡å‰æ²¿ photonic/hybrid QML å·¥ä½œ**ï¼Œæ¶µç›– kernel methodsã€QCNNã€QLSTMã€QGANã€reservoir computing ç­‰ã€‚
- æ‰€æœ‰å¤ç°å®éªŒæ¨¡å—åŒ–å‘å¸ƒäº GitHubï¼Œä½œä¸ºå…±äº« baselineï¼Œæ¨åŠ¨ç¤¾åŒºæ ‡å‡†åŒ–è¯„ä¼°ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | MerLin | å…¶ä»–æ¡†æ¶ï¼ˆå¦‚ PennyLane, Perceval, DeepQuantumï¼‰ |
|------|--------|---------------------------------------------|
| Photonic åŸç”Ÿæ”¯æŒ | âœ… å®Œæ•´ Fock space + SLOS åŠ é€Ÿ | âš ï¸ å¤šæ•°ä¾§é‡ CV æˆ– gate-emulation |
| PyTorch æ·±åº¦é›†æˆ | âœ… åŸç”Ÿ autograd + nn.Module å…¼å®¹ | âš ï¸ æ¥å£æŠ½è±¡å±‚æ¬¡è¾ƒé«˜ï¼Œçµæ´»æ€§å—é™ |
| å¯å¾®åˆ†å¼ºæ¨¡æ‹Ÿ | âœ… æ”¯æŒ exact state computation ä¸ analytic gradients | âš ï¸ å¤šæ•°ä»…æ”¯æŒé‡‡æ ·æˆ–å¼±æ¨¡æ‹Ÿ |
| çœŸå® QPU è”åŠ¨ | âœ… æ”¯æŒè¿œç¨‹è°ƒç”¨ Quandela QPU | âœ… Perceval æ”¯æŒï¼Œä½†æ—  ML ç”Ÿæ€æ•´åˆ |
| ç»Ÿä¸€åŸºå‡†å¹³å° | âœ… å†…å»º 18 é¡¹å¤ç°å®éªŒ | âŒ å¤šæ•°æ— ç³»ç»Ÿæ€§ benchmark æ”¯æŒ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä½¿ç”¨æƒ…å†µ
MerLin çš„å¤ç°å®éªŒè¦†ç›–å¤šä¸ªæ ‡å‡† ML ä»»åŠ¡ä¸æ•°æ®é›†ï¼š

| ä»»åŠ¡ç±»åˆ« | ä½¿ç”¨æ•°æ®é›† |
|--------|-----------|
| å›¾åƒåˆ†ç±» | MNIST, CIFAR-10ï¼ˆå‰5ç±»ï¼‰, Custom BASï¼ˆäºŒå€¼å›¾åƒï¼‰ |
| æ–‡æœ¬æƒ…æ„Ÿåˆ†æ | SST-2ï¼ˆSentiment Analysisï¼‰ |
| åˆæˆæ•°æ®é›† | Spiral datasetï¼ˆå¸¦å™ªå£°ï¼‰ã€Moon datasetï¼ˆéçº¿æ€§å¯åˆ†ï¼‰ |
| æ—¶é—´åºåˆ—é¢„æµ‹ | Lorenz systemï¼ˆæ··æ²ŒåŠ¨åŠ›å­¦ï¼‰ |
| ç”Ÿæˆä»»åŠ¡ | MNISTï¼ˆpatch-based generation for QGANï¼‰ |

> æ³¨ï¼šéƒ¨åˆ†å®éªŒä½¿ç”¨åˆæˆæ•°æ®ä»¥éªŒè¯æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼ˆexpressivityï¼‰è€Œéè¿½æ±‚ SOTA æ€§èƒ½ã€‚

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **è®­ç»ƒæµç¨‹**ï¼šé‡‡ç”¨æ ‡å‡† PyTorch è®­ç»ƒå¾ªç¯ï¼ˆAdam optimizer, CrossEntropyLossï¼‰ï¼Œæ”¯æŒ batch trainingã€‚
- **æµ‹é‡ç­–ç•¥**ï¼š
  - `probs`: è¾“å‡ºå…¨æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºåˆ†ç±»ï¼‰
  - `partial measurement`: å±€éƒ¨æ¨¡å¼æµ‹é‡ï¼ˆç”¨äº QCNN poolingï¼‰
  - `amplitudes`: è¿”å›é‡å­æ€æŒ¯å¹…ï¼ˆç”¨äº kernel æ„é€ ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - åˆ†ç±»å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰
  - SSIMï¼ˆç”¨äºå›¾åƒç”Ÿæˆè´¨é‡ï¼‰
  - å‚æ•°æ•ˆç‡ï¼ˆ#params è¾¾åˆ° â‰¥90% å‡†ç¡®ç‡ï¼‰
  - è®­ç»ƒé€Ÿåº¦ï¼ˆepoch time, speedup ratioï¼‰
  - å¯¹æŠ—é²æ£’æ€§ï¼ˆclean vs. adversarial accuracyï¼‰

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
MerLin åœ¨å¤ç°è¿‡ç¨‹ä¸­ä¸ä»¥ä¸‹åŸºçº¿è¿›è¡Œäº†æ¨ªå‘æ¯”è¾ƒï¼š
- **Classical baselines**ï¼šä¼ ç»Ÿ CNNã€RNNã€SVMã€ResNet
- **Gate-based QML**ï¼šQiskit å®ç°çš„ QNNã€QLSTMã€qSSL
- **Photonic-only simulators**ï¼šåŸå§‹ Perceval å®ç°
- **Hybrid models**ï¼šHQNNã€DQNNã€QRKD

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»ï¼ˆæ¥è‡ª Table Iï¼‰

| æ–¹æ³• | ä»»åŠ¡ | MerLin ç»“æœ | åŸå§‹æ–‡çŒ®ç»“æœ | æå‡/å·®å¼‚ |
|------|------|------------|--------------|----------|
| **Photonic QCNN [19]** | MNIST (0 vs 1) | **98.8Â±1.0%** | 93.1Â±3.6% | â†‘ ~5.7% |
| | Custom BAS | **98.2Â±2.2%** | 92.7Â±2.1% | â†‘ ~5.5% |
| **qSSL [29]** | CIFAR-10 (5 classes) | **49.2% acc**, Ã—0.97 speed | 48.4% (Qiskit), Ã—0.08 speed | æ›´å¿« + æ›´é«˜ç²¾åº¦ |
| **QLSTM [31]** | Function fitting | ä¸ gate-based QLSTM ç›¸å½“ | â€” | éªŒè¯ photonic æ›¿ä»£å¯è¡Œæ€§ |
| **QGAN [37]** | MNIST generation | ç›¸åŒ SSIMï¼Œ**è®­ç»ƒæé€Ÿè¾¾ 15Ã—** | â€” | ä¼˜åŒ–å™¨å‡çº§ï¼ˆSGD > SPSAï¼‰å¸¦æ¥æ˜¾è‘—åŠ é€Ÿ |
| **DQNN [34]** | MNIST åˆ†ç±» | æµ‹è¯•å‡†ç¡®ç‡å·® <4%ï¼Œ**è®­ç»ƒ epoch å‡å°‘ 4Ã—** | â€” | ADAM æ›¿æ¢ COBYLA æ˜¾è‘—æå‡æ•ˆç‡ |
| **Fidelity Kernel [15]** | Kernel learning | éšè®­ç»ƒé›†å¢å¤§æ€§èƒ½æå‡è¶‹åŠ¿ä¸€è‡´ | â€” | éªŒè¯ photonic å®ç°æœ‰æ•ˆæ€§ |
| **Adversarial Robustness [39]** | MNIST | Clean: ~98%, Adv(BIM Îµ=0.1): ~15% â†’ **å¯¹æŠ—è®­ç»ƒåæ¢å¤è‡³ ~95%** | â€” | è¡¨æ˜å¯¹æŠ—è®­ç»ƒæœ‰æ•ˆ |

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰
- **ç¼–ç æ–¹å¼å¯¹é²æ£’æ€§å½±å“**ï¼š
  - **Amplitude encoding**ï¼šææ˜“å—å°æ‰°åŠ¨æ”»å‡»ï¼ˆÎµ å¾ˆå°æ—¶æ€§èƒ½éª¤é™ï¼‰
  - **Angle encoding**ï¼šè¡¨ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—é²æ£’æ€§
  - â **å»ºè®®åœ¨å®‰å…¨æ•æ„Ÿåœºæ™¯ä¼˜å…ˆä½¿ç”¨ angle encoding**

- **ç»´åº¦å‹ç¼© vs. è¡¨è¾¾åŠ›æƒè¡¡**ï¼š
  - å¯¹ amplitude encoding æ–½åŠ  PCAï¼ˆ256â†’64ï¼‰å¯ç¼“è§£è¿‡æ‹Ÿåˆå¹¶æé«˜é²æ£’æ€§ï¼Œä½†ç‰ºç‰² high-dimensional representational powerã€‚

- **å…‰å­æ•°é‡ä¸è¡¨è¾¾åŠ›å…³ç³»**ï¼š
  - å¤ç° [14] å‘ç°ï¼šå¢åŠ è¾“å…¥å…‰å­æ•° `n` æ˜¾è‘—æå‡ VQC çš„ Fourier é¢‘è°±å®½åº¦ï¼Œä»è€Œå¢å¼ºæ¨¡å‹ expressivity â€”â€” **å…‰å­æ•°æ˜¯æ§åˆ¶æ¨¡å‹å®¹é‡çš„å…³é”®è¶…å‚**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Photonic QML æ¨¡å‹å¯åœ¨å¤šç§ä»»åŠ¡ä¸­åª²ç¾ç”šè‡³è¶…è¶Š gate-based QML**ï¼š
   - åœ¨ç›¸åŒä»»åŠ¡ä¸‹ï¼Œphotonic-native å®ç°ï¼ˆå¦‚ MerLinï¼‰ç›¸æ¯” Qiskit å®ç°ä¸ä»…æ›´å¿«ï¼Œæœ‰æ—¶è¿˜èƒ½å–å¾—æ›´é«˜å‡†ç¡®ç‡ï¼ˆå¦‚ qSSLï¼‰ã€‚
   
2. **MerLin æ˜¾è‘—æå‡äº†ä»¿çœŸæ•ˆç‡ä¸å¼€å‘ä¾¿æ·æ€§**ï¼š
   - åˆ©ç”¨ TorchScript ç¼–è¯‘ä¸ç¨€ç–è®¡ç®—å›¾ä¼˜åŒ–ï¼ŒSLOS ä»¿çœŸé€Ÿåº¦ç›¸æ¯”åŸç”Ÿ Perceval æå‡å¯è¾¾ **æ•°ä¸ªæ•°é‡çº§**ã€‚
   - PyTorch åŸç”Ÿé›†æˆæå¤§ç®€åŒ–äº†è°ƒè¯•ã€å¯è§†åŒ–ä¸æ··åˆå»ºæ¨¡æµç¨‹ã€‚

3. **ç³»ç»Ÿæ€§å¤ç°æ­ç¤ºâ€œé‡å­ä¼˜åŠ¿â€éœ€è°¨æ…è§£è¯»**ï¼š
   - å¤šæ•°æ‰€è°“â€œé‡å­ä¼˜è¶Šâ€å®åˆ™æºäºæ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å·¥ç¨‹æˆ–ä¼˜åŒ–æŠ€å·§ï¼Œè€Œéæœ¬è´¨é‡å­æœºåˆ¶ã€‚
   - MerLin æä¾›é€æ˜å®éªŒç¯å¢ƒï¼Œæœ‰åŠ©äºå‰¥ç¦»è™šå‡å¢ç›Šï¼Œèšç„¦çœŸæ­£æœ‰æ„ä¹‰çš„æ”¹è¿›ã€‚

4. **Photon-count æ˜¯ photonic QML çš„å…³é”®èµ„æºå‚æ•°**ï¼š
   - è¾“å…¥å…‰å­æ•°ç›´æ¥å½±å“ Hilbert space dimension ä¸æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œæ˜¯ photonic å¹³å°ç‰¹æœ‰çš„ç¼©æ”¾ç»´åº¦ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **æ¨¡æ‹Ÿè§„æ¨¡å—é™**ï¼šç”±äº SLOS éœ€å­˜å‚¨å®Œæ•´ Fock state vectorï¼Œç›®å‰å®ç”¨ä¸Šé™çº¦ä¸º **n â‰¤ 20 å…‰å­**ï¼Œè¶…å‡ºåå†…å­˜çˆ†ç‚¸ã€‚
- **ä»…æ”¯æŒç¦»æ•£å˜é‡ï¼ˆDVï¼‰å…‰å­è®¡ç®—**ï¼šæš‚æœªæ¶µç›– continuous-variableï¼ˆCVï¼‰æ¨¡å‹ï¼ˆå¦‚ Gaussian boson samplingï¼‰ã€‚
- **ä¾èµ– Quandela ç¡¬ä»¶ç”Ÿæ€**ï¼šç›®å‰ä»…æ”¯æŒå…¶è‡ªæœ‰ QPUï¼Œå°šæœªæ¥å…¥å…¶ä»– photonic å¹³å°ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- **æ‰©å±•è‡³æ›´å¤§è§„æ¨¡æ¨¡æ‹Ÿ**ï¼šå¼•å…¥è¿‘ä¼¼æ–¹æ³•ï¼ˆå¦‚ tensor network truncationã€Monte Carlo samplingï¼‰çªç ´ exact simulation é™åˆ¶ã€‚
- **æ”¯æŒæ›´å¤šç¡¬ä»¶åç«¯**ï¼šæ¥å…¥å…¶ä»– photonic æˆ– hybrid QPUï¼ˆå¦‚ Xanadu, PsiQuantumï¼‰ã€‚
- **æ„å»ºè‡ªåŠ¨åŒ– benchmark suite**ï¼šå»ºç«‹ç±»ä¼¼ MLPerf çš„é‡å­æœºå™¨å­¦ä¹ è¯„æµ‹æ ‡å‡†ï¼Œæ¨åŠ¨ç¤¾åŒºè§„èŒƒåŒ–ã€‚
- **æ¢ç´¢ emergent å…‰å­å…ƒä»¶**ï¼šå¦‚ photonic memristorã€nonlinear elementsï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹éçº¿æ€§ä¸è®°å¿†èƒ½åŠ›ã€‚

---

> ğŸ”— **é¡¹ç›®åœ°å€**ï¼š  
> - MerLin æ¡†æ¶: [https://github.com/merlinquantum/merlin](https://github.com/merlinquantum/merlin)  
> - å¤ç°å®éªŒä»“åº“: [https://github.com/merlinquantum/reproduced_papers](https://github.com/merlinquantum/reproduced_papers)

ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
MerLin ä¸åªæ˜¯ä¸€ä¸ª simulatorï¼Œè€Œæ˜¯ä¸€ä¸ª **benchmark-drivenã€hardware-awareã€ML-ecosystem-native çš„ discovery engine**ï¼Œæ—¨åœ¨æ¨åŠ¨ photonic QML è¿›å…¥å¯å¤ç°ã€å¯æ¯”è¾ƒã€å¯ååŒè®¾è®¡çš„æ–°é˜¶æ®µã€‚

</details>

---

### 11. [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)

**Authors**: Xuecheng Zou, Yu Tang, Bingbing Wang  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.10845v1  

#### Abstract
Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile di...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰çš„ **Knowledge Graph Completion (KGC)** æ–¹æ³•åœ¨èåˆé¢„è®­ç»ƒè¯­ä¹‰ï¼ˆå¦‚ PLMsï¼‰ä¸å›¾æ‹“æ‰‘ç»“æ„ï¼ˆå¦‚ GNNsï¼‰æ—¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§ç“¶é¢ˆï¼š**è¢«åŠ¨çš„ç»“æ„-è¯­ä¹‰å¯¹é½**ï¼ˆPassive Structural-Semantic Alignmentï¼‰ã€‚è¿™å¯¼è‡´ä¸¤å¤§ç—…ç†ç°è±¡ï¼š
- **Representation Collapse**ï¼šåœ¨ç¨€ç–å­å›¾ä¸­ï¼Œå®ä½“ç¼ºä¹è¶³å¤Ÿçš„æ‹“æ‰‘æ”¯æ’‘ï¼Œè¡¨ç¤ºåç¼©ï¼›
- **Identity Redundancy**ï¼šåœ¨å¯†é›†ç°‡ä¸­ï¼Œæ˜¾å¼è‡ªç¯å¼•å…¥ä¸å¯çº¦çš„å™ªå£°ã€‚

æ­¤å¤–ï¼Œå¤šæ•°æ¨¡å‹å­˜åœ¨ **Inference-time Distribution Shift** â€”â€”è®­ç»ƒæ—¶ä½¿ç”¨ç»“æ„ä¿¡æ¯ï¼Œæ¨ç†æ—¶ç¦ç”¨ï¼Œé€ æˆè¡¨å¾æ¼‚ç§»ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡º **SynergyKGC**ï¼Œä¸€ç§é«˜ç²¾åº¦ã€æŒ‡ä»¤é©±åŠ¨çš„åŒæ¨¡æ€ååŒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**Cross-Modal Synergy Expert**
- å¼•å…¥ **relation-aware cross-attention** æœºåˆ¶ï¼Œå°†è¯­ä¹‰å‘é‡ä½œä¸ºâ€œæŸ¥è¯¢æŒ‡ä»¤â€ä¸»åŠ¨æ£€ç´¢å¹¶è¿‡æ»¤æ‹“æ‰‘ä¸Šä¸‹æ–‡ï¼›
- æ›¿ä»£ä¼ ç»Ÿ GNN çš„è¢«åŠ¨é‚»å±…èšåˆï¼Œå®ç°åŠ¨æ€ã€æœ‰é€‰æ‹©æ€§çš„ç»“æ„ä¿¡æ¯èåˆã€‚

#### ï¼ˆ2ï¼‰**Dual-Axis Consistency Principle**
- **æ¶æ„è½´ä¸€è‡´æ€§**ï¼ˆArchitectural Axisï¼‰ï¼šQuery Tower ä¸ Entity Tower åœ¨ç»“æ„èåˆä¸Šä¿æŒå¯¹ç§°ååŒï¼›
- **ç”Ÿå‘½å‘¨æœŸè½´ä¸€è‡´æ€§**ï¼ˆLifecycle Axisï¼‰ï¼šè®­ç»ƒä¸æ¨ç†é˜¶æ®µå‡æ¿€æ´» Synergy Expertï¼Œæ¶ˆé™¤åˆ†å¸ƒåç§»ã€‚

#### ï¼ˆ3ï¼‰**Density-Aware Identity Anchoring (IA) ç­–ç•¥**
- æå‡ºâ€œç»“æ„å³èº«ä»½â€ï¼ˆStructure ~ Identityï¼‰ç°è±¡ç†è®ºï¼šåœ¨ç¨ å¯†å›¾ä¸­å†—ä½™çš„èº«ä»½ä¿¡å·å¯è¢«æŠ‘åˆ¶ï¼Œåœ¨ç¨€ç–å›¾ä¸­åˆ™éœ€ä¿ç•™ä»¥æä¾›ä½ç½®é”šç‚¹ï¼›
- è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦åŠ å…¥å®ä½“è‡ªèº«åµŒå…¥ï¼ˆidentity anchorï¼‰ï¼Œç¼“è§£ç¨€ç–ä¸‹çš„åç¼©ä¸ç¨ å¯†ä¸­çš„è¿‡å¹³æ»‘ã€‚

#### ï¼ˆ4ï¼‰**Instantaneous "Catch-up Effect"**
- æ¨¡å‹æ— éœ€é•¿å‘¨æœŸ warm-upï¼ˆä¼ ç»Ÿæ–¹æ³•å¸¸ >30 epochsï¼‰ï¼Œåœ¨ Synergy Expert æ¿€æ´»åç«‹å³è§¦å‘åŒå‘åŒæ­¥ï¼Œå¿«é€Ÿæ”¶æ•›è‡³é«˜æ€§èƒ½çŠ¶æ€ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ ProgKGC, SimKGCï¼‰ | SynergyKGC |
|------|-------------------------------|-----------|
| ç»“æ„èåˆæ–¹å¼ | è¢«åŠ¨æ·»åŠ ã€é™æ€èåˆ | ä¸»åŠ¨æ£€ç´¢ã€æŒ‡ä»¤é©±åŠ¨ |
| è®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§ | ä¸ä¸€è‡´ï¼ˆæ¨ç†å»ç»“æ„åŒ–ï¼‰ | ä¸¥æ ¼ä¸€è‡´ï¼ˆåŒå¡”å®æ—¶ååŒï¼‰ |
| å¯†åº¦é€‚åº”èƒ½åŠ› | å›ºå®šç­–ç•¥ï¼Œéš¾ä»¥å…¼é¡¾ç¨€ç–/ç¨ å¯† | åŠ¨æ€åˆ‡æ¢ï¼Œå¯†åº¦æ„ŸçŸ¥é”šå®š |
| æ”¶æ•›é€Ÿåº¦ | éœ€è¦é•¿æ—¶é—´è¯­ä¹‰é¢„çƒ­ | å¿«é€Ÿâ€œè¿½èµ¶æ•ˆåº”â€ï¼Œæ˜¾è‘—æé€Ÿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
åœ¨ä¸¤ä¸ªæ ‡å‡† benchmark ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå…·æœ‰æåŒ–æ‹“æ‰‘ä¸è¯­ä¹‰ç‰¹æ€§ï¼š

| æ•°æ®é›† | ç‰¹å¾æè¿° |
|--------|---------|
| **FB15k-237** | ç¨ å¯†å…³ç³»å›¾ï¼ˆP50=22ï¼‰ï¼Œæ–‡æœ¬ä¸°å¯Œï¼ˆå¹³å‡ 114.6 tokensï¼‰ |
| **WN18RR** | å±‚æ¬¡ç¨€ç–å›¾ï¼ˆP50=3ï¼‰ï¼Œè¯­ä¹‰ç®€æ´ï¼ˆå¹³å‡ 17.1 tokensï¼‰ |

> è¡¨æ ¼è§åŸæ–‡ Table 1ï¼Œæ¶µç›– #Entities, #Relations, å¯†åº¦åˆ†ä½æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ã€‚

---

### ğŸ§ª å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼š
  - è¯­ä¹‰ç¼–ç å™¨ï¼š`bert-base-uncased`
  - ååŒä¸“å®¶ï¼š4-head Synergy Expertï¼ˆd=768ï¼‰
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œbatch size=768ï¼Œå•å¡ A100
- **å­¦ä¹ ç‡**ï¼šä» {1e-5, 5e-5, 5e-4} ä¸­è°ƒä¼˜
- **æ¸©åº¦å‚æ•°**ï¼šT=0.05
- **Dropout**ï¼š0.1
- **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼š
  - Phase Iï¼šä»…è¯­ä¹‰è®­ç»ƒï¼ˆcontrastive lossï¼‰
  - Phase IIï¼šæ¿€æ´» Synergy Expertï¼Œè”åˆä¼˜åŒ–ï¼ˆNCE + MSE alignmentï¼‰

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
é‡‡ç”¨æ ‡å‡† **filtered setting** ä¸‹çš„æ’åºæŒ‡æ ‡ï¼š
- **MRR**ï¼ˆMean Reciprocal Rankï¼‰
- **Hits@k**ï¼ˆk âˆˆ {1, 3, 10}ï¼‰
- **MR**ï¼ˆMean Rankï¼Œç”¨äºè¡¡é‡å…¨å±€æ’åºç¨³å®šæ€§ï¼‰

---

### âš”ï¸ åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¶µç›–ä¸»æµ KGC èŒƒå¼ï¼š
- **Embedding-based**ï¼šTransE, RotatE, DistMult, ComplEx, TuckER
- **PLM-based / Hybrid**ï¼šKG-BERT, StAR, LP-BERT, KG-S2S, SimKGC, ProgKGC

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 2ï¼‰

| Method | FB15k-237 (MRR / H@1 / H@3 / H@10) | WN18RR (MRR / H@1 / H@3 / H@10) |
|--------|-------------------------------------|----------------------------------|
| **ProgKGC (SOTA)** | 34.4 / 25.5 / 37.5 / 52.3 | 68.2 / 59.7 / 73.9 / 83.4 |
| **SynergyKGC (Ours)** | **39.9 / 30.2 / 43.6 / 59.4** | **74.2 / 67.7 / 78.5 / 85.5** |
| **Improvement** | **+5.5 / +4.7 / +6.1 / +7.1** | **+6.0 / +8.0 / +4.6 / +2.1** |

> ğŸ’¡ **æ ¸å¿ƒçªç ´**ï¼šåœ¨ **WN18RR ä¸Š Hits@1 æå‡ +8.0%**ï¼Œåˆ›ä¸‹æ–° SOTAã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆTable 4 & Figure 4ï¼‰

#### ï¼ˆ1ï¼‰ç»„ä»¶æ¶ˆèåˆ†æï¼ˆAblation Studyï¼‰
ç§»é™¤å…³é”®æ¨¡å—åçš„æ€§èƒ½ä¸‹é™æ˜¾è‘—ï¼š

| æ¨¡å— | WN18RR (H@1) ä¸‹é™ | FB15k-237 (H@1) ä¸‹é™ | è¯´æ˜ |
|------|------------------|--------------------|------|
| w/o **Cross-Attention** | â†“25.7% | â†“1.8% | å¯¹ç¨€ç–å›¾è‡³å…³é‡è¦ï¼Œâ€œå®šä½æ”¯æ¶â€ |
| w/o **Adaptive Gate** | â†“16.5% | â†“2.9% | æŠ‘åˆ¶ç¨ å¯†å›¾å™ªå£°çš„å…³é”®æ»¤æ³¢å™¨ |
| w/o **MSE Alignment** | â†“1.7% | â†“1.7% | ç»´æŒè¯­ä¹‰ä¸€è‡´æ€§åŸºç¡€ |

> è¡¨æ˜å„æ¨¡å—åœ¨ä¸åŒå¯†åº¦åœºæ™¯ä¸‹å‘æŒ¥å·®å¼‚åŒ–ä½œç”¨ï¼ŒéªŒè¯ topology-aware è®¾è®¡å¿…è¦æ€§ã€‚

---

#### ï¼ˆ2ï¼‰Dual-Tower Consistency å½±å“ï¼ˆFigure 4ï¼‰
- è‹¥åªåœ¨ Query Tower å¯ç”¨ Synergyï¼ŒEntity Tower ä¿æŒé™æ€ â†’ æ€§èƒ½å¤§å¹…ä¸‹é™ï¼›
- åªæœ‰åœ¨ **è®­ç»ƒä¸æ¨ç†é˜¶æ®µéƒ½å¯ç”¨åŒå¡”ååŒ**ï¼Œæ‰èƒ½é¿å… distribution shiftï¼Œè¾¾åˆ°æœ€ä¼˜ã€‚

---

#### ï¼ˆ3ï¼‰Topological Scaling åˆ†æï¼ˆTable 3ï¼‰
| æ•°æ®é›† | æœ€ä¼˜è·³æ•° | è§‚å¯Ÿ |
|-------|--------|------|
| **FB15k-237 (Dense)** | 2-hop | å±€éƒ¨é‚»åŸŸå³å¯è§£æ­§ |
| **WN18RR (Sparse)** | 1-hopï¼ˆç²¾åº¦ï¼‰ä½† 5-hopï¼ˆå…¨å±€æ’åºï¼‰ | æµ…å±‚é”šå®š top-tierï¼Œæ·±å±‚æå‡ global rankingï¼ˆMR æ˜¾è‘—æ”¹å–„ï¼‰ |

---

#### ï¼ˆ4ï¼‰Identity Anchoring æ•æ„Ÿæ€§åˆ†æï¼ˆTable 5ï¼‰
- åœ¨ **WN18RR** ä¸­ï¼Œä¸è®¾é˜ˆå€¼ï¼ˆä¿ç•™æ‰€æœ‰ identityï¼‰æ•ˆæœæœ€ä½³ï¼ˆMRR=74.2ï¼‰ï¼›
- åœ¨ **FB15k-237** ä¸­ï¼Œä»…å½“ `threshold=1` æ—¶æœ€ä¼˜ï¼Œæ›´é«˜é˜ˆå€¼å¼•å…¥å™ªå£°ï¼›
- è¯æ˜ IA ç­–ç•¥å¿…é¡»ä¾èµ–å›¾æœ¬èº«çš„æ‹“æ‰‘å¯†åº¦ï¼Œè€Œéç»Ÿä¸€è®¾å®šã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ç»“æ„æ€§å¼‚è´¨æ€§æ˜¯ KGC çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼šä¸èƒ½ç”¨ç»Ÿä¸€ç­–ç•¥å¤„ç†ç¨€ç–ä¸ç¨ å¯†å­å›¾ã€‚
2. **ä¸»åŠ¨ååŒä¼˜äºè¢«åŠ¨èåˆ**ï¼šé€šè¿‡è¯­ä¹‰æ„å›¾å¼•å¯¼ç»“æ„æ£€ç´¢ï¼Œå¤§å¹…æå‡æ•ˆç‡ä¸ç²¾åº¦ã€‚
3. **Dual-Axis Consistency æ˜¯ç¨³å®šæ€§çš„åŸºçŸ³**ï¼šæ¶æ„å¯¹ç§° + è®­æ¨ä¸€è‡´ï¼Œæœ‰æ•ˆé˜²æ­¢ manifold driftã€‚
4. **Identity Anchoring å…·æœ‰å¯†åº¦ä¾èµ–æ€§**ï¼šâ€œç»“æ„å³èº«ä»½â€ç†è®ºæˆç«‹ï¼Œä¸ºç¨€ç–å›¾æä¾›å¿…è¦é”šç‚¹ã€‚
5. **SynergyKGC å®ç°å¿«é€Ÿæ”¶æ•›**ï¼šå¾—ç›Šäºâ€œcatch-up effectâ€ï¼Œå¯åœ¨æ›´å°‘ epoch å†…è¶…è¶Šéœ€é•¿ warm-up çš„æ¨¡å‹ã€‚

---

### âš ï¸ æ–¹æ³•å±€é™æ€§
- å½“å‰è®¾è®¡ä¾èµ–äºé«˜è´¨é‡çš„ entity description æ–‡æœ¬ï¼Œè‹¥æ–‡æœ¬ç¼ºå¤±æˆ–ä½è´¨ï¼Œè¯­ä¹‰ä¸“å®¶æ€§èƒ½å—é™ï¼›
- è™½ç„¶æ”¯æŒå¤šè·³èšåˆï¼Œä½†è®¡ç®—å¤æ‚åº¦éšé‚»å±…æ•°é‡å¢é•¿ï¼Œæç«¯ hub èŠ‚ç‚¹å¯èƒ½å½±å“æ•ˆç‡ï¼›
- å½“å‰å®éªŒé›†ä¸­äºé™æ€ KGï¼Œæœªè€ƒè™‘æ—¶é—´åŠ¨æ€æ€§ï¼ˆTemporal KGCï¼‰æ‰©å±•ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **Temporal Knowledge Graphs**ï¼Œç»“åˆæ—¶åºæ³¨æ„åŠ›æœºåˆ¶ï¼›
- æ¢ç´¢ **few-shot / zero-shot KGC** åœºæ™¯ä¸‹çš„è¿ç§»èƒ½åŠ›ï¼›
- è¿›ä¸€æ­¥è½»é‡åŒ– Synergy Expertï¼Œæå‡å¤§è§„æ¨¡éƒ¨ç½²æ•ˆç‡ï¼›
- å°†â€œæŒ‡ä»¤é©±åŠ¨ååŒâ€èŒƒå¼æ¨å¹¿è‡³å…¶ä»–å¤šæ¨¡æ€ç»“æ„åŒ–ä»»åŠ¡ï¼ˆå¦‚æ¨èç³»ç»Ÿã€ç”Ÿç‰©ç½‘ç»œæ¨ç†ï¼‰ã€‚

---

## âœ… æ€»ç»“
SynergyKGC é€šè¿‡æå‡º **æŒ‡ä»¤é©±åŠ¨çš„è·¨æ¨¡æ€ååŒæœºåˆ¶** å’Œ **åŒé‡ä¸€è‡´æ€§åŸåˆ™**ï¼ŒæˆåŠŸè§£å†³äº† KGC ä¸­é•¿æœŸå­˜åœ¨çš„æ‹“æ‰‘å¼‚è´¨æ€§éš¾é¢˜ã€‚å…¶å®éªŒè¡¨ç°ä¸ä»…åˆ·æ–°äº† WN18RR ç­‰ç¨€ç–åŸºå‡†çš„è®°å½•ï¼ˆHits@1 +8.0%ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯æ­ç¤ºäº†ä¸€ä¸ªé€šç”¨åŸåˆ™ï¼š**åœ¨éå‡åŒ€ç»“æ„æ•°æ®ä¸­ï¼Œé²æ£’çš„ä¿¡æ¯æ•´åˆå¿…é¡»æ˜¯è‡ªé€‚åº”ã€åŠ¨æ€ä¸”è®­ç»ƒ-æ¨ç†ä¸€è‡´çš„**ã€‚è¯¥å·¥ä½œä¸ºä¸‹ä¸€ä»£é«˜ç²¾åº¦ KGC æ¡†æ¶æä¾›äº†é‡è¦èŒƒå¼å‚è€ƒã€‚

> ğŸ”— å¼€æºåœ°å€ï¼š[https://github.com/XuechengZou-2001/SynergyKGC-main](https://github.com/XuechengZou-2001/SynergyKGC-main)

</details>

---

### 12. [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)

**Authors**: Lipai Huang, Kai Yin, Chia-Fu Liu, Ali Mostafavi  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.10312v1  

#### Abstract
R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each s...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šR2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿæ´ªæ°´ç¾æŸé¢„æµ‹æ¨¡å‹ï¼ˆå¦‚ FloodDamageCastï¼‰ä¾èµ–**ç›‘ç£å­¦ä¹ **ï¼Œéœ€è¦ä¸ºæ¯ä¸ªåŒºåŸŸå•ç‹¬æ”¶é›†æ•°æ®ã€ç‰¹å¾å·¥ç¨‹ã€è®­ç»ƒå’Œè°ƒä¼˜æ¨¡å‹ï¼Œä¸”éœ€å¤„ç†ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚è¿™å¯¼è‡´å…¶åœ¨æ–°åŒºåŸŸéƒ¨ç½²æ—¶æˆæœ¬é«˜ã€æ‰©å±•æ€§å·®ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- å¦‚ä½•åœ¨ä¸è¿›è¡Œä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼ˆtask-specific trainingï¼‰çš„å‰æä¸‹å®ç°é«˜ç²¾åº¦ç¾æŸ nowcastingï¼›
- å¦‚ä½•æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ¨ç†é€æ˜åº¦ï¼›
- å¦‚ä½•åœ¨èµ„æºå—é™åœºæ™¯ä¸‹å®ç°é«˜æ•ˆéƒ¨ç½²ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡º **R2RAG-Flood** â€”â€” ä¸€ç§**æ— éœ€è®­ç»ƒçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶**ï¼ˆtraining-free Retrieval-Augmented Generation, RAGï¼‰ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åœ°ç†ç©ºé—´æ¨ç†æœºåˆ¶ï¼Œç”¨äºæ´ªæ¶è´¢äº§æŸå¤±ç¨‹åº¦ï¼ˆProperty Damage Extent, PDEï¼‰çš„å®æ—¶é¢„æµ‹ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **æ„å»ºâ€œä»¥æ¨ç†ä¸ºä¸­å¿ƒâ€çš„çŸ¥è¯†åº“**ï¼ˆreasoning-centric knowledge baseï¼‰
   - å¯¹å†å²æ ‡æ³¨æ ·æœ¬ï¼Œä¸ä»…å­˜å‚¨ç»“æ„åŒ–ç‰¹å¾å’Œæ ‡ç­¾ï¼Œè¿˜åˆ©ç”¨ LLM è‡ªåŠ¨ç”ŸæˆåŒ…å«å®Œæ•´é€»è¾‘é“¾çš„ `<think>...</think>` æ¨ç†è½¨è¿¹ã€‚
   - æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š`{tabular predictors, text-mode summary, reasoning trajectory}`ã€‚
   - æ”¯æŒåç»­é€šè¿‡æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹çš„â€œæ€è€ƒè¿‡ç¨‹â€æ¥å¼•å¯¼æ–°æ ·æœ¬é¢„æµ‹ã€‚

2. **åŸºäºåˆ†å¸ƒå·®å¼‚åˆ†æçš„ç‰¹å¾æ’åºä¸æç¤ºä¼˜åŒ–**
   - ä½¿ç”¨ **Jensen-Shannon (JS)** å’Œ **Kolmogorov-Smirnov (KS)** ç»Ÿè®¡é‡é‡åŒ–å„ç‰¹å¾åœ¨ä¸åŒ PDE ç±»åˆ«é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚
   - æ„å»º **divergence-informed feature profile**ï¼Œç”¨äºï¼š
     - æ–‡æœ¬æ‘˜è¦ä¸­çš„ç‰¹å¾æ’åºï¼ˆé‡è¦ç‰¹å¾ä¼˜å…ˆæè¿°ï¼‰ï¼›
     - è‡ªç”±æ ·æœ¬ï¼ˆfree-shotï¼‰é€‰æ‹©ä¾æ®ï¼›
     - ä¸‹é™è§„åˆ™ï¼ˆdowngrade rulesï¼‰è§¦å‘çº¿ç´¢ã€‚

3. **ä¸¤é˜¶æ®µé¢„æµ‹ + æ¡ä»¶é™çº§æœºåˆ¶**ï¼ˆConditional Downgrade Mechanismï¼‰
   - é¢„æµ‹åˆ†ä¸¤æ­¥ï¼šå…ˆåˆ¤æ–­æ˜¯å¦å‘ç”ŸæŸåï¼ˆoccurrenceï¼‰ï¼Œå†åˆ¤æ–­ä¸¥é‡ç¨‹åº¦ï¼ˆseverityï¼‰ã€‚
   - å¼•å…¥åŸºäºè§„åˆ™çš„**è‡ªåŠ¨é™çº§æ£€æŸ¥**ï¼šå½“ `<think>` ä¸­çš„å™è¿°è¯æ®ä¸è¶³ä»¥æ”¯æŒé«˜ä¸¥é‡æ€§é¢„æµ‹æ—¶ï¼Œå¼ºåˆ¶å°†é¢„æµ‹ä» `2â†’1` æˆ– `1â†’0`ã€‚
   - æœ‰æ•ˆç¼“è§£ LLM è¿‡åº¦è‡ªä¿¡å¯¼è‡´çš„è¯¯æŠ¥é—®é¢˜ã€‚

4. **å¤šæºä¸Šä¸‹æ–‡å¢å¼ºæç¤ºè®¾è®¡**
   - åœ¨é¢„æµ‹æ—¶åŠ¨æ€æ³¨å…¥ä¸‰ç§ä¿¡æ¯ï¼š
     - åœ°ç†é‚»è¿‘é‚»å±…ï¼ˆwithin 1kmï¼‰åŠå…¶æ¨ç†è½¨è¿¹ï¼›
     - å…¸å‹åŸå‹ï¼ˆprototypesï¼‰â€”â€” æœ€å…·ä»£è¡¨æ€§çš„å„ç±»åˆ«æ ·æœ¬ï¼›
     - è¾¹ç•Œå›°éš¾æ ·æœ¬ï¼ˆhard-boundary casesï¼‰â€”â€” æ¥è¿‘å†³ç­–è¾¹ç•Œçš„æ¨¡ç³Šæ¡ˆä¾‹ã€‚
   - æ”¯æŒ HUC12 æµåŸŸå†…æœ¬åœ°æ£€ç´¢ï¼Œä¸è¶³æ—¶å›é€€åˆ°å…¨å±€åº“ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **éƒ¨ç½²æ•ˆç‡** | å®Œå…¨å…è®­ç»ƒï¼ˆtraining-freeï¼‰ï¼Œé€‚ç”¨äºå¿«é€Ÿè¿ç§»è‡³æ–°åŒºåŸŸ |
| **å¯è§£é‡Šæ€§** | è¾“å‡ºç»“æ„åŒ–æ¨ç†è·¯å¾„ï¼Œæ”¯æŒäº‹åå½’å› åˆ†æ |
| **é²æ£’æ€§** | åˆ©ç”¨çœŸå®å†å²æ¡ˆä¾‹çš„æ¨ç†æ¨¡å¼ï¼Œé¿å…çº¯æ–‡æœ¬å½’çº³åå·® |
| **æˆæœ¬æ•ˆç›Š** | è½»é‡çº§ LLM å˜ä½“åœ¨â€œä¸¥é‡æ€§æ¯å•ä½æˆæœ¬â€ä¸Šæ˜¾è‘—ä¼˜äºç›‘ç£æ¨¡å‹å’Œå¤§å‹ LLM |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **ç ”ç©¶åŒºåŸŸ**ï¼šç¾å›½å¾·å…‹è¨æ–¯å·å“ˆé‡Œæ–¯å¿ï¼ˆHarris Countyï¼‰ï¼Œé£“é£ Harveyï¼ˆ2017å¹´ï¼‰äº‹ä»¶ã€‚
- **ç©ºé—´ç²’åº¦**ï¼šç»Ÿä¸€èšåˆä¸º 500m Ã— 500m ç½‘æ ¼å•å…ƒï¼Œå¹¶æŒ‰ **HUC12 æµåŸŸç¼–ç ** åˆ†ç»„ã€‚
- **æ€»æ ·æœ¬æ•°**ï¼šè®­ç»ƒé›† 10,764ï¼›æµ‹è¯•é›† 4,614ã€‚
- **PDE æ ‡ç­¾æ„å»º**ï¼š
  - åŸºäº NFIP + FEMA IA æ´ªæ°´ç´¢èµ”æ•°æ®ï¼›
  - å½’ä¸€åŒ–ååˆ’åˆ†ä¸ºä¸‰çº§ï¼š
    - Low (0): æ— æˆ–è½»å¾®æŸå¤±ï¼›
    - Medium (1): ä¸­ç­‰æŸå¤±ï¼›
    - High (2): é‡å¤§æŸå¤±ï¼›
  - åˆ†å¸ƒæ¯”ä¾‹ï¼š44.7% / 40.6% / 14.6%

### ç‰¹å¾å˜é‡ï¼ˆå…±14é¡¹ï¼‰
æ¥è‡ªå¤šä¸ªå¼€æºæ•°æ®åº“ï¼Œæ¶µç›–å››å¤§ç±»ï¼š
| ç±»åˆ« | ç‰¹å¾ç¤ºä¾‹ |
|------|--------|
| Built Environment | FAR, Building Age, Foundation Height, POI Number |
| Topography | Elevation, HAND, Imperviousness, Terrain Roughness |
| Hydrology | Distance to Stream, Flood Claims in Past 50 Years |
| Event-specific | Maximum Rainfall |

---

### å®éªŒè®¾ç½®
#### LLM Backbonesï¼ˆå…±7ç§ï¼‰
| ç±»å‹ | æ¨¡å‹åç§° |
|------|---------|
| Proprietary | `gpt-4o-mini`, `gpt-4o`, `gpt-4.1`, `gpt-5-mini` |
| Open-source | `Llama-3.1-70B-Instruct`, `Qwen3-30B-A3B-Instruct-2507`, `DeepSeek-R1`ï¼ˆAPIè°ƒç”¨ï¼‰ |

æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ç›¸åŒ prompt æ¨¡æ¿ï¼Œ**æ— ä»»ä½• fine-tuning**ï¼Œä»…é€šè¿‡ context-augmented prompting å®ç°é¢„æµ‹ã€‚

---

### è¯„ä¼°æŒ‡æ ‡

#### é¢„æµ‹æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Overall Accuracy** | ä¸‰åˆ†ç±»æ•´ä½“å‡†ç¡®ç‡ |
| **Macro-F1** | å¹³è¡¡å„ç±»åˆ«çš„ F1 åˆ†æ•° |
| **Damage Class Accuracy** | ä»…é™ä¸­/é«˜æŸæ ·æœ¬ï¼ˆclass 1 & 2ï¼‰çš„å­é›†å‡†ç¡®ç‡ |
| **Severity Score** | åºæ•°è·ç¦»å¾—åˆ†ï¼š<br>$ \text{score}_i = 1 - \frac{|y_i - \hat{y}_i|}{2} $ï¼Œæ»¡åˆ† 1.0 |
| **Recallâ‚‚** | é«˜æŸç±»ï¼ˆclass 2ï¼‰å¬å›ç‡ |

#### æˆæœ¬æ•ˆç‡æŒ‡æ ‡
- **CostIdx**ï¼šæ¯æ ·æœ¬ç­‰æ•ˆç¾å…ƒæˆæœ¬
  - ç›‘ç£æ¨¡å‹ï¼šåŒ…å«è®­ç»ƒ + æ¨ç† GPU å®ä¾‹è´¹ç”¨ï¼ˆEC2 g6e.xlargeï¼‰
  - GPTç³»åˆ—ï¼šåŸºäº API è¾“å…¥/è¾“å‡º token è®¡è´¹
  - å¼€æºæ¨¡å‹ï¼šåŸºäºé‡åŒ–æ¨ç†ä¸‹çš„ GPU è¿è¡Œæ—¶é—´æŠ˜ç®—
- **Efficiency (Severity/CostIdx)**ï¼šå•ä½æˆæœ¬è·å¾—çš„ä¸¥é‡æ€§è¯„åˆ†ï¼Œè¡¡é‡æ€§ä»·æ¯”

#### æ¨ç†è´¨é‡è¯„ä¼°ï¼ˆInstance-level Reasoning Metricsï¼‰
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **LRA** (Label-Reasoning Alignment) | æ¨ç†å†…å®¹æ˜¯å¦æ”¯æŒæœ€ç»ˆé¢„æµ‹æ ‡ç­¾ |
| **SFC** (Salient Feature Coverage) | æ˜¯å¦æåŠå…³é”®åˆ¤åˆ«ç‰¹å¾ï¼ˆåŸºäº divergence rankingï¼‰ |
| **FDC** (Feature Direction Consistency) | ç‰¹å¾æè¿°æ–¹å‘æ˜¯å¦ä¸æ•°å€¼ä¸€è‡´ï¼ˆå¦‚â€œé«˜é™é›¨â€å¯¹åº”å®é™…é«˜å€¼ï¼‰ |
| **PAS** (Prototype Alignment Score) | æ˜¯å¦æ¨¡ä»¿å…¸å‹åŸå‹çš„æ¨ç†æ¨¡å¼ |
| **BTS** (Boundary Tradeoff Score) | æ˜¯å¦åœ¨è¾¹ç•Œæ ·æœ¬ä¸­ä½“ç°æ­£åè¯æ®æƒè¡¡ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Supervised Baseline**: `FloodDamageCast*`  
  - åŸºäº Gradient Boosted Tree çš„ç›‘ç£æ¨¡å‹
  - ä½¿ç”¨ç›¸åŒçš„14ç»´ç»“æ„åŒ–ç‰¹å¾è¾“å…¥
  - åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šè®­ç»ƒå¹¶è¯„ä¼°

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### æ€§èƒ½æ±‡æ€»ï¼ˆè§ Table 6ï¼‰

| Model | Overall Acc | Damage Class Acc | Severity Score | Recallâ‚‚ | CostIdx | Efficiency |
|-------|-------------|------------------|----------------|---------|---------|------------|
| **FloodDamageCast*** (baseline) | **0.714** | 0.859 | 0.844 | 0.672 | 0.030 | 28.6 |
| **gpt-4.1** | 0.668 | 0.816 | **0.825** | 0.439 | 0.133 | **6.2** |
| **gpt-5-mini** | 0.647 | **0.867** | 0.809 | **0.591** | 0.023 | 35.9 |
| **gpt-4o-mini** | 0.660 | 0.859 | 0.819 | 0.346 | **0.010** | **81.9** âœ… |
| **Llama-3.1-70B** | 0.613 | 0.896 âœ… | 0.787 | 0.529 | 0.199 | 4.0 |
| **Qwen3-30B** | 0.642 | 0.811 | 0.807 | 0.576 | 0.132 | 6.1 |
| **DeepSeek-R1** | 0.644 | 0.757 | 0.801 | 0.458 | 0.050 | 16.0 |

> âœ… è¡¨ç¤ºè¯¥åˆ—æœ€ä¼˜ï¼›**åŠ ç²—**è¡¨ç¤ºå…¨å±€æœ€ä½³

---

### å…³é”®å‘ç°
- R2RAG-Flood å¤šæ•° LLM å˜ä½“åœ¨ **Damage Class Accuracy** ä¸Šæ¥è¿‘ç”šè‡³è¶…è¿‡ç›‘ç£åŸºçº¿ï¼ˆæœ€é«˜è¾¾ **0.896**ï¼‰ï¼›
- å°½ç®¡æ•´ä½“å‡†ç¡®ç‡ç•¥ä½ï¼ˆ~0.61â€“0.67 vs 0.714ï¼‰ï¼Œä½†åœ¨**ä¸¥é‡æ€§æ’åºä¸€è‡´æ€§**ï¼ˆSeverity Scoreï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ˆæœ€é«˜ 0.825ï¼‰ï¼›
- **è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ gpt-4o-miniï¼‰åœ¨æ•ˆç‡ä¸Šç¢¾å‹å…¶ä»–æ¨¡å‹**ï¼š
  - æ•ˆç‡è¾¾åˆ° **81.9**ï¼Œè¿œé«˜äºåŸºçº¿ï¼ˆ28.6ï¼‰å’Œå¤§æ¨¡å‹ï¼ˆ4.0â€“6.2ï¼‰ï¼›
  - é€‚åˆèµ„æºæ•æ„Ÿã€éœ€å¿«é€Ÿéƒ¨ç½²çš„åº”ç”¨åœºæ™¯ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

å››ä¸ªé€æ­¥å¢å¼ºçš„é…ç½®ï¼š
1. **Text-mode only**ï¼šä»…ç›®æ ‡æ–‡æœ¬æ‘˜è¦
2. **+ Neighbor reasoning**ï¼šåŠ å…¥æœ€å¤š3ä¸ªåœ°ç†é‚»è¿‘æ ·æœ¬åŠå…¶æ¨ç†
3. **+ Free-shots**ï¼šé‚»å±…ä¸è¶³æ—¶è¡¥å……åŸå‹ä¸è¾¹ç•Œæ ·ä¾‹
4. **+ Downgrade**ï¼šå¯ç”¨æ¡ä»¶é™çº§æœºåˆ¶

#### ç»“æœè¶‹åŠ¿ï¼ˆå›¾3ï¼‰ï¼š
- æ‰€æœ‰ LLM åœ¨å¼•å…¥ **neighbor reasoning** å Macro-F1 æ˜¾è‘—æå‡ â†’ è¡¨æ˜åœ°ç†ç±»æ¯”æ¨ç†æœ‰æ•ˆï¼›
- åŠ å…¥ **free-shots** åè¿›ä¸€æ­¥ç¨³å®šç¨€ç–åŒºåŸŸé¢„æµ‹ï¼›
- **Downgrade æœºåˆ¶å¸¦æ¥å°å¹…ä½†ç¨³å®šçš„æ­£å‘å½±å“**ï¼Œè¯´æ˜å…¶æœ‰æ•ˆçº æ­£è¿‡åº¦é¢„æµ‹ï¼›
- å®Œæ•´æµç¨‹ï¼ˆIVï¼‰æ•ˆæœæœ€ä¼˜ï¼ŒéªŒè¯äº†æ•´ä½“è®¾è®¡åˆç†æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦ç»“è®º
1. âœ… **R2RAG-Flood æ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒå³å¯å®ç°ç¾æŸ nowcasting çš„ LLM æ¨ç†æ¡†æ¶**ï¼Œé€šè¿‡æ£€ç´¢å†å²æ¡ˆä¾‹çš„â€œæ€è€ƒè¿‡ç¨‹â€å®ç°çŸ¥è¯†è¿ç§»ã€‚
2. âœ… **æ¨ç†è½¨è¿¹ + ä¸Šä¸‹æ–‡æç¤º + è§„åˆ™æ ¡æ­£** çš„ç»„åˆæ˜¾è‘—æå‡äº†é¢„æµ‹åˆç†æ€§å’Œç¨³å®šæ€§ã€‚
3. âœ… **è½»é‡çº§ LLM åœ¨æˆæœ¬æ•ˆç‡ä¸Šå…·æœ‰å·¨å¤§ä¼˜åŠ¿**ï¼Œå°¤å…¶é€‚åˆåº”æ€¥å“åº”åœºæ™¯ä¸‹çš„è¾¹ç¼˜éƒ¨ç½²ã€‚
4. âœ… æ–¹æ³•åœ¨ä¿æŒè¾ƒé«˜ damage class accuracy å’Œ severity consistency çš„åŒæ—¶ï¼Œæä¾›å®Œæ•´çš„è§£é‡Šæ€§è¾“å‡ºã€‚

---

### å±€é™æ€§
1. **åœ°åŸŸæ³›åŒ–èƒ½åŠ›æœªå……åˆ†éªŒè¯**ï¼šç›®å‰ä»…åœ¨ Harris County + Hurricane Harvey åœºæ™¯ä¸‹æµ‹è¯•ï¼Œç»“æœå¯èƒ½å—åŒºåŸŸç‰¹æ€§å½±å“ã€‚
2. **ä¾èµ–é«˜è´¨é‡å†å²æ•°æ®**ï¼šè‹¥æŸåœ°åŒºç¼ºä¹è¶³å¤Ÿæ ‡æ³¨æ ·æœ¬ï¼Œæ£€ç´¢è´¨é‡ä¸‹é™ï¼Œå½±å“æ€§èƒ½ã€‚
3. **è‡ªåŠ¨åŒ–è¯„ä¼°å±€é™**ï¼šå½“å‰ reasoning metrics ä¸ºå¯å‘å¼è§„åˆ™é©±åŠ¨ï¼Œéš¾ä»¥å®Œå…¨æ•æ‰ä¸“å®¶çº§æ¨ç†æ·±åº¦ã€‚
4. **å›ºå®š prompt è®¾è®¡**ï¼šå°šæœªèåˆäººç±»ä¸“å®¶å‚ä¸çš„åŠ¨æ€æ¨¡æ¿ä¼˜åŒ–ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **è·¨åŒºåŸŸã€å¤šç¾å®³ç±»å‹æ‰©å±•**ï¼šåœ¨æ›´å¤šåŸå¸‚å’Œé£æš´äº‹ä»¶ä¸­éªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚
2. **å¼•å…¥å¤šæ¨¡æ€è¯æ®**ï¼šèåˆé¥æ„Ÿå½±åƒã€ç¤¾äº¤åª’ä½“æ–‡æœ¬æŠ¥å‘Šç­‰éç»“æ„åŒ–æ•°æ®ã€‚
3. **æ¢ç´¢å¤šç§è§£ç ç­–ç•¥**ï¼šæµ‹è¯•ä¸åŒ temperatureã€beam search ç­‰å¯¹æ€§èƒ½ä¸æˆæœ¬çš„å½±å“ã€‚
4. **ä¸“å®¶ååŒè®¾è®¡ prompt ä¸ free-shot é€‰æ‹©æ ‡å‡†**ï¼šé€šè¿‡è°ƒæŸ¥é—®å·ç­‰æ–¹å¼è·å–é¢†åŸŸä¸“å®¶åå¥½ï¼Œæ›¿ä»£å½“å‰è‡ªåŠ¨åŒ–é€‰æ‹©æœºåˆ¶ã€‚
5. **ç”Ÿå‘½å‘¨æœŸæˆæœ¬åˆ†æ**ï¼šå¼€å±•é¢å‘å®é™…ä¸šåŠ¡è´Ÿè½½çš„ç«¯åˆ°ç«¯æˆæœ¬å»ºæ¨¡ï¼Œè¶…è¶Šå½“å‰ evaluation-time accountingã€‚

--- 

> **ä¸€å¥è¯æ€»ç»“**ï¼š  
> R2RAG-Flood æˆåŠŸå°† LLM çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ä¸åœ°ç†ç©ºé—´ç¾æŸé¢„æµ‹ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€æ¡**å…è®­ç»ƒã€å¼ºè§£é‡Šã€é«˜æ•ˆç‡**çš„æ–°æŠ€æœ¯è·¯å¾„ï¼Œä¸ºç¾å®³åº”æ€¥å“åº”æä¾›äº†æ›´å…·å®ç”¨ä»·å€¼çš„ AI å·¥å…·ã€‚

</details>

---

### 13. [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)

**Authors**: Kanghyun Noh, Jinheon Choi, Yulwha Kim  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.10431v1  

#### Abstract
Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠQTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMsã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **é«˜è®¡ç®—å¼€é”€**ï¼šå¤§é‡ FLOPs å¯¼è‡´æ¨ç†å»¶è¿Ÿé«˜ã€‚
- **é«˜å†…å­˜å ç”¨**ï¼šå…¨ç²¾åº¦æƒé‡æ¶ˆè€—å¤§é‡æ˜¾å­˜ã€‚

å·²æœ‰ä¸¤ç§ä¸»æµä¼˜åŒ–æŠ€æœ¯ï¼š
- **Token-adaptive layer execution**ï¼ˆå¦‚ D-LLMï¼‰ï¼šé€šè¿‡åŠ¨æ€è·³è¿‡éƒ¨åˆ† transformer å±‚å‡å°‘ FLOPsã€‚
- **Quantization**ï¼šé™ä½æƒé‡ç²¾åº¦ä»¥å‡å°æ¨¡å‹ä½“ç§¯å’Œå†…å­˜å¸¦å®½éœ€æ±‚ã€‚

ç„¶è€Œï¼Œ**ç›´æ¥å°†äºŒè€…ç»“åˆä¼šå¯¼è‡´æ˜¾è‘—çš„å‡†ç¡®ç‡ä¸‹é™**ã€‚åŸå› åœ¨äº token-adaptive æ¨¡å‹æœ¬èº«å·²å‡å°‘äº†å†—ä½™ï¼ˆè®­ç»ƒè·¯å¾„å•ä¸€ + å‚æ•°å‚ä¸å°‘ï¼‰ï¼Œè€Œé‡åŒ–è¿›ä¸€æ­¥å¼•å…¥æ‰°åŠ¨ï¼Œä½¿æ¨¡å‹æ›´è„†å¼±ã€‚

> QTALE æ­£æ˜¯ä¸ºäº†è§£å†³â€œ**token-adaptive æ‰§è¡Œä¸é‡åŒ–éš¾ä»¥ååŒ**â€è¿™ä¸€å…³é”®é—®é¢˜è€Œæå‡ºã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šQTALE
QTALE æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç° **token-adaptive execution ä¸ quantization çš„æ— ç¼é›†æˆ**ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰Quantization-Robust Training Strategyï¼ˆé‡åŒ–é²æ£’æ€§è®­ç»ƒç­–ç•¥ï¼‰
- å¼•å…¥ **ç†µæœ€å¤§åŒ–ç›®æ ‡å‡½æ•° $ \mathcal{L}_{\text{entropy}} $** æ¥æ­£åˆ™åŒ– router çš„è¾“å‡º logitsã€‚
- ç›®æ ‡æ˜¯è®© execute å’Œ bypass çš„ logit å·®è·å˜å°ï¼Œæå‡è·¯ç”±å†³ç­–çš„ä¸ç¡®å®šæ€§ï¼ˆstochasticityï¼‰ï¼Œä»è€Œé¼“åŠ±æ›´å¤šæ ·åŒ–çš„æ‰§è¡Œè·¯å¾„åœ¨ fine-tuning ä¸­è¢«æ¢ç´¢ã€‚
- è¿™å¢å¼ºäº† **training-path redundancy**ï¼Œé˜²æ­¢æŸäº›å±‚é•¿æœŸä¸è¢«æ‰§è¡Œè€Œå¯¼è‡´æ¬ è®­ç»ƒã€‚

#### ï¼ˆ2ï¼‰Post-Training Execution Ratio Adjustment Mechanismï¼ˆæ¨ç†é˜¶æ®µæ‰§è¡Œæ¯”ä¾‹è°ƒèŠ‚æœºåˆ¶ï¼‰
- åœ¨æ¨ç†æ—¶é€šè¿‡è°ƒæ•´ threshold $ \theta $ åŠ¨æ€æ§åˆ¶ layer bypass è¡Œä¸ºã€‚
- æ”¯æŒçµæ´»å¢åŠ æ‰§è¡Œå±‚æ•°ï¼ˆå³æé«˜ execution ratioï¼‰ï¼Œä»¥ **æŒ‰éœ€æ¢å¤å‚æ•°å†—ä½™åº¦**ï¼Œå¢å¼ºå¯¹é‡åŒ–å™ªå£°çš„é²æ£’æ€§ã€‚
- ä½¿ç”¨ç®€å•çš„ grid search å³å¯åœ¨å°æ ¡å‡†é›†ä¸Šæ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | D-LLMï¼ˆBaselineï¼‰ | QTALEï¼ˆæœ¬æ–‡ï¼‰ |
|------|------------------|-------------|
| è·¯å¾„å¤šæ ·æ€§ | ä½ï¼ˆæ”¶æ•›åˆ°å›ºå®šå­é›†æ‰§è¡Œï¼‰ | é«˜ï¼ˆç†µæ­£åˆ™åŒ–ä¿ƒè¿›éšæœºæ¢ç´¢ï¼‰ |
| å¯¹é‡åŒ–çš„é²æ£’æ€§ | å·®ï¼ˆå¾®å°æ‰°åŠ¨å¯¼è‡´è·¯å¾„æ¼‚ç§»ï¼‰ | å¼ºï¼ˆå¹³æ»‘è·¯ç”±è¾¹ç•Œï¼‰ |
| å¯è°ƒæ€§ | å›ºå®šæ‰§è¡Œæ¯”ä¾‹ | æ¨ç†æ—¶å¯è°ƒ execution ratio |
| å‡†ç¡®æ€§ï¼ˆ+é‡åŒ–åï¼‰ | æ˜æ˜¾ä¸‹é™ | æ¥è¿‘ full-modelï¼Œå·®è· <0.5% |

> âœ… **QTALE æˆåŠŸæ¡¥æ¥äº† FLOPs é™ä½ä¸ memory footprint é™ä½çš„æŠ€æœ¯é¸¿æ²Ÿï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®çš„è”åˆå‹ç¼©ã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **CommonsenseQA Benchmark Suite**ï¼š
  - PIQA, BoolQ, SIQA, ARCe, ARCc, Winogrande (Winogr.), OBQA
- **å…¶ä»–è¯„ä¼°ä»»åŠ¡**ï¼š
  - MMLUï¼ˆå¤šä»»åŠ¡ç†è§£ï¼‰
  - Stanford-Alpaca å’Œ SAMSum æ•°æ®é›†ä¸Šçš„ **Perplexity (PPL)** æµ‹è¯„

æ‰€æœ‰å®éªŒå‡é‡‡ç”¨ **zero-shot setting**ï¼Œé¿å…ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒå¹²æ‰°ç»“æœã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
| é¡¹ç›® | è®¾ç½®è¯´æ˜ |
|------|--------|
| **æ¨¡å‹** | LLaMA2-7B, LLaMA3.1-8B, LLaMA3.2-3B |
| **é‡åŒ–æ–¹å¼** | ä½¿ç”¨ AWQ è¿›è¡Œ Post-Training Quantization (PTQ)<br>- 4-bit ä¸ 3-bit æ•´æ•°é‡åŒ–<br>- Group size = 128 |
| **Token-adaptive å®ç°** | åŸºäº D-LLM æ¶æ„ï¼Œåœ¨æ¯å±‚æ·»åŠ è½»é‡ MLP router |
| **è®­ç»ƒé…ç½®** | Fine-tuning è¶…å‚å¤ç”¨åŸå§‹ D-LLM è®¾ç½®ï¼ˆå­¦ä¹ ç‡ã€epoch æ•°ç­‰ï¼‰ |
| **è¯„ä¼°æŒ‡æ ‡** | - Zero-shot Accuracy (%)<br>- Perplexity (PPL)<br>- FLOPsï¼ˆå½’ä¸€åŒ–ï¼‰<br>- Memory footprint (GB)<br>- Inference Latency / Speedup |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **Full-layer Execution** | ä¸è¿›è¡Œ layer skipping çš„æ ‡å‡† fine-tuned æ¨¡å‹ï¼ˆä¸Šé™å‚è€ƒï¼‰ |
| **AWQ** | ä»…é‡åŒ–ï¼Œæ—  token-adaptive |
| **D-LLM** | ä»… token-adaptive æ‰§è¡Œ |
| **D-LLM + AWQ** | æœ´ç´ ç»„åˆï¼šå…ˆåº”ç”¨ D-LLM å†é‡åŒ– â†’ ä¸»è¦å¯¹æ¯”å¯¹è±¡ |
| **QTALE (ours)** | æå‡ºçš„æ–¹æ³•ï¼šå¸¦ç†µæ­£åˆ™è®­ç»ƒ + æ¨ç†æœŸ ratio è°ƒæ•´ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»£è¡¨æ€§ç»“æœï¼‰

#### â–¶ï¸ åœ¨ LLaMA3.2-3B + 4-bit AWQ ä¸Šçš„ CSQA å‡†ç¡®ç‡ï¼š
| æ–¹æ³• | Accuracy (%) |
|------|--------------|
| Full-layer (16-bit) | 80.51 |
| D-LLM (4-bit) | 73.96 âŒï¼ˆâ†“6.55%ï¼‰ |
| **QTALE (4-bit)** | **78.41** âœ…ï¼ˆæ¥è¿‘ full modelï¼‰ |

> â¤ QTALE æ¯” D-LLM æå‡ **+4.45%**ï¼Œå‡ ä¹è¿½å¹³é‡åŒ–åçš„ full modelï¼ˆ77.55%ï¼‰

#### â–¶ï¸ åœ¨ LLaMA2-7B + 3-bit AWQ ä¸Šçš„ç»“æœï¼š
| æ–¹æ³• | CSQA Accuracy (%) |
|------|--------------------|
| Full (3-bit) | 72.22 |
| D-LLM (3-bit) | 70.57 |
| **QTALE (3-bit)** | **72.79** âœ…

> â¤ QTALE ä¸ä»…æ²¡æœ‰æŸå¤±ï¼Œåè€Œç•¥è¶… full modelï¼

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”æ€»ç»“
- **D-LLM + Quantization ç»„åˆä¸¥é‡é€€åŒ–**ï¼š
  - å¹³å‡ accuracy ä¸‹é™å¯è¾¾ 5~7%ï¼Œå°¤å…¶åœ¨ä½æ¯”ç‰¹ï¼ˆ3-bitï¼‰ä¸‹æ›´æ˜æ˜¾ã€‚
- **QTALE æˆåŠŸç»´æŒ accuracy**ï¼š
  - ä¸ quantization-only æ¨¡å‹ä¹‹é—´çš„å·®è· **å§‹ç»ˆæ§åˆ¶åœ¨ 0.5% ä»¥å†…**ï¼ˆCommonsenseQA ä¸Šï¼‰ã€‚
- **FLOPs é™ä½çº¦ 46%**ï¼Œ**Memory footprint é™ä½è‡³ ~4.5GBï¼ˆåŸ 13.5GBï¼‰**ï¼Œå®ç°åŒé‡å‹ç¼©ã€‚

#### å›¾è¡¨æ”¯æŒï¼ˆFigure 6ï¼‰ï¼š
- QTALE åœ¨ **FLOPs-Memory æŠ˜è¡·æ›²çº¿ä¸Šæ˜¾è‘—ä¼˜äº baseline**ï¼Œè¾¾åˆ°å¸•ç´¯æ‰˜å‰æ²¿ã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### è¡¨æ ¼ 4ï¼šå„ç»„ä»¶è´¡çŒ®åˆ†æï¼ˆLLaMA3.2-3B, 4-bit, CSQAï¼‰
| æ–¹æ³• | Accuracy (%) | æ‰§è¡Œæ¯”ä¾‹ |
|------|-------------|---------|
| D-LLMï¼ˆæ— ä»»ä½•æ”¹è¿›ï¼‰ | 73.31 | 0.54 |
| + $ \mathcal{L}_{\text{entropy}} $ï¼ˆä»…è®­ç»ƒæ”¹è¿›ï¼‰ | 75.12 | 0.54 |
| + Threshold adjustment $ \theta $ï¼ˆä»…æ¨ç†è°ƒèŠ‚ï¼‰ | 74.88 | 0.57 |
| **å®Œæ•´ QTALEï¼ˆä¸¤è€…ç»“åˆï¼‰** | **77.62** | 0.58 |

> âœ… ä¸¤ä¸ªç»„ä»¶å‡æœ‰ç‹¬ç«‹å¢ç›Šï¼Œ**è”åˆä½¿ç”¨æ•ˆæœæœ€ä½³**ã€‚

#### å…³é”®è§‚å¯Ÿï¼š
- ä»…é  entropy æ­£åˆ™æ— æ³•æ”¹å˜æœ€ç»ˆæ‰§è¡Œæ¯”ä¾‹ï¼Œä½†æå‡äº†è·¯å¾„ç¨³å®šæ€§ã€‚
- ä»…é  threshold è°ƒèŠ‚è‹¥ç¼ºä¹è®­ç»ƒå¤šæ ·æ€§ï¼Œåˆ™æå‡æœ‰é™ã€‚
- **å¿…é¡»ä¸¤è€…ç»“åˆæ‰èƒ½å®ç°çœŸæ­£çš„ quantization robustness**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Token-adaptive ä¸ quantization çš„å†²çªæ ¹æºæ˜¯â€œå†—ä½™ç¼ºå¤±â€**ï¼š
   - D-LLM ç±»æ–¹æ³•å› è®­ç»ƒè·¯å¾„å•ä¸€ã€å‚æ•°åˆ©ç”¨ç‡ä½ï¼Œå¤©ç„¶å¯¹é‡åŒ–æ•æ„Ÿã€‚
2. **QTALE æœ‰æ•ˆé‡å»ºäº†å†—ä½™æ€§**ï¼š
   - é€šè¿‡ç†µæ­£åˆ™å¢å¼ºè®­ç»ƒå¤šæ ·æ€§ï¼ˆtraining-path redundancyï¼‰ã€‚
   - é€šè¿‡æ¨ç†æœŸè°ƒèŠ‚ reintroduce å‚æ•°å†—ä½™ï¼ˆparameter redundancyï¼‰ã€‚
3. **å®ç°äº†é«˜æ•ˆä¸å‡†ç¡®çš„ç»Ÿä¸€**ï¼š
   - åŒæ—¶é™ä½ FLOPsï¼ˆâ‰ˆ46%â†‘ speedupï¼‰å’Œ memoryï¼ˆâ†“68%ï¼‰ã€‚
   - accuracy æŸå¤±å¯å¿½ç•¥ï¼ˆ<0.5% vs quantized full modelï¼‰ã€‚
4. **å…¼å®¹æ€§å¼º**ï¼š
   - é€‚ç”¨äºä¸åŒ PTQ æ–¹æ³•ï¼ˆAWQã€MagR+GPTQ å‡éªŒè¯æœ‰æ•ˆï¼‰ã€‚
   - å¯æ‰©å±•è‡³å…¶ä»–å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚ pruningï¼Œè§ Table 6ï¼‰ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ– router æ¨¡å—å¼•å…¥é¢å¤–è®­ç»ƒæˆæœ¬**ï¼š
   - å°½ç®¡ router å¾ˆè½»é‡ï¼Œä½†ä»éœ€ fine-tuning é˜¶æ®µä»‹å…¥ï¼Œéå®Œå…¨å…è®­ç»ƒã€‚
2. **æ‰§è¡Œæ¯”ä¾‹è°ƒèŠ‚éœ€è¦ä¸€ä¸ªå°è§„æ¨¡æ ¡å‡†é›†**ï¼š
   - è™½ç„¶ grid search å¿«é€Ÿå®ç”¨ï¼Œä½†åœ¨ä¸¥æ ¼ zero-data åœºæ™¯ä¸‹ä»éœ€å‡è®¾å­˜åœ¨å°‘é‡æ ·æœ¬ã€‚
3. **ç›®å‰èšç„¦äº PTQï¼Œæœªæ¶‰åŠ QATï¼ˆQuantization-Aware Trainingï¼‰**ï¼š
   - è‹¥ç»“åˆ QAT å¯èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œä½†å¤æ‚åº¦ä¸Šå‡ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **è‡ªåŠ¨åŒ– threshold selection**ï¼š
   - æ¢ç´¢æ— éœ€äººå·¥æœç´¢çš„è‡ªé€‚åº” threshold æ§åˆ¶æœºåˆ¶ï¼ˆå¦‚åŸºäºè¾“å…¥éš¾åº¦é¢„æµ‹ï¼‰ã€‚
2. **æ‰©å±•åˆ° dynamic sparse training æˆ– MoE æ¶æ„**ï¼š
   - å°†è·¯å¾„å¤šæ ·æ€§æ€æƒ³è¿ç§»åˆ°æ›´å¹¿æ³›çš„ç¨€ç–åŒ–èŒƒå¼ä¸­ã€‚
3. **è·¨æ¨¡æ€æˆ–é•¿åºåˆ—åœºæ™¯çš„åº”ç”¨**ï¼š
   - éªŒè¯ QTALE åœ¨ vision-language æ¨¡å‹æˆ– long-context æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚
4. **ç¡¬ä»¶æ„ŸçŸ¥è”åˆä¼˜åŒ–**ï¼š
   - ç»“åˆå®é™… GPU memory bandwidth ä¸ compute capability è®¾è®¡ç«¯åˆ°ç«¯æœ€ä¼˜ç­–ç•¥ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **QTALE é€šè¿‡â€œè®­ç»ƒæ—¶é¼“åŠ±è·¯å¾„å¤šæ ·æ€§ + æ¨ç†æ—¶æŒ‰éœ€æ¢å¤å†—ä½™â€ï¼Œé¦–æ¬¡å®ç°äº† token-adaptive layer execution ä¸ quantization çš„ç¨³å®šååŒï¼Œåœ¨å¤§å¹…é™ä½ FLOPs ä¸ memory çš„åŒæ—¶ï¼Œå‡ ä¹æ— æŸåœ°ä¿ç•™äº†æ¨¡å‹å‡†ç¡®æ€§ï¼Œä¸ºé«˜æ•ˆ LLM éƒ¨ç½²æä¾›äº†ç»Ÿä¸€å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚**

</details>

---

### 14. [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)

**Authors**: Williams Jonathan, Tureci Esin  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.10520v1  

#### Abstract
Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives su...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼ˆå¦‚ **GRPO**ï¼‰åœ¨åº”ç”¨äº **Looped Language Models (LoopLMs)** æ—¶æ•ˆæœä¸ä½³ã€‚å…¶æ ¹æœ¬åŸå› åœ¨äºè¿™äº›æ–¹æ³•åªå¯¹ç”Ÿæˆå‰çš„**æœ€ç»ˆéšçŠ¶æ€**ï¼ˆfinal latent stateï¼‰è¿›è¡Œå¥–åŠ±ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹å†…éƒ¨å¤šæ­¥è¿­ä»£æ¨ç†çš„å®Œæ•´â€œæ€ç»´è½¨è¿¹â€ï¼ˆthought trajectoryï¼‰ã€‚è¿™ç§â€œç»ˆç«¯å¥–åŠ±ç“¶é¢ˆâ€ä¸ LoopLMs å¤šæ­¥å†…éƒ¨è®¡ç®—çš„æœ¬è´¨å­˜åœ¨æ ¹æœ¬æ€§é”™é…ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **RLTT (Reward Latent Thought Trajectories)**ï¼Œä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **å°†å¥–åŠ±ä¿¡å·åˆ†å¸ƒåˆ°æ•´ä¸ªéšçŠ¶æ€æ¨ç†è½¨è¿¹ä¸Š**ï¼Œè€Œéä»…å¥–åŠ±æœ€åä¸€ä¸ªå¾ªç¯æ­¥éª¤ã€‚
- é€šè¿‡èšåˆæ¯ä¸ªå†…éƒ¨å¾ªç¯ï¼ˆloopï¼‰äº§ç”Ÿçš„ next-token åˆ†å¸ƒï¼Œå¹¶åŸºäºæ­¤æ„å»ºç­–ç•¥æ¢¯åº¦ï¼Œå®ç°å¯¹å…¨è¿‡ç¨‹çš„ä¿¡ç”¨åˆ†é…ï¼ˆcredit assignmentï¼‰ã€‚
- è¯¥æ–¹æ³•æ— éœ€ä¾èµ–å¤–éƒ¨éªŒè¯å™¨ï¼ˆexternal verifiersï¼‰ï¼Œå¯ç›´æ¥æ›¿ä»£ GRPOï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´å¯†é›†çš„ä¿¡ç”¨åˆ†é…**ï¼šè§£å†³äº† GRPO ä¸­å¥–åŠ±ä¿¡å·éœ€åå‘ä¼ æ’­å¤šä¸ªéšå±‚çš„éš¾é¢˜ã€‚
- **æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›**ï¼šè®­ç»ƒå‡ºçš„ç­–ç•¥åœ¨æ•°å­¦å’Œéæ•°å­¦ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚
- **æ›´é«˜çš„æ¨ç†æ•ˆç‡**ï¼šè¯±å¯¼æ¨¡å‹æ›´å¿«æ”¶æ•›åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå‡å°‘å†—ä½™æ€è€ƒã€‚
- **å®ç°ç®€å•**ï¼šå¯æ— ç¼é›†æˆåˆ°ç°æœ‰ LoopLM æ¶æ„ä¸­ï¼Œæ— éœ€å¤æ‚ä¿®æ”¹ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
#### æ•°å­¦æ¨ç†åŸºå‡†ï¼ˆMath Benchmarksï¼‰ï¼š
- **MATH-500**ï¼šæ ‡å‡†æ•°å­¦é—®é¢˜é›†åˆã€‚
- **AIME24**ï¼šç¾å›½é‚€è¯·æ•°å­¦è€ƒè¯•é¢˜ï¼Œéš¾åº¦é«˜ã€‚
- **BeyondAIME**ï¼šè¶…è¶Šé«˜ä¸­å¥¥èµ›æ°´å¹³çš„æŒ‘æˆ˜æ€§æ•°å­¦é¢˜ã€‚
- **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜ã€‚

#### éæ•°å­¦æ¨ç†åŸºå‡†ï¼ˆNon-Math Benchmarksï¼‰ï¼š
- **ARC-C**ï¼šAI2 æ¨ç†æŒ‘æˆ˜ï¼Œé€»è¾‘ä¸å¸¸è¯†æ¨ç†ã€‚
- **MMLU-ST**ï¼šå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆSTEM å­é›†ï¼‰ã€‚
- **GPQA**ï¼šç ”ç©¶ç”Ÿçº§è°·æ­Œæ— æ³•å›ç­”çš„é—®é¢˜ï¼Œè€ƒå¯Ÿå¤šè·³äº‹å®æ¨ç†ã€‚
- **MBPP**ï¼šé¢å‘ç¼–ç¨‹çš„ä»»åŠ¡ï¼Œè¯„ä¼°ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚

> æ‰€æœ‰éæ•°å­¦ä»»åŠ¡å‡ä¸º**é›¶æ ·æœ¬è¿ç§»è¯„ä¼°**ï¼Œæ¨¡å‹ä»…åœ¨æ•°å­¦æ•°æ®ä¸Šè®­ç»ƒã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **åŸºç¡€æ¨¡å‹**ï¼š`Ouro-2.6B-Thinking`ï¼Œä¸€ä¸ªå¼€æºçš„ LoopLMã€‚
- **è®­ç»ƒæ¡ä»¶**ï¼šä¸¥æ ¼æ§åˆ¶è®¡ç®—èµ„æºåŒ¹é…ï¼Œä¸ GRPO åœ¨ç›¸åŒ rollout é¢„ç®—ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡ç­‰æ¡ä»¶ä¸‹è®­ç»ƒã€‚
- **è¯„ä¼°æ–¹å¼**ï¼š
  - ä½¿ç”¨**ç¡®å®šæ€§è§£ç **ï¼ˆdeterministic decodingï¼‰ã€‚
  - é‡‡ç”¨ **exact-match** ä½œä¸ºå‡†ç¡®ç‡æŒ‡æ ‡ã€‚
  - è®¾ç½®å›ºå®šçš„æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆå¦‚ MATH-500 ä¸º 2048 tokensï¼‰ã€‚
- **å¥–åŠ±å‡½æ•°**ï¼šåŸºäºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„äºŒå€¼å¥–åŠ±ï¼ˆ0/1ï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **SFT**ï¼ˆç›‘ç£å¾®è°ƒï¼‰
- **GRPO**ï¼ˆGroup Relative Policy Optimizationï¼‰â€”â€”å½“å‰ä¸»æµçš„ RLVR æ–¹æ³•
- **+RLTT**ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2ï¼‰

| Model | MATH-500 | AIME24 | BeyondAIME | GSM8K | Avg. Math |
|-------|----------|--------|------------|-------|-----------|
| +GRPO | 71.6     | 16.7   | 6.0        | 59.7  | 38.5      |
| **+RLTT** | **86.0** | **33.3** | **16.0**   | **94.0** | **57.3** |
| **æå‡ (+)** | **+14.4** | **+16.6** | **+10.0**  | **+34.3** | **+18.8** |

| Model | ARC-C | MMLU-ST | GPQA | MBPP | Avg. Non-Math |
|-------|-------|---------|------|------|----------------|
| +GRPO | 93.7  | 86.1    | 19.7 | 61.3 | 65.2           |
| **+RLTT** | **94.4** | **89.6** | **38.4** | **64.6** | **71.8**       |
| **æå‡ (+)** | **+0.7** | **+3.5** | **+18.7** | **+3.3** | **+6.6**         |

> âœ… **æ‰€æœ‰æå‡å‡ç»é…å¯¹ t æ£€éªŒç¡®è®¤å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆp < 0.05ï¼‰**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **å…¨é¢è¶…è¶Š GRPO**ï¼šåœ¨æ‰€æœ‰æ•°å­¦ä¸éæ•°å­¦åŸºå‡†ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ã€‚
- **æœ€éš¾ä»»åŠ¡æå‡æœ€å¤§**ï¼šåœ¨ AIME24 å’Œ BeyondAIME ä¸Šåˆ†åˆ«æå‡ **+16.6%** å’Œ **+10.0%**ï¼Œè¯´æ˜ RLTT ç‰¹åˆ«æ“…é•¿å¤„ç†éœ€è¦æŒç»­å¤šæ­¥æ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚
- **é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›å¼º**ï¼šå°½ç®¡åªåœ¨æ•°å­¦æ•°æ®ä¸Šè®­ç»ƒï¼ŒRLTT åœ¨ GPQA ä¸Šå‡ ä¹**ç¿»å€**äº GRPOï¼ˆ+18.7%ï¼‰ï¼Œè¡¨æ˜å…¶æå‡äº†é€šç”¨çš„éšå¼æ¨ç†èƒ½åŠ›ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰ä¸åŒå¾ªç¯æƒé‡ç­–ç•¥çš„å½±å“ï¼ˆTable 5ï¼‰
æµ‹è¯•äº†ä¸‰ç§æƒé‡æ–¹æ¡ˆï¼š
- **Uniform**ï¼ˆå‡åŒ€åŠ æƒï¼‰
- **Progressive**ï¼ˆè¶Šåæƒé‡è¶Šé«˜ï¼‰
- **Exit-probability**ï¼ˆåŸºäº Ouro çš„æ—©é€€æœºåˆ¶æ¦‚ç‡ï¼‰

> ğŸ” **å‘ç°**ï¼šä¸åŒç­–ç•¥é—´æ€§èƒ½å·®å¼‚å¾ˆå°ï¼Œè¯´æ˜ RLTT çš„ä¼˜åŠ¿ä¸»è¦æ¥è‡ªäºâ€œå…¨ç¨‹ä¿¡ç”¨åˆ†é…â€è¿™ä¸€æ ¸å¿ƒæœºåˆ¶ï¼Œè€Œéå…·ä½“æƒé‡è®¾è®¡ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒå¾ªç¯æ¬¡æ•°ä¸‹çš„è¡¨ç°ï¼ˆTable 9ï¼‰
åœ¨ 1â€“4 ä¸ªå¾ªç¯ä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š
- RLTT åœ¨æ‰€æœ‰å¾ªç¯æ•°ä¸‹å‡ä¼˜äº GRPOã€‚
- åœ¨ **1â€“2 ä¸ªå¾ªç¯**çš„å—é™æ¡ä»¶ä¸‹ï¼ŒRLTT ä¼˜åŠ¿å°¤ä¸ºæ˜æ˜¾ï¼ˆå¦‚ GSM8K ä¸Š +26.2% @1 loopï¼‰ï¼Œè¯æ˜å…¶æ—©æœŸæ¨ç†æ›´é«˜æ•ˆã€‚

#### ï¼ˆ3ï¼‰è§£ç é•¿åº¦é²æ£’æ€§ï¼ˆTable 3ï¼‰
åœ¨ä¸åŒ token é¢„ç®—ï¼ˆ1024â€“4096ï¼‰ä¸‹æµ‹è¯• MATH-500ï¼š
- RLTT åœ¨æ‰€æœ‰é¢„ç®—ä¸‹å‡ä¼˜äº GRPOã€‚
- å³ä½¿åœ¨è¿œè¶…è®­ç»ƒé•¿åº¦ï¼ˆ4096 tokensï¼‰çš„æƒ…å†µä¸‹ä»ä¿æŒé¢†å…ˆï¼Œè¯´æ˜æœªè¿‡æ‹Ÿåˆç‰¹å®šé•¿åº¦ã€‚

#### ï¼ˆ4ï¼‰Pass@k åˆ†æï¼ˆFigure 6ï¼‰
- RLTT åœ¨å¢åŠ é‡‡æ ·æ•° k æ—¶æ€§èƒ½æå‡æ›´é™¡å³­ï¼Œè¯´æ˜å…¶ä¿ç•™äº†æ›´å¤šæœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œ**ä½ç†µæºäºâ€œå¯æ§è‡ªä¿¡â€è€Œéâ€œæ¨¡å¼åç¼©â€**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **è¿‡ç¨‹å¥–åŠ±ä¼˜äºç»“æœå¥–åŠ±**ï¼šå¯¹ LoopLMs è€Œè¨€ï¼Œå¥–åŠ±å®Œæ•´çš„â€œæ€ç»´è½¨è¿¹â€æ¯”åªå¥–åŠ±æœ€ç»ˆè¾“å‡ºæ›´æœ‰æ•ˆã€‚
2. **RLTT æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡**ï¼šæ¨¡å‹èƒ½ä»¥æ›´å°‘çš„ token å’Œå¾ªç¯æ¬¡æ•°è¾¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œå‡å°‘â€œè¿‡åº¦æ€è€ƒâ€ã€‚
3. **æ³›åŒ–èƒ½åŠ›å¼º**ï¼šä¸ä»…åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¿˜èƒ½é›¶æ ·æœ¬è¿ç§»åˆ°é€»è¾‘ã€é—®ç­”ã€ç¼–ç¨‹ç­‰éæ•°å­¦é¢†åŸŸã€‚
4. **ç†è®ºæ”¯æŒ**ï¼šè®ºæ–‡ä»ç†è®ºä¸Šè¯æ˜ï¼Œè½¨è¿¹çº§ä¿¡ç”¨åˆ†é…ä¼šè¯±å¯¼æ›´çŸ­çš„æœ€ä¼˜è§£ç é•¿åº¦ï¼ˆTheorem A.5ï¼‰ï¼Œè§£é‡Šäº†å…¶é«˜æ•ˆæ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å†…å­˜å¼€é”€å¢åŠ **ï¼šéœ€å­˜å‚¨æ¯è½®å¾ªç¯çš„ log-probabilitiesï¼Œå¯¼è‡´ GPU ä¸Šå¯æ‰“åŒ…çš„ token æ•°å‡åŠï¼ˆä» 16384 é™è‡³ 8192ï¼‰ã€‚
- **æ¶æ„ä¾èµ–æ€§å¼º**ï¼šç›®å‰ä»…é€‚ç”¨äºå…·æœ‰æ˜¾å¼å¾ªç¯ç»“æ„çš„ **LoopLMs**ï¼Œéš¾ä»¥ç›´æ¥ç”¨äºæ ‡å‡† Transformerã€‚
- **å›ºå®šå¾ªç¯æ·±åº¦**ï¼šå®éªŒä¸­æœªä½¿ç”¨ Ouro åŸç”Ÿçš„è‡ªé€‚åº”æ—©é€€æœºåˆ¶ï¼Œç‰ºç‰²äº†åŠ¨æ€è®¡ç®—åˆ†é…çš„èƒ½åŠ›ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘**å†…å­˜é«˜æ•ˆçš„å®ç°**ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚
- å°† RLTT ä¸**è‡ªé€‚åº”æ—©é€€æœºåˆ¶**ç»“åˆï¼Œåœ¨ä¿æŒè½¨è¿¹å¥–åŠ±çš„åŒæ—¶æ¢å¤åŠ¨æ€è®¡ç®—èƒ½åŠ›ã€‚
- æ¢ç´¢ RLTT åœ¨å…¶ä»–ç±»å‹çš„éšå¼æ¨ç†æ¨¡å‹ï¼ˆå¦‚ Huginnï¼‰ä¸Šçš„é€‚ç”¨æ€§ã€‚
- ç ”ç©¶å¦‚ä½•å°†è¯¥æ€æƒ³æ‰©å±•è‡³éå¾ªç¯æ¶æ„ï¼Œä¾‹å¦‚é€šè¿‡ä¸­é—´å±‚ç›‘ç£æ¨¡æ‹Ÿâ€œè½¨è¿¹â€ã€‚

> ğŸ’¡ **æ€»ç»“**ï¼šRLTT é€šè¿‡â€œå¥–åŠ±è¿‡ç¨‹è€Œéä»…ä»…ç»“æœâ€ï¼ŒæˆåŠŸå¯¹é½äº†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸ LoopLMs çš„å†…åœ¨è®¡ç®—æœºåˆ¶ï¼Œä¸ºæå‡å¤§æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 15. [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)

**Authors**: Xinyu Yuan, Yan Qiao, Zonghui Wang, Wenzhi Chen  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.11057v1  

#### Abstract
The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in ba...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDivide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

æœ¬æ–‡èšç„¦äº**Multi-Commodity Flow (MCF)** é—®é¢˜ï¼Œå³åœ¨å¤æ‚ç½‘ç»œæ‹“æ‰‘ä¸­ä¸ºå¤šä¸ªå•†å“æµï¼ˆå¦‚ç½‘ç»œæµé‡ã€ç‰©æµè¿è¾“ç­‰ï¼‰åˆ†é…è·¯å¾„ï¼Œä»¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆå¦‚æœ€å°åŒ–æœ€å¤§é“¾è·¯åˆ©ç”¨ç‡ MLUã€æœ€å¤§åŒ–æ€»ååé‡ MTF æˆ–å¹¶å‘æµ MCFï¼‰ã€‚è¯¥é—®é¢˜æ˜¯ç½‘ç»œèµ„æºè°ƒåº¦ã€äº¤é€šå·¥ç¨‹ã€äº‘è®¡ç®—ç­‰é¢†åŸŸä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

éšç€ç°ä»£ç³»ç»Ÿè§„æ¨¡æ‰©å¤§ï¼ˆæ•°åƒèŠ‚ç‚¹ã€ç™¾ä¸‡çº§æµé‡ï¼‰ï¼Œä¼ ç»ŸåŸºäº **Linear Programming (LP)** çš„æ±‚è§£å™¨é¢ä¸´ä¸¥é‡ç“¶é¢ˆï¼šè®¡ç®—æ—¶é—´éšå˜é‡æ•°å‘ˆæ¬¡ç«‹æ–¹å¢é•¿ï¼ˆ$O(d^{2.3729}$)ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§éœ€æ±‚ã€‚è€Œç°æœ‰çš„ **Machine Learning (ML)** æ–¹æ³•è™½å¿«ï¼Œä½†å­˜åœ¨æ³›åŒ–èƒ½åŠ›å·®ã€å¯¹æœªè§ç¯å¢ƒæ•æ„Ÿã€éœ€å¤§é‡é‡è®­ç»ƒç­‰é—®é¢˜ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

ä½œè€…æå‡º **PRAM (Partitioned Resource Allocation with Multimodal Language Models)**ï¼Œæ˜¯é¦–ä¸ªå°† **Multimodal Language Models (MLMs)** åº”ç”¨äº MCF é—®é¢˜çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯æ¦‚æ‹¬ä¸º â€œ**Divide, Harmonize, Then Conquer**â€ï¼š

1. **Divide (åˆ’åˆ†)**ï¼š  
   å°†å…¨å±€ MCF é—®é¢˜æŒ‰**æºèŠ‚ç‚¹ (source)** åˆ’åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼ˆsubproblemsï¼‰ï¼Œæ¯ä¸ªå­ä»»åŠ¡å¤„ç†ä»åŒä¸€æºå‡ºå‘çš„æ‰€æœ‰æµã€‚è¿™æ˜¾è‘—é™ä½äº†å•ä¸ªå­é—®é¢˜çš„ç»´åº¦ï¼Œå®ç°å¹¶è¡ŒåŒ–ã€‚

2. **Harmonize (åè°ƒ)**ï¼š  
   å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„ **multi-agent reinforcement learning (MARL)** æ¡†æ¶ï¼Œé€šè¿‡ **LoRA (Low-Rank Adaptation)** å’Œ **context learning** å®ç°æ™ºèƒ½ä½“é—´çš„é€šä¿¡ä¸ååŒã€‚é‡‡ç”¨ **counterfactual policy gradients** æ¥è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“å¯¹å…¨å±€ç›®æ ‡çš„è´¡çŒ®ï¼Œä»è€Œå­¦ä¹ åè°ƒç­–ç•¥ã€‚

3. **Conquer (è§£å†³)**ï¼š  
   ä½¿ç”¨é¢„è®­ç»ƒçš„ **MLM** ä½œä¸ºå…±äº«çš„â€œä»£ç†â€æ¨¡å‹ï¼Œæ¥æ”¶**å›¾åƒå½¢å¼çš„å­æ‹“æ‰‘å›¾**ï¼ˆvision encoder è¾“å…¥ï¼‰å’Œ**æ–‡æœ¬å½¢å¼çš„éœ€æ±‚ä¿¡æ¯**ï¼ˆprompt è¾“å…¥ï¼‰ï¼Œè”åˆæ¨ç†ç”Ÿæˆé«˜è´¨é‡çš„è·¯å¾„æƒé‡åˆ†é…æ–¹æ¡ˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | æ¯” LP æ±‚è§£å™¨å¿« **1~2 ä¸ªæ•°é‡çº§**ï¼ˆ10â€“100Ã—ï¼‰ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ‹“æ‰‘ä¸Šè¡¨ç°çªå‡ºã€‚ |
| **è´¨é‡** | æ€§èƒ½æ¥è¿‘æœ€ä¼˜ï¼ˆLP è§£ï¼‰ï¼Œå¹³å‡å·®è· <8%ï¼Œéƒ¨åˆ†åœºæ™¯ç”šè‡³ä¼˜äº LPï¼ˆå¦‚ CERNET ä¸Š MLU ä½ 21%ï¼‰ã€‚ |
| **æ³›åŒ–æ€§** | å¯¹é“¾è·¯æ•…éšœã€æµé‡çªå¢ç­‰æœªè§äº‹ä»¶è¡¨ç°å‡ºå¼ºé²æ£’æ€§ï¼ˆæ€§èƒ½ä¸‹é™ <10%ï¼‰ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ |
| **é€šç”¨æ€§** | æ”¯æŒå¤šç§ MCF ç›®æ ‡ï¼ˆMLU, MTF, MCFï¼‰ï¼Œä¸”ä¸ä¾èµ–ç‰¹å®š DNN æ¶æ„è®¾è®¡ï¼Œé€‚é…æ€§å¼ºã€‚ |
| **å¯æ‰©å±•æ€§** | é€šè¿‡é—®é¢˜åˆ’åˆ†å’Œè½»é‡é€‚é…ï¼Œæ¨¡å‹å‚æ•°å¢é•¿ç¼“æ…¢ï¼Œé€‚ç”¨äºè¶…å¤§è§„æ¨¡ç½‘ç»œã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†

å®éªŒæ¶µç›–ä¸¤ç±»æ•°æ®é›†ï¼š

#### (1) **çœŸå®ä¸–ç•Œæ•°æ®é›†**ï¼ˆå°è§„æ¨¡ï¼‰
- **MetaDB**, **MetaWEB**ï¼šç¤¾äº¤ç½‘ç»œé›†ç¾¤å†…éƒ¨æµé‡
- **Abilene**, **CERNET**, **GEANT**ï¼šå¹¿åŸŸç½‘ï¼ˆWANï¼‰æµé‡çŸ©é˜µï¼ˆpublic TM æ•°æ®ï¼‰

#### (2) **å¤§è§„æ¨¡åˆæˆæ•°æ®é›†**
- **GtsCe**, **Colt**, **UsCarrier**, **Cogentco**, **Kdl**ï¼šä» Internet Topology Zoo æå–çš„å¤§è§„æ¨¡æ‹“æ‰‘ï¼ˆ100â€“754 èŠ‚ç‚¹ï¼‰
- éœ€æ±‚ç”Ÿæˆæ–¹å¼ï¼š
  - **Gravity Model**ï¼ˆä¸»ç”¨ï¼‰
  - **Poisson Model**
  - **Bimodal Model**

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡å½’ä¸€åŒ–è‡³ LP æœ€ä¼˜è§£ï¼‰
- **Maximum Link Utilization (MLU)** â†“ï¼šè¶Šä½è¶Šå¥½ï¼Œåæ˜ ç½‘ç»œå¥å£®æ€§
- **Maximum Total Flow (MTF)** â†‘ï¼šè¶Šé«˜è¶Šå¥½ï¼Œåæ˜ ååèƒ½åŠ›
- **Maximum Concurrent Flow (MCF)** â†‘ï¼šè¶Šé«˜è¶Šå¥½ï¼Œåæ˜ å…¬å¹³æ€§

#### å®éªŒè®¾ç½®
- ä¸»å¹²æ¨¡å‹ï¼š`Qwen2.5-VL-7B-Instruct`ï¼ˆæˆªæ–­å‰ 8 å±‚ï¼‰
- é€‚é…æ–¹å¼ï¼šLoRA + context learning + MARL fine-tuning
- è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹ï¼š7:1:2
- ç¡¬ä»¶ï¼šåŒ NVIDIA A100 GPU

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **LP (Gurobi)** | ä¼˜åŒ–æ±‚è§£å™¨ | è¿‘ä¼¼æœ€ä¼˜ï¼Œä½†ææ…¢ |
| **LP-top** | å¯å‘å¼ | ä»…å¯¹ top 10% æµé‡ç”¨ LPï¼Œå…¶ä½™èµ°æœ€çŸ­è·¯å¾„ |
| **POP** | åˆ†æ²»ä¼˜åŒ– | å°†é—®é¢˜å¤åˆ¶ k ä»½å¹¶è¡Œæ±‚è§£ |
| **DRL** | å¼ºåŒ–å­¦ä¹  | ç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥ç½‘ç»œ |
| **HARP** | GNN-based ML | å›¾ç¥ç»ç½‘ç»œ + ä¸å˜æ€§è®¾è®¡ |
| **Aether** | å¤šæ™ºèƒ½ä½“ GNN | åŒºåˆ†å¤§å°æµï¼Œåˆ†åˆ«å¤„ç† |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | PRAM è¡¨ç° |
|------|-----------|
| **MLU å¹³å‡å·®è·** | <8% vs. LP æœ€ä¼˜ |
| **è¿è¡Œæ—¶åŠ é€Ÿæ¯”** | **10â€“100Ã—** å¿«äº LPï¼Œ**5â€“7Ã—** å¿«äº POP |
| **æœ€å¤§è§„æ¨¡æµ‹è¯•** | Kdl æ‹“æ‰‘ï¼ˆ754 èŠ‚ç‚¹ï¼Œ1790 é“¾è·¯ï¼‰ä¸‹ **<25 ç§’å®Œæˆä¸€æ¬¡åˆ†é…** |
| **é²æ£’æ€§** | åœ¨é“¾è·¯æ•…éšœæˆ–æµé‡æ³¢åŠ¨ä¸‹ï¼Œæ€§èƒ½ä¸‹é™ **<10%** |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### (1) å°è§„æ¨¡çœŸå®æ•°æ®é›†ï¼ˆFig. 6ï¼‰
- PRAM åœ¨æ‰€æœ‰ä¸‰ä¸ªç›®æ ‡ä¸Šå‡è¾¾åˆ°**ç¬¬äºŒå¥½æ€§èƒ½**ï¼Œä»…æ¬¡äºæ‹¥æœ‰å®Œç¾é¢„æµ‹çš„ LPã€‚
- åœ¨ **CERNET å’Œ GEANT** ä¸Šï¼ŒPRAM çš„ **MLU ç”šè‡³ä¼˜äº LP**ï¼ˆåˆ†åˆ«ä½ 21% å’Œ 45%ï¼‰ï¼Œè¡¨æ˜ MLM æ›´æ“…é•¿æ•æ‰ MLU çš„å‡¸æ€§ç»“æ„ã€‚
- HARP ä¸“ä¸º MLU è®¾è®¡ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°ç¬¬äºŒï¼Œä½†åœ¨ MTF/MCF ä¸Šè¾ƒå·®ã€‚

#### (2) å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ˆFig. 7ï¼‰
- PRAM åœ¨æ‰€æœ‰æ‹“æ‰‘ä¸Šå‡å®ç°**å¯æ‰©å±•çš„é«˜æ€§èƒ½**ï¼Œè´¨é‡æ¥è¿‘ LPï¼Œé€Ÿåº¦è¿œè¶…æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•ã€‚
- ç›¸æ¯” HARP å’Œ Aetherï¼š
  - MLU é™ä½ **6.1% å’Œ 17.2%**
  - MTF æå‡ **16.6% å’Œ 7.3%**
  - MCF æå‡ **24.8% å’Œ 13.5%**

### æ¶ˆèå®éªŒç»“æœ

#### (1) **æ˜¯å¦åˆ’åˆ†é—®é¢˜ï¼ˆw/o partitionï¼‰**
- æ— åˆ’åˆ†æ—¶ï¼Œå¯è®­ç»ƒå‚æ•°æ€¥å‰§è†¨èƒ€ï¼ˆKdl ä¸Šè¾¾ 31,600 MBï¼‰ï¼Œæ— æ³•å®é™…éƒ¨ç½²ã€‚
- æœ‰åˆ’åˆ†æ—¶ï¼Œå‚æ•°å¢é•¿å¹³ç¼“ï¼Œä½“ç°å…¶**å¯æ‰©å±•æ€§ä¼˜åŠ¿**ã€‚

#### (2) **å…³é”®ç»„ä»¶æ¶ˆèï¼ˆFig. 9bï¼‰**
- **w/o MLM**ï¼šæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå°¤å…¶åœ¨é“¾è·¯æ•…éšœåœºæ™¯ä¸‹ï¼Œè¯´æ˜ MLM çš„å…ˆéªŒçŸ¥è¯†å¯¹æ³›åŒ–è‡³å…³é‡è¦ã€‚
- **w/o LoRA / w/o context / w/o MARL**ï¼šæ€§èƒ½å‡æœ‰ä¸‹é™ï¼Œè¯æ˜ä¸‰è€…å…±åŒæ„æˆæœ‰æ•ˆçš„è½»é‡åè°ƒæœºåˆ¶ã€‚
- **w/o MARL**ï¼šæ— æ³•è¯„ä¼°ä¸ªä½“è´¡çŒ®ï¼Œå¯¼è‡´åè°ƒå¤±è´¥ã€‚

#### (3) **ä¸åŒ MLM ä¸»å¹²å¯¹æ¯”ï¼ˆFig. 19ï¼‰**
- `Qwen2.5-VL-7B` > `Qwen2.5-VL-3B`ï¼šå‚æ•°è¶Šå¤šï¼Œæ€§èƒ½è¶Šå¥½ã€‚
- `Qwen2.5-VL-7B` > `Llama-3.2-11B-Vision`ï¼šå°½ç®¡åè€…æ›´å¤§ï¼Œä½† Qwen å› æ›´å¼ºçš„æ•°å­¦æ¨ç†è®­ç»ƒï¼Œåœ¨ MCF ä¸Šè¡¨ç°æ›´ä¼˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. **MLM å¯ä½œä¸ºå¼ºå¤§çš„ MCF æ±‚è§£å™¨**ï¼š  
   MLM ä¸ä»…èƒ½ç†è§£å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾+æ–‡ï¼‰ï¼Œè¿˜èƒ½é€šè¿‡ **in-context learning** æ¨¡æ‹Ÿ **gradient descent** è¿‡ç¨‹ï¼Œé€¼è¿‘æœ€ä¼˜è§£ï¼ˆTheorem 2ï¼‰ã€‚

2. **Divide-and-Conquer + MARL æ˜¯æœ‰æ•ˆèŒƒå¼**ï¼š  
   é€šè¿‡æºèŠ‚ç‚¹åˆ’åˆ†å®ç°é«˜æ•ˆå¹¶è¡Œï¼Œå†é€šè¿‡è½»é‡ MARL åè°ƒï¼Œæ—¢ä¿è¯äº†é€Ÿåº¦ï¼Œåˆç»´æŒäº†å…¨å±€ä¸€è‡´æ€§ã€‚

3. **PRAM å…·å¤‡å¼ºç†è®ºä¿éšœ**ï¼š  
   - MCF ç›®æ ‡å‡½æ•°å…·æœ‰å‡¸/å‡¹æ€§ï¼ˆTheorem 1ï¼‰
   - MARL é€‚é…è¿‡ç¨‹æ”¶æ•›ï¼ˆLemma 1ï¼‰
   - MLM å¯æ¨¡æ‹Ÿ GD æ›´æ–°ï¼ˆTheorem 2ï¼‰

4. **PRAM æ³›åŒ–èƒ½åŠ›å¼º**ï¼š  
   åœ¨é“¾è·¯æ•…éšœã€æµé‡æ³¢åŠ¨ç­‰æœªè§åœºæ™¯ä¸‹ä»ä¿æŒç¨³å®šæ€§èƒ½ï¼Œè¿œè¶…ä¼ ç»Ÿ ML æ–¹æ³•ã€‚

### æ–¹æ³•çš„å±€é™æ€§

1. **è®­ç»ƒæˆæœ¬é«˜**ï¼šå°½ç®¡æ¨ç†å¿«ï¼Œä½† fine-tuning ä»éœ€å¤§é‡èµ„æºã€‚
2. **è§†è§‰ç¼–ç å¯èƒ½å¼•å…¥åå·®**ï¼šå°†æ‹“æ‰‘å›¾è½¬ä¸ºå›¾åƒå¯èƒ½å¯¼è‡´ä¿¡æ¯æŸå¤±æˆ–è¯¯è§£ï¼ˆå¦‚è¾¹é‡å ï¼‰ã€‚
3. **éè‡ªå›å½’ç”Ÿæˆ**ï¼šå½“å‰ä¸º non-autoregressive è¾“å‡ºï¼Œæœªæ¥å¯æ¢ç´¢ autoregressive å½¢å¼æå‡ç²¾åº¦ã€‚
4. **å†…å­˜å¼€é”€**ï¼šé‡å¤å¤„ç†ç›¸åŒå­å›¾ä»æ¶ˆè€—è¾ƒå¤šæ˜¾å­˜ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘

1. æ¢ç´¢æ›´é«˜æ•ˆçš„ **parameter-efficient fine-tuning** æ–¹æ³•ï¼ˆå¦‚ Adapterã€IAÂ³ï¼‰ã€‚
2. å¼•å…¥ **autoregressive generation** ä»¥æ”¯æŒæ›´å¤æ‚çš„å†³ç­–åºåˆ—ã€‚
3. ç»“åˆ **memory mechanisms** å‡å°‘é‡å¤è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚
4. æ‰©å±•è‡³åŠ¨æ€ MCFã€ä¸ç¡®å®šæ€§ä¼˜åŒ–ç­‰æ›´å¤æ‚åœºæ™¯ã€‚

---

> **ä»£ç å¼€æº**ï¼šhttps://github.com/Y-debug-sys/PRAM  
> **ä¼¦ç†å£°æ˜**ï¼šæœªä½¿ç”¨éå…¬å¼€æ•°æ®ï¼Œæ— éšç§æ³„éœ²é£é™©ã€‚

</details>

---

### 16. [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)

**Authors**: Mashrekur Rahman  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.10354v1  

#### Abstract
Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretabil...

---

### 17. [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)

**Authors**: Arnav Shah, Junzhe Li, Parsa Idehpour, Adibvafa Fallahpour, Brandon Wang, Sukjun Hwang, Bo Wang, Patrick D. Hsu, Hani Goodarzi, Albert Gu  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.10603v1  

#### Abstract
Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coh...

---

### 18. [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)

**Authors**: Jialin Liu, Zhaorui Zhang, Ray C. C. Cheung  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.11062v1  

#### Abstract
Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content...

---

### 19. [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)

**Authors**: Bo Xue, Yunchong Song, Fanghao Shao, Xuekai Zhu, Lin Chen, Luoyi Fu, Xinbing Wang, Zhouhan Lin  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.10583v1  

#### Abstract
Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but o...

---

### 20. [A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors](https://arxiv.org/abs/2602.10451)

**Authors**: Jinkyo Han, Bahador Bahmani  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.10451v1  

#### Abstract
Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a...

---

### 21. [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)

**Authors**: Abhishek Vijaya Kumar, Bhaskar Kataria, Byungsoo Oh, Emaad Manzoor, Rachee Singh  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.10986v1  

#### Abstract
In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reus...

---

### 22. [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)

**Authors**: Jingang Qu, David Holzm\"uller, Ga\"el Varoquaux, Marine Le Morvan  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.11139v1  

#### Abstract
Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classificatio...

---

### 23. [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)

**Authors**: Hongyu Zhao, Siyu Zhou, Haolin Yang, Zengyi Qin, Tianyi Zhou  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10480v1  

#### Abstract
Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical cons...

---

### 24. [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)

**Authors**: Kanta Yamaoka, Sumantrak Mukherjee, Thomas G\"artner, David Antony Selby, Stefan Konigorski, Eyke H\"ullermeier, Viktor Bengs, Sebastian Josef Vollmer  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10282v1  

#### Abstract
Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, ...

---

### 25. [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)

**Authors**: Jaeyeon Kim, Jonathan Geuter, David Alvarez-Melis, Sham Kakade, Sitan Chen  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10314v1  

#### Abstract
Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a train...

---

### 26. [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)

**Authors**: Shuai Han, Mehdi Dastani, Shihan Wang  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10598v1  

#### Abstract
Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In ...

---

### 27. [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)

**Authors**: Yusong Lin, Haiyang Wang, Shuzhe Wu, Lue Fan, Feiyang Pan, Sanyuan Zhao, Dandan Tu  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10999v1  

#### Abstract
Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to ...

---

### 28. [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)

**Authors**: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11136v1  

#### Abstract
As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems...

---

### 29. [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)

**Authors**: Weihao Liu, Dehai Min, Lu Cheng  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10229v1  

#### Abstract
While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has e...

---

### 30. [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)

**Authors**: Yuming Yan, Shuo Yang, Kai Tang, Sihong Chen, Yang Zhang, Ke Xu, Dan Hu, Qun Yu, Pengfei Hu, Edith C. H. Ngai  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10740v1  

#### Abstract
Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forget...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- kv cache, offload, State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
