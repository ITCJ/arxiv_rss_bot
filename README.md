# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2025-12-25 06:04:06 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)

**Authors**: Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 13.5  
**Type**: new  
**ArXiv ID**: 2512.20276v1  

#### Abstract
Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth rob...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
Vision-Language-Action (VLA) æ¨¡å‹åœ¨æœºå™¨äººæ„ŸçŸ¥ä¸æ§åˆ¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé•¿è§†é‡ä»»åŠ¡æ‰§è¡Œæ½œåŠ›ï¼Œä½†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²é¢ä¸´ä¸¥é‡çš„**é«˜æ¨ç†å»¶è¿Ÿç“¶é¢ˆ**ã€‚ç”±äºä¾èµ–è‡ªå›å½’è§£ç ï¼ˆautoregressive decodingï¼‰ï¼Œä¸»æµ VLA æ¨¡å‹åœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šé€šå¸¸åªèƒ½è¾¾åˆ° 3â€“5 FPSï¼Œè¿œä½äºåŠ¨æ€äº¤äº’æ‰€éœ€çš„ 20â€“30 Hz æ§åˆ¶é¢‘ç‡ã€‚è¿™ä¸€æ€§èƒ½å·®è·é™åˆ¶äº†å…¶åœ¨å®æ—¶ã€åŠ¨æ€åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

ç°æœ‰ä¼˜åŒ–æ–¹æ³•å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š
- **ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼ˆå¦‚ Continuous Batchingï¼‰é€‚ç”¨äºå¤šç”¨æˆ·æœåŠ¡å™¨åœºæ™¯ï¼Œä¸é€‚ç”¨äºå•ç”¨æˆ·æœºå™¨äººæ§åˆ¶ï¼›
- **è¾¹ç¼˜ä¼˜åŒ–æŠ€æœ¯**ï¼ˆå¦‚é‡åŒ–ã€è’¸é¦ï¼‰è™½èƒ½å‡å°æ¨¡å‹è§„æ¨¡ï¼Œä½†æ— æ³•æ”¹å˜è§£ç é˜¶æ®µå†…å­˜å—é™çš„æœ¬è´¨ï¼›
- **ç®—æ³•çº§æ”¹è¿›**ï¼ˆå¦‚å¹¶è¡Œè§£ç ã€éè‡ªå›å½’ç”Ÿæˆï¼‰å¸¸éœ€é‡æ–°è®­ç»ƒï¼Œå¯èƒ½æŸå®³ç²¾åº¦æˆ–å¼•å…¥åŠ¨ä½œä¸è¿ç»­æ€§ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡º **ActionFlow** â€”â€” ä¸€ç§é¢å‘èµ„æºå—é™è¾¹ç¼˜å¹³å°çš„**çº¯ç³»ç»Ÿçº§æ¨ç†åŠ é€Ÿæ¡†æ¶**ï¼Œæ— éœ€é‡è®­ç»ƒå³å¯æ˜¾è‘—æå‡ VLA æ¨ç†é€Ÿåº¦ã€‚

å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å•ä¸€ VLA è¯·æ±‚å†…éƒ¨çš„è‡ªå›å½’è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªç”±å¤šä¸ªâ€œå¾®è¯·æ±‚â€ï¼ˆmicro-requestsï¼‰ç»„æˆçš„æµæ°´çº¿ï¼š
- æ¯ä¸ªæ—¶é—´æ­¥åŒ…å«ä¸€æ¬¡ **Prefill**ï¼ˆè®¡ç®—å¯†é›†å‹ï¼‰å’Œå¤šæ¬¡ **Decode**ï¼ˆå†…å­˜å¯†é›†å‹ï¼‰æ“ä½œã€‚
- åˆ©ç”¨æœºå™¨äººæ§åˆ¶çš„è¿ç»­æ€§ï¼Œåœ¨ä¸åŒæ—¶é—´æ­¥ä¹‹é—´è¿›è¡Œè·¨è¯·æ±‚æµæ°´çº¿è°ƒåº¦ï¼ˆCross-Request Pipeliningï¼‰ï¼Œå°†å½“å‰å¸§çš„ Prefill é˜¶æ®µä¸å†å²å¸§çš„å¤šä¸ª Decode é˜¶æ®µåˆå¹¶ä¸ºä¸€ä¸ªæ‰¹é‡å¤„ç†çš„å¤§çŸ©é˜µè¿ç®—ã€‚

ä¸ºæ­¤ï¼Œä½œè€…è®¾è®¡äº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ï¼š
1. **Cross-Request State Packed Forward Operator**  
   å°†å¤šä¸ªåˆ†æ•£çš„å°è§„æ¨¡ Decode æ“ä½œèåˆä¸ºä¸€ä¸ªå¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ï¼Œæé«˜ç®—æœ¯å¼ºåº¦ï¼ˆarithmetic intensityï¼‰ï¼Œä½¿è´Ÿè½½ä»å†…å­˜ç“¶é¢ˆè½¬å‘è®¡ç®—ç“¶é¢ˆã€‚
   
2. **Unified KV Ring Buffer**  
   æ‰€æœ‰æ´»è·ƒè¯·æ±‚çš„ KV ç¼“å­˜ç»Ÿä¸€å­˜å‚¨åœ¨ä¸€ä¸ªç‰©ç†è¿ç»­çš„ç¯å½¢ç¼“å†²åŒºä¸­ï¼Œé¿å…ä¼ ç»Ÿæ–¹æ³•ä¸­é¢‘ç¹çš„æ•°æ®æ‹·è´ä¸ CPU-GPU åŒæ­¥å¼€é”€ï¼Œæ”¯æŒé«˜æ•ˆçš„ Variable-Length Attentionï¼ˆVarlen-Attentionï¼‰ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ActionFlow | ç°æœ‰æ–¹æ³• |
|------|-----------|---------|
| æ˜¯å¦éœ€è¦é‡è®­ç»ƒ | âŒ ä¸éœ€è¦ | âœ… å¤šæ•°éœ€è¦ï¼ˆå¦‚å¹¶è¡Œè§£ç ï¼‰ |
| æ˜¯å¦ä¾èµ–å¤–éƒ¨æ‰¹å¤„ç† | âŒ å•ç”¨æˆ·ä¹Ÿèƒ½åŠ é€Ÿ | âœ… é€šå¸¸ä¾èµ–å¤šç”¨æˆ·è¯·æ±‚ |
| å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡æå‡ | âœ… æ˜¾è‘—æå‡ï¼ˆä»å†…å­˜ç“¶é¢ˆè½¬ä¸ºè®¡ç®—é¥±å’Œï¼‰ | âš ï¸ æœ‰é™ |
| åŠŸèƒ½ä¸€è‡´æ€§ | âœ… å®Œå…¨ç­‰ä»·äºåŸå§‹æ¨¡å‹è¾“å‡º | âš ï¸ å¯èƒ½å¼•å…¥è¯¯å·®æˆ–åŠ¨ä½œè·³è·ƒ |
| æ­£äº¤æ€§ | âœ… å¯ä¸é‡åŒ–ã€Speculative Decoding ç­‰æ­£äº¤ç»“åˆ | â€” |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šActionFlow æ˜¯é¦–ä¸ªä¸“ä¸ºå•ç”¨æˆ·ã€ä½å»¶è¿Ÿæœºå™¨äººæ§åˆ¶åœºæ™¯è®¾è®¡çš„ç³»ç»Ÿçº§ VLA åŠ é€Ÿæ–¹æ¡ˆï¼Œé€šè¿‡é‡æ„æ‰§è¡Œæµç¨‹è€Œéä¿®æ”¹æ¨¡å‹æœ¬èº«ï¼Œå®ç°äº†é«˜æ•ˆä¸”æ— æŸçš„æ¨ç†åŠ é€Ÿã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ¨¡å‹ä¸å¹³å°**
- **æ¨¡å‹**ï¼šOpenVLA-7Bï¼ˆ70äº¿å‚æ•°ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼š
  - **NVIDIA Jetson AGX Orin**ï¼ˆ64GBï¼‰ï¼šä»£è¡¨èµ„æºå—é™çš„åµŒå…¥å¼æœºå™¨äººè®¾å¤‡
  - **NVIDIA RTX 5090**ï¼šä»£è¡¨é«˜æ€§èƒ½è¾¹ç¼˜å·¥ä½œç«™
- **è½¯ä»¶æ ˆ**ï¼šPyTorch 2.6.0, Transformers 4.49.0, CUDA 12.6

### **è¯„ä¼°æŒ‡æ ‡**
- **FPS**ï¼ˆFrames Per Secondï¼‰ï¼šæ ¸å¿ƒååé‡æŒ‡æ ‡ï¼Œè¡¡é‡æ¯ç§’å¯å®Œæˆçš„æ¨ç†å¸§æ•°
- **Latency**ï¼ˆmsï¼‰ï¼šç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿ
- **Speedup**ï¼šç›¸å¯¹äº Baseline çš„åŠ é€Ÿæ¯”
- **Success Rate (%)**ï¼šåœ¨ LIBERO åŸºå‡†ä¸ŠéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§ï¼Œç¡®ä¿æ— ç²¾åº¦æŸå¤±

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | æè¿° |
|------|------|
| **Baseline** | æ ‡å‡†è‡ªå›å½’æ¨ç†ï¼Œæ— ä»»ä½•ç³»ç»Ÿä¼˜åŒ– |
| **Naive Pipe** | å®ç°äº†è·¨è¯·æ±‚æµæ°´çº¿ï¼Œä½†ä½¿ç”¨åŠ¨æ€ KV ç¼“å­˜æ‹¼æ¥å’Œ CPU åè°ƒæ‰¹å¤„ç†ï¼ˆæœªèåˆç®—å­ï¼‰ |
| **ActionFlow** | å®Œæ•´å®ç°ï¼ŒåŒ…å« Cross-Request State Packed Forward å’Œ Unified KV Ring Buffer |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1ï¼‰**

| Method | Platform | FPS | Speedup |
|--------|----------|-----|---------|
| Baseline | AGX Orin | 1.25 Â± 0.01 | 1.00x |
| Naive Pipe | AGX Orin | 2.70 Â± 0.19 | 2.16x |
| **ActionFlow** | **AGX Orin** | **3.20 Â± 0.17** | **2.56x** |
| Baseline | RTX 5090 | 7.62 Â± 0.14 | 1.00x |
| Naive Pipe | RTX 5090 | 15.60 Â± 0.08 | 2.04x |
| **ActionFlow** | **RTX 5090** | **19.45 Â± 0.22** | **2.55x** |

> âœ… åœ¨ä¸¤ç§å¹³å°ä¸Šå‡å®ç°çº¦ **2.55x çš„ FPS æå‡**ï¼Œé¦–æ¬¡ä½¿ OpenVLA-7B åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ¥è¿‘å¯ç”¨æ§åˆ¶é¢‘ç‡ï¼ˆ~3.2 FPS â†’ æ¥è¿‘ 30Hz æ§åˆ¶éœ€æ±‚çš„ä¸€åŠï¼Œå·²å¯ç”¨äºéƒ¨åˆ†ä»»åŠ¡ï¼‰ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**
- **ActionFlow vs. Naive Pipe**ï¼š
  - åœ¨ AGX Orin ä¸Šé¢å¤–å¸¦æ¥ **18.5% æ€§èƒ½æå‡**ï¼ˆ3.20 vs. 2.70 FPSï¼‰
  - åœ¨ RTX 5090 ä¸Šé¢å¤–å¸¦æ¥ **24.7% æå‡**ï¼ˆ19.45 vs. 15.60 FPSï¼‰
- **è¯´æ˜**ï¼šCustom fused operatorsï¼ˆç‰¹åˆ«æ˜¯ `FUSEDROPEANDWRITEKV` å’Œ `INPLACESHIFTKV`ï¼‰æœ‰æ•ˆæ¶ˆé™¤äº† CPU-GPU åŒæ­¥æ°”æ³¡å’Œå†…å­˜å¤åˆ¶å¼€é”€ï¼Œæ˜¯æ€§èƒ½çªç ´çš„å…³é”®ã€‚

---

### **æ•æ„Ÿæ€§åˆ†æï¼ˆFigure 5ï¼‰**
- **è´Ÿè½½è¶Šé‡ï¼ŒåŠ é€Ÿæ•ˆæœè¶Šæ˜¾è‘—**ï¼š
  - å½“ Decode é•¿åº¦ $ K = 32 $ ä¸” Prefill è¾ƒé•¿æ—¶ï¼š
    - RTX 5090 ä¸Šè¾¾åˆ° **4.06x åŠ é€Ÿ**
    - AGX Orin ä¸Šè¾¾åˆ° **4.36x åŠ é€Ÿ**
- **å¯¹ Decode é•¿åº¦é²æ£’æ€§å¼º**ï¼š
  - Baseline éš $ K $ å¢åŠ æ€§èƒ½ä¸‹é™ 72%
  - ActionFlow ä»…ä¸‹é™ 25%ï¼Œè¡¨æ˜å…¶æˆåŠŸæ‘Šé”€äº†è‡ªå›å½’ä¸²è¡Œå¼€é”€

---

### **åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯ï¼ˆTable 2ï¼‰**

| Method | Platform | Spatial | Object | Goal | Long |
|--------|----------|--------|--------|------|------|
| OpenVLA (Baseline) | RTX 5090 | 84.4% | 73.8% | 74.4% | 51.4% |
| ActionFlow | RTX 5090 | 84.3% | 71.2% | 78.6% | 53.3% |
| ActionFlow | AGX Orin | 83.4% | 68.8% | 75.6% | 49.0% |

> âœ… æ‰€æœ‰ä»»åŠ¡ç±»åˆ«æˆåŠŸç‡æ³¢åŠ¨åœ¨ç»Ÿè®¡è¯¯å·®èŒƒå›´å†…ï¼Œè¯æ˜ **ActionFlow æ˜¯åŠŸèƒ½æ— æŸçš„ç³»ç»Ÿä¼˜åŒ–**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **VLA æ¨ç†ç“¶é¢ˆæœ¬è´¨åœ¨äº Decode é˜¶æ®µçš„ä½ç®—æœ¯å¼ºåº¦å’Œé«˜è°ƒåº¦å¼€é”€**ï¼Œè€Œéå•çº¯çš„æ¨¡å‹å¤§å°é—®é¢˜ã€‚
2. **å³ä½¿åœ¨å•ç”¨æˆ·åœºæ™¯ä¸‹ï¼Œä¹Ÿå¯é€šè¿‡â€œå†…éƒ¨å¾®è¯·æ±‚æµæ°´çº¿â€åˆ›é€ æ‰¹å¤„ç†æœºä¼š**ï¼Œè¿™æ˜¯ä¼ ç»Ÿ LLM æ¨ç†å¼•æ“å¿½è§†çš„æœºä¼šã€‚
3. **ç³»ç»Ÿçº§ä¼˜åŒ–å¯ä»¥ç‹¬ç«‹äºç®—æ³•æ”¹è¿›å‘æŒ¥ä½œç”¨**ï¼ŒActionFlow ä¸é‡åŒ–ã€Speculative Decoding ç­‰å®Œå…¨æ­£äº¤ï¼Œå…·å¤‡è‰¯å¥½æ‰©å±•æ€§ã€‚
4. **Unified KV Ring Buffer + Fused Operators èƒ½æ˜¾è‘—é™ä½åŒæ­¥ä¸å†…å­˜å¼€é”€**ï¼Œæ˜¯å®ç°é«˜æ•ˆæµæ°´çº¿çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **åŠ é€Ÿæ¯”å—é™äºåŠ¨ä½œåºåˆ—é•¿åº¦ $ K $**ï¼šå½“ $ K=1 $ æ—¶æ— æ³•å½¢æˆæœ‰æ•ˆæµæ°´çº¿ï¼Œæ”¶ç›Šè¾ƒå°ã€‚
- **éœ€è¦å›ºå®šæˆ–é¢„çŸ¥åŠ¨ä½œé•¿åº¦**ï¼šç›®å‰å‡è®¾æ¯ä¸ªä»»åŠ¡ç”Ÿæˆå›ºå®šé•¿åº¦çš„åŠ¨ä½œåºåˆ—ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºå˜é•¿è¾“å‡ºåœºæ™¯ã€‚
- **å†…å­˜å ç”¨ç•¥æœ‰å¢åŠ **ï¼šUnified KV Ring Buffer éœ€é¢„ç•™ç©ºé—´ç»™ $ K $ ä¸ªå¹¶å‘è¯·æ±‚ï¼Œå¯¹æç«¯å†…å­˜å—é™è®¾å¤‡å¯èƒ½æ„æˆæŒ‘æˆ˜ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. æ”¯æŒ **å¯å˜é•¿åº¦åŠ¨ä½œç”Ÿæˆ** çš„åŠ¨æ€æµæ°´çº¿è°ƒåº¦æœºåˆ¶
2. ç»“åˆ **Speculative Decoding** æˆ– **Early Exit** è¿›ä¸€æ­¥å‡å°‘å®é™…è§£ç æ­¥æ•°
3. æ¢ç´¢ä¸ **Vision Token Sparsification** ç­‰è§†è§‰å‰ç«¯ä¼˜åŒ–çš„ååŒ
4. æ‰©å±•è‡³æ›´å¤š VLA æ¶æ„ï¼ˆå¦‚åŸºäº Mamba çš„ Robo-Mambaï¼‰
5. åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šéƒ¨ç½²ï¼Œæµ‹è¯•é—­ç¯æ§åˆ¶ç¨³å®šæ€§

---

> ğŸš€ **æ€»ä½“è¯„ä»·**ï¼šActionFlow æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„ç³»ç»Ÿè§†è§’æ¥è§£å†³ VLA åœ¨è¾¹ç¼˜éƒ¨ç½²çš„å»¶è¿Ÿéš¾é¢˜ï¼Œå…¶â€œå°†æ—¶é—´ç»´åº¦è½¬åŒ–ä¸ºæ‰¹å¤„ç†ç»´åº¦â€çš„æ€æƒ³å…·æœ‰å¯å‘æ„ä¹‰ï¼Œä¸ºé€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼ˆembodied agentsï¼‰çš„å®æ—¶åŒ–é“ºå¹³äº†é“è·¯ã€‚

</details>

---

### 2. [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)

**Authors**: Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.20953v1  

#### Abstract
The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D par...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDiving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹åœ¨**å¼‚æ„GPUç¯å¢ƒ**ï¼ˆå°¤å…¶æ˜¯åŸºäºspot instanceçš„åŠ¨æ€èµ„æºé›†ç¾¤ï¼‰ä¸­è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ†å¸ƒå¼è®­ç»ƒæ—¶é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **å¯¹ç§°æ€§é™åˆ¶**ï¼šç°æœ‰3Då¹¶è¡Œï¼ˆData Parallelism, Tensor Parallelism, Pipeline Parallelismï¼‰ç³»ç»Ÿé€šå¸¸å‡è®¾GPUèµ„æºæ˜¯åŒæ„çš„ï¼Œå¼ºåˆ¶é‡‡ç”¨å¯¹ç§°çš„å¹¶è¡Œç»“æ„ï¼Œä¸¥é‡é™åˆ¶äº†åœ¨å¼‚æ„ç¯å¢ƒä¸‹çš„ä¼˜åŒ–ç©ºé—´ã€‚
2. **è´Ÿè½½ä¸å‡è¡¡**ï¼šä¸åŒè®¡ç®—èƒ½åŠ›å’Œå†…å­˜å®¹é‡çš„GPUæ··åˆä½¿ç”¨æ—¶ï¼Œè‹¥ä¸èƒ½æœ‰æ•ˆå¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œä¼šå¯¼è‡´é«˜æ€§èƒ½GPUç©ºé—²ç­‰å¾…ä½æ€§èƒ½GPUï¼Œå½¢æˆâ€œstragglerâ€ç“¶é¢ˆã€‚
3. **å¼¹æ€§æ¢å¤æ•ˆç‡ä½**ï¼šå½“spot instanceè¢«æŠ¢å åï¼Œä¼ ç»Ÿcheckpointæ¢å¤æœºåˆ¶ä¾èµ–ä»äº‘å­˜å‚¨ä¸‹è½½å®Œæ•´æ£€æŸ¥ç‚¹ï¼Œé€šä¿¡å¼€é”€å¤§ï¼Œæ¢å¤æ—¶é—´é•¿ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡º **AutoHet** â€”â€” ä¸€ä¸ªé¢å‘å¼‚æ„GPUé›†ç¾¤çš„è‡ªåŠ¨åŒ–3Då¹¶è¡Œè®­ç»ƒç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### âœ… æ”¯æŒéå¯¹ç§°3Då¹¶è¡Œç»“æ„
- **ç†è®ºçªç ´**ï¼šé€šè¿‡å®éªŒè¯æ˜ï¼š
  - **Tensor Parallelism (TP)** å¿…é¡»ä¿æŒ**è·¨DPç»„çš„å¯¹ç§°æ€§**ï¼Œå¦åˆ™AllReduceå‰éœ€é«˜ä»£ä»·è½¬ç½®æ“ä½œï¼Œå¸¦æ¥æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼ˆååä¸‹é™8%-49%ï¼‰ã€‚
  - **Pipeline Parallelism (PP)** å¯ä»¥åœ¨ä¸åŒDPç»„é—´**éå¯¹ç§°åˆ†å¸ƒ**ï¼Œä¸ºçµæ´»è°ƒåº¦æä¾›å¯èƒ½ã€‚
- åŸºäºæ­¤è§‚å¯Ÿï¼ŒAutoHetæ”¯æŒ**éå¯¹ç§°PP + å¯¹ç§°TP**ç»„åˆï¼Œæå¤§æ‰©å±•äº†å¯è¡Œçš„å¹¶è¡Œç­–ç•¥æœç´¢ç©ºé—´ã€‚

#### âœ… ä¸¤é˜¶æ®µä¼˜åŒ–æ¡†æ¶è®¾è®¡
å°†å¤æ‚çš„3Då¹¶è¡Œè§„åˆ’åˆ†è§£ä¸ºä¸¤ä¸ªå¯è§£å­é—®é¢˜ï¼š
1. **ç¬¬ä¸€é˜¶æ®µï¼šæœ€å¤§åŒ–æœ‰æ•ˆè®¡ç®—èƒ½åŠ›ï¼ˆEffective Computing Powerï¼‰**
   - æ„å»ºéçº¿æ€§æ•´æ•°è§„åˆ’æ¨¡å‹ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ¯è½®è¿­ä»£æ—¶é—´ã€‚
   - å¼•å…¥ `effective computing power` æŒ‡æ ‡ï¼Œç»¼åˆè€ƒè™‘å„DPç»„çš„è®¡ç®—èƒ½åŠ›ä¸pipeline bubble ratioã€‚
   - ä½¿ç”¨SCIPæ±‚è§£å™¨å¿«é€Ÿè·å¾—æœ€ä¼˜è®¾å¤‡åˆ†ç»„æ–¹æ¡ˆã€‚

2. **ç¬¬äºŒé˜¶æ®µï¼šGPUæ˜ å°„ä¸æ¨¡å‹åˆ’åˆ†**
   - è€ƒè™‘NVLinkä¸RDMAå¸¦å®½å·®å¼‚ï¼Œä¼˜å…ˆä¿éšœTPé€šä¿¡èµ°NVLinkã€‚
   - è®¾è®¡å¯å‘å¼ç®—æ³•è¿›è¡Œ**èŠ‚ç‚¹-é˜¶æ®µæ˜ å°„**ï¼ˆnode and stage mappingï¼‰ï¼Œå°†ä½ç®—åŠ›GPUåˆ†é…è‡³æ—©æœŸPPé˜¶æ®µï¼ˆå› å…¶å†…å­˜éœ€æ±‚é«˜ä½†è®¡ç®—è½»é‡ï¼‰ã€‚
   - åœ¨PPå†…éƒ¨è¿›è¡Œç»†ç²’åº¦**å±‚çº§è´Ÿè½½å‡è¡¡**ï¼ˆlayer-wise load balancingï¼‰ï¼Œä½¿å„é˜¶æ®µè®¡ç®—æ—¶é—´æ¥è¿‘ã€‚

#### âœ… é«˜æ•ˆå¼¹æ€§æ¢å¤æœºåˆ¶
- **å±‚çº§Checkpointç®¡ç†**ï¼š
  - å°†æ¨¡å‹å‚æ•°å’Œoptimizer statesæŒ‰**å±‚ï¼ˆlayerï¼‰ç²’åº¦**åˆ‡åˆ†ä¿å­˜ï¼Œè€ŒéGPUç²’åº¦ã€‚
  - ç»´æŠ¤**layer bitmap**è®°å½•æ¯ä¸ªcheckpointå—çš„ç‰©ç†ä½ç½®ã€‚
- **è‡ªé€‚åº”æ¢å¤ç­–ç•¥**ï¼š
  - ä¼˜å…ˆä»æœ¬åœ°èŠ‚ç‚¹åŠ è½½å·²æœ‰checkpointç‰‡æ®µã€‚
  - ä»…ä»äº‘ç«¯æ‹‰å–ç¼ºå¤±éƒ¨åˆ†ã€‚
  - æ”¯æŒTPç»´åº¦å˜åŒ–æ—¶çš„å‚æ•°é‡åˆ†åŒºï¼ˆsplit/concatenateï¼‰ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Megatron-LM / Whale | AutoHet |
|------|---------------------|---------|
| å¹¶è¡Œç»“æ„çµæ´»æ€§ | ä»…æ”¯æŒå¯¹ç§°3Då¹¶è¡Œ | âœ… æ”¯æŒéå¯¹ç§°PPã€çµæ´»DP/PPç»„åˆ |
| è´Ÿè½½å‡è¡¡èƒ½åŠ› | å›ºå®šåˆ†å±‚ï¼Œæ— è§†å¼‚æ„æ€§ | âœ… å±‚çº§åŠ¨æ€åˆ†é…ï¼ŒåŒ¹é…GPUç®—åŠ› |
| é€šä¿¡ä¼˜åŒ– | é»˜è®¤æ‹“æ‰‘ï¼Œæ— NVLinkæ„ŸçŸ¥ | âœ… é€šä¿¡ä¼˜å…ˆçº§è°ƒåº¦ï¼ˆTP > DP > PPï¼‰ |
| å¼¹æ€§æ¢å¤ | å…¨é‡checkpointä¸‹è½½ | âœ… åˆ†å±‚æœ¬åœ°ä¼˜å…ˆæ¢å¤ï¼Œå¤§å¹…æé€Ÿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒå¹³å°é…ç½®
- **GPUç±»å‹**ï¼šNVIDIA A100 (80GB), H800 (80GB), H20 (100GB)
- **é›†ç¾¤è§„æ¨¡**ï¼š4ä¸ªèŠ‚ç‚¹ï¼Œå…±24å—GPU
  - Node 0 & 3: A100 Ã—8
  - Node 1: H800 Ã—8
  - Node 2: H20 Ã—8
- **é€šä¿¡æ¶æ„**ï¼š
  - èŠ‚ç‚¹å†…ï¼šNVLinkï¼ˆ~600 GB/sï¼‰
  - èŠ‚ç‚¹é—´ï¼šRoCEv2ï¼ˆ400 Gbpsï¼‰

### æ¨¡å‹ä¸ä»»åŠ¡
- **æ¨¡å‹æ¶æ„**ï¼š
  - BERT-Large (340M)
  - GPT-3 (6.7B)
  - LLaMA (6.7B)
- æ‰€æœ‰æ¨¡å‹å‡åŸºäºTransformerç»“æ„ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **è®­ç»ƒååé‡ï¼ˆThroughputï¼‰**ï¼štokens/s
- **æ¢å¤æ—¶é—´ï¼ˆRecovery Timeï¼‰**ï¼špreemptionåé‡æ–°å¯åŠ¨è®­ç»ƒæ‰€éœ€æ—¶é—´
- **Planning Overhead**ï¼šç”Ÿæˆæœ€ä¼˜å¹¶è¡Œè®¡åˆ’çš„æ—¶é—´
- **Profiling Overhead**ï¼šæ€§èƒ½é‡‡æ ·è€—æ—¶

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Megatron-LM**ï¼šä¸»æµ3Då¹¶è¡Œæ¡†æ¶ï¼Œå‡è®¾åŒæ„ç¯å¢ƒ
- **Whale**ï¼šç¡¬ä»¶æ„ŸçŸ¥è´Ÿè½½å‡è¡¡ç³»ç»Ÿï¼Œæ”¯æŒå¼‚æ„DP/PP
- **Varuna**ï¼šä½æˆæœ¬å¼¹æ€§è®­ç»ƒç³»ç»Ÿï¼Œç”¨äºæ¢å¤æ€§èƒ½å¯¹æ¯”

> æ³¨ï¼šç”±äºMegatron-LMå’ŒWhaleç¼ºä¹è‡ªåŠ¨å¹¶è¡Œè§„åˆ’èƒ½åŠ›ï¼Œä½œè€…æŠ¥å‘Šå…¶â€œæœ€ä½³æ‰‹åŠ¨é…ç½®â€çš„ç»“æœä½œä¸ºä¸Šé™åŸºå‡†ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ è®­ç»ƒååæå‡ï¼ˆEnd-to-End Performanceï¼‰

| åœºæ™¯ | æ¨¡å‹ | AutoHet vs Megatron-LM | AutoHet vs Whale |
|------|------|------------------------|------------------|
| å‡åŒ€åˆ†å¸ƒ | BERT-Large | **1.38Ã—** | - |
| å‡åŒ€åˆ†å¸ƒ | GPT-3 | **1.53Ã—** | **1.27Ã—** |
| éå‡åŒ€åˆ†å¸ƒ | LLaMA | **1.79Ã—** | **1.51Ã—** |

> åœ¨éå‡åŒ€GPUåˆ†å¸ƒä¸‹ï¼ˆå¦‚ `4Ã—A100 + 2Ã—H800`ï¼‰ï¼ŒAutoHetå¯é€šè¿‡æ„å»ºæ›´åˆç†çš„éå¯¹ç§°PPç»“æ„é¿å…é•¿pipelineæ°”æ³¡ï¼Œè€ŒåŸºçº¿åªèƒ½é‡‡ç”¨ä½æ•ˆçš„é•¿é“¾pipelineã€‚

---

### ğŸ” æ¶ˆèå®éªŒä¸æ¨¡å—è´¡çŒ®åˆ†æï¼ˆBreakdown Analysisï¼‰

ä»¥GPT-3 6.7Båœ¨ `4Ã—A100 + 4Ã—H800` ä¸Šä¸ºä¾‹ï¼š

| æ¨¡å—æ·»åŠ é¡ºåº | ååå¢ç›Šï¼ˆç›¸å¯¹Baselineï¼‰ | è´¡çŒ®è¯´æ˜ |
|-------------|--------------------------|--------|
| Baselineï¼ˆåŸºç¡€Pipelineï¼‰ | 1.00Ã— | - |
| + Device Grouping | 1.11Ã— | å‡å°‘pipeline bubble |
| + Node & Stage Mapping | 1.16Ã— | åˆ©ç”¨NVLinkä¼˜åŒ–é€šä¿¡ |
| + Workload Balancing | **1.79Ã—** | å±‚çº§è´Ÿè½½å‡è¡¡åŒ¹é…ç®—åŠ› |

> ç»“æœè¡¨æ˜ï¼Œ**workload balancing** æ˜¯æ€§èƒ½æå‡çš„å…³é”®é©±åŠ¨åŠ›ã€‚

---

### âš™ï¸ è§„åˆ’ä¸å‰–é¢å¼€é”€ï¼ˆPlanning & Profiling Overheadï¼‰

| GPUæ•°é‡ | Planning Time | Profiling Time |
|--------|---------------|----------------|
| 16     | 1.23 s        | ~12 min        |
| 24     | 5.72 s        | ~13 min        |
| 32     | 16.96 s       | ~14 min        |
| 64     | 159.12 s      | ~15.4 min      |

> ç›¸æ¯”Alpaï¼ˆéœ€240åˆ†é’Ÿæœç´¢ç­–ç•¥ï¼‰ï¼ŒAutoHetçš„æ•°å­¦å»ºæ¨¡ç®€åŒ–ä½¿å…¶**è§„åˆ’é€Ÿåº¦å¿«è¿‘10å€**ï¼Œä¸”å¼€é”€éšè§„æ¨¡å¢é•¿ç¼“æ…¢ã€‚

---

### ğŸ”„ å¼¹æ€§æ¢å¤é€Ÿåº¦å¯¹æ¯”ï¼ˆElastic Recoveryï¼‰

| åœºæ™¯ | æ¢å¤æ–¹å¼ | AutoHet vs Varuna |
|------|----------|--------------------|
| å®Œå…¨æœ¬åœ°å¯ç”¨ | Local-only | **4.38Ã— æ›´å¿«** |
| éƒ¨åˆ†æœ¬åœ°ç¼ºå¤± | Hybrid (local + cloud) | **1.49Ã— æ›´å¿«** |
| å¤šèŠ‚ç‚¹æ‰©å®¹ | RDMA-accelerated | **3.59Ã— æ›´å¿«** |

> AutoHeté€šè¿‡**æœ¬åœ°ä¼˜å…ˆæ¢å¤ + RDMAä¼ è¾“**æ˜¾è‘—é™ä½æ¢å¤å»¶è¿Ÿï¼Œåœ¨å¤§è§„æ¨¡æ‰©å±•åœºæ™¯ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **TPå¿…é¡»å¯¹ç§°**ï¼šå› AllReduceå‰çŸ©é˜µè½¬ç½®å¼€é”€è¿‡å¤§ï¼Œéå¯¹ç§°TPä¼šå¸¦æ¥é«˜è¾¾49%çš„æ€§èƒ½æŸå¤±ã€‚
2. **PPå¯ä»¥éå¯¹ç§°**ï¼šå…è®¸ä¸åŒDPç»„æ‹¥æœ‰ä¸åŒçš„PPé˜¶æ®µæ•°ï¼Œä¸ºå¼‚æ„è°ƒåº¦æä¾›çµæ´»æ€§ã€‚
3. **è´Ÿè½½å‡è¡¡ â‰  å†…å­˜å¡«æ»¡**ï¼šåœ¨å¼‚æ„ç¯å¢ƒä¸­ï¼Œå•çº¯è¿½æ±‚GPUå†…å­˜åˆ©ç”¨ç‡ä¼šå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ï¼›åº”æ ¹æ®ç®—åŠ›æ¯”ä¾‹åˆ†é…å±‚æ•°ã€‚
4. **æ¢å¤ç­–ç•¥å†³å®šå¼¹æ€§æ•ˆç‡**ï¼šä¼ ç»Ÿçš„å…¨é‡checkpointæ¢å¤å·²æˆä¸ºç“¶é¢ˆï¼Œ**ç»†ç²’åº¦ã€æœ¬åœ°ä¼˜å…ˆã€æ”¯æŒé‡åˆ†åŒº**çš„æ¢å¤æœºåˆ¶è‡³å…³é‡è¦ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ–é¢„è®¾æ¨¡å‹ç»“æ„**ï¼šç›®å‰ä¸»è¦é€‚ç”¨äºé‡å¤å †å çš„Transformerç»“æ„ï¼ˆå¦‚GPTã€LLaMAï¼‰ï¼Œå¯¹å¤æ‚éå¯¹ç§°ç½‘ç»œæ”¯æŒæœ‰é™ã€‚
2. **æœªè€ƒè™‘CPU offloading**ï¼šæ‰€æœ‰çŠ¶æ€ä»ä¿ç•™åœ¨GPUæ˜¾å­˜æˆ–ä¸»æœºå†…å­˜ä¸­ï¼Œæœªé›†æˆCPU/GPUæ··åˆå†…å­˜ç®¡ç†ã€‚
3. **é™æ€è§„åˆ’ä¸ºä¸»**ï¼šè™½ç„¶æ”¯æŒspot instanceæ¢å¤ï¼Œä½†å¹¶è¡Œè®¡åˆ’æ›´æ–°ä»ä¸ºç¦»çº¿é‡è§„åˆ’ï¼Œå°šæœªå®ç°å®Œå…¨åœ¨çº¿åŠ¨æ€è°ƒæ•´ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ”¯æŒ**åŠ¨æ€åœ¨çº¿é‡è§„åˆ’**ï¼ˆDynamic Repartitioningï¼‰ï¼Œåº”å¯¹é¢‘ç¹çš„spot instanceå˜åŠ¨ã€‚
- æ‰©å±•è‡³**å¤šæ¨¡æ€å¤§æ¨¡å‹**ï¼ˆå¦‚Geminiã€Qwen-VLï¼‰ç­‰æ›´å¤æ‚æ¶æ„ã€‚
- ç»“åˆ**zeroå†—ä½™ä¼˜åŒ–å™¨**ï¼ˆZeROï¼‰è¿›ä¸€æ­¥é™ä½å†…å­˜å‹åŠ›ã€‚
- æ¢ç´¢**è·¨æ•°æ®ä¸­å¿ƒå¼‚æ„è®­ç»ƒ**åœºæ™¯ä¸‹çš„å¹¿åŸŸé€šä¿¡ä¼˜åŒ–ã€‚

---

## æ€»ç»“

AutoHet æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³**å¼‚æ„GPU + spot instance + 3Då¹¶è¡Œ**ä¸‰é‡æŒ‘æˆ˜çš„è‡ªåŠ¨åŒ–è®­ç»ƒæ¡†æ¶ã€‚å®ƒé€šè¿‡**ç†è®ºæ´å¯Ÿ â†’ æ•°å­¦å»ºæ¨¡ â†’ åˆ†å±‚ä¼˜åŒ– â†’ å¼¹æ€§æ¢å¤**çš„å®Œæ•´é—­ç¯ï¼Œåœ¨çœŸå®å¼‚æ„ç¯å¢ƒä¸‹å®ç°äº†æœ€é«˜è¾¾ **1.79Ã— çš„è®­ç»ƒååæå‡** å’Œ **4.38Ã— çš„æ¢å¤åŠ é€Ÿ**ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡AIè®­ç»ƒç³»ç»Ÿçš„å¼¹æ€§åŒ–ã€æ™ºèƒ½åŒ–æä¾›äº†é‡è¦å®è·µè·¯å¾„ã€‚

</details>

---

### 3. [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)

**Authors**: Alexandros Christoforos, Chadbourne Davis  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.20724v1  

#### Abstract
Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By se...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»ŸåŸºäº **diffusion** çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ DiffuSeqï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´ä¸¥é‡çš„ **è®¡ç®—æˆæœ¬é«˜** å’Œ **å†…å­˜å¼€é”€å¤§** é—®é¢˜ã€‚å…¶æ ¹æœ¬åŸå› åœ¨äºï¼š
- è‡ªæ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ $O(n^2)$ çš„æ—¶é—´ä¸ç©ºé—´å¤æ‚åº¦ï¼›
- æ‰©æ•£è¿‡ç¨‹éœ€è¦å¤šæ¬¡è¿­ä»£å»å™ªï¼Œæ¯æ­¥éƒ½æ¶‰åŠå…¨åºåˆ—æ³¨æ„åŠ›è®¡ç®—ï¼›
- é•¿åºåˆ—ä¸‹éš¾ä»¥ç»´æŒè¯­ä¹‰è¿è´¯æ€§å’Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡ã€‚

æ­¤å¤–ï¼Œä¸»æµ LLMs é€šå¸¸è®­ç»ƒäº 8K token ä»¥å†…ä¸Šä¸‹æ–‡ï¼Œåœ¨æ›´é•¿è¾“å…¥ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSA-DiffuSeq
ä½œè€…æå‡º **SA-DiffuSeq** â€”â€”ä¸€ç§èåˆç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attention, SAï¼‰ä¸æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆé•¿æ–‡æœ¬ç”Ÿæˆæ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰å¼•å…¥ **Sparse Attention æœºåˆ¶**
- é‡‡ç”¨ **æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆsliding window attentionï¼‰** å’Œ **è†¨èƒ€çª—å£ï¼ˆdilated sliding windowï¼‰** ç»“æ„ï¼Œå°†æ³¨æ„åŠ›å¤æ‚åº¦ä» $O(n^2)$ é™è‡³ $O(n \times w)$ï¼Œå…¶ä¸­ $w$ ä¸ºçª—å£å¤§å°ã€‚
- å¼•å…¥ **å…¨å±€æ³¨æ„åŠ›ï¼ˆglobal attentionï¼‰** åˆ°å…³é”® tokenï¼ˆå¦‚ [CLS]ï¼‰ï¼Œç¡®ä¿é‡è¦ä¿¡æ¯å¯è®¿é—®æ•´ä¸ªä¸Šä¸‹æ–‡ã€‚

#### ï¼ˆ2ï¼‰é›†æˆ **Mixture of Experts (MoE)** æ¶æ„
- åœ¨æ¯ä¸ª Transformer å±‚ä¸­åµŒå…¥å¤šä¸ªä¸“å®¶ç½‘ç»œï¼Œé€šè¿‡é—¨æ§æœºåˆ¶åŠ¨æ€è·¯ç”±ä¸åŒæ–‡æœ¬æ®µåˆ°æœ€ç›¸å…³çš„ä¸“å®¶ã€‚
- å®ç°â€œæŒ‰éœ€è®¡ç®—â€ï¼Œæå‡æ¨¡å‹å®¹é‡çš„åŒæ—¶æ§åˆ¶æ€»è®¡ç®—é‡å¢é•¿ã€‚

#### ï¼ˆ3ï¼‰è®¾è®¡ **è½¯å¸æ”¶çŠ¶æ€ï¼ˆsoft absorbing stateï¼‰**
- æ”¹è¿›åŸå§‹æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç¦»æ•£å¸æ”¶çŠ¶æ€ï¼Œæå‡ºè¿ç»­åŒ–çš„ **soft absorption**ï¼Œå¢å¼ºå»å™ªè½¨è¿¹ç¨³å®šæ€§ï¼ŒåŠ é€Ÿé‡å»ºæ”¶æ•›ã€‚

#### ï¼ˆ4ï¼‰ä¼˜åŒ–é‡‡æ ·ç­–ç•¥
- ä½¿ç”¨å…ˆè¿›æ±‚è§£å™¨ **DPM-solver++** æ˜¾è‘—å‡å°‘æ¨ç†é˜¶æ®µæ‰€éœ€çš„ diffusion steps æ•°é‡ï¼Œæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | è®­ç»ƒæ—¶é—´é™ä½çº¦ 15%ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼ˆè§ Table 7ï¼‰ |
| **è´¨é‡** | åœ¨å¤šç§æŒ‡æ ‡ä¸Šè¶…è¶Š DiffuSeqã€Longformer å’Œ GPT-4 |
| **å¯æ‰©å±•æ€§** | èƒ½ç¨³å®šå¤„ç†è¶…è¿‡ 8Kã€ç”šè‡³è¾¾ 16K token çš„é•¿åºåˆ— |
| **æ–°é¢–æ€§** | ç”Ÿæˆæ–‡æœ¬å¤šæ ·æ€§æ›´é«˜ï¼ˆ2-gram novelty æ›´ä¼˜ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒæ¶µç›–å››ä¸ªå…¸å‹é•¿æ–‡æœ¬ä»»åŠ¡åœºæ™¯ï¼š

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | ç‰¹ç‚¹ |
|--------|---------|------|
| **Arxiv Abstract Dataset** | ç§‘å­¦æ–‡çŒ®æ‘˜è¦ç”Ÿæˆ | é•¿æ–‡æ¡£ã€æŠ€æœ¯æ€§å¼ºã€ç»“æ„ä¸¥è°¨ |
| **HotpotQA** | å¤šè·³é—®ç­” | éœ€è¦è·¨æ®µè½æ¨ç†ä¸æ”¯æŒäº‹å®è¿½è¸ª |
| **Commonsense Conversation Dataset** | å¯¹è¯ç”Ÿæˆ | ä¸Šä¸‹æ–‡æ•æ„Ÿã€éœ€å¸¸è¯†ç†è§£ |
| **Quora Question Pairs (QQP)** | é—®å¥å¤è¿° | è¯­ä¹‰ä¿æŒ + è¡¨è¾¾å¤šæ ·åŒ– |

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹ç»“æ„**ï¼š
  - 12 å±‚ S12Transformerï¼Œæ¯å±‚ 12 ä¸ª attention headï¼›
  - åŸºäº Longformer çš„ç¨€ç–æ³¨æ„åŠ›é…ç½®ï¼›
  - MoE æ¯å±‚æ¿€æ´» top-k ä¸“å®¶ï¼ˆå…·ä½“æœªè¯¦è¿°ï¼‰ï¼›
- **æ‰©æ•£å‚æ•°**ï¼š
  - 2048 diffusion stepsï¼›
  - square-root noise scheduleï¼›
  - æ­£å‘æ‰©æ•£å…¬å¼ï¼š$ z_t = \sqrt{\alpha_t} z_0 + \sqrt{1-\alpha_t} e $ï¼Œ$e \sim \mathcal{N}(0,I)$ï¼›
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA A100 GPUsï¼›
- **è®­ç»ƒç­–ç•¥**ï¼šé€æ­¥å¢åŠ çª—å£å¤§å°å’Œåºåˆ—é•¿åº¦ï¼ˆcurriculum learning styleï¼‰ï¼›

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
ç»¼åˆè¯„ä¼°ç”Ÿæˆè´¨é‡ä¸å¤šæ ·æ€§ï¼š
- **BLEU**ï¼šè¡¡é‡ n-gram åŒ¹é…ç¨‹åº¦ï¼›
- **ROUGE-{1,2,L} (R1/R2/RL)**ï¼šä¾§é‡å¬å›ç‡ï¼Œé€‚ç”¨äºæ‘˜è¦ä»»åŠ¡ï¼›
- **BERTScore**ï¼šåŸºäº BERT çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ†ï¼›
- **Accuracy**ï¼šç”¨äº QQP åˆ†ç±»ä»»åŠ¡ï¼›
- **Answer EM/F1 & Support EM/F1**ï¼šHotpotQA ç‰¹æœ‰æŒ‡æ ‡ï¼Œåˆ†åˆ«è¯„ä»·ç­”æ¡ˆå‡†ç¡®æ€§å’Œè¯æ®å¥å­åŒ¹é…ï¼›
- **2-gram Novelty**ï¼šè¡¡é‡ç”Ÿæˆæ–‡æœ¬çš„ç‹¬ç‰¹æ€§ï¼›
- **Inference Time**ï¼šæ¨ç†å»¶è¿Ÿå¯¹æ¯”ã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **DiffuSeq**ï¼šåŸºç¡€æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼›
- **Longformer**ï¼šç»å…¸ç¨€ç–æ³¨æ„åŠ›æ¶æ„ï¼›
- **GPT-4**ï¼šä½œä¸ºå¼º baseline å‚è€ƒï¼ˆå¼•ç”¨è‡ª Achiam et al., 2023ï¼‰ï¼›
- åŒæ—¶ä¹Ÿå¯¹æ¯”äº†å„æ•°æ®é›†ä¸“ç”¨æ¨¡å‹ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰Arxiv Abstract Datasetï¼ˆç§‘å­¦æ‘˜è¦ï¼‰
| Model | R1 | R2 | RL |
|-------|----|----|-----|
| Longformer | 41.44 | 17.52 | 38.70 |
| DiffuSeq | 39.12 | 16.43 | 37.88 |
| **SA-DiffuSeq** | **44.41** | **18.73** | **39.89** |

> â• æå‡æ˜æ˜¾ï¼Œå°¤å…¶åœ¨ R1 å’Œ R2 ä¸Šé¢†å…ˆè¶… 5 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒé•¿åº¦ä¸‹çš„é²æ£’æ€§è¡¨ç°ï¼ˆArxivï¼‰
| Sequence Length | Model | R1 | R2 | RL |
|------------------|--------|-----|-----|------|
| 8K | SA-DiffuSeq | **46.85** | **19.72** | **41.35** |
| 12K | SA-DiffuSeq | **45.40** | **18.94** | **40.50** |
| 16K | SA-DiffuSeq | **43.60** | **18.11** | **39.75** |

> âœ”ï¸ SA-DiffuSeq åœ¨æ‰€æœ‰é•¿åº¦ä¸‹å‡ä¼˜äºåŸºçº¿ï¼Œä¸”éšé•¿åº¦å¢é•¿è¡°å‡ç¼“æ…¢ï¼Œä½“ç°è‰¯å¥½å¯æ‰©å±•æ€§ã€‚

#### ï¼ˆ3ï¼‰HotpotQAï¼ˆå¤šè·³é—®ç­”ï¼‰
| Model | Answer EM/F1 | Support EM/F1 |
|--------|---------------|----------------|
| Longformer | 71.21 / 82.42 | 65.11 / 89.50 |
| DiffuSeq | 70.91 / 81.43 | 64.60 / 88.51 |
| **SA-DiffuSeq** | **72.88 / 85.42** | **66.69 / 90.40** |

> ğŸ’¡ åœ¨æ”¯æŒå¥è¯†åˆ«å’Œæ”¯æŒ F1 ä¸Šä¼˜åŠ¿æ˜¾è‘—ï¼Œè¯´æ˜å…¶æ›´å¼ºçš„é•¿ç¨‹ä¾èµ–æ•æ‰èƒ½åŠ›ã€‚

#### ï¼ˆ4ï¼‰Commonsense Conversationï¼ˆå¯¹è¯ç”Ÿæˆï¼‰
| Model | BLEU | ROUGE-L | BERTScore |
|--------|--------|-----------|-------------|
| Longformer | 0.030 | 0.139 | 0.602 |
| DiffuSeq | 0.022 | 0.119 | 0.501 |
| **SA-DiffuSeq** | **0.049** | **0.233** | **0.628** |

> ğŸ¯ åœ¨ä¸‰é¡¹æŒ‡æ ‡å…¨é¢é¢†å…ˆï¼Œå°¤å…¶ BLEU æ¥è¿‘ç¿»å€ã€‚

#### ï¼ˆ5ï¼‰QQPï¼ˆé—®å¥å¤è¿°å‡†ç¡®æ€§ï¼‰
| Model | Accuracy |
|--------|----------|
| Longformer | 92.3 |
| DiffuSeq | 91.7 |
| **SA-DiffuSeq** | **95.3** |

> âœ… æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¡¨æ˜å…¶ä¼˜ç§€çš„è¯­ä¹‰ä¿çœŸèƒ½åŠ›ã€‚

#### ï¼ˆ6ï¼‰æ¨ç†æ•ˆç‡ä¸æ–°é¢–æ€§å¯¹æ¯”ï¼ˆTable 7ï¼‰
| Model | Inference Time | 2-gram Novelty |
|--------|----------------|----------------|
| SA-DiffuSeq | **0.80** | **0.90** |
| DiffuSeq | 1.00 | 0.75 |
| Longformer | 1.40 | 0.60 |

> âš–ï¸ SA-DiffuSeq å®ç°äº† **é«˜æ–°é¢–æ€§ä¸ä½å»¶è¿Ÿçš„è‰¯å¥½å¹³è¡¡**ï¼Œä¼˜äºä¸¤è€…ã€‚

---

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰

ç ”ç©¶äº†ä»¥ä¸‹ç»„ä»¶çš„å½±å“ï¼ˆä»¥ Arxiv æ•°æ®é›†ä¸ºä¾‹ï¼‰ï¼š

| é…ç½® | R1 | R2 | RL |
|------|-----|-----|------|
| å®Œæ•´ SA-DiffuSeqï¼ˆbaselineï¼‰ | 44.41 | 18.73 | 39.89 |
| ç§»é™¤ Sparse Attention | 42.52 | 17.99 | 38.41 |
| å‡å°‘ diffusion steps è‡³ 1024 | â†“è½»å¾® | â†“è½»å¾® | â†“è½»å¾® |
| å¢åŠ è‡³ 4096 steps | â†‘è‡³ 44.71 | â†’18.55 | â†‘è‡³ 40.20 |
| çª—å£å¤§å°æ”¹ä¸º 256 | 43.80 | 18.22 | 39.65 |
| çª—å£å¤§å°æ”¹ä¸º 1024 | 44.40 | 18.66 | 39.92 |

#### å‘ç°ï¼š
- **Sparse Attention æ˜¯æœ€å…³é”®ç»„ä»¶**ï¼Œç§»é™¤åæ€§èƒ½å¤§å¹…ä¸‹é™ï¼›
- æ›´å¤š diffusion steps å¯ç•¥å¾®æå‡è´¨é‡ï¼Œä½†è¾¹é™…æ•ˆç›Šé€’å‡ï¼›
- è¾ƒå¤§çš„ attention window æœ‰åŠ©äºæ€§èƒ½ï¼Œä½†ä¸å¦‚ SA æœ¬èº«å½±å“å¤§ï¼›
- è¡¨æ˜ SA ä¸ MoE ååŒä½œç”¨æ˜¯æˆåŠŸçš„å…³é”®ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ç»“æ„åŒ–ç¨€ç–æ€§ + æ‰©æ•£æ¨¡å‹ = é«˜æ•ˆé•¿æ–‡æœ¬ç”Ÿæˆæ–°èŒƒå¼**
   - å°† Sparse Attention æˆåŠŸèå…¥ diffusion æ¡†æ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†è®¡ç®—ç“¶é¢ˆï¼›
   - SA-DiffuSeq åœ¨ä¿æŒé«˜è´¨é‡ç”Ÿæˆçš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒä¸æ¨ç†æ•ˆç‡ã€‚

2. **åŠ¨æ€èµ„æºåˆ†é…æœºåˆ¶ï¼ˆMoEï¼‰å¢å¼ºè¡¨è¾¾åŠ›ä¸æ•ˆç‡**
   - MoE å…è®¸æ¨¡å‹æ ¹æ®ä¸åŒæ–‡æœ¬åŒºåŸŸçš„è¯­ä¹‰å¤æ‚åº¦çµæ´»è°ƒåº¦è®¡ç®—èµ„æºï¼›
   - å®ç°â€œæŒ‰éœ€è®¡ç®—â€ï¼Œé¿å…å¯¹ç®€å•éƒ¨åˆ†è¿‡åº¦æ¶ˆè€—ç®—åŠ›ã€‚

3. **è½¯å¸æ”¶çŠ¶æ€ä¸ DPM-solver++ æå‡é‡‡æ ·æ•ˆç‡**
   - æ–°è®¾è®¡çš„ soft absorbing state åŠ é€Ÿäº†å»å™ªæ”¶æ•›ï¼›
   - ç»“åˆ DPM-solver++ å¯å¤§å¹…å‡å°‘ inference stepsï¼ŒåŠ å¿«ç”Ÿæˆé€Ÿåº¦ã€‚

4. **åœ¨è¶…é•¿åºåˆ—ï¼ˆ>8K tokensï¼‰ä¸Šè¡¨ç°ç¨³å¥**
   - æˆåŠŸçªç ´å½“å‰å¤šæ•° LLMs çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼›
   - é€‚ç”¨äºç§‘ç ”å†™ä½œã€ä»£ç ç”Ÿæˆã€é•¿å¯¹è¯ç­‰çœŸå®åº”ç”¨åœºæ™¯ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **å®ç°å¤æ‚åº¦è¾ƒé«˜**ï¼šåŒæ—¶æ•´åˆ MoEã€Sparse Attentionã€Diffusionã€DPM-solver ç­‰å¤šä¸ªå‰æ²¿æ¨¡å—ï¼Œéƒ¨ç½²éš¾åº¦è¾ƒå¤§ï¼›
- **ä¸“å®¶è´Ÿè½½å‡è¡¡é—®é¢˜æœªæ·±å…¥è®¨è®º**ï¼šMoE å­˜åœ¨ä¸“å®¶åˆ©ç”¨ç‡ä¸å‡çš„é£é™©ï¼›
- **ç¼ºä¹äººç±»è¯„ä¼°**ï¼šç›®å‰ä»…ä¾èµ–è‡ªåŠ¨æŒ‡æ ‡ï¼Œç¼ºå°‘ human evaluation éªŒè¯ç”Ÿæˆè´¨é‡ï¼›
- **èƒ½è€—åˆ†æä¸è¶³**ï¼šè™½å¼ºè°ƒæ•ˆç‡ï¼Œä½†æœªæä¾› GPU åŠŸè€—æˆ–ç¢³è¶³è¿¹æ•°æ®ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´é«˜æ•ˆçš„ç¨€ç–æ¨¡å¼ï¼ˆå¦‚å¯å­¦ä¹ çš„ attention topologyï¼‰ï¼›
- å°† SA-DiffuSeq æ‰©å±•è‡³å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å›¾æ–‡ç”Ÿæˆï¼‰ï¼›
- è¿›ä¸€æ­¥å‹ç¼© diffusion stepsï¼Œè¿ˆå‘å®æ—¶ç”Ÿæˆï¼›
- å¼€å‘è½»é‡åŒ–ç‰ˆæœ¬ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼›
- å¼•å…¥å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¯æ§ç”Ÿæˆä¼˜åŒ–ã€‚

---

## æ€»ç»“

ğŸ“Œ **SA-DiffuSeq æ˜¯ä¸€é¡¹é¢å‘é•¿æ–‡æœ¬ç”Ÿæˆçš„é‡å¤§è¿›å±•**ï¼Œå®ƒé€šè¿‡å°† **Sparse Attention**ã€**Mixture of Experts** ä¸ **diffusion model** æ·±åº¦èåˆï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ•ˆç‡ä¸è´¨é‡ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚å®éªŒè¯æ˜å…¶åœ¨å¤šä¸ªé¢†åŸŸå®ç°äº† SOTA æ€§èƒ½ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œä¸ºæœªæ¥é«˜æ•ˆã€é«˜è´¨é‡çš„é•¿æ–‡æ¡£ç”Ÿæˆæä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 4. [SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression](https://arxiv.org/abs/2512.20635)

**Authors**: Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.20635v1  

#### Abstract
Transformer encoders are widely deployed in large-scale web services for natural language understanding tasks such as text classification, semantic retrieval, and content ranking. However, their high inference latency and memory consumption pose significant challenges for real-time serving and scala...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
Transformer ç¼–ç å™¨åœ¨å¤§è§„æ¨¡ Web æœåŠ¡ä¸­çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€è¯­ä¹‰æ£€ç´¢ã€å†…å®¹æ’åºï¼‰ä¸­è¢«å¹¿æ³›éƒ¨ç½²ï¼Œä½†å…¶é«˜æ¨ç†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—é™åˆ¶äº†å®æ—¶æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿™ç§ä½æ•ˆä¸»è¦æºäºæ¶æ„å†—ä½™ï¼Œå°¤å…¶æ˜¯ **attention æ¨¡å—ä¸­çš„å‚æ•°å†—ä½™** å’Œ **feed-forward network (FFN)** çš„è®¡ç®—å¼€é”€ã€‚

ä¼ ç»Ÿå‰ªææ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **ç²—ç²’åº¦æ–¹æ³•**ï¼ˆå¦‚å±‚ä¸¢å¼ƒã€å¤´å‰ªæï¼‰åœ¨æ¿€è¿›å‹ç¼©ä¸‹ç²¾åº¦éª¤é™ï¼›
- **ç»†ç²’åº¦æ–¹æ³•**ï¼ˆå¦‚ movement pruningï¼‰ä¾èµ–å¯å‘å¼è¯„åˆ†æˆ–å®šåˆ¶æ©ç ï¼Œéš¾ä»¥éƒ¨ç½²ï¼›
- å¤šæ•°æ–¹æ³•ä»…å…³æ³¨ attention å¤´å‰ªæï¼Œè€Œå¿½ç•¥å ä¸»å¯¼åœ°ä½çš„ FFN å­å±‚ï¼›
- åŠ¨æ€è·¯ç”±æœºåˆ¶å¼•å…¥æ¨ç†æ—¶å¼€é”€ï¼Œå½±å“å®é™…éƒ¨ç½²æ•ˆç‡ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šSHRPï¼ˆSpecialized Head Routing and Pruningï¼‰
SHRP æ˜¯ä¸€ç§æ–°å‹çš„ç»“æ„åŒ–å‰ªææ¡†æ¶ï¼Œé€šè¿‡å°†æ¯ä¸ª attention head è§†ä¸ºç‹¬ç«‹â€œä¸“å®¶â€ï¼Œå¹¶ç»“åˆå…±äº«è½»é‡çº§ Expander-FFN æ¥å®ç°é«˜æ•ˆç¼–ç å™¨å‹ç¼©ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **Expert Attention è®¾è®¡**
   - å°†æ ‡å‡† multi-head attention æ”¹é€ ä¸º **Expert Attention**ï¼Œæ¯ä¸ª head ä½œä¸ºç‹¬ç«‹ä¸“å®¶æ¨¡å—ï¼Œæ‹¥æœ‰ç‹¬ç«‹çš„ Q/K/V æŠ•å½±ï¼›
   - æ‰€æœ‰ head è¾“å‡ºä¸æ‹¼æ¥ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªå…±äº«çš„è½»é‡çº§ **Expander-FFN** è¿›è¡Œç»´åº¦æ¢å¤å’Œéçº¿æ€§å˜æ¢ï¼›
   - å®ç° attention å’Œ FFN å­å±‚çš„è”åˆå‰ªæã€‚

2. **ä¸¤é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥**
   - **ç¬¬ä¸€é˜¶æ®µï¼šBalanced Explorationï¼ˆè´Ÿè½½å‡è¡¡æ¢ç´¢ï¼‰**
     - å¼•å…¥åŸºäº KL æ•£åº¦çš„ load balancing lossï¼Œé˜²æ­¢æ—©æœŸä¸“å®¶å´©æºƒï¼Œç¡®ä¿æ‰€æœ‰ head éƒ½èƒ½è·å¾—æ¢¯åº¦ä¿¡å·ï¼›
   - **ç¬¬äºŒé˜¶æ®µï¼šExpert Specializationï¼ˆä¸“å®¶ä¸“ä¸šåŒ–ï¼‰**
     - å…³é—­å¹³è¡¡æŸå¤±ï¼Œå…è®¸é—¨æ§æœºåˆ¶è‡ªç„¶é€‰æ‹©æœ€ä¼˜ headï¼Œä¿ƒè¿› specializationï¼›
   - åˆ†å±‚é€æ­¥è½¬æ¢ï¼ˆlayer-wise conversionï¼‰ï¼Œé¿å…ä¸€æ¬¡æ€§æ›¿æ¢å¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šã€‚

3. **Top-1 è·¯ç”±ä¸ç¡®å®šæ€§å‰ªææœºåˆ¶**
   - ä½¿ç”¨ Top-1 gating ç­–ç•¥ï¼šæ¯å±‚æ¯ä¸ªè¾“å…¥åªæ¿€æ´»ä¸€ä¸ªå¾—åˆ†æœ€é«˜çš„ headï¼›
   - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•å„ head çš„ä½¿ç”¨é¢‘ç‡ï¼›
   - æ¨ç†å‰ç§»é™¤ä½é¢‘ä½¿ç”¨çš„ head åŠå…¶å¯¹åº”è·¯å¾„ï¼Œ**å®Œå…¨æ¶ˆé™¤åŠ¨æ€è·¯ç”±å¼€é”€**ï¼›
   - æœ€ç»ˆæ¨¡å‹æ˜¯é™æ€ã€ç´§å‡‘ã€æ— è·¯ç”±é€»è¾‘çš„ç¡®å®šæ€§ encoderã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | SHRP | ä¼ ç»Ÿæ–¹æ³• |
|------|------|---------|
| **å‰ªæç²’åº¦** | ç»“æ„åŒ–ã€æ¨¡å—åŒ–ï¼ˆhead + FFNï¼‰ | å¤šä¸º attention-only æˆ–éç»“æ„åŒ– |
| **è®­ç»ƒæœºåˆ¶** | æ¸è¿›å¼ + ä¸¤é˜¶æ®µä¼˜åŒ– | å•é˜¶æ®µå¾®è°ƒæˆ–å¤æ‚è¾…åŠ©æŸå¤± |
| **æ¨ç†æ•ˆç‡** | å®Œå…¨å»é™¤è·¯ç”±å¼€é”€ï¼Œé™æ€æ‰§è¡Œ | åŠ¨æ€è·¯ç”±å¸¦æ¥é¢å¤–å»¶è¿Ÿ |
| **éƒ¨ç½²å‹å¥½æ€§** | ç”Ÿæˆç¡®å®šæ€§æ¨¡å‹ï¼Œæ— éœ€ç‰¹æ®Šè¿è¡Œæ—¶æ”¯æŒ | é€šå¸¸éœ€å®šåˆ¶æ¨ç†å¼•æ“ |
| **å‹ç¼©èƒ½åŠ›** | æ›´å½»åº•ï¼ˆåŒæ—¶å‰ª attention å’Œ FFNï¼‰ | æœ‰é™ï¼ˆå¸¸ä¿ç•™å®Œæ•´ FFNï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨ **GLUE benchmark** ä¸Šçš„ä¸ƒä¸ªä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼š
  - MNLIï¼ˆè‡ªç„¶è¯­è¨€æ¨æ–­ï¼‰
  - QQPï¼ˆå¥å­å¯¹ç›¸ä¼¼åº¦ï¼‰
  - QNLIï¼ˆé—®ç­”è•´å«ï¼‰
  - SST-2ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰
  - MRPCï¼ˆé‡Šä¹‰æ£€æµ‹ï¼‰
  - RTEï¼ˆæ–‡æœ¬è•´å«ï¼‰
  - CoLAï¼ˆè¯­æ³•å¯æ¥å—æ€§åˆ¤æ–­ï¼‰
- ä¸»å¹²æ¨¡å‹ï¼š**BERT-base-uncased**ï¼ˆ12 å±‚ï¼Œæ¯å±‚ 12 ä¸ª headï¼‰

---

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼šå•å¼  NVIDIA A800 GPUï¼ˆ48GB æ˜¾å­˜ï¼‰
- **æ‰¹å¤§å°ï¼ˆBatch Sizeï¼‰**ï¼š64
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œå­¦ä¹ ç‡ $2 \times 10^{-5}$ï¼Œweight decay 0.01
- **å­¦ä¹ ç‡è°ƒåº¦**ï¼šcosine annealingï¼Œwarmup 10%
- **å‰ªæåè®®**ï¼š
  - é€æ­¥å°† Z/12 å±‚æ›¿æ¢ä¸º Expert Attention æ¨¡å—ï¼›
  - è®­ç»ƒç»“æŸåæ ¹æ®éªŒè¯é›†ä¸Šçš„ head ä½¿ç”¨é¢‘ç‡è¿›è¡Œ Top-1 å‰ªæï¼›
  - ç§»é™¤æœªé€‰ä¸­çš„ head å’Œ routerï¼Œå½¢æˆæœ€ç»ˆ compact modelã€‚

---

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Accuracy** | å„ä»»åŠ¡å‡†ç¡®ç‡ï¼ˆæˆ– MCC for CoLAï¼‰ |
| **Parameter Reduction (%)** | å‚æ•°å‡å°‘æ¯”ä¾‹ï¼ˆä¸å« embedding å±‚ï¼‰ |
| **FLOPs Remaining (%)** | æ¨ç†æ‰€éœ€ FLOPs å åŸå§‹æ¨¡å‹çš„æ¯”ä¾‹ |
| **Throughput (inf/s)** | æ¯ç§’å¤„ç†æ ·æœ¬æ•°ï¼ˆbatch size=64ï¼‰ |
| **Latency** | å•æ ·æœ¬æ¨ç†æ¯«ç§’æ•° |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¯”è¾ƒäº†å¤šç§ä¸»æµ attention head pruning æ–¹æ³•ï¼š
- **Michel et al. [13]**ï¼šåŸºäºæ¢¯åº¦é‡è¦æ€§çš„ head å‰ªæ
- **Voita et al. [18]**ï¼šä½¿ç”¨è¾…åŠ©æŸå¤±å¼•å¯¼äºŒå€¼åŒ– head é€‰æ‹©
- **Pipelined DSP & Joint DSP [11,12]**ï¼šè¿­ä»£å¼æˆ–ç«¯åˆ°ç«¯ä¼˜åŒ–å‰ªæ
- **STE [5]**ï¼šç›´é€šä¼°è®¡å™¨ç”¨äºå¯å¾® head æ©ç 

è¿™äº›æ–¹æ³•å‡ä¿æŒåŸå§‹ FFN ä¸å˜ï¼Œä»…å‰ª attention headsã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰

| å‰ªæå±‚æ•° | å‚æ•°å‡å°‘ | å‡†ç¡®ç‡ä¿ç•™ | ååæå‡ | FLOPs å‰©ä½™ |
|--------|----------|------------|-----------|-------------|
| 6/12   | 48.2%    | 93.1%      | 81.1%     | 51.7%       |
| 8/12   | 64.3%    | 89.2%      | 146.2%    | 35.6%       |
| 10/12  | 80.4%    | 89.6%      | 282.1%    | 19.6%       |
| **11/12** | **88.5%** | **84.4%** | **424.1%** | **11.5%** |

> âœ… **æç«¯å‹ç¼©åœºæ™¯ä¸‹ä»å…·å®ç”¨æ€§**ï¼šå³ä½¿å‰ªæ‰ 11/12 å±‚ï¼Œæ¨¡å‹ä»ä¿ç•™ 84.4% åŸå§‹æ€§èƒ½ï¼Œååæå‡ **4.2Ã—**ï¼ŒFLOPs é™è‡³ **11.5%**

---

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼ˆä»¥ MNLI ä¸ºä¾‹ï¼Œè§ Table 2ï¼‰

| æ–¹æ³• | ä¿ç•™ heads æ•° | å‡†ç¡®ç‡ | æ¨¡å‹å°ºå¯¸ç¼©å‡ |
|------|----------------|--------|--------------|
| Michel et al. | 12 | 0.4059 | 24.0% |
| Joint DSP | 12 | 0.7974 | 24.0% |
| **SHRP (Ours)** | **12** | **0.6853** | **88.5%** |

> ğŸ”º **ä¼˜åŠ¿æ˜æ˜¾**ï¼šåœ¨ç›¸åŒä¿ç•™ head æ•°ä¸‹ï¼ŒSHRP å®ç°æ›´å¤§å¹…åº¦çš„å‹ç¼©ï¼ˆ88.5% vs 24%ï¼‰ï¼Œå°½ç®¡ç»å¯¹å‡†ç¡®ç‡ç•¥ä½ï¼Œä½†å…¶å‹ç¼©æ¯”è¿œè¶…ä¼ ç»Ÿæ–¹æ³•ã€‚

æ­¤å¤–ï¼Œåœ¨ä¸­ç­‰å‰ªæç¨‹åº¦ä¸‹ï¼ˆå¦‚ 120 æˆ– 96 headsï¼‰ï¼ŒSHRP çš„å‡†ç¡®ç‡ä¸æœ€ä½³åŸºçº¿ç›¸å½“ç”šè‡³æ›´é«˜ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆTop-k è·¯ç”±åˆ†æï¼Œè§ Table 3ï¼‰

ç ”ç©¶ä¸åŒ Top-k è·¯ç”±ç­–ç•¥å¯¹å‰ªæåæ€§èƒ½çš„å½±å“ï¼ˆQQP ä»»åŠ¡ï¼‰ï¼š

| Top-k | å¹³å‡å‡†ç¡®ç‡å˜åŒ–ï¼ˆÎ”ï¼‰ |
|-------|--------------------|
| **Top-1** | **+0.0021** âœ… |
| Top-3 | -0.2490 âŒ |
| Top-5 | -0.2775 âŒ |
| Top-7 | -0.2817 âŒ |
| Top-9 | -0.2769 âŒ |
| Top-11 | -0.3039 âŒ |

> ğŸ” **å…³é”®å‘ç°**ï¼š
- **åªæœ‰ Top-1 è·¯ç”±èƒ½åœ¨å‰ªæåç»´æŒç”šè‡³è½»å¾®æå‡æ€§èƒ½**ï¼›
- æ›´å¤§çš„ k å¯¼è‡´æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œä¸”éš k å¢å¤§æ¶åŒ–ï¼›
- Top-1 æä¾›æ¸…æ™°çš„ usage statisticsï¼Œä¾¿äºè¯†åˆ«å†—ä½™ headï¼›
- Top-k å¼•å…¥ä¸“å®¶é‡å å’Œå¹²æ‰°ï¼Œå‰Šå¼±å‰ªææ•ˆæœã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Expert Attention + Expander-FFN æ¶æ„æœ‰æ•ˆè§£è€¦ attention ä¸ FFN**ï¼Œä½¿å¾—ä¸¤è€…å¯ä»¥ç»Ÿä¸€å‰ªæï¼Œå¤§å¹…æå‡å‹ç¼©æ½œåŠ›ã€‚
2. **Top-1 è·¯ç”±æ˜¯æœ€é€‚åˆå‰ªæä»»åŠ¡çš„æœºåˆ¶**ï¼šå®ƒä¸ä»…æœ€å°åŒ–æ¨ç†å¼€é”€ï¼Œè¿˜äº§ç”Ÿå¯è§£é‡Šçš„ usage patternsï¼Œæ”¯æŒå¹²å‡€å‰ªæã€‚
3. **æ¸è¿›å¼è®­ç»ƒ + ä¸¤é˜¶æ®µä¼˜åŒ–** æˆåŠŸè§£å†³äº† MoE ç±»ç»“æ„åœ¨ fine-tuning ä¸­æ˜“å´©æºƒçš„é—®é¢˜ï¼Œä¿éšœäº†è®­ç»ƒç¨³å®šæ€§ã€‚
4. **SHRP åœ¨æ¿€è¿›å‹ç¼©ä¸‹è¡¨ç°ç¨³å¥**ï¼šå³ä½¿åªå‰© 1/12 å±‚ï¼Œä»èƒ½ä¿ç•™è¶…è¿‡ 84% çš„åŸå§‹æ€§èƒ½ï¼Œé€‚ç”¨äºå»¶è¿Ÿæ•æ„Ÿçš„å¤§è§„æ¨¡éƒ¨ç½²åœºæ™¯ã€‚
5. **å‹ç¼©å¸¦æ¥çš„ä¸ä»…æ˜¯å‚æ•°å‡å°‘ï¼Œæ›´æ˜¯çœŸå®ååæå‡**ï¼šå¾—ç›Šäºè·¯ç”±ç§»é™¤å’Œç»“æ„ç®€åŒ–ï¼Œå®æµ‹ throughput æå‡è¾¾ 4.2Ã—ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä¸»è¦é’ˆå¯¹ **encoder-only æ¨¡å‹**ï¼ˆå¦‚ BERTï¼‰ï¼Œå°šæœªæ‰©å±•è‡³ decoder-onlyï¼ˆå¦‚ LLaMAï¼‰æˆ– encoder-decoder æ¶æ„ï¼›
- æç«¯å‰ªæï¼ˆå¦‚ 11/12ï¼‰è™½ä¿ç•™ä¸€å®šæ€§èƒ½ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ï¼ˆå¦‚ MNLIï¼‰ä¸Šä»æœ‰è¾ƒå¤§ä¸‹é™ï¼›
- Expander-FFN çš„è®¾è®¡å‡è®¾ head ç»´åº¦è¾ƒå°ï¼ˆ$d_{\text{head}} = d/h$ï¼‰ï¼Œå¯èƒ½é™åˆ¶åœ¨éå¸¸å®½æ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† SHRP æ‰©å±•è‡³ **decoder-based models**ï¼Œæ¢ç´¢è‡ªå›å½’ç”Ÿæˆä¸­çš„ç»“æ„åŒ–å‹ç¼©ï¼›
- æ¢ç´¢ **adaptive head æ•°é‡**ï¼ˆper-layer åŠ¨æ€å†³å®šä¿ç•™å‡ ä¸ª headï¼‰ï¼›
- ç»“åˆé‡åŒ–ã€è’¸é¦ç­‰æŠ€æœ¯è¿›ä¸€æ­¥æå‡å‹ç¼©æ•ˆç‡ï¼›
- åœ¨æ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ BERT-largeã€RoBERTaï¼‰ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼›
- æ¢ç´¢æ›´å¤šåº”ç”¨åœºæ™¯ï¼Œå¦‚ mobile inferenceã€è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚

---

## æ€»ç»“
SHRP æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„ Transformer ç¼–ç å™¨å‹ç¼©èŒƒå¼ï¼Œé€šè¿‡ **Expert Attention + Expander-FFN + Top-1 Routing & Pruning** çš„ç»„åˆï¼Œå®ç°äº†ï¼š
- é«˜å‹ç¼©æ¯”ï¼ˆæœ€é«˜ 88.5% å‚æ•°å‡å°‘ï¼‰ï¼›
- é«˜ä¿çœŸåº¦ï¼ˆ84.4% å‡†ç¡®ç‡ä¿ç•™ï¼‰ï¼›
- é«˜æ¨ç†æ•ˆç‡ï¼ˆ4.2Ã— ååæå‡ï¼‰ï¼›
- éƒ¨ç½²å‹å¥½æ€§ï¼ˆé™æ€ã€ç¡®å®šæ€§æ¨¡å‹ï¼‰ï¼›

è¯¥å·¥ä½œæ¨åŠ¨äº† model compression ä»â€œå¯å‘å¼å‰ªæâ€å‘â€œå¯è§£é‡Šã€å¯éƒ¨ç½²çš„ç³»ç»Ÿè®¾è®¡â€è½¬å˜ï¼Œå…·æœ‰é‡è¦çš„å·¥ä¸šåº”ç”¨ä»·å€¼ã€‚

</details>

---

### 5. [Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits](https://arxiv.org/abs/2512.20755)

**Authors**: Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.20755v1  

#### Abstract
Ensuring the safety and efficiency of AI systems is a central goal of modern research. Formal verification provides guarantees of neural network robustness, while early exits improve inference efficiency by enabling intermediate predictions. Yet verifying networks with early exits introduces new cha...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡èšç„¦äº**ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨æ•ˆç‡ä¸å®‰å…¨æ€§ä¹‹é—´çš„æƒè¡¡**ã€‚ä¸€æ–¹é¢ï¼Œ**Early Exit (EE)** æŠ€æœ¯é€šè¿‡å…è®¸æ¨¡å‹åœ¨ä¸­é—´å±‚æå‰ç»ˆæ­¢æ¨ç†æ¥æå‡æ¨ç†æ•ˆç‡ï¼›å¦ä¸€æ–¹é¢ï¼Œ**å½¢å¼åŒ–éªŒè¯ï¼ˆFormal Verificationï¼‰** è¢«ç”¨äºä¿è¯æ¨¡å‹çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œå°† EE å¼•å…¥åï¼Œå…¶åŠ¨æ€æ‰§è¡Œè·¯å¾„ï¼ˆconditional execution pathsï¼‰ç ´åäº†ä¼ ç»ŸéªŒè¯å·¥å…·å¯¹å›ºå®šè¾“å‡ºå±‚çš„å‡è®¾ï¼Œä½¿å¾—æ ‡å‡†éªŒè¯æ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨ã€‚

å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š
- å¦‚ä½•ä¸ºå¸¦æœ‰ Early Exit çš„ç¥ç»ç½‘ç»œå®šä¹‰åˆé€‚çš„**é²æ£’æ€§å±æ€§**ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ª**å¯æ‰©å±•ä¸”å®Œå¤‡çš„å½¢å¼åŒ–éªŒè¯æ¡†æ¶**æ¥å¤„ç† EE æ¶æ„ä¸­çš„æ¡ä»¶åˆ†æ”¯é€»è¾‘ï¼Ÿ

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº†é¦–ä¸ªé’ˆå¯¹ **DNN with Early Exits çš„å½¢å¼åŒ–éªŒè¯æ¡†æ¶**ï¼Œä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **å®šä¹‰äº†é€‚ç”¨äº EE æ¶æ„çš„å±€éƒ¨é²æ£’æ€§ï¼ˆlocal robustnessï¼‰æ€§è´¨**  
   - æå‡ºäº† `Pee` æ€§è´¨ï¼Œæ˜ç¡®è€ƒè™‘äº†â€œæŸä¸ª runner-up ç±»åˆ«åœ¨æŸä¸€å±‚èƒœå‡ºâ€ä¸”â€œæ­¤å‰æ²¡æœ‰ exit å·²ç»ç”±æ­£ç¡®ç±»åˆ«èƒœå‡ºâ€çš„åä¾‹åœºæ™¯ï¼Œé¿å…å› æ¡ä»¶åˆ†æ”¯å¯¼è‡´çš„è™šå‡ counterexampleã€‚

2. **æå‡ºäº†ä¸€ç§é€šç”¨çš„éªŒè¯ç®—æ³•ï¼ˆAlg. 1ï¼‰åŠå…¶ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆAlg. 2ï¼‰**
   - **Baseline ç®—æ³•ï¼ˆAlg. 1ï¼‰**ï¼šç³»ç»Ÿåœ°éå†æ‰€æœ‰ exit å’Œæ‰€æœ‰ runner-up ç±»åˆ«ï¼Œè°ƒç”¨åº•å±‚éªŒè¯å™¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ»¡è¶³ `Pee` çš„è¾“å…¥ã€‚
   - **ä¼˜åŒ–ç­–ç•¥ï¼ˆAlg. 2ï¼‰** åŒ…å«ä¸¤ä¸ªå…³é”®å¯å‘å¼æ”¹è¿›ï¼š
     - **Break Optimizationï¼ˆæ—©åœæœºåˆ¶ï¼‰**ï¼šè‹¥å½“å‰ exit ä¸­ä¸»ç±»åˆ«å¾—åˆ†è¶³å¤Ÿé«˜ï¼ˆ>1-Tï¼‰ï¼Œåˆ™æ— éœ€è¿›ä¸€æ­¥éªŒè¯è¯¥å±‚å…¶ä»–ç±»åˆ«ï¼Œç›´æ¥è·³è¿‡å†…å±‚å¾ªç¯ã€‚
     - **Continue Optimizationï¼ˆæå‰è¿”å› SAFEï¼‰**ï¼šè‹¥èƒ½è¯æ˜åœ¨å½“å‰ exit ä¸»ç±»åˆ«å§‹ç»ˆè·èƒœï¼Œåˆ™åç»­æ‰€æœ‰ exit éƒ½ä¸ä¼šäº§ç”Ÿåä¾‹ï¼Œå¯ç«‹å³è¿”å› SAFEã€‚

3. **ç†è®ºåˆ†ææ­ç¤ºäº†å¤æ‚åº¦ä¼˜åŠ¿**
   - åœ¨ **trace stability å‡è®¾ä¸‹**ï¼ˆå³é‚»åŸŸå†…æ‰€æœ‰è¾“å…¥å…·æœ‰ç›¸åŒæ‰§è¡Œè·¯å¾„ï¼‰ï¼ŒéªŒè¯å¤æ‚åº¦ä»ä¾èµ–æ•´ä¸ªç½‘ç»œè§„æ¨¡é™ä¸ºä»…ä¾èµ–å®é™…æ‰§è¡Œè·¯å¾„é•¿åº¦ã€‚
   - è¯æ˜äº†è¯¥é—®é¢˜å±äº **Fixed Parameter Tractable (FPT)**ï¼Œå‚æ•°ä¸º `kÂ·|T(x)|`ï¼ˆå±‚å®½ Ã— æ‰§è¡Œå±‚æ•°ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„æŒ‡æ•°çº§å¤æ‚åº¦ã€‚

4. **æå‡ºâ€œåˆ©ç”¨ EE æ¥åŠ é€Ÿæ™®é€šç½‘ç»œçš„éªŒè¯â€è¿™ä¸€æ–°è§†è§’**
   - å³ä½¿åŸå§‹æ¨¡å‹æ—  EEï¼Œä¹Ÿå¯é€šè¿‡æ·»åŠ  EE å¹¶ä½¿ç”¨æœ¬æ–‡æ–¹æ³•å®ç°æ›´é«˜æ•ˆçš„éªŒè¯ï¼Œä»è€Œå°† EE è§†ä¸ºä¸€ç§**å¢å¼ºå¯éªŒè¯æ€§ï¼ˆverifiabilityï¼‰çš„æ‰‹æ®µ**ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|--------|--------|
| æ”¯æŒ EE æ¶æ„ | âŒ ä¸æ”¯æŒ | âœ… æ˜¾å¼å»ºæ¨¡æ¡ä»¶åˆ†æ”¯ |
| éªŒè¯æ•ˆç‡ | éœ€åˆ†æå®Œæ•´ç½‘ç»œ | å¯æå‰ç»ˆæ­¢ï¼Œå‡å°‘å†—ä½™æŸ¥è¯¢ |
| å¯æ‰©å±•æ€§ | æŒ‡æ•°å¤æ‚åº¦ | FPT å¤æ‚åº¦ï¼ˆåœ¨ trace ç¨³å®šæ—¶ï¼‰ |
| å®ç”¨ä»·å€¼ | ä»…å…³æ³¨å®‰å…¨ | åŒæ—¶å…¼é¡¾æ•ˆç‡ä¸å®‰å…¨ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **MNIST**ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œ10ç±»ï¼Œç°åº¦å›¾ï¼ˆ1Ã—28Ã—28ï¼‰
- **CIFAR-10**ï¼šå½©è‰²å›¾åƒåˆ†ç±»ï¼Œ10ç±»ï¼ŒRGB å›¾åƒï¼ˆ3Ã—32Ã—32ï¼‰
- **CIFAR-100**ï¼šæ›´å¤æ‚çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œ100ç±»

### æ¨¡å‹æ¶æ„
- **Fully Connected (FC-6)**ï¼š6 å±‚å…¨è¿æ¥ç½‘ç»œï¼Œç”¨äº MNIST
- **LeNet-5 (CNN)**ï¼šç»å…¸å·ç§¯ç½‘ç»œï¼Œç”¨äº CIFAR-10
- **ResNet-18**ï¼šæ®‹å·®ç½‘ç»œï¼Œä¿®æ”¹ç‰ˆï¼ˆMaxPool â†’ AvgPoolï¼‰ï¼Œç”¨äº CIFAR-10 å’Œ CIFAR-100
- **VGG-16**ï¼šæ·±å±‚å·ç§¯ç½‘ç»œï¼Œç”¨äº CIFAR-10

### å®éªŒè®¾ç½®
- **Early Exit è®¾è®¡**ï¼šæ¯ä¸ª exit æ˜¯ä¸€ä¸ªé™„åŠ çš„å…¨è¿æ¥å±‚ + SoftMaxï¼Œé˜ˆå€¼ `T âˆˆ {0.6, 0.7, ..., 0.9}`ï¼Œé»˜è®¤ `T=0.9`
- **éªŒè¯å·¥å…·**ï¼šåŸºäº **Alpha-Beta CROWN** å®ç°éªŒè¯æŸ¥è¯¢ï¼ˆæ”¯æŒ ReLU å’Œ SoftMaxï¼‰
- **æ‰°åŠ¨èŒƒå›´ Îµ**ï¼šæµ‹è¯•å¤šä¸ª Îµ å€¼ `{0.001, 0.005, 0.01, 0.05, 0.1}`
- **è¶…æ—¶é™åˆ¶**ï¼šæ¯æ ·æœ¬æœ€å¤š 30 åˆ†é’Ÿ

### è¯„ä¼°æŒ‡æ ‡
- **éªŒè¯æ—¶é—´ï¼ˆVerification Timeï¼‰**ï¼šSAFE / UNSAFE æŸ¥è¯¢çš„å¹³å‡è€—æ—¶
- **ç»“æœåˆ†å¸ƒ**ï¼šSAFE / UNSAFE / UNKNOWNï¼ˆè¶…æ—¶æˆ–å¤±è´¥ï¼‰çš„æ•°é‡ç»Ÿè®¡
- **Robustness Ratio**ï¼š`#SAFE / (#SAFE + #UNSAFE)`
- **Exit å±‚ç›¸å…³æ€§çƒ­åŠ›å›¾**ï¼šæ¯”è¾ƒæ¨ç† exit ä¸éªŒè¯ exit çš„ä¸€è‡´æ€§

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Alg. 1 vs Alg. 2**ï¼šéªŒè¯ä¼˜åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§
- **With EE vs Without EEï¼ˆvanillaï¼‰**ï¼šè¯„ä¼° EE å¯¹éªŒè¯æ•ˆç‡çš„å½±å“
- **ä¸åŒ exit ä½ç½®ï¼ˆRN_ee1 vs RN_ee2ï¼‰**ï¼šç ”ç©¶ exit æ”¾ç½®å¯¹æ€§èƒ½çš„å½±å“

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### è¡¨ 1ï¼šéªŒè¯è¿è¡Œæ—¶é—´å¯¹æ¯”ï¼ˆå•ä½ï¼šç§’ï¼‰

| Benchmark | æ–¹æ³• | UNSAFEï¼ˆå‡å€¼ï¼‰ | SAFEï¼ˆå‡å€¼ï¼‰ |
|----------|------|----------------|--------------|
| MNIST, FC6 | Alg. 1 | 0.527 | 13.037 |
|           | Alg. 2 | 0.600 | **1.143**ï¼ˆæé€Ÿ ~11Ã—ï¼‰ |
| CIFAR10, LeNet | Alg. 1 | 2.250 | 56.927 |
|                | Alg. 2 | 2.279 | **7.255**ï¼ˆæé€Ÿ ~7.8Ã—ï¼‰ |
| CIFAR10, ResNet18 | Alg. 1 | 3.518 | 409.395 |
|                    | Alg. 2 | 5.679 | **42.646**ï¼ˆæé€Ÿ ~9.6Ã—ï¼‰ |
| CIFAR10, VGG16 | Alg. 1 | 5.412 | âŒ è¶…æ—¶æ— æ³•å®Œæˆ |
|                 | Alg. 2 | 8.823 | **1532.4 ç§’ï¼ˆ~25.5 minï¼‰**

> âœ… **ç»“è®º**ï¼šAlg. 2 åœ¨ SAFE æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äº Alg. 1ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨¡å‹ä¸Šæ•ˆæœæ›´æ˜æ˜¾ã€‚

#### è¡¨ 2ï¼šUNKNOWN ç»“æœæ•°é‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

| Benchmark | Alg. 1 | Alg. 2 |
|----------|--------|--------|
| MNIST, FC6 | 672 | 652 |
| CIFAR10, LeNet | 253 | 256 |
| CIFAR10, ResNet18 | 159 | 162 |
| CIFAR10, VGG16 | 68 | **53**ï¼ˆâ†“22%ï¼‰ |

> âœ… **ç»“è®º**ï¼šAlg. 2 å‡å°‘äº†è¶…æ—¶æƒ…å†µï¼Œæå‡äº†æ±‚è§£èƒ½åŠ›ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### å›¾ 2ï¼šEE æ¨¡å‹ vs Vanilla æ¨¡å‹çš„éªŒè¯æ—¶é—´å¯¹æ¯”
- åœ¨ **LeNet-5ã€ResNet-18ã€FC-6** ä¸Šï¼Œ**å¸¦ EE çš„æ¨¡å‹éªŒè¯é€Ÿåº¦æ›´å¿«**ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒéš¾çš„ SAFE æŸ¥è¯¢ä¸Šã€‚
- **ResNet-18 on CIFAR-100** å®éªŒæ˜¾ç¤ºï¼š
  - Vanilla æ¨¡å‹ï¼š2 å°æ—¶è¶…æ—¶æœªå®Œæˆ SAFE éªŒè¯
  - EE æ¨¡å‹ï¼šçº¦ **1 å°æ—¶å®Œæˆæ‰€æœ‰ SAFE éªŒè¯**
- è¿™è¡¨æ˜ EE ä¸ä»…ä¸æŸå®³å¯éªŒè¯æ€§ï¼Œåè€Œèƒ½**æ˜¾è‘—æå‡éªŒè¯æ•ˆç‡**ã€‚

#### å›¾ 3 & å›¾ 8ï¼šInference Exit vs Verification Exit çƒ­åŠ›å›¾
- å¯¹äº **SAFE æ ·æœ¬**ï¼Œæ¨ç† exit ä¸éªŒè¯ exit é«˜åº¦ä¸€è‡´ï¼ˆå¼ºç›¸å…³æ€§ï¼‰
- å¯¹äº **UNSAFE æ ·æœ¬**ï¼Œå¤šæ•°åä¾‹åœ¨ç¬¬ä¸€ä¸ª exit å°±è¢«å‘ç°
- æ”¯æŒäº† **trace stability å‡è®¾æˆç«‹**ï¼Œè¯´æ˜æ—©æœŸé€€å‡ºç¡®å®å‡å°‘äº†éœ€è¦éªŒè¯çš„ç½‘ç»œéƒ¨åˆ†

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### å›¾ 9ï¼šç®—æ³•å˜ä½“å¯¹æ¯”ï¼ˆAlg. 1, 2, 3, 4ï¼‰
- **Alg. 3ï¼ˆä»… breakï¼‰** > Alg. 1ï¼šbreak ä¼˜åŒ–æœ‰æ•ˆ
- **Alg. 4ï¼ˆä»… continueï¼‰** > Alg. 3ï¼šcontinue ä¼˜åŒ–è¿›ä¸€æ­¥æå‡
- **Alg. 2ï¼ˆä¸¤è€…ç»“åˆï¼‰** > æ‰€æœ‰å•ä¸€ä¼˜åŒ–ï¼šç»„åˆæ•ˆæœæœ€ä½³
- åœ¨ **SAFE æ¡ˆä¾‹ä¸­æå‡æ˜¾è‘—**ï¼ŒUNSAFE æ¡ˆä¾‹å½±å“å°ï¼ˆå›  counterexample é€šå¸¸å¿«é€Ÿæ‰¾åˆ°ï¼‰

> âœ… **ç»“è®º**ï¼šä¸¤ç§ä¼˜åŒ–äº’è¡¥ï¼Œè”åˆä½¿ç”¨è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Early Exit ä¸ä»…æå‡æ¨ç†æ•ˆç‡ï¼Œä¹Ÿå¢å¼ºäº†å¯éªŒè¯æ€§ï¼ˆverifiabilityï¼‰**
   - ç”±äºå¤šæ•°è¾“å…¥åœ¨æµ…å±‚ exit å°±åšå‡ºå†³ç­–ï¼ŒéªŒè¯åªéœ€åˆ†æç½‘ç»œå‰ç¼€ï¼Œå¤§å¹…é™ä½è®¡ç®—è´Ÿæ‹…ã€‚
   
2. âœ… **æå‡ºçš„ `Pee` é²æ£’æ€§å®šä¹‰åˆç†ä¸”å¯éªŒè¯**
   - æ­£ç¡®æ•æ‰äº† EE æ¶æ„ä¸‹çš„æ½œåœ¨é£é™©è·¯å¾„ï¼Œé¿å…è¯¯åˆ¤ã€‚

3. âœ… **Alg. 2 çš„ä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æå‡éªŒè¯æ•ˆç‡**
   - æœ€é«˜å¯è¾¾ **10Ã— åŠ é€Ÿ**ï¼Œå¹¶åœ¨åŸæœ¬æ— æ³•å®Œæˆçš„ä»»åŠ¡ï¼ˆå¦‚ VGG-16ï¼‰ä¸ŠæˆåŠŸéªŒè¯ã€‚

4. âœ… **trace stability å‡è®¾åœ¨å®è·µä¸­å¹¿æ³›æˆç«‹**
   - æ¨ç†è·¯å¾„ä¸éªŒè¯è·¯å¾„é«˜åº¦ä¸€è‡´ï¼Œæ”¯æŒäº† FPT å¤æ‚åº¦åˆ†æçš„å‰æã€‚

5. âœ… **exit ä½ç½®å½±å“é²æ£’æ€§ä¸æ•ˆç‡æƒè¡¡**
   - æ›´æ—©çš„ exitï¼ˆå¦‚ ResNet ç¬¬1å—åï¼‰å¸¦æ¥æ›´é«˜é²æ£’æ€§ï¼ˆæ›´å¤š SAFEï¼‰ï¼Œä½†ç²¾åº¦ç•¥é™ï¼›
   - æ›´æ·±çš„ exit æå‡ç²¾åº¦ï¼Œä½†å¢åŠ éªŒè¯æˆæœ¬å’Œ UNKNOWN æ•°é‡ã€‚

6. âœ… **æ–¹æ³•ç¡¬ä»¶æ— å…³**
   - åœ¨ Apple M3 å’Œ NVIDIA A100 ä¸Šå‡è§‚å¯Ÿåˆ°ä¸€è‡´çš„åŠ é€Ÿè¶‹åŠ¿ï¼ˆè§ Table 6ï¼‰ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ–åº•å±‚éªŒè¯å™¨å¯¹ SoftMax çš„æ”¯æŒ**
   - å½“å‰æ–¹æ³•éœ€å¤„ç† SoftMax æ¿€æ´»å‡½æ•°ï¼ŒæŸäº›å·¥å…·ï¼ˆå¦‚ Marabouï¼‰å°šä¸å®Œå…¨æ”¯æŒã€‚
   
2. **trace stability å¹¶éæ€»æ˜¯æˆç«‹**
   - åœ¨æç«¯æ‰°åŠ¨ä¸‹ï¼Œé‚»åŸŸå†…è¾“å…¥å¯èƒ½è§¦å‘ä¸åŒçš„ exit è·¯å¾„ï¼Œæ­¤æ—¶æ— æ³•äº«å— FPT ä¼˜åŠ¿ã€‚

3. **EE è®­ç»ƒå¯èƒ½ç‰ºç‰²ä¸€å®šç²¾åº¦**
   - æ·»åŠ  exit å¹¶ fine-tune å¯èƒ½è½»å¾®é™ä½æ•´ä½“ accuracyï¼Œéœ€æƒè¡¡ accuracy vs efficiency vs robustnessã€‚

4. **ç›®å‰èšç„¦äº local robustness**
   - å…¶ä»–æ€§è´¨ï¼ˆå¦‚ fairness, safety constraintsï¼‰è™½å¯æ‰©å±•ï¼Œä½†éœ€å®šåˆ¶åŒ– predicate ç¼–ç ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³å…¶ä»–æ€§è´¨éªŒè¯**
   - å¦‚ safetyï¼ˆæ§åˆ¶åŠ¨ä½œè¾¹ç•Œï¼‰ã€fairnessï¼ˆæ•æ„Ÿå±æ€§ä¸å˜æ€§ï¼‰ç­‰ã€‚

2. **å¹¶è¡ŒåŒ–éªŒè¯è¿‡ç¨‹**
   - åˆ©ç”¨åˆ†å¸ƒå¼è®¡ç®—åŠ é€Ÿå¤š exit / å¤šç±»åˆ«æŸ¥è¯¢ã€‚

3. **æ¢ç´¢æ›´å¤æ‚çš„ exit æ¡ä»¶å‡½æ•°**
   - è¶…è¶Šç®€å•çš„ SoftMax é˜ˆå€¼ï¼Œå¼•å…¥ learned gating æˆ– accumulated confidenceã€‚

4. **ç»“åˆ certified training æ–¹æ³•**
   - å°† EE ä¸ certified training æ­£äº¤ç»“åˆï¼ŒåŒæ—¶æå‡é²æ£’æ€§ä¸éªŒè¯æ•ˆç‡ã€‚

5. **åº”ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ Transformersï¼‰**
   - æ¢ç´¢ EE åœ¨ NLP æ¨¡å‹ä¸­çš„éªŒè¯æ½œåŠ›ã€‚

---

> **æ€»ç»“**ï¼šæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å»ºç«‹äº† **DNN with Early Exits çš„å½¢å¼åŒ–éªŒè¯æ¡†æ¶**ï¼Œä¸ä»…è§£å†³äº† EE æ¶æ„å¸¦æ¥çš„éªŒè¯æŒ‘æˆ˜ï¼Œè¿˜æ„å¤–å‘ç° EE å¯ä½œä¸º**æå‡æ¨¡å‹å¯éªŒè¯æ€§çš„å·¥å…·**ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç®—æ³•ä¼˜åŒ–ä¸ç†è®ºåˆ†æï¼Œå®ç°äº†åœ¨ä¿æŒ soundness ä¸ completeness çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡éªŒè¯æ•ˆç‡ï¼Œä¸ºé«˜æ•ˆä¸”å®‰å…¨çš„ AI ç³»ç»Ÿæä¾›äº†æ–°çš„è®¾è®¡èŒƒå¼ã€‚

</details>

---

### 6. [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)

**Authors**: Zhuo Yang, Yeyun Chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.20135v2  

#### Abstract
Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åˆ†å­ç¼–è¾‘ï¼ˆMolecular Editingï¼‰å’Œåˆ†å­ä¼˜åŒ–ï¼ˆMolecular Optimizationï¼‰æ˜¯è¯ç‰©è®¾è®¡ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨**å•æ­¥ç”ŸæˆèŒƒå¼**ï¼ˆsingle-pass generationï¼‰ï¼Œç¼ºä¹å¯¹å¤šæ­¥å†³ç­–è¿‡ç¨‹çš„å»ºæ¨¡ï¼Œéš¾ä»¥æœ‰æ•ˆæ•´åˆåŒ–å­¦å·¥å…·åé¦ˆï¼ˆå¦‚æœ‰æ•ˆæ€§ã€ç›¸ä¼¼æ€§ã€æ€§è´¨è¯„ä¼°ï¼‰ã€‚è¿™å¯¼è‡´æ¨¡å‹å®¹æ˜“ç”ŸæˆåŒ–å­¦æ— æ•ˆç»“æ„ï¼ˆhallucinationï¼‰ï¼Œä¸”æ— æ³•å®ç°å¯è§£é‡Šã€å¯æ§çš„è¿­ä»£ä¼˜åŒ–ã€‚

æ­¤å¤–ï¼Œç°æœ‰LLMæ–¹æ³•å¤§å¤šåœç•™åœ¨é™æ€æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæœªå°†åˆ†å­è®¾è®¡è§†ä¸ºä¸€ä¸ªéœ€è¦**æ¨ç†ã€å·¥å…·è°ƒç”¨ä¸åé¦ˆå¾ªç¯**çš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰è¡Œä¸ºã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **MolAct** â€”â€” ä¸€ç§åŸºäº**æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ **ï¼ˆAgentic Reinforcement Learning, Agentic RLï¼‰çš„æ¡†æ¶ï¼Œé¦–æ¬¡å°†åˆ†å­è®¾è®¡å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¤šæ­¥ã€å·¥å…·å¢å¼ºçš„å†³ç­–è¿‡ç¨‹ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
- **ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**ï¼ˆTwo-stage Training Paradigmï¼‰ï¼š
  - **Stage 1**: é¢„è®­ç»ƒ MolEditAgentï¼Œä¸“æ³¨äºåŸºç¡€ç¼–è¾‘èƒ½åŠ›ï¼ˆadd/delete/substitute åŠŸèƒ½å›¢ï¼‰ï¼Œé€šè¿‡ validity å’Œ similarity åé¦ˆå­¦ä¹ æ­£ç¡®æ“ä½œã€‚
  - **Stage 2**: åœ¨ Stage 1 æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ MolOptAgentï¼Œè¿›è¡Œå¤æ‚å±æ€§ä¼˜åŒ–ï¼ˆå¦‚ LogPã€solubilityï¼‰ï¼Œå¤ç”¨å·²å­¦ç¼–è¾‘ç­–ç•¥å¹¶å¼•å…¥ property oracle åé¦ˆã€‚
- **å·¥å…·å¢å¼ºçš„äº¤äº’æœºåˆ¶**ï¼ˆTool-Augmented Interactionï¼‰ï¼š
  - æ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥å¯é€‰æ‹©æ‰§è¡Œï¼šâ‘  ç¼–è¾‘æ“ä½œï¼›â‘¡ è°ƒç”¨å¤–éƒ¨åŒ–å­¦å·¥å…·ï¼ˆvalidity checker, similarity calculator, property predictorï¼‰è·å–åé¦ˆï¼›â‘¢ ç»ˆæ­¢ã€‚
  - æ‰€æœ‰å·¥å…·è¾“å‡ºä½œä¸ºä¸Šä¸‹æ–‡ä¿ç•™ï¼Œä»…å¯¹ agent ç”Ÿæˆçš„ token è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼ˆmasked updateï¼‰ï¼Œç¡®ä¿ç¨³å®šä¿¡ç”¨åˆ†é…ã€‚
- **Group-Relative Policy Optimization (GRPO)**ï¼š
  - å¤šæ¡ rollout å…±äº«åŒä¸€è¾“å…¥ promptï¼Œå¹¶åœ¨å…¶å†…éƒ¨ç›¸å¯¹å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°ï¼Œæå‡é•¿ç¨‹ä»»åŠ¡ä¸­ RL è®­ç»ƒç¨³å®šæ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼ ç»Ÿæ–¹æ³• / LLM Baseline | MolAct |
|------|------------------------|--------|
| å†³ç­–æ¨¡å¼ | å•æ­¥ç”Ÿæˆ | å¤šè½®â€œæ€è€ƒâ†’å·¥å…·è°ƒç”¨â†’åé¦ˆâ€å¾ªç¯ |
| åŒ–å­¦æœ‰æ•ˆæ€§æ§åˆ¶ | ä¾èµ–æ¨¡å‹å†…éšçŸ¥è¯† | æ˜¾å¼è°ƒç”¨å·¥å…·éªŒè¯ï¼Œé¿å…æ— æ•ˆç»“æ„ |
| å¯è§£é‡Šæ€§ä¸å¯æ§æ€§ | é»‘ç®±ç”Ÿæˆ | æ¯æ­¥æ“ä½œå¯è¿½è¸ªï¼Œæ”¯æŒä¸­é—´åé¦ˆè°ƒæ•´ |
| æ€§èƒ½è¿ç§» | ä»é›¶å¼€å§‹ä¼˜åŒ– | åˆ©ç”¨ç¼–è¾‘é¢„è®­ç»ƒçŸ¥è¯†ï¼ŒåŠ é€Ÿä¼˜åŒ–æ”¶æ•› |
| æˆåŠŸç‡ | ä½ï¼ˆå°¤å…¶å°æ¨¡å‹ï¼‰ | æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯å¤§æ¨¡å‹ä¸‹ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨ **ChemCoTDatasets**ï¼ˆLi et al., 2025ï¼‰æ„å»ºè®­ç»ƒæ•°æ®ã€‚
- ä»ä¸­æå– source SMILES + ç¼–è¾‘æŒ‡ä»¤ æˆ– ä¼˜åŒ–ç›®æ ‡ï¼Œå½¢æˆ RL è®­ç»ƒæ ·æœ¬ã€‚
- åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µçš„æ•°æ®ï¼š
  - **Stage 1ï¼ˆEditingï¼‰**ï¼šåŠŸèƒ½å›¢æ·»åŠ ã€åˆ é™¤ã€æ›¿æ¢ä»»åŠ¡ã€‚
  - **Stage 2ï¼ˆOptimizationï¼‰**ï¼šä»¥æ”¹å–„ LogPã€solubilityã€QEDã€DRD2ã€JNK3ã€GSK3Î² ç­‰ä¸ºç›®æ ‡çš„ä»»åŠ¡ã€‚
- æ‰€æœ‰åˆ†å­ç»è¿‡ canonicalization å’Œ validity checkï¼Œä¿è¯è¾“å…¥ä¸€è‡´æ€§ã€‚

### å®éªŒè®¾ç½®
- **Backbone æ¨¡å‹**ï¼šåŸºäº Qwen-2.5-3B å’Œ Qwen-2.5-7B æ„å»º MolEditAgent å’Œ MolOptAgentã€‚
- **è®­ç»ƒæ–¹å¼**ï¼š
  - ä½¿ç”¨ GRPO è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚
  - æ¯ä¸ª prompt å¤åˆ¶ä¸º K æ¡å¹¶è¡Œ rolloutã€‚
  - æœ€å¤§äº¤äº’è½®æ•°ï¼ˆturn budgetï¼‰è®¾ä¸º 16ã€‚
- **å·¥å…·æ¥å£**ï¼š
  - æ”¯æŒè°ƒç”¨ RDKit å·¥å…·è¿›è¡Œ validity checkã€Murcko scaffold similarityï¼ˆTanimotoï¼‰ã€ä»¥åŠå¤šä¸ª property oracleã€‚
- **æ¨ç†æµç¨‹**ï¼šåŒæ ·éµå¾ª multi-turn å¾ªç¯ï¼Œç›´åˆ° terminate æˆ–è¾¾åˆ°é¢„ç®—ã€‚

### è¯„ä¼°æŒ‡æ ‡
| ä»»åŠ¡ç±»å‹ | ä¸»è¦æŒ‡æ ‡ | å«ä¹‰ |
|---------|--------|------|
| **Molecular Editing** | Pass@1 (Acc.) | æ˜¯å¦æˆåŠŸå®ŒæˆæŒ‡å®šç¼–è¾‘æ“ä½œ |
| | Validity (%) | ç”Ÿæˆåˆ†å­æ˜¯å¦åŒ–å­¦æœ‰æ•ˆ |
| **Molecular Optimization** | Î” (Mean Property Improvement) | å±æ€§å¹³å‡æå‡å€¼ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ |
| | SR% (Success Rate) | æˆåŠŸæå‡å±æ€§çš„æ¯”ä¾‹ï¼ˆ%ï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¶µç›–å¤šç§ä¸»æµ LLM å’Œä¸“ç”¨æ¨¡å‹ï¼Œåˆ†ä¸ºä¸¤ç±»ï¼š
- **With Thinking (W/ Thinking)**ï¼šå…·å¤‡é“¾å¼æ€ç»´èƒ½åŠ›çš„é—­æºæˆ–å¼€æºæ¨¡å‹ï¼Œå¦‚ Gemini-2.5-pro-thinkã€Claude3.7-sonnet-thinkã€DeepSeek-R1ã€‚
- **Without Thinking (W/o Thinking)**ï¼šæ ‡å‡†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œå¦‚ GPT-4oã€Qwen ç³»åˆ—ã€Llama ç³»åˆ—ç­‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### åˆ†å­ç¼–è¾‘ä»»åŠ¡ï¼ˆMolecular Editingï¼‰
#### å‡†ç¡®ç‡è¡¨ç°ï¼ˆTable 1ï¼‰
| æ¨¡å‹ | Add (%) | Delete (%) | Substitute (%) |
|------|--------|-----------|---------------|
| **MolEditAgent-7B** | **90.0** | **80.0** | **78.3** |
| Gemini-2.5-pro-think | 100.0 | 85.0 | 81.7 |
| GPT-4o | 80.0 | 80.0 | 65.0 |
| DeepSeek-R1 | 70.0 | 70.0 | 68.3 |

- MolEditAgent-7B åœ¨ Add ä¸Šä»…æ¬¡äº Geminiï¼Œåœ¨ Delete å’Œ Sub ä¸Šä¼˜äºå¤šæ•° baselineã€‚
- MolEditAgent-3B è¡¨ç°è‰¯å¥½ï¼ˆAdd: 80.0%, Delete: 70.0%ï¼‰ï¼Œä½†åœ¨ Sub ä»… 16.7%ï¼Œè¯´æ˜å¤æ‚æ“ä½œéœ€æ›´å¤§å®¹é‡æ¨¡å‹ã€‚

#### åŒ–å­¦æœ‰æ•ˆæ€§ï¼ˆTable 2ï¼‰
| æ¨¡å‹ | Add (%) | Delete (%) | Substitute (%) |
|------|--------|-----------|---------------|
| **MolEditAgent-7B** | **100.0** | **95.0** | **98.0** |
| Qwen2.5-7B-Instruct | 75.0 | 70.0 | 65.0 |
| **MolEditAgent-3B** | **95.0** | **80.0** | **71.7** |
| Qwen-2.5-3B-Instruct | 60.0 | 55.0 | 65.0 |

âœ… **ç»“è®º**ï¼šMolAct æ˜¾è‘—æå‡äº†ç”Ÿæˆåˆ†å­çš„åŒ–å­¦æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯ 7B æ¨¡å‹æ¥è¿‘å®Œç¾ã€‚

---

### åˆ†å­ä¼˜åŒ–ä»»åŠ¡ï¼ˆMolecular Optimizationï¼‰
#### æ€§èƒ½å¯¹æ¯”ï¼ˆTable 3ï¼‰
| æ¨¡å‹ | LogP Î” | LogP SR% | Solubility Î” | Solubility SR% | DRD2 Î” | DRD2 SR% | GSK3Î² Î” | GSK3Î² SR% |
|------|-------|----------|-------------|----------------|--------|----------|----------|------------|
| **MolOptAgent-7B** | **0.89** | **92** | **1.42** | **84** | 0.02 | 38 | 0.04 | 36 |
| Gemini-2.5-pro-think | -0.28 | 81 | 1.91 | 92 | 0.35 | 74 | 0.04 | 68 |
| Claude3.7-sonnet-think | 0.41 | 81 | 0.59 | 77 | 0.18 | 66 | 0.01 | 57 |
| DeepSeek-R1 | 0.36 | 74 | 1.48 | 97 | 0.10 | 62 | -0.02 | 41 |

- **LogP ä¼˜åŒ–æœ€å¼º**ï¼šMolOptAgent-7B å®ç°æœ€é«˜ Î”ï¼ˆ0.89ï¼‰å’Œ SR%ï¼ˆ92%ï¼‰ï¼Œæ˜¾è‘—è¶…è¶Šæ‰€æœ‰ baselineã€‚
- **Solubility æ¥è¿‘æœ€ä¼˜**ï¼šÎ”=1.42ï¼Œä»…æ¬¡äº Gemini å’Œ DeepSeek-R1ï¼ŒSR%=84% ä»å…·ç«äº‰åŠ›ã€‚
- **ç”Ÿç‰©æ´»æ€§æ–¹é¢ä¸­ç­‰åä¸Š**ï¼šåœ¨ DRD2 å’Œ GSK3Î² ä¸Šæœ‰æ­£å‘æ”¹è¿›ï¼Œè™½ä¸åŠæœ€å¼ºæ¨¡å‹ï¼Œä½†ä¼˜äºå¤šæ•° baselineã€‚
- **QED å’Œ JNK3 è¡¨ç°ä¸€èˆ¬**ï¼šä»æœ‰æå‡ç©ºé—´ã€‚

#### å°æ¨¡å‹è¡¨ç°ï¼ˆMolOptAgent-3Bï¼‰
- æ‰€æœ‰ä»»åŠ¡ SR% ä»…ä¸º 3â€“12%ï¼Œè¿œä½äº 7B ç‰ˆæœ¬ï¼Œè¡¨æ˜å°æ¨¡å‹éš¾ä»¥åœ¨æœ‰é™æ­¥æ•°å†…æœ‰æ•ˆæ‰§è¡Œç­–ç•¥ã€‚

---

### æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰

#### ä¸€é˜¶æ®µ vs ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆTable 4ï¼‰
| æ¨¡å‹ | LogP SR% | Solubility SR% | QED SR% | ... |
|------|--------|---------------|--------|-----|
| Qwen-2.5-7B-Instructï¼ˆone-stageï¼‰ | 0 | 0 | 12 | ... |
| Qwen-2.5-3B-Instructï¼ˆone-stageï¼‰ | 0 | 0 | 0 | ... |
| **MolOptAgent-7B**ï¼ˆtwo-stageï¼‰ | **92** | **84** | **35** | ... |
| **MolOptAgent-3B**ï¼ˆtwo-stageï¼‰ | **12** | **8** | **5** | ... |

ğŸ“Œ **å…³é”®å‘ç°**ï¼šå³ä½¿æ‹¥æœ‰ç›¸åŒå·¥å…·å’Œ RL è®¾ç½®ï¼Œ**æ²¡æœ‰ç¼–è¾‘é¢„è®­ç»ƒçš„ä¸€é˜¶æ®µæ–¹æ³•å®Œå…¨å¤±è´¥**ï¼Œè¯æ˜ä¸¤é˜¶æ®µè®¾è®¡è‡³å…³é‡è¦ã€‚

#### æ¨¡å‹å®¹é‡å½±å“ï¼ˆFigure 4â€“6ï¼‰
- **å“åº”é•¿åº¦åˆ†ææ˜¾ç¤º**ï¼š
  - MolOptAgent-3B åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°æç«¯å“åº”å³°å€¼ï¼ˆé«˜è¾¾ 5000 tokensï¼‰ï¼Œåæ˜ å…¶æ¨ç†å†—ä½™ã€å·¥å…·è°ƒåº¦æ··ä¹±ã€‚
  - MolOptAgent-7B å“åº”æ›´çŸ­ä¸”ç¨³å®šï¼Œä½“ç°é«˜æ•ˆå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚
- å°½ç®¡ä¸¤è€… validation reward ç›¸ä¼¼ï¼Œä½† **success rate å·®å¼‚å·¨å¤§ï¼ˆ3â€“12% vs 14â€“92%ï¼‰**ï¼Œè¯´æ˜ reward ä¸ç­‰äºå¯æ‰§è¡Œæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å¤šæ­¥ã€å·¥å…·å¢å¼ºçš„å†³ç­–æ¡†æ¶æ˜¾è‘—æå‡åˆ†å­ç¼–è¾‘ä¸ä¼˜åŒ–æ•ˆæœ**ï¼š
   - æ˜¾å¼å¼•å…¥ validityã€similarityã€property å·¥å…·åé¦ˆï¼Œæå¤§æé«˜äº†ç”Ÿæˆåˆ†å­çš„æœ‰æ•ˆæ€§å’Œä»»åŠ¡æˆåŠŸç‡ã€‚
2. âœ… **ä¸¤é˜¶æ®µè®­ç»ƒæ˜¯æˆåŠŸçš„å…³é”®**ï¼š
   - å…ˆæŒæ¡åŸºæœ¬ç¼–è¾‘æŠ€èƒ½ï¼Œå†è¿ç§»åˆ°å±æ€§ä¼˜åŒ–ä»»åŠ¡ï¼Œä½¿ç­–ç•¥å­¦ä¹ æ›´åŠ ç¨³å®šé«˜æ•ˆã€‚
   - æ— æ­¤é¢„è®­ç»ƒåˆ™ RL å®Œå…¨æ— æ³•æ”¶æ•›åˆ°æœ‰ç”¨ç­–ç•¥ã€‚
3. âœ… **æ¨¡å‹å®¹é‡ç›´æ¥å½±å“ç­–ç•¥æ‰§è¡ŒåŠ›**ï¼š
   - æ›´å¤§çš„ backboneï¼ˆ7Bï¼‰ä¸ä»…èƒ½è·å¾—æ›´é«˜ rewardï¼Œæ›´èƒ½å°†å¥–åŠ±è½¬åŒ–ä¸ºå®é™…å¯è¡Œçš„æ“ä½œåºåˆ—ã€‚
   - å°æ¨¡å‹ï¼ˆ3Bï¼‰è™½èƒ½æ‹Ÿåˆ rewardï¼Œä½†æ— æ³•åœ¨äº¤äº’é¢„ç®—å†…æœ‰æ•ˆæ‰§è¡Œã€‚
4. âœ… **MolAct å®ç°é«˜åŒ–å­¦æœ‰æ•ˆæ€§ï¼ˆ95â€“100%ï¼‰ä¸ç«äº‰æ€§ä¼˜åŒ–æ€§èƒ½**ï¼š
   - ç‰¹åˆ«æ˜¯åœ¨ LogP ä¼˜åŒ–ä¸Šå¤§å¹…é¢†å…ˆï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…è¯ç‰©è®¾è®¡åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. â— **éƒ¨åˆ†ç›®æ ‡ä¼˜åŒ–èƒ½åŠ›æœ‰é™**ï¼š
   - å¦‚ JNK3ã€QED ç­‰ä»»åŠ¡è¡¨ç°å¹³å¹³ï¼Œå¯èƒ½å› é€šç”¨ backbone ç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚
2. â— **æœªè€ƒè™‘åˆæˆå¯è¡Œæ€§**ï¼ˆsynthetic feasibilityï¼‰ï¼š
   - å½“å‰æ¡†æ¶ä¸å»ºæ¨¡ååº”è·¯å¾„æˆ–å¯åˆæˆæ€§ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆéš¾ä»¥å®éªŒå®¤å®ç°çš„åˆ†å­ã€‚
3. â— **å°æ¨¡å‹æ‰§è¡Œæ•ˆç‡ä½ä¸‹**ï¼š
   - 3B æ¨¡å‹å­˜åœ¨ä¸¥é‡å·¥å…·æ»¥ç”¨å’Œå“åº”è†¨èƒ€é—®é¢˜ï¼Œé™åˆ¶å…¶éƒ¨ç½²å®ç”¨æ€§ã€‚
4. â— **ä¾èµ–é«˜è´¨é‡å¤–éƒ¨å·¥å…·**ï¼š
   - å·¥å…·ç²¾åº¦ç›´æ¥å½±å“æœ€ç»ˆç»“æœï¼Œè‹¥ property oracle ä¸å‡†ï¼Œåˆ™ä¼˜åŒ–æ–¹å‘å¯èƒ½é”™è¯¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. ğŸ”§ **é›†æˆååº”æ„ŸçŸ¥æ¨¡å—**ï¼š
   - å¼•å…¥ reaction-aware constraints æˆ– retrosynthesis å·¥å…·ï¼Œæå‡ç”Ÿæˆåˆ†å­çš„å¯åˆæˆæ€§ã€‚
2. ğŸ”„ **æ”¹è¿›å°æ¨¡å‹ç­–ç•¥æ‰§è¡Œèƒ½åŠ›**ï¼š
   - è®¾è®¡ error recovery æœºåˆ¶ã€adaptive planning æˆ– distillation æ–¹æ³•ï¼Œæå‡å°æ¨¡å‹ executabilityã€‚
3. ğŸ“š **å¼€å‘æ›´ç²¾ç»†çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**ï¼š
   - è¶…è¶Šç®€å•çš„ä¸¤é˜¶æ®µï¼Œæ¢ç´¢æ¸è¿›å¼ curriculumï¼Œé€‚é…ä¸åŒéš¾åº¦çš„ç›®æ ‡ç»„åˆã€‚
4. ğŸ¯ **å¢å¼ºé¢†åŸŸç‰¹å¼‚æ€§èƒ½åŠ›**ï¼š
   - å¯¹ç‰¹å®šé¶ç‚¹ï¼ˆå¦‚ JNK3ï¼‰å¼•å…¥ domain-specific tools æˆ– knowledge injectionã€‚
5. ğŸ§ª **æ‰©å±•è‡³å…¶ä»–ç§‘å­¦å‘ç°ä»»åŠ¡**ï¼š
   - å°† MolAct æ¡†æ¶æ¨å¹¿åˆ°ææ–™è®¾è®¡ã€å‚¬åŒ–å‰‚ä¼˜åŒ–ç­‰å…¶ä»–ç§‘å­¦å†³ç­–ä»»åŠ¡ã€‚

---

> ğŸ’¬ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **MolAct æ˜¯é¦–ä¸ªå°†åˆ†å­è®¾è®¡å½¢å¼åŒ–ä¸º Agentic RL é—®é¢˜çš„å·¥ä½œï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒ + å·¥å…·å¢å¼ºäº¤äº’ï¼Œåœ¨ä¿æŒé«˜åŒ–å­¦æœ‰æ•ˆæ€§çš„åŒæ—¶å®ç°äº†é¢†å…ˆçš„å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½ï¼Œä¸ºå¯è§£é‡Šã€å¯æ§çš„ AI è¾…åŠ©è¯ç‰©è®¾è®¡æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 7. [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)

**Authors**: Shize Liang, Hongzhi Wang  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.20949v1  

#### Abstract
Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Neural Probe-Based Hallucination Detection for Large Language Models è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“äº§ç”Ÿ**å¹»è§‰ï¼ˆhallucinationï¼‰**ï¼Œå³è¾“å‡ºç¼ºä¹äº‹å®ä¾æ®æˆ–é”™è¯¯çš„å†…å®¹ã€‚è¿™åœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸå°¤ä¸ºå±é™©ã€‚ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **åŸºäºä¸ç¡®å®šæ€§çš„æ–¹æ³•**ï¼ˆå¦‚ perplexityã€entropyï¼‰å¸¸åœ¨é«˜ç½®ä¿¡åº¦ä¸‹ä»äº§ç”Ÿé”™è¯¯åˆ¤æ–­ï¼›
- **åŸºäºæ£€ç´¢çš„æ–¹æ³•**ä¾èµ–å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå—é™äºæ£€ç´¢æ•ˆç‡å’ŒçŸ¥è¯†è¦†ç›–èŒƒå›´ï¼Œéš¾ä»¥å®æ—¶åº”ç”¨ï¼›
- **ä¼ ç»Ÿçº¿æ€§æ¢é’ˆï¼ˆlinear probesï¼‰** è™½è½»é‡ä¸”å¯å®æ—¶è¿è¡Œï¼Œä½†åªèƒ½æ•æ‰æµ…å±‚è¯­ä¹‰æ¨¡å¼ï¼Œæ— æ³•å»ºæ¨¡æ·±å±‚éšè—çŠ¶æ€ä¸­çš„éçº¿æ€§è¯­ä¹‰ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºä¸€ç§**åŸºäºç¥ç»ç½‘ç»œæ¢é’ˆçš„ token-level å¹»è§‰æ£€æµ‹æ¡†æ¶**ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

- **MLP Probe æ¶æ„**ï¼šå†»ç»“ LLM å‚æ•°ï¼Œåœ¨ä¸­é—´å±‚æ’å…¥è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºæ¢é’ˆï¼Œå¯¹é«˜ç»´éšè—çŠ¶æ€è¿›è¡Œ**éçº¿æ€§å»ºæ¨¡**ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚è¯­ä¹‰ç©ºé—´ä¸­çš„å¹»è§‰ç‰¹å¾ã€‚
- **å¤šç›®æ ‡è”åˆæŸå¤±å‡½æ•°ï¼ˆmulti-objective joint lossï¼‰**ï¼š
  - åŒ…å« `Lfocal`ï¼ˆèšç„¦éš¾æ ·æœ¬ï¼‰ã€`Lspan`ï¼ˆå¢å¼ºå®ä½“è·¨åº¦ä¸€è‡´æ€§ï¼‰ã€`Lsparse`ï¼ˆç¨€ç–æ­£åˆ™åŒ–æå‡å®šä½ç²¾åº¦ï¼‰ã€`LKL`ï¼ˆä¿æŒè¯­è¨€æ¨¡å‹åˆ†å¸ƒç¨³å®šæ€§ï¼‰ã€‚
  - å®ç°è¯­ä¹‰å¯åˆ†æ€§ã€è®­ç»ƒç¨³å®šæ€§å’Œå±€éƒ¨æ•æ„Ÿæ€§çš„ååŒä¼˜åŒ–ã€‚
- **Layer Position-Probe Performance å“åº”æ¨¡å‹ + è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨æœç´¢æœ€ä¼˜æ¢é’ˆæ’å…¥å±‚**ï¼š
  - å»ºç«‹â€œå±‚ä½ç½®-æ¢é’ˆæ€§èƒ½â€æ•°å­¦æ¨¡å‹ï¼Œå®šä¹‰åŸºäº KL æ•£åº¦çš„**è¡¨ç¤ºå¯åˆ†æ€§åº¦é‡**ï¼›
  - ä½¿ç”¨ **Bayesian Optimizationï¼ˆBOï¼‰** è‡ªåŠ¨å¯»æ‰¾æœ€ä½³æ¢é’ˆæ’å…¥å±‚ï¼Œæ›¿ä»£äººå·¥ç»éªŒé€‰æ‹©ï¼Œç†è®ºè¯æ˜å…¶æ¸è¿‘æœ€ä¼˜æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **å‡†ç¡®æ€§** | æ˜¾è‘—ä¼˜äº uncertainty-based å’Œ retrieval-based æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Š AUC å’Œ R@0.1 æå‡æ˜æ˜¾ |
| **å®æ—¶æ€§** | æ¢é’ˆæœºåˆ¶æ— éœ€å¤–éƒ¨æ£€ç´¢ï¼Œå¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶æ£€æµ‹ token-level å¹»è§‰ |
| **æ³›åŒ–èƒ½åŠ›** | åœ¨è·¨é¢†åŸŸï¼ˆåŒ»å­¦ã€å¸¸è¯†ã€æ³•å¾‹ï¼‰ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ |
| **è‡ªåŠ¨åŒ–ç¨‹åº¦** | åˆ©ç”¨ BO è‡ªåŠ¨ç¡®å®šæœ€ä¼˜æ¢é’ˆå±‚ï¼Œå‡å°‘äººå·¥è°ƒå‚æˆæœ¬ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **LongFact-annotations**ï¼šåŸºäº LongFact å’Œ LongFact++ æ„é€ çš„é•¿æ–‡æœ¬è¯­æ–™ï¼Œä¿ç•™ token å¯¹é½ï¼Œæ ‡æ³¨å®ä½“çº§å¹»è§‰ï¼ˆäººç‰©ã€ç»„ç»‡ã€åœ°ç‚¹ã€æ—¥æœŸã€å¼•ç”¨ç­‰ï¼‰ï¼Œç”±å…·å¤‡ç½‘é¡µæ£€ç´¢èƒ½åŠ›çš„ Claude 4 Sonnet è‡ªåŠ¨ç”Ÿæˆå¹¶éªŒè¯æ ‡ç­¾ã€‚
- å››ç±» prompt ç±»å‹ï¼šä¸»é¢˜èšç„¦æŸ¥è¯¢ã€åäººä¼ è®°ã€å¼•æ–‡ç”Ÿæˆã€é‡Œç¨‹ç¢‘æ¡ˆä»¶ç›¸å…³æ³•å¾‹é—®é¢˜ã€‚
- å®éªŒè¯„ä¼°æ¶µç›–å››ä¸ªä»£è¡¨æ€§ç”Ÿæˆæ•°æ®é›†ï¼š
  1. **LongFact**ï¼ˆQwen2.5-7B-Instruct ç”Ÿæˆï¼‰
  2. **LongFact-Augmented**ï¼ˆåŒä¸Šï¼‰
  3. **HealthBench**ï¼ˆMeta-Llama-3.1-8B-Instruct ç”Ÿæˆï¼ŒåŒ»å­¦é—®ç­”ï¼‰
  4. **TriviaQA**ï¼ˆMeta-Llama-3.1-8B-Instruct ç”Ÿæˆï¼Œå¸¸è¯†é—®ç­”ï¼‰

### å®éªŒè®¾ç½®
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-7B-Instruct å’Œ Meta-Llama-3.1-8B-Instruct
- **æ¢é’ˆç±»å‹å¯¹æ¯”**ï¼š
  - Linear Probeï¼ˆçº¿æ€§åˆ†ç±»å™¨ï¼‰
  - MLP Probeï¼ˆæœ¬æ–‡æå‡ºï¼‰
- **è®­ç»ƒæ–¹å¼**ï¼šä»…è®­ç»ƒæ¢é’ˆå‚æ•°ï¼ŒLLM ä¸»å¹²å‚æ•°å†»ç»“
- **é»˜è®¤æ¢é’ˆåˆå§‹ä½ç½®**ï¼š$ l = \lfloor 0.95 \times \text{num\_layers} \rfloor $

### è¯„ä¼°æŒ‡æ ‡
- **AUC**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯ï¼Œè¡¡é‡æ•´ä½“åˆ¤åˆ«èƒ½åŠ›
- **R@0.1 FPR**ï¼ˆRecall at 0.1 False Positive Rateï¼‰ï¼šä½è¯¯æŠ¥ç‡ä¸‹çš„å¬å›ç‡ï¼Œåæ˜ é«˜ç²¾åº¦åœºæ™¯ä¸‹çš„æ£€æµ‹èƒ½åŠ›
- **Accuracy / Precision / Recall**ï¼štoken-level åˆ†ç±»æ€§èƒ½
- **Probe Loss & LM Loss**ï¼šåˆ†ææ¢é’ˆæ‹Ÿåˆèƒ½åŠ›å’Œå¯¹ä¸»æ¨¡å‹å¹²æ‰°

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Perplexity** | Uncertainty-based | è¡¡é‡ä¸‹ä¸€ä¸ª token çš„ä¸ç¡®å®šæ€§ |
| **Entropy** | Uncertainty-based | åŸºäºè¯­ä¹‰ç†µä¼°è®¡ä¸ç¡®å®šæ€§ |
| **Linear Probe** | Probe-based | ä¼ ç»Ÿçº¿æ€§åˆ†ç±»å™¨æ¢é’ˆï¼Œç”¨äºå¯¹æ¯” |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| Dataset | Method | AUC â†‘ | R@0.1 â†‘ | Accuracy | Precision | Recall |
|--------|--------|-------|---------|----------|-----------|--------|
| **LongFact** | Perplexity | 0.4623 | 0.0879 | â€” | â€” | â€” |
| | Entropy | 0.5124 | 0.1076 | â€” | â€” | â€” |
| | Linear Probe | 0.9022 | 0.6891 | 0.9022 | 0.8829 | 0.5280 |
| | **MLP Probe (Ours)** | **0.9528** | **0.7024** | **0.9528** | **0.8902** | **0.5715** |
| **LongFact-Aug** | MLP Probe | 0.9404 | 0.6395 | 0.9404 | 0.4828 | 0.0784 |
| **HealthBench** | MLP Probe | 0.9549 | **0.7695** | 0.9549 | 0.3231 | **0.4904** |
| **TriviaQA** | Linear Probe | 0.8336 | 0.1730 | 0.8336 | 0.1178 | 0.3255 |
| | **MLP Probe (Ours)** | **0.9223** | **0.6891** | **0.9223** | **0.3886 (+270%)** | **0.4477 (+37%)** |

> æ³¨ï¼šTriviaQA ä¸Š Precision æå‡è¶…è¿‡ **270%**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **MLP Probe æ˜¾è‘—ä¼˜äºæ‰€æœ‰ baseline**ï¼š
  - åœ¨ LongFact ä¸Šï¼Œç›¸æ¯” linear probeï¼ŒAUC æå‡çº¦ **5.6%**ï¼ŒR@0.1 æå‡ **2.3%**ï¼ŒRecall æå‡ **8.2%**ï¼›
  - åœ¨ TriviaQA ä¸Šï¼ŒR@0.1 æå‡é«˜è¾¾ **516%**ï¼ˆä» 0.173 â†’ 0.6891ï¼‰ï¼Œæ˜¾ç¤ºå…¶åœ¨ä½è¯¯æŠ¥æ¡ä»¶ä¸‹æå¼ºçš„æ—©æœŸå¬å›èƒ½åŠ›ï¼›
  - ä¸ç¡®å®šæ€§æ–¹æ³•ï¼ˆPerplexity/Entropyï¼‰æ€§èƒ½æ¥è¿‘éšæœºçŒœæµ‹ï¼ˆAUC â‰ˆ 0.5ï¼‰ï¼Œè¯´æ˜ä»…é æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä¸è¶³ä»¥è¯†åˆ«å¹»è§‰ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰

| ç§»é™¤ç»„ä»¶ | Î”AUC â†“ | Î”R@0.1 â†“ | ç»“è®º |
|--------|--------|----------|------|
| `Lfocal` | -0.3720 | -0.4414 | è‡³å…³é‡è¦ï¼Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼Œæå‡å¯¹å¹»è§‰ token çš„æ•æ„Ÿæ€§ |
| `Lsparse` | -0.0567 | -0.1393 | æŠ‘åˆ¶è¿‡åº¦æ¿€æ´»ï¼Œæé«˜å®šä½ç²¾åº¦ |
| `CKL` | -0.0234 | -0.0552 | ç»´æŒè¯­è¨€æ¨¡å‹åˆ†å¸ƒä¸€è‡´æ€§ï¼Œæå‡é²æ£’æ€§ |
| `Lspan` | -0.0132 | -0.1706 | å¢å¼ºè¿ç»­å¹»è§‰ span çš„æ£€æµ‹èƒ½åŠ›ï¼Œæ˜¾è‘—å½±å“ R@0.1 |

> å¤šç›®æ ‡æŸå¤±å„éƒ¨åˆ†ååŒä½œç”¨ï¼Œ`Lfocal` æ˜¯æ ¸å¿ƒé©±åŠ¨åŠ›ï¼Œå…¶ä½™æä¾›äº’è¡¥å¢ç›Šã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **MLP æ¢é’ˆèƒ½æœ‰æ•ˆæ•æ‰æ·±å±‚éšè—çŠ¶æ€ä¸­çš„éçº¿æ€§å¹»è§‰æ¨¡å¼**ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçº¿æ€§æ¢é’ˆï¼›
2. **token-level å®ä½“çº§å¹»è§‰æ£€æµ‹å…·æœ‰å¯è¡Œæ€§ä¸å®ç”¨æ€§**ï¼šå®ä½“è¾¹ç•Œæ¸…æ™°ã€æ”¯æŒå®æ—¶éªŒè¯ï¼Œé€‚åˆåœ¨çº¿ç³»ç»Ÿéƒ¨ç½²ï¼›
3. **å¤šç›®æ ‡è”åˆæŸå¤±å‡½æ•°æœ‰æ•ˆå¹³è¡¡äº†ç²¾åº¦ã€ç¨³å®šæ€§ä¸ç»“æ„ä¸€è‡´æ€§**ï¼Œå°¤å…¶åœ¨ä½è¯¯æŠ¥åœºæ™¯ä¸‹æå‡æ˜¾è‘—ï¼›
4. **æœ€ä¼˜æ¢é’ˆæ’å…¥å±‚å¹¶éå›ºå®šé«˜å±‚ï¼Œéœ€åŠ¨æ€ç¡®å®š**ï¼šé€šè¿‡è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨æœç´¢ï¼ŒQwen2.5-7B æœ€ä¼˜å±‚ä¸º **29**ï¼ŒLlama3.1-8B ä¸º **22**ï¼ˆè§ Figure 6ï¼‰ï¼ŒéªŒè¯äº†â€œä¸­é«˜å±‚è¯­ä¹‰æŠ½è±¡åŒºâ€æ˜¯æœ€ä½³æ¢æµ‹åŒºåŸŸï¼›
5. **ç†è®ºä¿è¯**ï¼šæ‰€æè´å¶æ–¯ä¼˜åŒ–ç­–ç•¥æ»¡è¶³æ¸è¿‘æœ€ä¼˜æ€§ï¼ˆTheorem 1ï¼‰ï¼Œç´¯è®¡é—æ†¾éšè¿­ä»£æ”¶æ•›è‡³é›¶ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ–¹æ³•èšç„¦äº **entity-level hallucinations**ï¼Œå°šæœªæ‰©å±•åˆ° assertion-level æˆ– relation-level å¹»è§‰ï¼›
- æ¢é’ˆè®­ç»ƒä¾èµ–é«˜è´¨é‡ token-level æ ‡æ³¨æ•°æ®ï¼Œç›®å‰ä¸»è¦ä¾é  LLM è‡ªåŠ¨æ ‡æ³¨ï¼Œå¯èƒ½å­˜åœ¨å™ªå£°ï¼›
- è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶ï¼ˆä¸åŒæ¶æ„çš„æœ€ä¼˜å±‚è§„å¾‹æ˜¯å¦ä¸€è‡´ï¼Ÿï¼‰ï¼›
- å®é™…éƒ¨ç½²æ—¶éœ€è€ƒè™‘æ¢é’ˆå¸¦æ¥çš„é¢å¤–æ¨ç†å»¶è¿Ÿï¼ˆå°½ç®¡è½»é‡ï¼Œä½†ä»å¢åŠ è®¡ç®—è´Ÿæ‹…ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°†æ£€æµ‹ç²’åº¦ä» entity æ‰©å±•åˆ° claimã€relation å’Œ eventï¼›
- æ¢ç´¢ **cross-lingual å’Œ multimodal åœºæ™¯ä¸‹çš„å¹»è§‰æ£€æµ‹**ï¼›
- ç»“åˆ retrieval-based æ–¹æ³•å½¢æˆ hybrid æ£€æµ‹ç³»ç»Ÿï¼Œå…¼é¡¾å†…éƒ¨è¡¨å¾ä¸å¤–éƒ¨è¯æ®ï¼›
- ç ”ç©¶æ¢é’ˆçš„å¯è§£é‡Šæ€§ï¼Œåå‘æŒ‡å¯¼ LLM å¾®è°ƒä»¥å‡å°‘å¹»è§‰ç”Ÿæˆï¼›
- æ¢ç´¢æ›´é«˜æ•ˆçš„æ¢é’ˆç»“æ„ï¼ˆå¦‚ LoRA-based probingï¼‰ä»¥é™ä½èµ„æºæ¶ˆè€—ã€‚

--- 

> âœ… æ€»ç»“ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆã€å¯è§£é‡Šã€å®æ—¶çš„ token-level å¹»è§‰æ£€æµ‹æ–°èŒƒå¼ï¼Œé€šè¿‡ MLP æ¢é’ˆ + å¤šç›®æ ‡æŸå¤± + è´å¶æ–¯å±‚æœç´¢ä¸‰é‡åˆ›æ–°ï¼Œåœ¨å¤šä¸ªæƒå¨æ•°æ®é›†ä¸Šå®ç°äº† SOTA æ€§èƒ½ï¼Œä¸ºæå‡ LLM å¯é æ€§æä¾›äº†å®ç”¨å·¥å…·å’Œç†è®ºåŸºç¡€ã€‚

</details>

---

### 8. [RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks](https://arxiv.org/abs/2512.20920)

**Authors**: Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.20920v1  

#### Abstract
Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šRevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿›è¡Œ **Full-Parameter Fine-Tuning** æ—¶é¢ä¸´ä¸¥é‡çš„å†…å­˜ç“¶é¢ˆã€‚ç”±äºåå‘ä¼ æ’­éœ€è¦ç¼“å­˜å¤§é‡ä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationsï¼‰ï¼Œå¯¼è‡´å³°å€¼æ˜¾å­˜å ç”¨æé«˜ï¼Œéš¾ä»¥åœ¨å•å¼ æ¶ˆè´¹çº§æˆ–æœåŠ¡å™¨çº§ GPU ä¸Šå®Œæˆè®­ç»ƒã€‚å°½ç®¡å·²æœ‰å¦‚ **ZeRO** å’Œ **FSDP** ç­‰åˆ†å¸ƒå¼ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†å®ƒä»¬ä¾èµ–å¤šå¡é€šä¿¡ï¼Œå¢åŠ äº†ç¡¬ä»¶æˆæœ¬å¹¶é™ä½äº†è®­ç»ƒé€Ÿåº¦ï¼›è€Œ **PEFT æ–¹æ³•ï¼ˆå¦‚ LoRAï¼‰** è™½ç„¶èŠ‚çœå†…å­˜ï¼Œå´åªæ›´æ–°å°‘é‡å‚æ•°ï¼Œç‰ºç‰²äº†æ¨¡å‹æ€§èƒ½ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šRevFFN
æœ¬æ–‡æå‡º **RevFFN** â€”â€” ä¸€ç§åŸºäºå¯é€†ç½‘ç»œï¼ˆreversible networksï¼‰è®¾è®¡çš„é«˜æ•ˆå…¨å‚æ•°å¾®è°ƒæ¡†æ¶ï¼Œä¸“ä¸º **Mixture-of-Experts (MoE)** æ¶æ„çš„å¤§æ¨¡å‹å®šåˆ¶ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å°† Transformer å±‚çš„éšè—çŠ¶æ€æ‹†åˆ†ä¸ºä¸¤ä¸ªæµï¼ˆ$X_1$, $X_2$ï¼‰
- è®¾è®¡è€¦åˆæ›´æ–°è§„åˆ™ï¼Œä½¿å¾—æ¯ä¸€å±‚çš„è¾“å…¥å¯ä»¥ä»è¾“å‡ºç²¾ç¡®é‡æ„ï¼ˆå³æ„å»ºåŒå°„æ˜ å°„ï¼‰
- åœ¨åå‘ä¼ æ’­æ—¶åŠ¨æ€é‡è®¡ç®—æ¿€æ´»å€¼ï¼Œæ— éœ€é¢„å…ˆå­˜å‚¨å¤§å¤šæ•°ä¸­é—´æ¿€æ´»

#### åˆ›æ–°ç»„ä»¶ï¼š
1. **Reversible Block ç»“æ„**  
   - ä¿®æ”¹æ ‡å‡† Transformer å—ç»“æ„ï¼Œé‡‡ç”¨è·¨åˆ†æ”¯æ³¨æ„åŠ›ä¸éå¯¹ç§° FFN æ›´æ–°æœºåˆ¶
   - å®ç°è¾“å…¥å¯ä»è¾“å‡ºæ— æŸæ¢å¤ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜éœ€æ±‚

2. **è½»é‡çº§æŠ•å½±é€‚é…å™¨ï¼ˆProjection Adaptersï¼‰**  
   - å¼•å…¥ä¸¤ä¸ªå°å‹çº¿æ€§å±‚ $P: \mathbb{R}^{d/2} \to \mathbb{R}^d$ å’Œ $P': \mathbb{R}^d \to \mathbb{R}^{d/2}$ï¼Œæ¡¥æ¥é¢„è®­ç»ƒæƒé‡ä¸åŠç»´è¾“å…¥ä¹‹é—´çš„ç»´åº¦ä¸åŒ¹é…
   - ä¸ç ´ååŸå§‹ MoE æ¶æ„ï¼Œä¿ç•™ä¸“å®¶å®¹é‡å’Œè·¯ç”±é€»è¾‘

3. **ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆTwo-Stage Training Scheduleï¼‰**
   - **Stage 1: Adapter Warm-up**  
     å†»ç»“ä¸»å¹²å‚æ•°ï¼Œä»…è®­ç»ƒæŠ•å½±é€‚é…å™¨ï¼Œç¨³å®šå­ç©ºé—´å¯¹é½
   - **Stage 2: Joint Fine-tuning**  
     è§£å†»æ‰€æœ‰ Transformer å‚æ•°ï¼ˆé™¤ MoE è·¯ç”±å™¨å¤–ï¼‰ï¼Œè”åˆä¼˜åŒ–ä¸“å®¶æƒé‡å’Œé€‚é…å™¨

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | RevFFN | PEFTï¼ˆå¦‚ LoRAï¼‰ | åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚ ZeROï¼‰ |
|------|--------|------------------|------------------------|
| æ˜¾å­˜æ•ˆç‡ | é«˜ï¼ˆå‡å°‘ ~50%ï¼‰ | æé«˜ | ä¸­ç­‰ï¼ˆæ€»å†…å­˜ä¸å˜ï¼‰ |
| æ€§èƒ½æ½œåŠ› | å…¨å‚æ•°æ›´æ–° â†’ æ›´å¼ºè¡¨è¾¾èƒ½åŠ› | å—é™äºå°å‚æ•°æ›´æ–° | æ”¯æŒå…¨å‚æ•°æ›´æ–° |
| ç¡¬ä»¶è¦æ±‚ | å• GPU å¯è¡Œ | å• GPU å¯è¡Œ | å¤š GPU å¿…éœ€ |
| è®­ç»ƒé€Ÿåº¦ | è¾ƒä½ååï¼ˆå› é‡è®¡ç®—ï¼‰ | é«˜ | å—é™äºé€šä¿¡å¼€é”€ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šRevFFN åœ¨ä¿æŒæ¥è¿‘ PEFT çº§åˆ«çš„å†…å­˜æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº† **Full-Parameter Fine-Tuning**ï¼Œå…¼é¡¾é«˜æ€§èƒ½ä¸ä½èµ„æºæ¶ˆè€—ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **databricks-dolly-15k**ï¼šä¸€ä¸ªå¼€æºçš„æŒ‡ä»¤è·Ÿéšï¼ˆinstruction-followingï¼‰æ•°æ®é›†ï¼Œç”¨äº fine-tuning æ‰€æœ‰æ¨¡å‹ã€‚

### ğŸ’» å®éªŒå¹³å°
- **ç¡¬ä»¶**ï¼šå•å¼  NVIDIA H800 GPUï¼ˆ80GB VRAMï¼‰
- **è½¯ä»¶æ¡†æ¶**ï¼šPyTorch + HuggingFace Transformers

### ğŸ§ª åŸºçº¿æ–¹æ³•å¯¹æ¯”
#### ï¼ˆ1ï¼‰Parameter-Efficient Fine-Tuning (PEFT)
- **LoRA** [10]
- **DoRA** [19]
- **(IA)^3** [20]

#### ï¼ˆ2ï¼‰Memory-Efficient Full Fine-Tuning
- **SFT + Activation Checkpointing** [21]
- **LoMO** [22]
- **GaLore** [23]

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Peak VRAM Usage (GB)** â†“ | è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§ GPU æ˜¾å­˜å ç”¨ï¼Œè¡¡é‡å†…å­˜æ•ˆç‡ |
| **Training Throughput (samples/sec)** â†‘ | æ¯ç§’å¤„ç†æ ·æœ¬æ•°ï¼Œåæ˜ è®­ç»ƒé€Ÿåº¦ |
| **ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½** | åŒ…æ‹¬ï¼š<br>â€¢ **MMLU (%)**ï¼šå¤šä»»åŠ¡è¯­è¨€ç†è§£<br>â€¢ **GSM8K (%)**ï¼šæ•°å­¦æ¨ç†èƒ½åŠ›<br>â€¢ **Multilingual (%)**ï¼šè·¨è¯­è¨€è¡¨ç°<br>â€¢ **MT-Bench Score**ï¼šå¯¹è¯ä¸å¤šè½®æŒ‡ä»¤éµå¾ªèƒ½åŠ› |

### ğŸ§© æ¨¡å‹åŸºç¡€
- **Qwen1.5-MoE-A2.7B**ï¼šå…·æœ‰ 2.7B æ¿€æ´»å‚æ•°çš„ MoE æ¶æ„ LLM
- æ‰€æœ‰æ–¹æ³•å‡åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶å°½å¯èƒ½å¢å¤§ batch size ä»¥å……åˆ†åˆ©ç”¨ 80GB æ˜¾å­˜

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š è¡¨1ï¼šæ˜¾å­˜ä¸é€Ÿåº¦å¯¹æ¯”ï¼ˆSingle H800ï¼‰

| æ–¹æ³• | Peak VRAM (GB) â†“ | Throughput (samples/s) â†‘ |
|------|------------------|----------------------------|
| **LoRA** | 18.2 | 75.4 |
| **DoRA** | 19.5 | 71.8 |
| **(IA)Â³** | 17.9 | 74.1 |
| **SFT + Checkpointing** | 65.4 | 19.7 |
| **LoMO** | 42.2 | 17.3 |
| **GaLore** | 45.1 | 35.2 |
| **RevFFN** | **39.5** | **24.6** |

> ğŸ”¹ **RevFFN æ˜¾å­˜ä»…ä¸º SFT çš„ 60%ï¼ˆâ†“49%ï¼‰**ï¼Œä¼˜äº GaLore å’Œ LoMO  
> ğŸ”¹ ååä½äº PEFTï¼Œä½†é«˜äºä¼ ç»Ÿ SFTï¼Œè¯´æ˜é‡è®¡ç®—ä»£ä»·å¯æ§

---

### ğŸ“ˆ è¡¨2ï¼šä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | MMLU (%) â†‘ | GSM8K (%) â†‘ | Multilingual (%) â†‘ | MT-Bench â†‘ |
|------|------------|-------------|--------------------|-----------|
| Base Model | 62.4 | 61.2 | 40.4 | 6.25 |
| LoRA | 65.2 | 71.5 | 38.5 | 7.18 |
| DoRA | 65.7 | 70.8 | 38.9 | 7.25 |
| (IA)Â³ | 65.0 | 70.2 | 38.2 | 7.15 |
| SFT + Checkpointing | 66.1 | 74.8 | 39.5 | 7.52 |
| LoMO | 66.2 | 74.6 | 39.3 | 7.50 |
| GaLore | 66.3 | 74.2 | 39.2 | 7.46 |
| **RevFFN** | **66.7** | **75.1** | **38.8** | **7.65** |

> âœ… **RevFFN åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€ä¼˜æ€§èƒ½**ï¼Œå°¤å…¶åœ¨ MMLU å’Œ GSM8K ä¸Šè¶…è¶Šæœ€å¼º SFT åŸºçº¿  
> âš ï¸ Multilingual ç•¥ä½äº Baseï¼Œä½†ä»ä¼˜äºå¤šæ•° PEFT æ–¹æ³•

---

### ğŸ” è¡¨3ï¼šæ¶ˆèå®éªŒï¼ˆAblation Study on MMLUï¼‰

| é…ç½® | MMLU (%) |
|------|----------|
| RevFFNï¼ˆå®Œæ•´æ–¹æ³•ï¼‰ | **66.7** |
| w/o Stage 1ï¼ˆç›´æ¥è”åˆè®­ç»ƒï¼‰ | 57.1 |
| w/o Stage 2ï¼ˆä»…è®­ç»ƒæŠ•å½±å±‚ï¼‰ | 54.5 |

> ğŸ”¹ ä¸¤é˜¶æ®µè®­ç»ƒè‡³å…³é‡è¦ï¼š
> - ç¼ºå°‘ **Adapter Warm-up** å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™
> - ç¼ºå°‘ **Joint Fine-tuning** åˆ™é™åˆ¶æ¨¡å‹è¡¨è¾¾åŠ›ï¼Œé€€åŒ–ä¸º PEFT ç±»æ–¹æ³•

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **RevFFN æˆåŠŸå®ç°å• GPU ä¸Šçš„ MoE æ¨¡å‹å…¨å‚æ•°å¾®è°ƒ**ï¼Œå°†å³°å€¼æ˜¾å­˜ä» 65.4GB é™è‡³ 39.5GBï¼ˆâ†“49%ï¼‰ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯ã€‚
2. **æ€§èƒ½å…¨é¢è¶…è¶Š PEFT ä¸ä¸»æµå†…å­˜ä¼˜åŒ–æ–¹æ³•**ï¼Œè¯æ˜å…¨å‚æ•°æ›´æ–°çš„é‡è¦æ€§ä¸å¯æ›¿ä»£ã€‚
3. **å¯é€†å—è®¾è®¡ + æŠ•å½±é€‚é…å™¨ + ä¸¤é˜¶æ®µè®­ç»ƒ** å½¢æˆååŒæ•ˆåº”ï¼Œåœ¨ä¸ç ´å MoE æ¶æ„çš„å‰æä¸‹è¾¾æˆé«˜æ•ˆè®­ç»ƒã€‚
4. **æ¶ˆèå®éªŒè¯æ˜å„æ¨¡å—ä¸å¯æˆ–ç¼º**ï¼Œç‰¹åˆ«æ˜¯ä¸¤é˜¶æ®µç­–ç•¥å¯¹ç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½èµ·å†³å®šæ€§ä½œç”¨ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **è®¡ç®—å¼€é”€å¢åŠ **ï¼šç”±äºåå‘ä¼ æ’­ä¸­éœ€é‡è®¡ç®—æ¿€æ´»ï¼Œè®­ç»ƒååè¾ƒä½ï¼ˆ~24.6 samples/sï¼‰ï¼Œä¸é€‚åˆè¿½æ±‚æè‡´é€Ÿåº¦çš„åœºæ™¯ã€‚
- **ç»“æ„ä¿®æ”¹å¤æ‚åº¦æå‡**ï¼šç›¸æ¯” LoRA ç­‰æ’å…¥å¼æ–¹æ³•ï¼ŒRevFFN éœ€é‡æ„ Transformer å—ç»“æ„ï¼Œé›†æˆéš¾åº¦æ›´é«˜ã€‚
- å½“å‰ä»…éªŒè¯äº Qwen1.5-MoE-A2.7Bï¼Œå°šæœªæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡ MoE æ¨¡å‹ï¼ˆå¦‚ >10Bï¼‰ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤§è§„æ¨¡çš„ MoE æ¨¡å‹ï¼ˆå¦‚ Qwen-Max æˆ– Mixtral çº§åˆ«ï¼‰
- æ¢ç´¢ä¸å…¶ä»–å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ã€knowledge distillationï¼‰ç»“åˆ
- è¿›ä¸€æ­¥ä¼˜åŒ–é‡è®¡ç®—è·¯å¾„ï¼Œæå‡è®­ç»ƒåå
- æ¨å¹¿è‡³ Vision-Language Models æˆ–å…¶ä»–æ¶æ„ï¼ˆå¦‚ Diffusion Modelsï¼‰

---

## âœ… æ€»ä½“è¯„ä»·
**RevFFN æ˜¯ä¸€é¡¹å…¼å…·å®ç”¨æ€§ä¸ç†è®ºæ·±åº¦çš„å·¥ä½œ**ï¼Œå®ƒé€šè¿‡å¼•å…¥ **reversible networks** çš„æ€æƒ³ï¼Œå·§å¦™è§£å†³äº† MoE ç±»å¤§æ¨¡å‹åœ¨å•å¡ç¯å¢ƒä¸‹æ— æ³•è¿›è¡Œå…¨å‚æ•°å¾®è°ƒçš„éš¾é¢˜ã€‚å…¶åœ¨æ€§èƒ½ä¸å†…å­˜ä¹‹é—´å–å¾—äº†ä¼˜å¼‚å¹³è¡¡ï¼Œä¸ºæœªæ¥â€œå¹³æ°‘åŒ–â€å¤§æ¨¡å‹å¾®è°ƒæä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 9. [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)

**Authors**: Chaithra, Kamesh Kadimisetty, Biju R Mohan  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.20082v2  

#### Abstract
Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we pro...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAdaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs, RAG and Reinforcement Learning Approaches

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿé‡‘èæƒ…æ„Ÿåˆ†ææ¨¡å‹å­˜åœ¨ä»¥ä¸‹å±€é™ï¼š
- **ç¼ºä¹å¸‚åœºåé¦ˆæœºåˆ¶**ï¼šå¤§å¤šæ•°æ¨¡å‹ä»…åŸºäºæ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œæœªè€ƒè™‘å®é™…è‚¡ç¥¨ä»·æ ¼å˜åŠ¨å¯¹é¢„æµ‹ç»“æœçš„å½±å“ã€‚
- **ä¸Šä¸‹æ–‡ä¸è¶³**ï¼šæ–°é—»æ ‡é¢˜é€šå¸¸ç®€çŸ­ä¸”ç¼ºä¹èƒŒæ™¯ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å‡†ç¡®åˆ¤æ–­æƒ…æ„Ÿææ€§ã€‚
- **é™æ€ä¿¡æ¯æºæƒé‡**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤šæºä¿¡æ¯èåˆæ—¶é‡‡ç”¨å›ºå®šæƒé‡ï¼Œæ— æ³•åŠ¨æ€é€‚åº”ä¸åŒæ¥æºçš„å¯é æ€§å˜åŒ–ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**è‡ªé€‚åº”é‡‘èæƒ…æ„Ÿåˆ†ææ¡†æ¶**ï¼Œç»“åˆ **Instruction-Tuned LLMsã€Retrieval-Augmented Generation (RAG)** å’Œ **Reinforcement Learning (PPO)**ï¼Œå®ç°å¸‚åœºæ„ŸçŸ¥çš„æƒ…æ„Ÿå»ºæ¨¡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **Instruction-Tuned LLaMA 3.2 3B æ¨¡å‹å¾®è°ƒ**
   - åœ¨ SentiFin æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œæå‡æ¨¡å‹å¯¹å°åº¦é‡‘èå¸‚åœºè¯­å¢ƒçš„ç†è§£èƒ½åŠ›ã€‚

2. **åŠ¨æ€ RAG ä¸Šä¸‹æ–‡å¢å¼ºæœºåˆ¶**
   - æ„å»ºå¤šæºé‡‘èæ–°é—»æ•°æ®åº“ï¼Œé€šè¿‡ sentence embedding å’Œ cosine similarity åŠ¨æ€æ£€ç´¢ç›¸å…³èƒŒæ™¯ä¿¡æ¯ã€‚
   - å¼•å…¥**å¸¦æƒé‡çš„ç›¸ä¼¼åº¦è¯„åˆ†**ï¼Œä¾æ®æ–°é—»æ¥æºçš„å¯ä¿¡åº¦è°ƒæ•´æ£€ç´¢ä¼˜å…ˆçº§ã€‚

3. **å¸‚åœºåé¦ˆé©±åŠ¨çš„æºæƒé‡æ›´æ–°æœºåˆ¶**
   - è®¾è®¡ä¸€ä¸ªåé¦ˆæ¨¡å—ï¼Œå°†æ¨¡å‹é¢„æµ‹æƒ…æ„Ÿä¸æ¬¡æ—¥è‚¡ä»·æ¶¨è·Œå¯¹æ¯”ï¼Œè‹¥ä¸€è‡´åˆ™å¢åŠ è¯¥æ¶ˆæ¯æ¥æºçš„æƒé‡ï¼Œåä¹‹é™ä½ã€‚
   - ä½¿ç”¨è½»é‡çº§æ¢¯åº¦è§„åˆ™ï¼ˆclamped æ›´æ–°ï¼‰å®ç°åœ¨çº¿å­¦ä¹ ã€‚

4. **åŸºäº PPO çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥**
   - å°†æºæƒé‡åˆ†é…å»ºæ¨¡ä¸º RL ä»»åŠ¡ï¼Œä½¿ç”¨ Proximal Policy Optimization (PPO) å­¦ä¹ é•¿æœŸæœ€ä¼˜çš„åŠ æƒç­–ç•¥ã€‚
   - æé«˜ç³»ç»Ÿåœ¨æœªçŸ¥æ—¶é—´çª—å£å’Œè‚¡ç¥¨ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **å‡†ç¡®æ€§** | æ˜¾è‘—ä¼˜äºä¼ ç»Ÿ LLM å’Œé™æ€ RAG æ–¹æ³• |
| **å¸‚åœºå¯¹é½æ€§** | é¢„æµ‹ç»“æœæ›´è´´è¿‘çœŸå®è‚¡ä»·èµ°åŠ¿ï¼ˆmarket alignmentï¼‰ |
| **åŠ¨æ€é€‚åº”æ€§** | èƒ½éšæ—¶é—´è‡ªåŠ¨è°ƒæ•´ä¿¡æ¯æºä¿¡ä»»åº¦ï¼Œå…·å¤‡æŒç»­è¿›åŒ–èƒ½åŠ› |
| **å¯æ‰©å±•æ€§** | å¯é›†æˆæ›´å¼º LLM æˆ–å¼•å…¥åŸºæœ¬é¢å› å­è¿›ä¸€æ­¥ä¼˜åŒ– |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†

| æ•°æ®é›† | æè¿° |
|--------|------|
| **SentiFin Dataset** | ç”¨äº instruction tuning çš„è®­ç»ƒé›†ï¼ŒåŒ…å« 10,572 æ¡å°åº¦è‚¡å¸‚æ–°é—»æ ‡é¢˜ï¼Œä¸‰ç±»æ ‡ç­¾ï¼ˆpositive / neutral / negativeï¼‰ï¼ŒæŒ‰ 80:20 åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ã€‚ |
| **NIFTY 50 æ–°é—»æ•°æ®é›†** | è‡ªå»ºæµ‹è¯•ç”¨æ•°æ®é›†ï¼Œæ¶µç›– 2024â€“2025 å¹´é—´ 8,000 æ¡æ¥è‡ª Yahoo Financeã€MoneyControl ç­‰ä¸»æµè´¢ç»åª’ä½“çš„æ–°é—»ï¼Œç”¨äº RAG æ£€ç´¢ä¸è¯„ä¼°ã€‚ |
| **Ground Truth æ ‡ç­¾ç”Ÿæˆæ–¹å¼** | åŸºäºæ¬¡æ—¥è‚¡ç¥¨å›æŠ¥ç‡ç›¸å¯¹äºè¿‡å»30å¤©æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®å®šä¹‰ï¼š<br>â€¢ æ­£å‘ï¼šreturn > mean + std<br>â€¢ è´Ÿå‘ï¼šreturn < mean â€“ std<br>â€¢ ä¸­æ€§ï¼šå…¶ä½™æƒ…å†µ |

### âš™ï¸ å®éªŒè®¾ç½®

- **æ¨¡å‹æ¶æ„**ï¼š
  - ä¸»å¹²æ¨¡å‹ï¼š`unsloth/Llama-3.2-3B-Instruct`
  - å¾®è°ƒæŠ€æœ¯ï¼šQLoRAï¼ˆ4-bit é‡åŒ– + LoRA adapterï¼‰
  - åµŒå…¥æ¨¡å‹ï¼š`all-MiniLM-L6-v2` ç”¨äºå¥å­ç¼–ç 

- **RAG æµç¨‹**ï¼š
  1. ç»™å®šè¾“å…¥æ ‡é¢˜ â†’ æ£€ç´¢å‰å3å¤©å†…çš„å€™é€‰æ–‡ç« 
  2. è¿‡æ»¤å«æƒ…æ„Ÿå…³é”®è¯ï¼ˆå¦‚â€œä¸Šæ¶¨â€ã€â€œä¸‹è·Œâ€ï¼‰çš„æ–‡ç« 
  3. è®¡ç®— cosine similarity Ã— source weight å¾—åŠ æƒå¾—åˆ†
  4. é€‰å– top-k æ–‡ç« æ‹¼æ¥æˆ prompt è¾“å…¥ LLM

- **å¼ºåŒ–å­¦ä¹ é…ç½®**ï¼š
  - Agentï¼šPPO ç®—æ³•
  - Stateï¼šå½“å‰ source weights + å†å²å‡†ç¡®ç‡ + å¸‚åœºæŒ‡æ ‡
  - Actionï¼šè¾“å‡ºæ–°çš„ normalized source weight å‘é‡
  - Rewardï¼šé¢„æµ‹æ­£ç¡® +1ï¼Œé”™è¯¯ -1ï¼›ä¸­æ€§åŒºé—´ï¼ˆÂ±0.5%ï¼‰ä¸å¥–åŠ±

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å…¬å¼è¯´æ˜ |
|------|---------|
| **Accuracy** | æ­£ç¡®é¢„æµ‹æ ·æœ¬å æ¯” |
| **Weighted F1-Score** | æŒ‰ç±»åˆ«æ”¯æŒåº¦åŠ æƒçš„ F1 å¹³å‡å€¼ï¼Œç¼“è§£ç±»åˆ«ä¸å¹³è¡¡å½±å“ |
| **Market Alignment** | é¢„æµ‹æƒ…æ„Ÿä¸å®é™…è‚¡ä»·å˜åŠ¨æ–¹å‘çš„ä¸€è‡´æ€§ç¨‹åº¦ï¼ˆéšå¼è¯„ä»·ï¼‰ |

### ğŸ”€ å¯¹æ¯”çš„åŸºçº¿æ–¹æ³•

| æ–¹æ³• | æè¿° |
|------|------|
| **Fine-Tuned LLaMA 3.2** | ä»…å¾®è°ƒæ— ä¸Šä¸‹æ–‡ |
| **RAG + Static Weights** | å›ºå®šæºæƒé‡çš„ RAG |
| **RAG + Market Feedback (Cosine/WOC)** | åŸºäºåé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡ |
| **FinBERT** | é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹ |
| **RoBERTa** | é€šç”¨ Transformer æ¨¡å‹ |
| **Price Context Variants** | åŠ å…¥å‰3æ—¥ä»·æ ¼æè¿°ä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š æ€§èƒ½æ±‡æ€»è¡¨ï¼ˆæ¥è‡ª Table 4ï¼‰

| æ¨¡å‹å˜ä½“ | Accuracy | Weighted F1 |
|----------|----------|-------------|
| Fine-Tuned LLaMA 3.2 | 0.5520 | 0.5375 |
| RAG + Without Market Feedback | 0.6094 | 0.5722 |
| RAG + With Market Feedback (WOC) | **0.6153** | **0.5746** |
| RAG + PPO Optimized Weights | 0.6109 | 0.5733 |
| RAG + Price Context (Best) | **0.6650** | 0.5674 |
| FinBERT | 0.4852 | 0.5027 |
| RoBERTa | 0.5800 | 0.5551 |

> æ³¨ï¼šåŠ å…¥ price context å accuracy æ˜¾è‘—ä¸Šå‡ï¼Œä½† F1 ä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹åå‘é¢„æµ‹å¤šæ•°ç±»ï¼ˆneutralï¼‰ï¼Œå­˜åœ¨åå·®ã€‚

### ğŸ” å…³é”®å‘ç°

- **RAG æ˜¾è‘—æå‡æ€§èƒ½**ï¼šç›¸æ¯”çº¯å¾®è°ƒæ¨¡å‹ï¼ŒRAG æå‡ accuracy è¾¾ **+5.74%**ï¼ŒéªŒè¯äº†ä¸Šä¸‹æ–‡è¡¥å……çš„é‡è¦æ€§ã€‚
- **å¸‚åœºåé¦ˆæœºåˆ¶æœ‰æ•ˆ**ï¼šå¼•å…¥åŠ¨æ€æƒé‡åï¼Œaccuracy å’Œ F1 å‡æœ‰æå‡ï¼Œå°¤å…¶ WOC æ–¹æ³•è¡¨ç°æœ€ä½³ã€‚
- **PPO æä¾›æ›´ç¨³å®šç­–ç•¥**ï¼šè™½ç„¶ç»å¯¹æŒ‡æ ‡ç•¥ä½äºç›´æ¥åé¦ˆæ³•ï¼Œä½† PPO å­¦åˆ°çš„æƒé‡æ›´å…·æ³›åŒ–æ€§å’Œç¨³å®šæ€§ã€‚
- **Price Context æå‡ accuracy ä½†æŸå®³ F1**ï¼šå›  neutral ç±»å æ¯”è¾ƒé«˜ï¼ˆ~70%ï¼‰ï¼ŒåŠ å…¥ä»·æ ¼ä¿¡æ¯åæ¨¡å‹å€¾å‘é¢„æµ‹ä¸­æ€§ï¼Œé€ æˆç±»åˆ«å¤±è¡¡ã€‚

### ğŸ“‰ æ¶ˆèå®éªŒç»“æœï¼ˆéšå«åˆ†æï¼‰

- **ç§»é™¤ RAG** â†’ æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼ˆâ†“ ~6% accuracyï¼‰ï¼Œè¯æ˜ä¸Šä¸‹æ–‡æ£€ç´¢å¿…è¦ã€‚
- **ä½¿ç”¨é™æ€ source weights** vs **åŠ¨æ€ feedback** â†’ åè€…æ›´ä¼˜ï¼Œè¯´æ˜åŠ¨æ€è°ƒæ•´å¯ä¿¡æºé‡è¦ã€‚
- **PPO vs æ‰‹å·¥åé¦ˆæ›´æ–°** â†’ PPO æ›´é€‚åˆé•¿æœŸéƒ¨ç½²ï¼Œé¿å…çŸ­æœŸæ³¢åŠ¨å¹²æ‰°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦ç»“è®º

1. **Instruction-tuned LLM ç»“åˆ RAG å¯æ˜¾è‘—æå‡é‡‘èæƒ…æ„Ÿåˆ†ææ•ˆæœ**ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç¼ºå¤±çš„æ–°é—»æ ‡é¢˜æ—¶ã€‚
2. **å¸‚åœºåé¦ˆæœºåˆ¶ä½¿æ¨¡å‹å…·å¤‡â€œå¸‚åœºæ„è¯†â€**ï¼Œèƒ½å¤Ÿæ ¹æ®å®é™…è‚¡ä»·ååº”ä¸æ–­æ ¡å‡†ä¿¡æ¯æºå¯ä¿¡åº¦ã€‚
3. **PPO å¼ºåŒ–å­¦ä¹ å¯ç”¨äºå­¦ä¹ é²æ£’çš„ source weighting ç­–ç•¥**ï¼Œè™½æ€§èƒ½å¢ç›Šæœ‰é™ï¼Œä½†åœ¨è·¨æ—¶é—´å’Œè·¨è‚¡ç¥¨åœºæ™¯ä¸‹æ›´å…·æ³›åŒ–æ½œåŠ›ã€‚
4. **çœŸå®å¸‚åœºè¡Œä¸ºå¯ä½œä¸ºç›‘ç£ä¿¡å·æ›¿ä»£äººå·¥æ ‡æ³¨**ï¼Œå®ç°ä½æˆæœ¬ã€é«˜ä¸€è‡´æ€§ ground truth æ„å»ºã€‚

### âš ï¸ å±€é™æ€§

- **ä¾èµ–å¤šæºæ–°é—»æ•°æ®**ï¼šè‹¥æŸäº›å…¬å¸æ–°é—»ç¨€ç–ï¼Œåˆ™ RAG æ•ˆæœå—é™ã€‚
- **Price Context æ”¹å–„ accuracy ä½†æ¶åŒ– F1**ï¼šæç¤ºéœ€è®¾è®¡æ›´ç²¾ç»†çš„ä¸Šä¸‹æ–‡èåˆæœºåˆ¶ã€‚
- **æœªè€ƒè™‘åŸºæœ¬é¢å› ç´ **ï¼šå¦‚è´¢æŠ¥æ•°æ®ã€è¡Œä¸šè¶‹åŠ¿ç­‰éæƒ…ç»ªé©±åŠ¨çš„ä»·æ ¼å˜åŠ¨æœªè¢«å»ºæ¨¡ã€‚
- **Neutral ç±»åˆ«ä¸»å¯¼å½±å“è¯„ä¼°**ï¼šæ•°æ®åˆ†å¸ƒä¸å‡è¡¡å¯èƒ½å¯¼è‡´æ¨¡å‹åè§ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

- å¼•å…¥ **fundamental indicators**ï¼ˆå¦‚ P/E ratio, EPSï¼‰ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œæå‡å¯¹éæƒ…ç»ªé©±åŠ¨è¡Œæƒ…çš„è¯†åˆ«ã€‚
- æ¢ç´¢ **cross-stock æˆ– sector-level context propagation**ï¼Œåˆ©ç”¨åŒè¡Œä¸šè‚¡ç¥¨é—´çš„è”åŠ¨æ•ˆåº”ã€‚
- æ‰©å±•è‡³ **multi-modal è¾“å…¥**ï¼ˆå¦‚å›¾è¡¨ã€å…¬å‘ŠPDFï¼‰ï¼Œå¢å¼ºä¿¡æ¯è¦†ç›–å¹¿åº¦ã€‚
- å°†æ¡†æ¶åº”ç”¨äº **algorithmic trading ç­–ç•¥ç”Ÿæˆ**ï¼Œå®ç°ä»æƒ…æ„Ÿç†è§£åˆ°å†³ç­–æ‰§è¡Œçš„é—­ç¯ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªèåˆ **Instruction-Tuned LLMã€RAG ä¸ PPO å¼ºåŒ–å­¦ä¹ ** çš„è‡ªé€‚åº”é‡‘èæƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œåœ¨ NIFTY 50 æ•°æ®ä¸Šå®ç°äº†æ›´é«˜ç²¾åº¦ä¸å¸‚åœºå¯¹é½æ€§ï¼Œæ¨åŠ¨äº† LLM å‘â€œå¸‚åœºæ„ŸçŸ¥å‹â€æ™ºèƒ½ä½“çš„å‘å±•ã€‚

</details>

---

### 10. [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)

**Authors**: NVIDIA,  :, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frank Sun, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt, Gargi Prasad, George Armstrong, Gerald Shen, Gorkem Batmaz, Grigor Nalbandyan, Haifeng Qian, Harsh Sharma, Hayley Ross, Helen Ngo, Herbert Hum, Herman Sahota, Hexin Wang, Himanshu Soni, Hiren Upadhyay, Huizi Mao, Huy C Nguyen, Huy Q Nguyen, Iain Cunningham, Ido Galil, Ido Shahaf, Igor Gitman, Ilya Loshchilov, Itamar Schen, Itay Levy, Ivan Moshkov, Izik Golan, Izzy Putterman, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jatin Mitra, Jeffrey Glick, Jenny Chen, Jesse Oliver, Jian Zhang, Jiaqi Zeng, Jie Lou, Jimmy Zhang, Jinhang Choi, Jining Huang, Joey Conway, Joey Guman, John Kamalu, Johnny Greco, Jonathan Cohen, Joseph Jennings, Joyjit Daw, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kai Xu, Kan Zhu, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kevin Shih, Kezhi Kong, Khushi Bhardwaj, Kirthi Shankar, Krishna C. Puvvada, Krzysztof Pawelec, Kumar Anik, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Li Ding, Lizzie Wei, Lucas Liebenwein, Luis Vega, Maanu Grover, Maarten Van Segbroeck, Maer Rodrigues de Melo, Mahdi Nazemi, Makesh Narsimhan Sreedhar, Manoj Kilaru, Maor Ashkenazi, Marc Romeijn, Marcin Chochowski, Mark Cai, Markus Kliegl, Maryam Moosaei, Matt Kulka, Matvei Novikov, Mehrzad Samadi, Melissa Corpuz, Mengru Wang, Meredith Price, Michael Andersch, Michael Boone, Michael Evans, Miguel Martinez, Mikail Khona, Mike Chrzanowski, Minseok Lee, Mohammad Dabbah, Mohammad Shoeybi, Mostofa Patwary, Nabin Mulepati, Najeeb Nabwani, Natalie Hereth, Nave Assaf, Negar Habibi, Neta Zmora, Netanel Haber, Nicola Sessions, Nidhi Bhatia, Nikhil Jukar, Nikki Pope, Nikolai Ludwig, Nima Tajbakhsh, Nir Ailon, Nirmal Juluru, Nishant Sharma, Oleksii Hrinchuk, Oleksii Kuchaiev, Olivier Delalleau, Oluwatobi Olabiyi, Omer Ullman Argov, Omri Puny, Oren Tropp, Ouye Xie, Parth Chadha, Pasha Shamis, Paul Gibbons, Pavlo Molchanov, Pawel Morkisz, Peter Dykas, Peter Jin, Pinky Xu, Piotr Januszewski, Pranav Prashant Thombre, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Qing Miao, Qiyu Wan, Rabeeh Karimi Mahabadi, Rachit Garg, Ran El-Yaniv, Ran Zilberstein, Rasoul Shafipour, Rich Harang, Rick Izzo, Rima Shahbazyan, Rishabh Garg, Ritika Borkar, Ritu Gala, Riyad Islam, Robert Hesse, Roger Waleffe, Rohit Watve, Roi Koren, Ruoxi Zhang, Russell Hewett, Russell J. Hewett, Ryan Prenger, Ryan Timbrook, Sadegh Mahdavi, Sahil Modi, Samuel Kriman, Sangkug Lim, Sanjay Kariyappa, Sanjeev Satheesh, Saori Kaji, Satish Pasumarthi, Saurav Muralidharan, Sean Narentharen, Sean Narenthiran, Seonmyeong Bak, Sergey Kashirsky, Seth Poulos, Shahar Mor, Shanmugam Ramasamy, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shiqing Fan, Shreya Gopal, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Simeng Sun, Smita Ithape, Somshubra Majumdar, Soumye Singhal, Stas Sergienko, Stefania Alborghetti, Stephen Ge, Sugam Dipak Devare, Sumeet Kumar Barua, Suseella Panguluri, Suyog Gupta, Sweta Priyadarshi, Syeda Nahida Akter, Tan Bui, Teodor-Dumitru Ene, Terry Kong, Thanh Do, Tijmen Blankevoort, Tim Moon, Tom Balough, Tomer Asida, Tomer Bar Natan, Tomer Ronen, Tugrul Konuk, Twinkle Vashishth, Udi Karpas, Ushnish De, Vahid Noorozi, Vahid Noroozi, Venkat Srinivasan, Venmugil Elango, Victor Cui, Vijay Korthikanti, Vinay Rao, Vitaly Kurin, Vitaly Lavrukhin, Vladimir Anisimov, Wanli Jiang, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenfei Zhou, Will Jennings, William Zhang, Wojciech Prazuch, Xiaowei Ren, Yashaswi Karnati, Yejin Choi, Yev Meyer, Yi-Fu Wu, Yian Zhang, Yigong Qin, Ying Lin, Yonatan Geifman, Yonggan Fu, Yoshi Subara, Yoshi Suhara, Yubo Gao, Zach Moshe, Zhen Dong, Zhongbo Zhu, Zihan Liu, Zijia Chen, Zijie Yan  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.20856v1  

#### Abstract
We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# NVIDIA Nemotron 3: Efficient and Open Intelligence è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
NVIDIA Nemotron 3 æ—¨åœ¨è§£å†³å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨**æ¨ç†æ•ˆç‡ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›ä»¥åŠéƒ¨ç½²æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜**ã€‚å…·ä½“åŒ…æ‹¬ï¼š
- ä¼ ç»Ÿ Transformer MoE æ¶æ„åœ¨é•¿åºåˆ—ç”Ÿæˆæ—¶å›  KV Cache è†¨èƒ€å¯¼è‡´æ¨ç†ååä½ï¼›
- é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ç™¾ä¸‡çº§ tokenï¼‰åœºæ™¯ä¸‹ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å¤–æ¨æ€§èƒ½ä¸‹é™ï¼›
- å¤šæ­¥æ¨ç†ä¸å·¥å…·è°ƒç”¨ç­‰ agentic ä»»åŠ¡éœ€è¦æ›´å¼ºçš„è§„åˆ’ä¸æ§åˆ¶èƒ½åŠ›ï¼›
- æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²å¯¹é«˜ç²¾åº¦æ•°å€¼æ ¼å¼ä¾èµ–é«˜ï¼Œé™åˆ¶ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
Nemotron 3 å®¶æ—æ¨¡å‹ï¼ˆNano, Super, Ultraï¼‰æå‡ºäº†ä¸€ç³»åˆ—æŠ€æœ¯åˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰**Hybrid Mamba-Transformer MoE æ¶æ„**
- ç»“åˆ **Mamba-2** å’Œ **MoE** å±‚ï¼Œå‡å°‘æ˜‚è´µçš„ self-attention å±‚æ•°é‡ï¼Œä»…ä¿ç•™å°‘é‡ç”¨äºå…³é”®æ³¨æ„åŠ›æ“ä½œã€‚
- åˆ©ç”¨ Mamba çš„å¸¸é‡çŠ¶æ€å­˜å‚¨ç‰¹æ€§ï¼Œæ˜¾è‘—é™ä½ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…å­˜å¼€é”€å’Œå»¶è¿Ÿã€‚
- æ”¯æŒé«˜è¾¾ **1M tokens** çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

#### ï¼ˆ2ï¼‰**LatentMoE æ¶æ„ï¼ˆSuper & Ultraï¼‰**
- åœ¨ MoE ä¸­å¼•å…¥â€œæ½œåœ¨ç©ºé—´â€è·¯ç”±æœºåˆ¶ï¼šå°†è¾“å…¥ä»éšè—ç»´åº¦ $d$ æŠ•å½±åˆ°æ›´å°çš„ latent ç»´åº¦ $l < d$ è¿›è¡Œä¸“å®¶è®¡ç®—ã€‚
- å‡å°‘ all-to-all é€šä¿¡é‡å’Œå‚æ•°åŠ è½½é‡ï¼ˆå‹ç¼©å› å­çº¦ $d/l \approx 4\times$ï¼‰ï¼ŒèŠ‚çœèµ„æºç”¨äºå¢åŠ ä¸“å®¶æ€»æ•°å’Œæ¿€æ´»ä¸“å®¶æ•°ï¼ˆ$K'$ï¼‰ã€‚
- å®ç°æ›´é«˜éçº¿æ€§å®¹é‡å’Œä¸“å®¶å¤šæ ·æ€§ï¼Œæå‡å‡†ç¡®ç‡è€Œä¸ç‰ºç‰²æ¨ç†æ•ˆç‡ã€‚

#### ï¼ˆ3ï¼‰**Multi-Token Prediction (MTP)**
- å¼•å…¥ MTP æ¨¡å—é¢„æµ‹å¤šä¸ªæœªæ¥ tokenï¼Œæä¾›æ›´å¯†é›†çš„ç›‘ç£ä¿¡å·ï¼Œå¢å¼ºå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚
- MTP è¾“å‡ºå¯ç›´æ¥ä½œä¸º speculative decoding çš„ draft tokensï¼Œå®ç°ç«¯åˆ°ç«¯åŠ é€Ÿã€‚
- å¯¹ Super å’Œ Ultra æ¨¡å‹é›†æˆï¼Œå¸¦æ¥æ¨ç†é€Ÿåº¦å’Œè´¨é‡åŒé‡æ”¶ç›Šã€‚

#### ï¼ˆ4ï¼‰**NVFP4 å…¨æµç¨‹è®­ç»ƒ**
- é¦–æ¬¡åœ¨æ··åˆæ¶æ„ä¸Šç¨³å®šä½¿ç”¨ **NVFP4** æ•°å€¼æ ¼å¼è¿›è¡Œé¢„è®­ç»ƒï¼ˆæœ€é«˜è¾¾ 25T tokensï¼‰ã€‚
- ä½¿ç”¨ cuBLAS åŸç”Ÿæ”¯æŒçš„ NVFP4 GEMMsï¼Œç›¸æ¯” FP8 æä¾› **3Ã— æ›´é«˜çš„å³°å€¼åå**ã€‚
- å…³é”®å±‚ï¼ˆå¦‚ QKVã€Mamba è¾“å‡ºã€MTPï¼‰ä¿æŒ BF16/MXFP8 ç²¾åº¦ä»¥ç»´æŒç¨³å®šæ€§ã€‚

#### ï¼ˆ5ï¼‰**å¤šç¯å¢ƒå¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼ˆMulti-environment RLï¼‰**
- è®¾è®¡æ¶µç›–æ•°å­¦ã€ç¼–ç¨‹ã€æŒ‡ä»¤éµå¾ªã€æœç´¢ã€èŠå¤©ã€é•¿æ–‡æœ¬ç†è§£ç­‰å¤šä¸ªé¢†åŸŸçš„ RL ç¯å¢ƒé›†åˆã€‚
- æ‰€æœ‰ä»»åŠ¡**å¹¶è¡Œè”åˆè®­ç»ƒ**ï¼Œé¿å…é˜¶æ®µæ€§è®­ç»ƒå¸¦æ¥çš„èƒ½åŠ›é€€åŒ–ã€‚
- ä½¿ç”¨ GRPO + masked importance sampling æå‡è®­ç»ƒç¨³å®šæ€§ã€‚

#### ï¼ˆ6ï¼‰**æ¨ç†æ—¶ç»†ç²’åº¦æ¨ç†é¢„ç®—æ§åˆ¶ï¼ˆGranular Reasoning Budget Controlï¼‰**
- ç”¨æˆ·å¯åœ¨æ¨ç†æ—¶æŒ‡å®šâ€œæ€è€ƒ tokenâ€çš„æœ€å¤§æ•°é‡ã€‚
- è¾¾åˆ°é¢„ç®—åè‡ªåŠ¨åˆ‡æ¢è‡³æœ€ç»ˆå›ç­”ç”Ÿæˆï¼Œå®ç°**ç²¾åº¦ä¸å»¶è¿Ÿçš„çµæ´»å¹³è¡¡**ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Nemotron 3 | ä¼ ç»Ÿ MoE / Dense LLM |
|------|------------|------------------------|
| æ¨ç†åå | æ˜¾è‘—æ›´é«˜ï¼ˆå°¤å…¶é•¿åºåˆ—ï¼‰ | å—é™äº KV Cache å¢é•¿ |
| ä¸Šä¸‹æ–‡æ‰©å±•èƒ½åŠ› | æ”¯æŒ up to 1M tokensï¼Œæ—  RoPE å¤–æ¨é—®é¢˜ | RoPE å¤–æ¨æ€§èƒ½éª¤é™ |
| MoE æ•ˆç‡ | LatentMoE æå‡ç²¾åº¦åŒæ—¶ä¸å¢å¼€é”€ | é€šä¿¡ç“¶é¢ˆä¸¥é‡ |
| è®­ç»ƒæ•ˆç‡ | NVFP4 åŠ é€Ÿè®­ç»ƒï¼Œé™ä½æ˜¾å­˜å ç”¨ | å¤šç”¨ BF16/FP8ï¼Œæ•ˆç‡è¾ƒä½ |
| æ¨ç†çµæ´»æ€§ | æ”¯æŒåŠ¨æ€æ¨ç†é¢„ç®—æ§åˆ¶ | å›ºå®šæ¨ç†è·¯å¾„ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **é¢„è®­ç»ƒæ•°æ®**ï¼šè¶…è¿‡ **10 trillion tokens** çš„å¤šæ ·åŒ–æ–‡æœ¬æ•°æ®ï¼ˆæœªå…¬å¼€ç»†èŠ‚ï¼Œä½†å£°æ˜åŒ…å«ä»£ç ã€ç§‘å­¦æ–‡çŒ®ã€ç½‘é¡µã€ä¹¦ç±ç­‰ï¼‰ã€‚
- **é•¿ä¸Šä¸‹æ–‡ä¸“é¡¹æ•°æ®**ï¼šåˆæˆæ•°æ®ç”¨äºæ”¯æŒï¼š
  - é•¿è·ç¦»æ£€ç´¢ï¼ˆlong-range retrievalï¼‰
  - å¤šè·³æ¨ç†ï¼ˆmulti-hop reasoningï¼‰
  - å¤šæ–‡æ¡£èšåˆï¼ˆmulti-document info aggregationï¼‰
- **RL åè®­ç»ƒç¯å¢ƒ**ï¼š
  - æ•°å­¦æ¨ç†ï¼ˆAIME25, Math 500ï¼‰
  - ç¼–ç¨‹ç«èµ›ï¼ˆLiveCodeBench, SciCodeï¼‰
  - å·¥å…·ä½¿ç”¨ï¼ˆIFBenchï¼‰
  - æŒ‡ä»¤è·Ÿéšï¼ˆT2-Benchï¼‰
  - é•¿ä¸Šä¸‹æ–‡é—®ç­”ï¼ˆRULER @ 1Mï¼‰
  - å¸¸è¯†æ¨ç†ï¼ˆARC-Challenge, HellaSwag ç­‰ï¼‰

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
| ç±»åˆ« | è®¾ç½® |
|------|------|
| æ¨¡å‹è§„æ¨¡ | Nano: 30B params (A3B); Super/Ultra: æ›´å¤§è§„æ¨¡ï¼ˆA8B ablation ä½¿ç”¨ 8B active paramsï¼‰ |
| åºåˆ—é•¿åº¦ | CPT: 512k; SFT: 256k; RL: up to 32k; æµ‹è¯•æ”¯æŒ 1M |
| æ¨ç†æ¨¡å¼ | æ”¯æŒ greedy decoding ä¸ speculative decodingï¼ˆåˆ©ç”¨ MTPï¼‰ |
| è¯„ä¼°æŒ‡æ ‡ | - å‡†ç¡®ç‡ï¼ˆaccï¼‰<br>- CoT EMï¼ˆChain-of-Thought Exact Matchï¼‰<br>- RULER scoreï¼ˆé•¿ä¸Šä¸‹æ–‡æ£€ç´¢èƒ½åŠ›ï¼‰<br>- Negative Log-Likelihood (NLL)<br>- ååé‡ï¼ˆtokens/secï¼‰<br>- æ¨ç†å»¶è¿Ÿ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Qwen3-30B-A3B-Thinking**
- **GPT-OSS-20B-A4B**
- **Standard MoE æ¶æ„ï¼ˆç›¸åŒå‚æ•°é‡ï¼‰**
- **BF16 è®­ç»ƒæ¨¡å‹ï¼ˆç”¨äº NVFP4 å¯¹æ¯”ï¼‰**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰æ•´ä½“æ€§èƒ½å¯¹æ¯”ï¼ˆå›¾2ï¼‰
| æ¨¡å‹ | Arena-Hard-v2 | AIME25 | IFBench | SWE-Bench | LCB v6 | RULER @ 1M |
|------|-------------|--------|---------|-----------|--------|------------|
| **Nemotron-3-Nano-30B-A3B** | **99.2** | **98.7** | **91.7** | **89.1** | **85.0** | **54.19** |
| Qwen3-30B-A3B-Thinking | 97.8 | 96.3 | 86.8 | 80.7 | 77.5 | 47.5 |
| GPT-OSS-20B-A4B | 91.7 | 85.0 | 68.2 | 66.0 | 61.0 | 38.8 |

> âœ… Nemotron Nano åœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†ä¸Šå‡å–å¾— SOTA è¡¨ç°ã€‚

#### ï¼ˆ2ï¼‰æ¨ç†ååä¼˜åŠ¿ï¼ˆå›¾2ï¼‰
- **Nemotron-3-Nano æ¯” Qwen3-30B-A3B ååé«˜å‡º 3.3Ã—**
- åœ¨æ›´é•¿åºåˆ—ä¸‹ä¼˜åŠ¿è¿›ä¸€æ­¥æ‰©å¤§ï¼ˆdue to Mamba-2 çš„å¸¸é‡çŠ¶æ€ï¼‰

#### ï¼ˆ3ï¼‰LatentMoE æ¶ˆèå®éªŒï¼ˆè¡¨1ï¼‰
| æ¨¡å‹ | MMLU-Pro | MMLU | Code | Math | Commonsense Understanding |
|------|----------|-------|------|------|----------------------------|
| Standard MoE (8.09B active) | 48.30 | 70.10 | 51.95 | 78.32 | 81.73 |
| **LatentMoE (8.02B active)** | **52.87** | **72.11** | **55.14** | **80.19** | **82.10** |

> âœ… LatentMoE åœ¨å‡ ä¹å…¨éƒ¨ä»»åŠ¡ä¸Šä¸€è‡´ä¼˜äºæ ‡å‡† MoEï¼Œä¸”æ´»è·ƒå‚æ•°æ›´å°‘ã€‚

#### ï¼ˆ4ï¼‰MTP æ¶ˆèå®éªŒï¼ˆè¡¨2ï¼‰
åœ¨ 8B active MoE åŸºç¡€æ¨¡å‹ä¸ŠåŠ å…¥ MTP çš„å¹³å‡æå‡çº¦ **+2.4%**ï¼š
| ä»»åŠ¡ | Baseline | +MTP | Î” |
|------|----------|-------|----|
| MMLU | 70.06 | 71.26 | +1.20 |
| MMLU-Pro (CoT) | 45.05 | 47.84 | +2.79 |
| GSM8K | 82.49 | 84.46 | +1.97 |
| RACE | 84.02 | 85.36 | +1.34 |

> âœ… MTP æ˜¾è‘—æå‡å¤šç±»ä»»åŠ¡è¡¨ç°ï¼Œå¹¶å¤©ç„¶æ”¯æŒ speculative decodingã€‚

#### ï¼ˆ5ï¼‰NVFP4 è®­ç»ƒæ•ˆæœï¼ˆå›¾4 & å›¾5ï¼‰
- **è®­ç»ƒæŸå¤±å·®è· <1%**ï¼ˆvs BF16ï¼‰ï¼ŒéªŒè¯æŸå¤±å·®è·è¿›ä¸€æ­¥ç¼©å°è‡³ **<0.6%**ï¼ˆåœ¨æ›´å¤§æ¨¡å‹ä¸Šï¼‰ã€‚
- ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è½¨è¿¹å‡ ä¹é‡åˆï¼ˆå›¾5ï¼‰ï¼Œè¡¨æ˜ NVFP4 ä¸æŸå®³æœ€ç»ˆæ€§èƒ½ã€‚
- åœ¨ GB300 ä¸Šï¼ŒNVFP4 GEMM ååä¸º FP8 çš„ **3Ã—**ã€‚

#### ï¼ˆ6ï¼‰é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼ˆè¡¨3 & å›¾6ï¼‰
| æ¨¡å‹ | 128k | 256k | 512k | **1M** |
|------|------|------|------|--------|
| Nemotron-Nano-12B-v2-Base | 85.13 | 79.85 | 75.12 | **23.43** |
| **Nemotron-3-Nano-30B-A3B-Base** | 74.48 | 71.67 | 66.02 | **54.19** |

> âœ… MoE Hybrid æ¶æ„åœ¨ 1M ä¸Šä¸‹æ–‡ä¸‹è¡¨ç°æ›´ç¨³å¥ï¼Œè¡°å‡å¹³ç¼“ï¼ˆgraceful degradationï¼‰ï¼Œè€Œ Dense Hybrid æ€¥å‰§ä¸‹é™ã€‚

- **å›¾6ï¼šä»£ç æ•°æ®ä¸Šçš„ç´¯ç§¯ NLL éš token ä½ç½®ä¸‹é™ï¼ˆRÂ²=0.883ï¼‰**  
  âœ è¡¨æ˜æ¨¡å‹èƒ½æœ‰æ•ˆåˆ©ç”¨é•¿è¾¾ 1M çš„ä¸Šä¸‹æ–‡è¿›è¡Œé¢„æµ‹ã€‚

#### ï¼ˆ7ï¼‰æ¨ç†é¢„ç®—æ§åˆ¶ï¼ˆå›¾8ï¼‰
- éšç€å…è®¸çš„ thinking token æ•°é‡å¢åŠ ï¼Œå‡†ç¡®ç‡ç¨³æ­¥ä¸Šå‡ã€‚
- ç”¨æˆ·å¯æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©ç²¾åº¦-å»¶è¿ŸæŠ˜è¡·ç‚¹ï¼ˆe.g., 2K â†’ 16K tokensï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Hybrid Mamba-Transformer MoE æ˜¯é«˜æ•ˆæ¨ç†çš„ç†æƒ³æ¶æ„**ï¼Œå°¤å…¶é€‚åˆé•¿åºåˆ—å’Œ agentic åœºæ™¯ã€‚
2. **LatentMoE æˆåŠŸæ‰“ç ´ MoE ä¸­é€šä¿¡å¼€é”€ä¸æ¨¡å‹è´¨é‡çš„çŸ›ç›¾**ï¼Œå®ç°â€œæ›´å¥½åˆä¸æ…¢â€ã€‚
3. **NVFP4 å¯ç¨³å®šåº”ç”¨äºå¤§è§„æ¨¡æ··åˆæ¶æ„è®­ç»ƒ**ï¼Œå¸¦æ¥æ˜¾è‘—ç¡¬ä»¶æ•ˆç‡æå‡ï¼Œä¸”ä¸å½±å“ä¸‹æ¸¸æ€§èƒ½ã€‚
4. **MTP ä¸ä»…æå‡è®­ç»ƒè´¨é‡ï¼Œè¿˜ä¸º speculative decoding æä¾›åŸç”Ÿæ”¯æŒ**ï¼Œæ˜¯æ¨ç†åŠ é€Ÿçš„å…³é”®ç»„ä»¶ã€‚
5. **å¤šç¯å¢ƒå¹¶è¡Œ RL è®­ç»ƒæ¯”é˜¶æ®µå¼è®­ç»ƒæ›´ç¨³å®šã€å…¨é¢**ï¼Œæœ‰åŠ©äºæ„å»ºé€šç”¨ agent åŸºåº§ã€‚
6. **æ¨ç†æ—¶é¢„ç®—æ§åˆ¶æä¾›äº†å‰æ‰€æœªæœ‰çš„çµæ´»æ€§**ï¼Œé€‚ç”¨äºä¸åŒ SLA è¦æ±‚çš„åº”ç”¨åœºæ™¯ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **LatentMoE çš„è®¾è®¡ä¾èµ– careful dimensioné€‰æ‹©ï¼ˆ$l$ï¼‰å’Œè·¯ç”±æœºåˆ¶ä¼˜åŒ–**ï¼Œå¯èƒ½å¢åŠ è°ƒå‚å¤æ‚åº¦ã€‚
- **Mamba å±‚å¯¹æŸäº›ç»“æ„åŒ–æ¨ç†ä»»åŠ¡çš„æ”¯æŒå°šå¾…æ·±å…¥éªŒè¯**ï¼ˆå¦‚ç¬¦å·é€»è¾‘ï¼‰ã€‚
- **NVFP4 éœ€è¦ç‰¹å®šç¡¬ä»¶æ”¯æŒï¼ˆå¦‚ GB300ï¼‰æ‰èƒ½å‘æŒ¥æœ€å¤§ä¼˜åŠ¿**ï¼Œé€šç”¨æ€§å—é™ã€‚
- å½“å‰ä»…å‘å¸ƒ Nano æ¨¡å‹ï¼ŒSuper å’Œ Ultra å°šæœªå¼€æºï¼Œå®é™…éƒ¨ç½²éªŒè¯ä»éœ€ç­‰å¾…ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€æ”¾ Super å’Œ Ultra æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç ä¸æ•°æ®ã€‚
- æ¢ç´¢æ›´å¤šç¨€ç–åŒ–ä¸å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚ Compression èŠ‚æåŠçš„å·¥ä½œï¼‰ã€‚
- æ‰©å±• RL ç¯å¢ƒè¦†ç›–æ›´å¤š real-world agent åœºæ™¯ï¼ˆå¦‚è‡ªåŠ¨åŒ–åŠå…¬ã€æ™ºèƒ½ä½“åä½œï¼‰ã€‚
- è¿›ä¸€æ­¥ä¼˜åŒ– MTP ä¸ speculative decoding çš„ååŒç­–ç•¥ã€‚
- æ¨åŠ¨ NVFP4 åœ¨å…¶ä»–æ¶æ„ï¼ˆå¦‚çº¯ Transformer æˆ– RWKVï¼‰ä¸­çš„åº”ç”¨ã€‚

---

> ğŸ”š **æ€»ç»“**ï¼šNVIDIA Nemotron 3 æ˜¯ä¸€ä¸ªé¢å‘ **é«˜æ•ˆã€å¼€æ”¾ã€å¼ºæ¨ç†èƒ½åŠ›çš„ä¸‹ä¸€ä»£å¼€æº LLM å®¶æ—**ã€‚å®ƒé€šè¿‡ **Hybrid æ¶æ„ + LatentMoE + MTP + NVFP4 + å¤šç¯å¢ƒ RL + æ¨ç†é¢„ç®—æ§åˆ¶** çš„ç»„åˆæ‹³ï¼Œåœ¨å‡†ç¡®æ€§ã€ååã€é•¿ä¸Šä¸‹æ–‡ã€çµæ´»æ€§ç­‰æ–¹é¢å®ç°äº†å…¨é¢çªç ´ï¼Œæ ‡å¿—ç€ agentic AI åŸºåº§æ¨¡å‹è¿›å…¥æ–°é˜¶æ®µã€‚

</details>

---

### 11. [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)

**Authors**: Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20061v1  

#### Abstract
Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential f...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šScaling Reinforcement Learning for Content Moderation with Large Language Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

æœ¬æ–‡èšç„¦äºå¤§è§„æ¨¡ **Content Moderationï¼ˆå†…å®¹å®¡æ ¸ï¼‰** ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨æ ‡ç­¾ç¨€ç–ã€æ”¿ç­–å®šä¹‰åŠ¨æ€æ¼”è¿›ã€ä¸”éœ€è¦å¤æ‚æ¨ç†çš„ç°å®åœºæ™¯ä¸­ï¼Œè®­ç»ƒå‡ºå…·å¤‡â€œä¸“å®¶çº§â€å‡†ç¡®ç‡çš„ **Large Language Modelsï¼ˆLLMsï¼‰**ã€‚

ä¼ ç»Ÿæ–¹æ³•å¦‚ **Supervised Fine-Tuningï¼ˆSFTï¼‰** åœ¨é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶è¡¨ç°å—é™ï¼Œè€Œç®€å•çš„ **Reinforcement Learningï¼ˆRLï¼‰** æ–¹æ³•å®¹æ˜“å‡ºç°å¥–åŠ±é»‘å®¢ï¼ˆreward hackingï¼‰ã€æ¨ç†åç¼©ï¼ˆreasoning collapseï¼‰å’Œç½®ä¿¡åº¦æåŒ–ç­‰é—®é¢˜ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†å¦‚ä½•è§„æ¨¡åŒ–åº”ç”¨ RL æ¥æå‡ LLM åœ¨çœŸå®å·¥ä¸šçº§å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

1. **æå‡ºäº†ä¸€å¥—å¯æ‰©å±•çš„ RL è®­ç»ƒæ¡†æ¶ç”¨äºå†…å®¹å®¡æ ¸**  
   é¦–æ¬¡åœ¨å·¥ä¸šçº§å†…å®¹å®¡æ ¸åœºæ™¯ä¸­ç³»ç»Ÿæ€§åœ°éªŒè¯äº† RL çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€æ•´å¥—é€‚ç”¨äºè¯¥é¢†åŸŸçš„ RL è®­ç»ƒé…æ–¹ï¼ˆtraining recipeï¼‰ï¼ŒåŒ…æ‹¬ï¼š
   - ä½¿ç”¨ **Group Relative Policy Optimizationï¼ˆGRPOï¼‰** æ›¿ä»£ PPOï¼Œæå‡è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ã€‚
   - è®¾è®¡å¤šç»´åº¦ **Reward Shaping** ç­–ç•¥ï¼Œç»“åˆå‡†ç¡®æ€§ã€æ ¼å¼ã€é•¿åº¦ä¸æ¨ç†è´¨é‡å¥–åŠ±ã€‚
   - å¼•å…¥ **Rubric-Based Reasoning Rewardï¼ˆRrubï¼‰**ï¼Œå¯¹æ¨¡å‹çš„å®Œæ•´æ¨ç†é“¾è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ï¼Œè¶…è¶Šä»…ä¾èµ–æœ€ç»ˆæ ‡ç­¾çš„ç›‘ç£æ–¹å¼ã€‚
   - æå‡º **Reflection-Aided Prompting** å’Œ **Monte-Carlo Score Aggregation** æ¥ç¼“è§£ç½®ä¿¡åº¦åŒå³°åˆ†å¸ƒé—®é¢˜ã€‚

2. **æå‡º Disagreement Filtering æ•°æ®ç­›é€‰ç­–ç•¥**  
   åˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆå¤šä¸ªå“åº”çš„ä¸€è‡´æ€§æ¥è¯†åˆ«é«˜ä»·å€¼è®­ç»ƒæ ·æœ¬ï¼ˆå³â€œæœ‰äº‰è®®â€çš„æ ·æœ¬ï¼‰ï¼Œæ˜¾è‘—æå‡ RL çš„æ•°æ®æ•ˆç‡ã€‚

3. **æ­ç¤º RL åœ¨å†…å®¹å®¡æ ¸ä¸­çš„å¯é¢„æµ‹ç¼©æ”¾è§„å¾‹**  
   å‘ç° RL è¡¨ç°å‡º **sigmoid-like scaling behavior**ï¼šæ€§èƒ½éšè®­ç»ƒæ•°æ®é‡ã€rollout æ•°é‡å’Œä¼˜åŒ–æ­¥æ•°å¹³æ»‘ä¸Šå‡åè¶‹äºé¥±å’Œï¼Œä¸ºèµ„æºåˆ†é…æä¾›å®ç”¨æŒ‡å¯¼ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•°æ®æ•ˆç‡** | RL è¾¾åˆ°ä¸ SFT ç›¸å½“ç”šè‡³æ›´ä¼˜æ€§èƒ½æ‰€éœ€çš„æ•°æ®é‡å°‘ **10â€“100 å€**ï¼Œç‰¹åˆ«é€‚åˆæ ‡æ³¨æˆæœ¬é«˜çš„é¢†åŸŸã€‚ |
| **æ¨ç†èƒ½åŠ›** | èƒ½æ›´å¥½å¤„ç†éœ€å¤æ‚ã€æ¡ä»¶æ€§æ¨ç†çš„ä»»åŠ¡ï¼Œé¿å… SFT å¯¹æµ…å±‚æ¨¡å¼çš„è¿‡æ‹Ÿåˆã€‚ |
| **çµæ´»æ€§** | ä¸ä¾èµ–é™æ€æ ‡æ³¨ï¼Œå¯é€šè¿‡ LLM-as-Judge æˆ– rubric åŠ¨æ€è°ƒæ•´åé¦ˆä¿¡å·ï¼Œé€‚åº”æ”¿ç­–å˜åŒ–ã€‚ |
| **ç¨³å®šæ€§å¢å¼º** | æ‰€ææŠ€æœ¯æœ‰æ•ˆç¼“è§£äº† RL å¸¸è§å¤±è´¥æ¨¡å¼ï¼ˆå¦‚ reward hackingã€length collapseï¼‰ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- ä½¿ç”¨æ¥è‡ª **Meta Platforms, Inc.** çš„ä¸‰ä¸ªçœŸå®ç”Ÿäº§ç¯å¢ƒä¸­çš„å†…å®¹å®¡æ ¸ä»»åŠ¡ï¼ˆTask1, Task2, Task3ï¼‰ï¼Œå‡ä¸ºåŸºäºæ–‡æœ¬çš„ **Policy-Violation Classification**ï¼ˆæ˜¯å¦è¿åæŸé¡¹ç¤¾åŒºå‡†åˆ™ï¼‰ã€‚
- æ•°æ®åŒ…å«ç”¨æˆ·èµ„æ–™ä¿¡æ¯ï¼ˆç”¨æˆ·åã€bioã€å›½å®¶ã€ç²‰ä¸æ•°ç­‰ï¼‰åŠå¤šæ¨¡æ€å†…å®¹ç‰‡æ®µã€‚
- æ¯ä¸ªä»»åŠ¡ä»…æœ‰æ•°ç™¾è‡³æ•°åƒæ¡ç”±ä¸“å®¶æ ‡æ³¨çš„é«˜è´¨é‡æ ·æœ¬ï¼Œä½“ç°å…¸å‹çš„ **label scarcity** åœºæ™¯ã€‚

---

### **å®éªŒè®¾ç½®**

- **æ¨¡å‹æ¶æ„**ï¼šä¸»è¦ä½¿ç”¨ **Qwen2.5-VL-7B** å’Œ **Qwen-3-8B** ç­‰å¼€æº LLMã€‚
- **è¾“å…¥å½¢å¼**ï¼šç»“æ„åŒ– promptï¼Œå¼•å¯¼æ¨¡å‹æŒ‰æ­¥éª¤æ€è€ƒå¹¶è¾“å‡º JSON æ ¼å¼åˆ¤æ–­ç»“æœã€‚
- **RL ç®—æ³•**ï¼šé‡‡ç”¨ **GRPOï¼ˆGroup Relative Policy Optimizationï¼‰**ï¼Œæ— éœ€æ˜¾å¼å€¼å‡½æ•°ï¼Œè®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ã€‚
- **è®­ç»ƒæµç¨‹**ï¼š
  - **Baseline**: SFTï¼ˆSupervised Fine-Tuningï¼‰
  - **Two-Stage**: SFT â†’ RL
  - **RL-Only**: ç›´æ¥åœ¨ base model ä¸Šè¿›è¡Œ RL
- **Rollout è®¾ç½®**ï¼šæ¯è½®ç”Ÿæˆå¤šä¸ªå“åº”ï¼ˆN=8~128ï¼‰ï¼Œç”± reward model æ‰“åˆ†åç”¨äºç­–ç•¥æ›´æ–°ã€‚

---

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **R@P90** | Recall at Precision â‰¥ 90%ï¼Œè¡¡é‡é«˜ç²¾åº¦ä¸‹çš„å¬å›èƒ½åŠ›ï¼Œæ˜¯å·¥ä¸šéƒ¨ç½²çš„å…³é”®æŒ‡æ ‡ã€‚ |
| **PRAUC** | Precision-Recall Area Under Curveï¼Œç»¼åˆè¯„ä¼°åˆ†ç±»æ€§èƒ½ã€‚ |
| **F1 Score** | å¹³è¡¡ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„å¸¸ç”¨æŒ‡æ ‡ã€‚ |
| **Faithfulness / Factuality** | ä½¿ç”¨ LLM-as-Judgeï¼ˆGemini-2.5-Proï¼‰å’Œ HHEM æ¨¡å‹è¯„ä¼°æ¨ç†å¿ å®æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚ |
| **Throughput (tokens/s/GPU)** | è¯„ä¼°è®­ç»ƒæ¡†æ¶æ•ˆç‡ã€‚ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | æè¿° |
|------|------|
| **SFT-COT** | å¸¦æ€ç»´é“¾çš„ç›‘ç£å¾®è°ƒï¼Œä¸»æµ baselineã€‚ |
| **Zero-shot / Few-shot** | æ— è®­ç»ƒæˆ–æå°‘é‡ç¤ºä¾‹æç¤ºã€‚ |
| **RL-Only** | ä»…ç”¨ RL å¾®è°ƒ base modelã€‚ |
| **SFT â†’ RL** | å…ˆ SFT å† RL å¾®è°ƒã€‚ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **RL æ˜¾è‘—ä¼˜äº SFTï¼Œå°¤å…¶åœ¨å°æ•°æ®åœºæ™¯ä¸‹**

- åœ¨ Task1ã€Task2ã€Task3 ä¸Šï¼Œ**RL-Only æ¨¡å‹ä»…ç”¨ ~200â€“800 æ ·æœ¬å³å¯åŒ¹æ•Œç”šè‡³è¶…è¿‡ SFT æ¨¡å‹åœ¨æ•°ä¸‡æ ·æœ¬ä¸Šçš„è¡¨ç°**ã€‚
- å›¾ 6 æ˜¾ç¤ºï¼Œåœ¨ Task2 ä¸Šï¼ŒRL+SFT ä½¿ç”¨ 836 ä¸ªæ ·æœ¬å³è¾¾åˆ° SFT ä½¿ç”¨ 60K æ ·æœ¬çš„æ€§èƒ½æ°´å¹³ â†’ **çº¦ 100Ã— æ•°æ®æ•ˆç‡æå‡**ã€‚

#### âœ… **RL å±•ç°å‡º sigmoid-like ç¼©æ”¾è¡Œä¸º**

- å›¾ 7 æ˜¾ç¤ºï¼Œéšç€è®­ç»ƒ token æ•°å¢åŠ ï¼Œæ€§èƒ½å…ˆå¿«é€Ÿä¸Šå‡ï¼Œåœ¨ ~2.4B tokens åè¶‹äºé¥±å’Œã€‚
- å›¾ 8 æ˜¾ç¤ºï¼Œéšç€ rollout æ•°é‡å¢åŠ ï¼Œ`maj@N`ï¼ˆå¤šæ•°æ­£ç¡®ç‡ï¼‰æŒç»­æå‡ï¼Œä½†å¢ç›Šé€’å‡ã€‚

#### âœ… **Disagreement Filtering æå¤§æå‡æ•°æ®æ•ˆç‡**

| æ•°æ®å­é›† | å¤§å° | F1 | PRAUC |
|--------|-----|----|-------|
| All | 677 | 0.87 | 0.85 |
| Easy | 566 | 0.79 | 0.80 |
| Disagreement | 61 | **0.88** | **0.90** |

> ä»…ç”¨ **61 ä¸ªâ€œæœ‰äº‰è®®â€æ ·æœ¬è®­ç»ƒ RL** å³å¯è¾¾åˆ°æ¥è¿‘å…¨é‡æ•°æ®çš„æ•ˆæœï¼Œè¡¨æ˜è¯¥ç­–ç•¥èƒ½ç²¾å‡†ç­›é€‰é«˜ä¿¡æ¯é‡æ ·æœ¬ã€‚

#### âœ… **Reward Shaping æ˜¾è‘—æå‡æ€§èƒ½ä¸é²æ£’æ€§**

| Reward Setup | F1 (Qwen-38B, Task1) |
|-------------|------------------------|
| Racc + Rfmt (baseline) | 0.49 |
| + Rlen | 0.49 |
| + Rrub (**å®Œæ•´å¥–åŠ±**) | **0.61** (+12%) |

> åŠ å…¥ **Rubric-Based Reasoning Reward** å F1 æå‡ 12%ï¼Œè¯´æ˜å¯¹æ¨ç†è¿‡ç¨‹çš„è´¨é‡ç›‘ç£è‡³å…³é‡è¦ã€‚

#### âœ… **Monte-Carlo Sampling æ”¹å–„ç½®ä¿¡åº¦æ ¡å‡†**

- è¡¨ 2 æ˜¾ç¤ºï¼Œåœ¨ Task2 ä¸Šä½¿ç”¨ N=4 rollouts è¿›è¡Œ MC é‡‡æ ·ï¼ŒR@P90 æå‡ 0.05ã€‚
- å›¾ 4 æ˜¾ç¤ºï¼ŒMC æ–¹æ³•ä½¿åŸæœ¬åŒå³°çš„ç½®ä¿¡åº¦åˆ†å¸ƒæ›´åŠ é›†ä¸­ï¼Œå‡å°‘æç«¯é¢„æµ‹ã€‚

#### âœ… **è®­ç»ƒæ¡†æ¶æ•ˆç‡å¯¹æ¯”**

| æ¡†æ¶ | Throughput (tokens/s/GPU) | ç›¸å¯¹æå‡ |
|------|----------------------------|----------|
| HuggingFace TRL | 1854 | 1.0Ã— |
| **VeRL** | **4600** | **2.5Ã—** |

> VeRL å› å…¶ HybridFlow æ¶æ„å®ç°æ›´é«˜ååï¼Œæ›´é€‚åˆå¤§è§„æ¨¡ RL è®­ç»ƒã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **RL åœ¨å†…å®¹å®¡æ ¸ä¸­å…·æœ‰æé«˜çš„æ•°æ®æ•ˆç‡**  
   - åœ¨ä¸“å®¶æ ‡æ³¨ç¨€ç¼ºçš„åœºæ™¯ä¸‹ï¼ŒRL å¯å®ç° **10â€“100Ã— æ›´é«˜çš„æ•°æ®æ•ˆç‡**ï¼Œè¿œè¶… SFTã€‚

2. **RL éµå¾ªå¯é¢„æµ‹çš„ sigmoid-like ç¼©æ”¾è§„å¾‹**  
   - æ€§èƒ½éšæ•°æ®ã€rollout æ•°ã€è®­ç»ƒæ­¥æ•°å¢é•¿è€Œå¹³ç¨³ä¸Šå‡åé¥±å’Œï¼Œä¸ºå·¥ä¸šéƒ¨ç½²æä¾›èµ„æºè§„åˆ’ä¾æ®ã€‚

3. **çº¯ RL è®­ç»ƒæ˜“å¯¼è‡´é€€åŒ–è¡Œä¸ºï¼Œå¿…é¡»é€šè¿‡å¥–åŠ±å¡‘å½¢æ§åˆ¶**  
   - è§‚å¯Ÿåˆ°å…¸å‹é—®é¢˜ï¼š**reward hacking**ï¼ˆè·³è¿‡æ¨ç†ç›´æ¥è¾“å‡ºæ ‡ç­¾ï¼‰ã€**length collapse**ï¼ˆè§£é‡Šè¶Šæ¥è¶ŠçŸ­ï¼‰ã€**bimodal probability distribution**ï¼ˆç½®ä¿¡åº¦è¿‡é«˜æˆ–è¿‡ä½ï¼‰ã€‚

4. **Rubric-Based Reward æ˜¯å…³é”®åˆ›æ–°**  
   - é€šè¿‡å¯¹æ¨ç†é“¾çš„ç»“æ„åŒ–è¯„åˆ†ï¼ˆè€Œéä»…çœ‹æœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„ **faithfulness** å’Œ **factuality**ã€‚

5. **Disagreement Filtering æ˜¯é«˜æ•ˆæ•°æ®é€‰æ‹©ç­–ç•¥**  
   - è‡ªåŠ¨ç­›é€‰â€œæ¨¡å‹ä¸ç¡®å®šâ€çš„æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ï¼Œå¯åœ¨æå°æ•°æ®é‡ä¸‹è·å¾—é«˜æ€§èƒ½ã€‚

6. **Two-Stageï¼ˆSFT â†’ RLï¼‰é€šå¸¸ä¼˜äº RL-Only**  
   - SFT æä¾›ç¨³å®šæ¨ç†æ¨¡æ¿ï¼Œé˜²æ­¢ RL-Only å‡ºç°æŒ‡ä»¤å¹»è§‰ï¼›ä½†åœ¨å¤§æ•°æ® SFT ä¸‹ RL å¢ç›Šæœ‰é™ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **ä¾èµ–é«˜è´¨é‡ reward model æˆ– LLM-as-Judge**  
   - è‹¥ judge æ¨¡å‹ä¸å¯é ï¼Œä¼šå¼•å…¥å™ªå£°åé¦ˆï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§ã€‚

2. **è®¡ç®—å¼€é”€è¾ƒå¤§**  
   - å°½ç®¡ GRPO æ›´é«˜æ•ˆï¼Œä½†ä»éœ€å¤§é‡ rollout å’Œæ¨ç†ï¼Œå¯¹ç®—åŠ›è¦æ±‚é«˜ã€‚

3. **éš¾ä»¥å®Œå…¨æ¶ˆé™¤ post-hoc rationalization**  
   - æ¨¡å‹ä»å¯èƒ½ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸æˆç«‹çš„æ¨ç†æ¥è¿åˆå¥–åŠ±ã€‚

4. **æ³›åŒ–åˆ°æ–°æ”¿ç­–ä»éœ€é‡æ–°è®­ç»ƒ**  
   - å½“æ”¿ç­–å˜æ›´æ—¶ï¼Œéœ€é‡æ–°æ„å»º reward rubric å¹¶æ”¶é›†æ–°æ•°æ®ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **è‡ªåŠ¨åŒ– rubric æ„å»ºä¸ reward modeling**  
   - æ¢ç´¢ä»è‡ªç„¶è¯­è¨€æ”¿ç­–è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œè¯„åˆ†è§„åˆ™ã€‚

2. **åœ¨çº¿å­¦ä¹ ä¸æŒç»­é€‚åº”æœºåˆ¶**  
   - ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰æˆ–è‡ªåŠ¨æ¢æµ‹ç³»ç»Ÿï¼Œå®ç°æ”¿ç­–å˜åŒ–ä¸‹çš„æŒç»­å¯¹é½ã€‚

3. **è·¨ä»»åŠ¡è¿ç§» RL ç­–ç•¥**  
   - å¼€å‘é€šç”¨ RL æ¡†æ¶ï¼Œå‡å°‘æ¯ä¸ªæ–°ä»»åŠ¡æ‰€éœ€çš„å®šåˆ¶åŒ–å·¥ç¨‹ã€‚

4. **é™ä½æ¨ç†æ—¶æˆæœ¬**  
   - ä¼˜åŒ– MC sampling å’Œ reflection prompting çš„å»¶è¿Ÿï¼Œä½¿å…¶æ›´é€‚åˆå®æ—¶å®¡æ ¸åœºæ™¯ã€‚

5. **å¤šæ¨¡æ€å†…å®¹å®¡æ ¸æ‰©å±•**  
   - å°†å½“å‰æ–‡æœ¬ä¸»å¯¼çš„æ–¹æ³•æ‹“å±•è‡³å›¾åƒã€è§†é¢‘ç­‰å¤šæ¨¡æ€å†…å®¹ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡è¯æ˜äº† **Reinforcement Learning** æ˜¯è§£å†³å·¥ä¸šçº§å†…å®¹å®¡æ ¸ä¸­æ ‡æ³¨ç¨€ç¼ºä¸å¤æ‚æ¨ç†éš¾é¢˜çš„æœ‰æ•ˆè·¯å¾„ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ **reward shapingã€reflection promptingã€disagreement filtering** ç­‰æŠ€æœ¯ï¼Œå®ç°äº†æ¯” SFT é«˜è¾¾ **100Ã— çš„æ•°æ®æ•ˆç‡æå‡**ï¼Œå¹¶æ­ç¤ºäº† RL åœ¨çœŸå®åœºæ™¯ä¸­çš„å¯é¢„æµ‹ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿæä¾›äº†é‡è¦å®è·µæŒ‡å—ã€‚

</details>

---

### 12. [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)

**Authors**: Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.21204v1  

#### Abstract
Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰çš„è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ï¼ˆå¦‚ HuBERTã€WavLMï¼‰åœ¨å­¦ä¹ è¯­è¨€è¡¨ç¤ºæ—¶éœ€è¦æ•°åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ï¼Œä¸”å¯¹å£°å­¦å’Œä¸Šä¸‹æ–‡å˜åŒ–æ•æ„Ÿï¼Œå¯¼è‡´å…¶åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„é€‚åº”èƒ½åŠ›å·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å©´å„¿ä»…éœ€æ•°ç™¾å°æ—¶çš„è¯­éŸ³æš´éœ²å³å¯æŒæ¡æ–°è¯­è¨€çš„åŸºæœ¬å•ä½ï¼Œæ˜¾ç¤ºå‡ºæé«˜çš„å­¦ä¹ æ•ˆç‡ã€‚æœ¬æ–‡æ—¨åœ¨ç¼©å°è¿™ä¸€â€œæ•ˆç‡é¸¿æ²Ÿâ€ï¼Œå®ç°**åŸºäºæå°‘æœªæ ‡æ³¨è¯­éŸ³æ•°æ®çš„å¿«é€Ÿè¯­è¨€é€‚åº”**ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº† **SpidR-Adapt**ï¼Œä¸€ä¸ªé€šç”¨çš„è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œä¸“ä¸ºå°‘æ ·æœ¬ï¼ˆfew-shotï¼‰è¯­è¨€é€‚åº”è®¾è®¡ï¼ŒåŒ…å«ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼š

- **Multi-task Adaptive Pre-training (MAdaPT)**  
  å°†ä½èµ„æºè¯­éŸ³è¡¨ç¤ºå­¦ä¹ å»ºæ¨¡ä¸ºå…ƒå­¦ä¹ ï¼ˆmeta-learningï¼‰é—®é¢˜ï¼Œé‡‡ç”¨åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼ˆbi-level optimizationï¼‰ã€‚æ¯ä¸ªâ€œepisodeâ€æ¨¡æ‹Ÿä¸€æ¬¡ä½èµ„æºè¯­è¨€çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡è·¨ä»»åŠ¡å­¦ä¹ æ„å»ºå¯è¿ç§»çš„å½’çº³åç½®ï¼ˆinductive biasesï¼‰ï¼Œæå‡å¯¹æ–°è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚

- **First-Order Bi-Level Optimization (FOBLO)**  
  é’ˆå¯¹ MAdaPT ä¸­è®¡ç®—æ˜‚è´µçš„äºŒé˜¶ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºä¸€ç§é«˜æ•ˆçš„å¯å‘å¼ä¸€é˜¶è¿‘ä¼¼ç®—æ³• FOBLOã€‚è¯¥æ–¹æ³•é¿å…äº†å†…å¾ªç¯æ¢¯åº¦çš„é«˜é˜¶å¯¼æ•°è®¡ç®—ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼Œä½¿å¤§è§„æ¨¡å…ƒè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚

- **Interleaved Supervision**  
  åœ¨é¢„è®­ç»ƒé˜¶æ®µäº¤æ›¿ä½¿ç”¨è‡ªç›‘ç£ï¼ˆSSLï¼‰å’Œå°‘é‡éŸ³ç´ çº§ç›‘ç£ä¿¡å·è¿›è¡Œè®­ç»ƒï¼Œå½¢æˆæ›´é²æ£’çš„åˆå§‹åŒ–ã€‚è¿™ç§æ··åˆç­–ç•¥æ¨¡ä»¿äº†äººç±»å¬è§‰ç³»ç»Ÿå¯¹è¯­éŸ³å˜å¼‚çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒæ ‡ç­¾é«˜æ•ˆæ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ•°æ®æ•ˆç‡æé«˜**ï¼šä»…ç”¨ **<1å°æ—¶ç›®æ ‡è¯­è¨€éŸ³é¢‘** å³å¯è¾¾åˆ°ç”šè‡³è¶…è¶Šä¼ ç»Ÿæ–¹æ³•åœ¨ 6,000 å°æ—¶æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ï¼Œ**æ•°æ®æ•ˆç‡æå‡è¶…è¿‡ 100Ã—**ã€‚
- **æ¶æ„æ— å…³æ€§**ï¼šè™½ç„¶ä»¥ SpidR ä¸ºéª¨å¹²ï¼Œä½† MAdaPT-FOBLO æ¡†æ¶å¯æ¨å¹¿è‡³å…¶ä»– SSL æ¨¡å‹ã€‚
- **æ›´å¼ºçš„ OoD æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨æœªè§è¿‡çš„è¯­è¨€ä¸Šè¡¨ç°å‡ºå“è¶Šçš„é€‚åº”é€Ÿåº¦å’Œæ€§èƒ½ï¼Œä¼˜äºæ ‡å‡†å¤šä»»åŠ¡è®­ç»ƒå’Œ Reptile ç­‰å…ƒå­¦ä¹ åŸºçº¿ã€‚
- **ç¨³å®šè®­ç»ƒæœºåˆ¶**ï¼šå¼•å…¥ active forgettingï¼ˆä¸»åŠ¨é—å¿˜ï¼‰é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆæºè¯­è¨€ç‰¹å®šçŸ¥è¯†ï¼Œå¹¶ç»“åˆ interleaved supervision æå‡å…ƒè®­ç»ƒç¨³å®šæ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **æºè¯­è¨€ï¼ˆSource Languagesï¼‰**ï¼š19 ç§è¯­è¨€ï¼Œæ¥è‡ª **VoxPopuli** å’Œ **VoxCommunis Corpus**ï¼Œæ¯ç§æä¾›çº¦ 300 å°æ—¶æœªæ ‡æ³¨æ•°æ® + æœ€å¤š 50 å°æ—¶éŸ³ç´ å¯¹é½æ ‡æ³¨æ•°æ®ã€‚
- **ç›®æ ‡è¯­è¨€ï¼ˆTarget Languagesï¼‰**ï¼šå…± 8 ç§ï¼ˆ5 å¼€å‘ + 3 æµ‹è¯•ï¼‰ï¼Œä¸æºè¯­è¨€æ— é‡å ï¼Œç”¨äºè¯„ä¼° out-of-domain (OoD) é€‚åº”èƒ½åŠ›ã€‚
- **å°è§„æ¨¡é€‚é…å­é›†**ï¼šæ¯ä¸ªç›®æ ‡è¯­è¨€æ„é€ å››ä¸ªå­é›†ï¼š**10åˆ†é’Ÿã€1å°æ—¶ã€10å°æ—¶ã€100å°æ—¶**æœªæ ‡æ³¨éŸ³é¢‘ã€‚
- **é¢†åŸŸå†…å¯¹ç…§ç»„ï¼ˆIn-Domain PTï¼‰**ï¼šä¸ºå…¬å¹³æ¯”è¾ƒï¼Œä¸ºç›®æ ‡æµ‹è¯•è¯­è¨€é¢å¤–æ”¶é›† **6,000 å°æ—¶**æ ‡æ³¨æ•°æ®ä½œä¸ºâ€œoracleâ€åŸºå‡†ã€‚

### å®éªŒè®¾ç½®
- **éª¨å¹²æ¨¡å‹**ï¼šåŸºäº **SpidR**ï¼ˆstate-of-the-art è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ï¼‰ï¼Œå…·å¤‡å­¦ç”Ÿ-æ•™å¸ˆç»“æ„ä¸åœ¨çº¿èšç±»æœºåˆ¶ã€‚
- **å…ƒè®­ç»ƒæµç¨‹**ï¼š
  - å…ƒåˆå§‹åŒ–ï¼ˆMeta-Initializationï¼‰ï¼šå…ˆè¿›è¡Œ multi-task pre-trainingï¼Œåˆ†ä¸ºä¸¤ç§ç­–ç•¥ï¼š
    - `Multi-Task-PT[SSL]`ï¼šçº¯è‡ªç›‘ç£
    - `Multi-Task-PT[SSL/SL]`ï¼šinterleaved supervisionï¼ˆæ¯10æ­¥æ’å…¥ä¸€æ¬¡ç›‘ç£è®­ç»ƒï¼‰
  - å…ƒè®­ç»ƒï¼ˆMeta-Trainingï¼‰ï¼šè¿è¡Œ 800 ä¸ª episodesï¼Œæ¯ä¸ª episode åŒ…å«ï¼š
    - å†…å¾ªç¯ï¼ˆInner-loopï¼‰ï¼š1,800 æ­¥è‡ªç›‘ç£é€‚åº”ï¼ˆä½¿ç”¨ 10 å°æ—¶éšæœºæºè¯­è¨€ç‰‡æ®µï¼‰
    - å¤–å¾ªç¯ï¼ˆOuter-loopï¼‰ï¼š200 æ­¥ç›‘ç£æ›´æ–°ï¼ˆä½¿ç”¨æ ‡æ³¨æ•°æ®åé¦ˆï¼‰
  - ä¸»åŠ¨é—å¿˜ï¼ˆActive Forgettingï¼‰ï¼šæ¯æ¬¡å†…å¾ªç¯å¼€å§‹å‰é‡ç½®é¢„æµ‹å¤´ï¼ˆheadsï¼‰å’Œç æœ¬ï¼ˆcodebooksï¼‰ï¼Œä¿ƒè¿›è·¨è¯­è¨€æŠ½è±¡ã€‚
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä½¿ç”¨ 16 GPUs å¹¶è¡Œè®­ç»ƒã€‚

### è¯„ä¼°æŒ‡æ ‡
- **ABX discriminability**ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼šè¡¡é‡æ¨¡å‹åŒºåˆ†éŸ³ç´ çš„èƒ½åŠ›ï¼Œåˆ† within-speaker å’Œ across-speaker æ¡ä»¶ã€‚
- **Spoken Language Modeling (SLM)** æ€§èƒ½ï¼š
  - **sWUGGY**ï¼šè¯é¡¹è¯†åˆ«ï¼ˆlexicalï¼‰
  - **sBLIMP**ï¼šè¯­æ³•åˆ¤æ–­ï¼ˆsyntaxï¼‰
  - **tSC**ï¼ˆtopic Story Clozeï¼‰ï¼šè¯è¯­è¿è´¯æ€§ï¼ˆdiscourseï¼‰
- **Phoneme Discovery Benchmark**ï¼š
  - **PNMI**ï¼ˆPhone-normalized Mutual Informationï¼‰ï¼šè¶Šé«˜è¶Šå¥½
  - **PER**ï¼ˆPhoneme Error Rateï¼‰ï¼šè¶Šä½è¶Šå¥½

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **In-Domain Mono-Task-PT** | ä½¿ç”¨å®Œæ•´ 6k å°æ—¶ç›®æ ‡è¯­è¨€æ•°æ®è®­ç»ƒçš„æ ‡å‡†æ¨¡å‹ï¼ˆoracle ä¸Šé™ï¼‰ |
| **Multi-Task-PT[SSL]** | æ ‡å‡†å¤šä»»åŠ¡é¢„è®­ç»ƒï¼ˆçº¯è‡ªç›‘ç£ï¼‰ |
| **Multi-Task-PT[SSL/SL]** | å¤šä»»åŠ¡é¢„è®­ç»ƒ + äº¤é”™ç›‘ç£ |
| **MAdaPT-Reptile** | ä½¿ç”¨ Reptile å¯å‘å¼çš„å…ƒå­¦ä¹ å˜ä½“ï¼ˆçº¯è‡ªç›‘ç£å¤–å¾ªç¯ï¼‰ |
| **MAdaPT-FOBLO** | æœ¬æ–‡æå‡ºçš„å®Œæ•´æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… æ•°æ®æ•ˆç‡ï¼ˆABX ä¸‹é™é€Ÿåº¦ï¼‰
- å¦‚å›¾ 2 æ‰€ç¤ºï¼Œ**MAdaPT-FOBLO** åœ¨ä»…ä½¿ç”¨ **1å°æ—¶ç›®æ ‡è¯­è¨€æ•°æ®**åï¼ŒABX è¡¨ç°å³è¿½å¹³ In-Domain PT æ¨¡å‹ã€‚
- ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMulti-Task-PT éœ€è¦ >100 å°æ—¶æ‰èƒ½æ¥è¿‘ç›¸åŒæ°´å¹³ï¼Œè¡¨æ˜å…¶ç¼ºä¹æœ‰æ•ˆé€‚åº”æœºåˆ¶ã€‚
- æ•°æ®æ•ˆç‡æå‡è¾¾ **100å€ä»¥ä¸Š**ã€‚

#### âœ… ä¸‹æ¸¸ SLM æ€§èƒ½ï¼ˆTable 2ï¼‰
| æ–¹æ³• | sWUGGY/sBLIMP/tSC å¹³å‡å¾—åˆ† (%) |
|------|-------------------------------|
| In-Domain Mono-Task-PT | 61.85 |
| Multi-Task-PT[SSL] | 61.07 |
| **MAdaPT-FOBLO** | **62.89** âœ… |
| MAdaPT-Reptile | 62.60 |

ğŸ‘‰ ç»“æœæ˜¾ç¤ºï¼š**å³ä½¿åªç”¨äº†æå°‘é‡æ•°æ®ï¼ŒSpidR-Adapt çš„ä¸‹æ¸¸è¯­è¨€å»ºæ¨¡æ€§èƒ½å·²å…¨é¢è¶…è¶Šä½¿ç”¨ 6,000 å°æ—¶æ•°æ®è®­ç»ƒçš„ in-domain æ¨¡å‹**ã€‚

#### âœ… éŸ³ç´ å‘ç°åŸºå‡†ï¼ˆTable 3ï¼‰
| æ–¹æ³• | PNMI â†‘ | PER â†“ | ABX (within) â†“ |
|------|--------|-------|----------------|
| Multi-Task-PT[SSL] + HuBERT | 0.58 | 76.01% | 6.62 |
| Multi-Task-PT[SSL] + SpidR | 0.66 | 60.17% | 4.83 |
| MAdaPT-Reptile | 0.69 | 38.27% | 4.12 |
| **MAdaPT-FOBLO** | **0.71** | **37.70%** | **4.09** |

ğŸ‘‰ æ˜¾è‘—ä¼˜äº HuBERT å’Œ Reptile å˜ä½“ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œéª¨å¹²é€‰æ‹©çš„é‡è¦æ€§ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ğŸ”¹ Active Forgettingï¼ˆè¡¨ 10â€“11ï¼‰
- ç§»é™¤ active forgetting å¯¼è‡´ ABX åˆ†æ•°æ˜æ˜¾ä¸Šå‡ï¼ˆæ€§èƒ½ä¸‹é™ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬ï¼ˆ0hï¼‰æ¡ä»¶ä¸‹ã€‚
- è¡¨æ˜è¯¥æœºåˆ¶æœ‰æ•ˆé˜²æ­¢æ¨¡å‹å›ºåŒ–äºå†å²ä»»åŠ¡ï¼Œå¢å¼ºå¯å¡‘æ€§ã€‚

#### ğŸ”¹ Meta-Initializationï¼ˆè¡¨ 12â€“13ï¼‰
- **éšæœºåˆå§‹åŒ–**å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼ŒABX åˆ†æ•°æé«˜ï¼ˆ>30%ï¼‰ï¼Œæ— æ³•æ”¶æ•›ã€‚
- `Multi-Task-PT[SSL/SL]` åˆå§‹åŒ–ä¼˜äºçº¯ SSL åˆå§‹åŒ–ï¼Œè¯´æ˜ **interleaved supervision æ„å»ºäº†æ›´ä¼˜çš„èµ·ç‚¹**ã€‚

#### ğŸ”¹ Meta-Learning Rate Î²ï¼ˆè¡¨ 14ï¼‰
- Î² = 0.01 æ—¶æ€§èƒ½æœ€ä½³ï¼›è¿‡å¤§ï¼ˆå¦‚ Î²=1ï¼‰å¯¼è‡´éœ‡è¡ï¼Œè¿‡å°åˆ™æ”¶æ•›æ…¢ã€‚
- æœ€ç»ˆæ‰€æœ‰å®éªŒå‡é‡‡ç”¨ Î² = 0.01ã€‚

#### ğŸ”¹ å±‚çº§åˆ†æï¼ˆå›¾ 4ï¼‰
- æœ€ä½³åˆ¤åˆ«æ€§èƒ½å‡ºç°åœ¨ä¸­é—´å±‚ï¼ˆç¬¬6æˆ–ç¬¬8å±‚ï¼‰ï¼Œä¸ç›‘ç£å¤´æ‰€åœ¨ä½ç½®ä¸€è‡´ã€‚
- æ·±å±‚åè€Œæ€§èƒ½ä¸‹é™ï¼Œæç¤ºé«˜å±‚è¡¨ç¤ºå¯èƒ½è¿‡åº¦æŠ½è±¡æˆ–ä»»åŠ¡åç¦»ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å…ƒå­¦ä¹ æ¡†æ¶å¯æå¤§æå‡è¯­éŸ³æ¨¡å‹çš„æ•°æ®æ•ˆç‡**ï¼šé€šè¿‡ MAdaPT è®¾è®¡ï¼Œæ¨¡å‹èƒ½åœ¨ <1 å°æ—¶æ•°æ®ä¸‹å®Œæˆé«˜è´¨é‡è¯­è¨€é€‚åº”ã€‚
2. **FOBLO æ˜¯é«˜æ•ˆå¯è¡Œçš„åŒå±‚ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šç›¸æ¯”ä¼ ç»ŸäºŒé˜¶æ–¹æ³•ï¼ŒFOBLO åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—è´Ÿæ‹…ï¼Œé€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚
3. **Interleaved Supervision è‡³å…³é‡è¦**ï¼šçº¯è‡ªç›‘ç£åˆå§‹åŒ–éš¾ä»¥æ”¯æ’‘ç¨³å®šå…ƒè®­ç»ƒï¼Œè€Œå°‘é‡ç›‘ç£ä¿¡å·å¯æ˜¾è‘—æ”¹å–„åˆå§‹çŠ¶æ€ã€‚
4. **Active Forgetting å¢å¼ºæ³›åŒ–èƒ½åŠ›**ï¼šå®šæœŸé‡ç½®å…³é”®æ¨¡å—æœ‰åŠ©äºæ‰“ç ´è¯­è¨€ç‰¹å¼‚æ€§ä¾èµ–ï¼Œæå‡è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚
5. **SpidR-Adapt è¶…è¶Šé¢†åŸŸå†…è®­ç»ƒæ¨¡å‹**ï¼šåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šï¼Œå…¶ few-shot è¡¨ç°å·²ä¼˜äºä½¿ç”¨ 6,000 å°æ—¶æ•°æ®è®­ç»ƒçš„ä¼ ç»Ÿæ¨¡å‹ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æºè¯­è¨€æ ‡æ³¨æ•°æ®**ï¼šå¤–å¾ªç¯ä»éœ€ç›‘ç£ä¿¡å·ï¼Œé™åˆ¶äº†å®Œå…¨æ— æ ‡ç­¾åœºæ™¯çš„åº”ç”¨ã€‚
- **åˆå§‹åŒ–æ•æ„Ÿ**ï¼šæ€§èƒ½é«˜åº¦ä¾èµ– meta-initialization è´¨é‡ï¼Œå°šæ— æ³•ä»é›¶å¼€å§‹è®­ç»ƒã€‚
- **æœªæ•´åˆ SLM å…ƒè®­ç»ƒ**ï¼šå½“å‰å…ƒå­¦ä¹ ä»…ä½œç”¨äºè¯­éŸ³ç¼–ç å™¨ï¼Œæœªæ‰©å±•åˆ°æ•´ä¸ª spoken language model è®­ç»ƒæµç¨‹ã€‚
- **è®¡ç®—æˆæœ¬è¾ƒé«˜**ï¼šå°½ç®¡ FOBLO é™ä½äº†å¤æ‚åº¦ï¼Œä½†æ•´ä½“å…ƒè®­ç»ƒä»éœ€å¤§é‡ GPU æ—¶é—´ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ— éœ€ç›‘ç£ä¿¡å·çš„å…¨è‡ªç›‘ç£å…ƒå­¦ä¹ æ¡†æ¶ã€‚
- å°†å…ƒå­¦ä¹ ç›´æ¥åº”ç”¨äº SLM è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡ç«¯åˆ°ç«¯æ•ˆç‡ã€‚
- æ‰©å±•è‡³æ›´å¤šè¯­è¨€å®¶æ—ï¼Œç ”ç©¶è·¨è¯­ç³»è¿ç§»èƒ½åŠ›ã€‚
- å‡å°‘å¯¹é«˜è´¨é‡éŸ³ç´ å¯¹é½æ•°æ®çš„ä¾èµ–ï¼Œæ¢ç´¢å¼±ç›‘ç£æˆ–ä¼ªæ ‡ç­¾ç­–ç•¥ã€‚

---

> ğŸ“¦ **å¼€æºä¿¡æ¯**ï¼šä½œè€…å·²å°†ä»£ç å’Œæ¨¡å‹æƒé‡å…¬å¼€äº GitHubï¼š[https://github.com/facebookresearch/spidr-adapt](https://github.com/facebookresearch/spidr-adapt)

</details>

---

### 13. [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)

**Authors**: Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20968v1  

#### Abstract
Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-At...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
- **åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶çš„é€šä¿¡ç“¶é¢ˆ**ï¼šéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰ä¸æ–­æ‰©å±•ï¼ˆå¦‚ç™¾ä¸‡çº§ tokenï¼‰ï¼Œæ³¨æ„åŠ›è®¡ç®—çš„è®¡ç®—é‡å’Œå†…å­˜éœ€æ±‚å‘ˆå¹³æ–¹å¢é•¿ï¼Œå¯¼è‡´å•è®¾å¤‡æ— æ³•æ‰¿è½½ã€‚
- **Ring-Attention çš„å¯æ‰©å±•æ€§é™åˆ¶**ï¼šå½“å‰ä¸»æµçš„ Ring-Attention è™½èƒ½é€šè¿‡ç¯çŠ¶é€šä¿¡é‡å è®¡ç®—ä¸é€šä¿¡ï¼Œä½†å…¶é€šä¿¡é‡éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œåœ¨å¤§è§„æ¨¡ GPU é›†ç¾¤ä¸­æˆä¸ºä¸¥é‡ç“¶é¢ˆï¼ˆå®éªŒæ˜¾ç¤ºé€šä¿¡å å‰å‘æ—¶é—´é«˜è¾¾ 91.5%ï¼‰ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
- **æå‡º Mesh-Attention**ï¼šä¸€ç§å…¨æ–°çš„åˆ†å¸ƒå¼æ³¨æ„åŠ›ç®—æ³•ï¼Œé‡æ–°è®¾è®¡äº†åˆ†å¸ƒå¼æ³¨æ„åŠ›çš„è®¾è®¡ç©ºé—´ã€‚
- **åŸºäº Assignment Matrix (AM) çš„å»ºæ¨¡è§†è§’**ï¼š
  - å°†æ¯ä¸ª Q-KV å¯¹çš„è®¡ç®—ä»»åŠ¡åˆ†é…æŠ½è±¡ä¸ºä¸€ä¸ªçŸ©é˜µï¼ˆAMï¼‰ï¼Œå…¶ä¸­ `AM[i][j]` è¡¨ç¤ºè´Ÿè´£è®¡ç®—ç¬¬ i ä¸ª Q å’Œç¬¬ j ä¸ª KV å—çš„ GPU IDã€‚
  - è¿™ç§çŸ©é˜µè§†è§’æ­ç¤ºäº†é€šä¿¡-è®¡ç®—æ¯”ï¼ˆCommCom ratioï¼‰ä¸æ•°æ®å±€éƒ¨æ€§ä¹‹é—´çš„å…³ç³»ã€‚
- **äºŒç»´ Tile åˆ†é…ç­–ç•¥**ï¼š
  - ä¸åŒäº Ring-Attention çš„ä¸€ç»´è¡Œ/åˆ—åˆ’åˆ†ï¼ˆå³æ¯ä¸ª GPU å¤„ç†æ•´è¡Œæˆ–æ•´åˆ—ï¼‰ï¼ŒMesh-Attention å°† AM åˆ’åˆ†ä¸ºäºŒç»´ tileï¼ˆaÃ—bï¼‰ï¼Œæ¯ä¸ª GPU è´Ÿè´£ä¸€ä¸ª tile å†…çš„æ‰€æœ‰ Q-KV è®¡ç®—ã€‚
  - æ”¯æŒçµæ´»è°ƒæ•´ tile å½¢çŠ¶ä»¥ä¼˜åŒ– CommCom ratioã€‚
- **Local Q-KV Property**ï¼š
  - å¼•å…¥â€œæœ¬åœ° Q-KV å±æ€§â€ï¼Œç¡®ä¿æ¯ä¸ª GPU è‡³å°‘èƒ½å¤„ç†å…¶æœ¬åœ° Q å’Œ KV çš„äº¤äº’ï¼Œå‡å°‘å†—ä½™é€šä¿¡ã€‚
  - é€šè¿‡æ—‹è½¬ KV ç»´åº¦ç´¢å¼•å®ç°è¯¥å±æ€§ã€‚
- **é«˜æ•ˆçš„è°ƒåº¦æœç´¢ç®—æ³•**ï¼š
  - æå‡ºä¸€ç§å¸¦çº¦æŸçš„è´ªå¿ƒè°ƒåº¦ç®—æ³•ï¼Œåœ¨ tile å†…è‡ªåŠ¨ç”Ÿæˆæœ€ä¼˜çš„ computation å’Œ communication é‡å é¡ºåºã€‚
  - å…³é”®æ´å¯Ÿï¼šâ€œå»¶è¿Ÿæ‰§è¡Œå·²å°±ç»ªçš„è®¡ç®—â€å¯ä»¥æ¢å–æ›´å¥½çš„é€šä¿¡é‡å æœºä¼šï¼ˆincreasing â€œprofitâ€ï¼‰ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ä¼˜åŠ¿ |
|------|------|
| **vs Ring-Attention** | æ˜¾è‘—é™ä½é€šä¿¡é‡ï¼ˆæœ€å¤šå‡å°‘ 85.4%ï¼‰ï¼Œæå‡é€Ÿåº¦ï¼ˆæœ€é«˜è¾¾ 3.4Ã— åŠ é€Ÿï¼‰ï¼Œå…·å¤‡æ›´å¼ºçš„å¼º/å¼±å¯æ‰©å±•æ€§ |
| **vs Ulysses** | ä¸å— attention head æ•°é‡é™åˆ¶ï¼Œæ”¯æŒä»»æ„è§„æ¨¡å¹¶è¡Œ |
| **vs StarTrail** | æ¶ˆé™¤å†—ä½™é€šä¿¡ï¼Œé€šä¿¡å¤æ‚åº¦æ›´ä½ï¼Œæ›´æ˜“äºé‡å  |
| **é€šç”¨æ€§** | Ring-Attention æ˜¯å…¶ç‰¹æ®Šå½¢å¼ï¼ˆå½“ a=1 æˆ– b=1 æ—¶ï¼‰ï¼Œå…·æœ‰æ›´é«˜çµæ´»æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
- è®ºæ–‡æœªä½¿ç”¨ä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒ/æ¨ç†è¯„æµ‹ã€‚
- å®éªŒåŸºäº**åˆæˆæ•°æ®**ï¼ˆsynthetic dataï¼‰è¿›è¡Œæ³¨æ„åŠ›å±‚çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹è¯„ä¼°ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„è®¡ç®—æ•ˆç‡ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼š256 GPU é›†ç¾¤ï¼ˆé™¤éç‰¹åˆ«è¯´æ˜ï¼‰
- **æ¨¡å‹é…ç½®**ï¼š
  - Attention heads: 32
  - Head dimension: 128
  - Hidden size: 4096
- **åºåˆ—é•¿åº¦**ï¼šä» 256K åˆ° 4M tokensï¼ˆå«å› æœæ©ç ï¼‰
- **å¹¶è¡Œè§„æ¨¡**ï¼š32ã€64ã€128ã€256 GPUs

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Throughput** | Attention æ­£åå‘ä¼ æ’­ååé‡ï¼ˆiter/s Ã— 10â»Â²ï¼‰ |
| **MFU (Model FLOPs Utilization)** | å®é™…åˆ©ç”¨çš„ FLOPs å ç†è®ºå³°å€¼çš„æ¯”ä¾‹ï¼Œåæ˜ ç¡¬ä»¶åˆ©ç”¨ç‡ |
| **Execution Time** | æ€»è¿è¡Œæ—¶é—´ï¼ˆåŒºåˆ† FWD/BWDï¼‰ |
| **Communication Volume** | æ¯ GPU é€šä¿¡æ•°æ®é‡ï¼ˆå•ä½ï¼šelementsï¼‰ |
| **Memory Usage** | å³°å€¼æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰ |
| **Scalability** | å¼º/å¼±å¯æ‰©å±•æ€§åˆ†æ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Ring-Attention**ï¼šä¸»è¦å¯¹æ¯”åŸºçº¿
- **Ulysses (DS-Ulysses)**ï¼šç”¨äºç†è®ºé€šä¿¡å¤æ‚åº¦æ¯”è¾ƒ
- **StarTrail**ï¼šç”¨äºé€šä¿¡æ¨¡å¼ä¸å¤æ‚åº¦åˆ†æ
- **GQA å˜ä½“**ï¼šåœ¨ Grouped-Query Attention åœºæ™¯ä¸‹éªŒè¯é²æ£’æ€§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆ256 GPUs, 1M åºåˆ—é•¿åº¦ï¼‰
| æŒ‡æ ‡ | Ring-Attention | Mesh-Attention | æå‡å¹…åº¦ |
|------|----------------|----------------|----------|
| **Throughput (FWD+BWD)** | ~2.3Ã—10â»Â² iter/s | ~7.8Ã—10â»Â² iter/s | **3.4Ã— åŠ é€Ÿ** |
| **å¹³å‡åŠ é€Ÿæ¯”ï¼ˆæ‰€æœ‰é…ç½®ï¼‰** | â€” | â€” | **2.9Ã—** |
| **MFU (%)** | ~4.1% | ~12.1% | **çº¦ 3Ã— æå‡** |
| **é€šä¿¡ç­‰å¾…æ—¶é—´å‡å°‘** | â€” | â€” | **å¹³å‡ 74.0%ï¼Œæœ€é«˜ 74.9%** |
| **é€šä¿¡é‡å‡å°‘** | â€” | â€” | **å¹³å‡ 79.0%ï¼Œæœ€é«˜ 85.4%** |

### ğŸ” ä¸å…¶ä»–é…ç½®çš„ç»“æœå¯¹æ¯”
- åœ¨ **32â€“256 GPUs** æ‰€æœ‰è§„æ¨¡ä¸‹å‡æ˜¾è‘—ä¼˜äº Ring-Attentionã€‚
- éšç€åºåˆ—é•¿åº¦å¢åŠ ï¼ŒMFU æé«˜ï¼Œè¡¨æ˜è®¡ç®—å æ¯”ä¸Šå‡ï¼Œé€šä¿¡ä¼˜åŒ–æ•ˆæœæ›´æ˜æ˜¾ã€‚
- å› æœæ©ç åœºæ™¯ä¸‹æ€§èƒ½å·®è·æ›´å¤§ï¼ˆå› æœ‰æ•ˆè®¡ç®—å‡åŠï¼Œé€šä¿¡ç›¸å¯¹æ›´é‡ï¼‰ã€‚

### ğŸ“‰ å¼º/å¼±å¯æ‰©å±•æ€§
| ç±»å‹ | ç»“æœ |
|------|------|
| **Strong Scaling (å›ºå®š 1M åºåˆ—)** | Ring æœ€ä½³åœ¨ 64 GPUsï¼ˆ37.5sï¼‰ï¼ŒMesh åœ¨ 128 GPUs è¾¾åˆ° 11.9sï¼ˆå¿« 3.15Ã—ï¼‰ï¼›Ring åœ¨ >64 åæ€§èƒ½ä¸‹é™ï¼ŒMesh å¯æŒç»­æ‰©å±• |
| **Weak Scaling (âˆš2 å¢é•¿åºåˆ—)** | Ring 256 vs 32 GPUs æ…¢ 3.74Ã—ï¼ŒMesh ä»…æ…¢ 2.83Ã—ï¼Œä½“ç°æ›´å¥½æ‰©å±•æ€§ |

### ğŸ” æ¶ˆèå®éªŒä¸é¢å¤–éªŒè¯
- **é€šä¿¡é‡åˆ†æå›¾ï¼ˆFig 9bï¼‰**ï¼šMesh-Attention æ¯ GPU é€šä¿¡é‡éš GPU æ•°å¢åŠ è€Œä¸‹é™ï¼ˆç†è®ºé¢„æµ‹ï¼‰ï¼Œè€Œ Ring å‡ ä¹ä¸å˜ã€‚
- **GQA åœºæ™¯æµ‹è¯•ï¼ˆFig 10ï¼‰**ï¼š
  - å³ä½¿åœ¨ KV å…±äº«ï¼ˆg=4/8ï¼‰ç¼“è§£é€šä¿¡å‹åŠ›çš„æƒ…å†µä¸‹ï¼ŒMesh-Attention ä»ä¿æŒæ˜¾è‘—ä¼˜åŠ¿ã€‚
  - å½“ g è¾ƒå°æ—¶ï¼ˆé€šä¿¡å‹åŠ›å¤§ï¼‰ï¼Œæ€§èƒ½å¢ç›Šæœ€å¤§ã€‚
- **å³°å€¼å†…å­˜åˆ†æï¼ˆTable 5ï¼‰**ï¼š
  - Mesh-Attention å†…å­˜æ›´é«˜ï¼ˆéœ€ç¼“å­˜å¤šä¸ª Q/KV chunksï¼‰ï¼Œä½†å±äº**ç¬æ€å†…å­˜**ï¼Œä¸å½±å“æ¢¯åº¦å›ä¼ æ‰€éœ€çš„æ¿€æ´»å­˜å‚¨ã€‚
  - å†…å­˜å¼€é”€æ˜¯æš‚æ—¶çš„ trade-offã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Ring-Attention å­˜åœ¨æç«¯å±€éƒ¨æ€§å¤±è¡¡**ï¼šQ å®Œç¾å±€éƒ¨ï¼ŒKV å®Œå…¨éå±€éƒ¨ â†’ å¯¼è‡´é«˜é€šä¿¡å¼€é”€ã€‚
2. **äºŒç»´ Tile åˆ†é… + Local Q-KV Property** å¯æœ‰æ•ˆå¹³è¡¡ Q å’Œ KV çš„é€šä¿¡å¼€é”€ï¼Œæ˜¾è‘—é™ä½ CommCom ratioã€‚
3. **Mesh-Attention çš„é€šä¿¡å¤æ‚åº¦ä¸º O(âˆšn)**ï¼Œä¼˜äº Ring-Attention çš„ O(n)ï¼Œç†è®ºä¸Šæœ‰æ›´ä¼˜çš„æ‰©å±•æ€§ã€‚
4. **è´ªå¿ƒè°ƒåº¦ç®—æ³•èƒ½é«˜æ•ˆæ¢ç´¢è°ƒåº¦ç©ºé—´**ï¼Œå®ç°é«˜åº¦é‡å çš„ computation å’Œ communicationã€‚
5. å®éªŒéªŒè¯åœ¨ 256 GPU è§„æ¨¡ä¸‹ï¼Œ**å¹³å‡ 2.9Ã— é€Ÿåº¦æå‡ï¼Œé€šä¿¡é‡å‡å°‘è¿‘ 80%**ï¼Œä¸”å¯æ‰©å±•æ€§å¼ºã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **æ›´é«˜çš„å³°å€¼å†…å­˜æ¶ˆè€—**ï¼šç”±äºéœ€è¦ç¼“å­˜å¤šä¸ªè¿œç¨‹ Q/KV chunksï¼Œå¯¹æ˜¾å­˜å®¹é‡è¦æ±‚æ›´é«˜ã€‚
- **è°ƒåº¦ä¾èµ–é™æ€ profiling**ï¼š`c_Q`, `c_KV`, `c_O` å‚æ•°éœ€é¢„å…ˆæµ‹é‡ï¼Œå¯èƒ½å—ç¡¬ä»¶å¼‚æ„å½±å“ã€‚
- **å½“å‰å®ç°å‡è®¾åŒæ­¥æ‰§è¡Œ**ï¼šå¯¹ç½‘ç»œæŠ–åŠ¨æˆ– GPU å¼‚æ„ç¯å¢ƒçš„å®¹é”™èƒ½åŠ›æœªæ·±å…¥æ¢è®¨ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢**å†…å­˜é«˜æ•ˆçš„è°ƒåº¦ç­–ç•¥**ï¼šæ›´æ—©é‡Šæ”¾ä¸å†ä½¿ç”¨çš„ Q/KV chunksï¼Œé™ä½å³°å€¼å†…å­˜ã€‚
- ç ”ç©¶**åŠ¨æ€è‡ªé€‚åº”è°ƒåº¦æœºåˆ¶**ï¼šæ ¹æ®å®æ—¶é€šä¿¡å»¶è¿ŸåŠ¨æ€è°ƒæ•´è®¡ç®—ä¸é€šä¿¡é¡ºåºã€‚
- æ‰©å±•è‡³**å¼‚æ„é›†ç¾¤ä¸ä½å¸¦å®½ç¯å¢ƒ**ï¼šè¿›ä¸€æ­¥éªŒè¯åœ¨çœŸå®ç”Ÿäº§ç³»ç»Ÿçš„ç¨³å®šæ€§ä¸æ”¶ç›Šã€‚
- ç»“åˆå…¶ä»–å¹¶è¡ŒèŒƒå¼ï¼ˆå¦‚ Tensor Parallelismï¼‰æ„å»ºç»Ÿä¸€çš„ Long Context å¹¶è¡Œæ¡†æ¶ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> Mesh-Attention é€šè¿‡å¼•å…¥ **Assignment Matrix æŠ½è±¡æ¨¡å‹** å’Œ **äºŒç»´ Tile åˆ†é…ç­–ç•¥**ï¼Œä»æ ¹æœ¬ä¸Šä¼˜åŒ–äº†åˆ†å¸ƒå¼æ³¨æ„åŠ›çš„é€šä¿¡æ¨¡å¼ï¼Œåœ¨ä¿æŒè®¡ç®—å®Œæ•´æ€§çš„åŒæ—¶å¤§å¹…é™ä½é€šä¿¡å¼€é”€ï¼Œå®ç°äº†æ¯” Ring-Attention æ›´é«˜æ•ˆç‡å’Œæ›´å¼ºå¯æ‰©å±•æ€§çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†æ–¹æ¡ˆã€‚

</details>

---

### 14. [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)

**Authors**: S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.21009v1  

#### Abstract
Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-worl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
- **åŠ¨æ€è¶…å›¾åˆ†æç¼ºä¹é«˜æ•ˆæ•°æ®ç»“æ„**ï¼šä¼ ç»Ÿå›¾æ¨¡å‹ä»…èƒ½è¡¨ç¤ºæˆå¯¹äº¤äº’ï¼ˆdyadic interactionsï¼‰ï¼Œè€Œç°å®ä¸–ç•Œä¸­çš„é«˜é˜¶äº¤äº’ï¼ˆå¦‚å¤šä½œè€…åˆä½œã€åŒ–å­¦ååº”ï¼‰éœ€ç”¨è¶…å›¾ï¼ˆhypergraphï¼‰å»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥å…·å¤§å¤šé’ˆå¯¹é™æ€è¶…å›¾æˆ–æ™®é€šå›¾ï¼Œç¼ºä¹æ”¯æŒå¤§è§„æ¨¡**åŠ¨æ€è¶…å›¾**ï¼ˆdynamic hypergraphsï¼‰çš„é«˜æ•ˆã€å¯æ‰©å±•çš„æ•°æ®ç»“æ„ã€‚
- **è¶…å›¾ä¸‰å…ƒç»„ï¼ˆtriadï¼‰è®¡æ•°æ•ˆç‡ä½ä¸‹**ï¼šè¶…å›¾ triad æ˜¯ä¸‰è§’å½¢åœ¨è¶…å›¾ä¸Šçš„æ¨å¹¿ï¼Œç”¨äºç¤¾åŒºæ£€æµ‹ã€æ¨èç³»ç»Ÿç­‰ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€æ›´æ–°æ—¶éœ€é‡æ–°è®¡ç®—æ•´ä¸ªå›¾ï¼Œè®¡ç®—å¼€é”€å·¨å¤§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
- **æå‡º ESCHER**ï¼šé¦–ä¸ªé¢å‘ GPU çš„ã€ä¸“ä¸ºå¤§è§„æ¨¡åŠ¨æ€è¶…å›¾è®¾è®¡çš„å¹¶è¡Œæ•°æ®ç»“æ„ï¼ˆGPU-centric parallel data structureï¼‰ï¼Œæ”¯æŒé«˜æ•ˆçš„è¶…è¾¹ï¼ˆhyperedgeï¼‰å’Œå…³è”é¡¶ç‚¹ï¼ˆincident vertexï¼‰çš„æ’å…¥ä¸åˆ é™¤ã€‚
- **æ ¸å¿ƒè®¾è®¡äº®ç‚¹**ï¼š
  - **åŒæ˜ å°„æ”¯æŒ**ï¼šç»Ÿä¸€æ”¯æŒ `h2v`ï¼ˆhyperedge-to-verticesï¼‰ã€`v2h`ï¼ˆvertex-to-hyperedgesï¼‰ã€`h2h`ï¼ˆhyperedge-to-neighborsï¼‰ä¸‰ç§æ˜ å°„ï¼Œå…¼å®¹å¤šç§è¶…å›¾è¡¨ç¤ºå½¢å¼ï¼ˆå¦‚ line graphã€clique graphï¼‰ã€‚
  - **å†…å­˜ç®¡ç†æœºåˆ¶**ï¼š
    - ä½¿ç”¨**é¢„åˆ†é… + åˆ†å—å†…å­˜**ï¼ˆflattened memory blocksï¼‰é¿å…é¢‘ç¹çš„ GPU åŠ¨æ€å†…å­˜åˆ†é…ã€‚
    - å¼•å…¥åŸºäº**å®Œå…¨äºŒå‰æ ‘çš„å—ç®¡ç†å™¨**ï¼ˆcomplete binary tree-based block managerï¼‰ï¼Œå®ç° O(log|E|) æ—¶é—´å†…æŸ¥æ‰¾å¯ç”¨å†…å­˜å—ï¼Œå¹¶æ”¯æŒå¹¶è¡Œæ’å…¥/åˆ é™¤ã€‚
  - **æº¢å‡ºé“¾å¼å­˜å‚¨**ï¼šå½“å•ä¸ªè¶…è¾¹è¿‡å¤§æ— æ³•æ”¾å…¥ä¸€ä¸ªå†…å­˜å—æ—¶ï¼Œé€šè¿‡æŒ‡é’ˆé“¾æ¥å¤šä¸ªå—ï¼Œç±»ä¼¼é“¾è¡¨ç»“æ„ã€‚
- **å¼€å‘ triad count æ›´æ–°æ¡†æ¶**ï¼šåŸºäº ESCHER æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„å¹¶è¡Œ triad è®¡æ•°æ›´æ–°ç®—æ³•ï¼Œæœ€å°åŒ–å†—ä½™è®¡ç®—ï¼Œä»…åœ¨å—å½±å“å­å›¾ä¸Šè¿›è¡Œå±€éƒ¨é‡ç®—ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ESCHER | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ MoCHyã€THyMe+ã€Hornetï¼‰ |
|------|--------|-------------------------------------|
| æ”¯æŒåŠ¨æ€æ€§ | âœ… å®Œå…¨æ”¯æŒå¢åˆ æ”¹ | âŒ å¤šä¸ºé™æ€åˆ†ææˆ–éƒ¨åˆ†åŠ¨æ€ |
| GPU å¹¶è¡Œä¼˜åŒ– | âœ… é«˜åº¦å¹¶è¡ŒåŒ–ï¼Œåˆ©ç”¨ warp å¯¹é½ | âš ï¸ éƒ¨åˆ†æœªé’ˆå¯¹ GPU è®¾è®¡ |
| å†…å­˜æ•ˆç‡ | âœ… æ‰¹é‡é¢„åˆ†é… + å—å¤ç”¨ | âŒ æ˜“äº§ç”Ÿç¢ç‰‡æˆ–é¢‘ç¹æ‹·è´ |
| é€šç”¨æ€§ | âœ… æ”¯æŒ hyperedge-basedã€incident-vertex-basedã€temporal triads | âŒ é€šå¸¸åªæ”¯æŒä¸€ç§ç±»å‹ |
| æ€§èƒ½ | âœ… æ˜¾è‘—åŠ é€Ÿï¼ˆæœ€é«˜è¾¾ 473.7Ã—ï¼‰ | âš ï¸ é€Ÿåº¦æ…¢ä¸”éš¾ä»¥æ‰©å±• |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| æ•°æ®é›† | è¶…è¾¹æ•° | é¡¶ç‚¹æ•° | æœ€å¤§åŸºæ•°ï¼ˆcardinalityï¼‰ | æ¥æº |
|-------|--------|--------|--------------------------|------|
| Coauth | 2.6M | 1.9M | 280 | [19] |
| Tags | 5.7M | 50K | 4 | [19] |
| Orkut | 6.3M | 3.1M | 27K | [20] |
| Threads | 9.7M | 2.7M | 67 | [19] |
| Random | 15M | 5M | 10K | åˆæˆ |

> æ³¨ï¼šæ‰€æœ‰å®éªŒå‡åœ¨ **NVIDIA A100 GPUï¼ˆ80GB HBM2eï¼‰** ä¸Šè¿è¡Œï¼Œä¸»æœºä¸º AMD EPYC Milan 7713 CPUã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **ä»»åŠ¡ç±»å‹**ï¼š
  - Hyperedge-based triad countingï¼ˆåŸºäº MoCHy [5]ï¼‰
  - Incident-vertex-based triad countingï¼ˆåŸºäº StatHyper [7]ï¼‰
  - Temporal triad countingï¼ˆåŸºäº THyMe+ [14]ï¼‰
- **åŠ¨æ€æ“ä½œæ¨¡å¼**ï¼š
  - æ‰¹é‡æ›´æ–°ï¼ˆbatch updateï¼‰ï¼šæ¯æ¬¡æ›´æ–°åŒ…å«ä¸€å®šæ•°é‡çš„ hyperedge æ’å…¥/åˆ é™¤ï¼ˆæ¯”ä¾‹å¯è°ƒï¼Œå¦‚ 50%/50%ï¼‰
  - é¡¶ç‚¹ä¿®æ”¹ï¼šåœ¨å·²æœ‰ hyperedge ä¸­æ·»åŠ /åˆ é™¤ incident vertices
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - æ‰§è¡Œæ—¶é—´ï¼ˆexecution timeï¼‰
  - åŠ é€Ÿæ¯”ï¼ˆspeedupï¼‰vs åŸºçº¿æ–¹æ³•
  - å¯æ‰©å±•æ€§ï¼ˆéšæ•°æ®è§„æ¨¡å¢é•¿çš„è¡¨ç°ï¼‰
  - å†…å­˜ä½¿ç”¨æƒ…å†µ

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦åŠ¨æ€ | æ˜¯å¦ GPU | æ”¯æŒ triad ç±»å‹ |
|------|------|-----------|------------|------------------|
| MoCHy [5] | Static hypergraph | âŒ | âŒï¼ˆå…±äº«å†…å­˜ï¼‰ / âœ…ï¼ˆæœ¬æ–‡å®ç°ï¼‰ | Hyperedge-based |
| THyMe+ [14] | Temporal hypergraph | âœ… | âŒï¼ˆCPUï¼‰ / âœ…ï¼ˆæœ¬æ–‡å®ç°ï¼‰ | Temporal |
| StatHyper [7] | Statistical model | âŒ | âŒï¼ˆR åŒ…ï¼‰ / âœ…ï¼ˆæœ¬æ–‡ CUDA å®ç°ï¼‰ | Incident-vertex-based |
| Hornet [12] | Dynamic graph | âœ… | âœ… | Vertex-basedï¼ˆæ™®é€šå›¾ï¼‰ |

> æ³¨ï¼šä½œè€…ä¸ºå…¬å¹³æ¯”è¾ƒï¼Œ**å®ç°äº† MoCHy å’Œ THyMe+ çš„ GPU ç‰ˆæœ¬ä½œä¸ºå¼ºåŸºçº¿**ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table IV å’Œ Figuresï¼‰

#### ï¼ˆ1ï¼‰Hyperedge-based Triad Counting vs MoCHy
- **å¹³å‡åŠ é€Ÿæ¯”**ï¼š**37.8Ã—**
- **æœ€å¤§åŠ é€Ÿæ¯”**ï¼š**104.5Ã—**
- å³ä½¿å¯¹æ¯”ä½œè€…è‡ªå·±å®ç°çš„ **GPU ç‰ˆ MoCHy**ï¼Œä»è¾¾åˆ°ï¼š
  - å¹³å‡ **19.5Ã—**
  - æœ€é«˜ **57.5Ã—**

> ğŸ’¡ åŸå› ï¼šMoCHy æ¯æ¬¡æ›´æ–°åéœ€é‡å»ºæ•´ä¸ªç»“æ„å¹¶å…¨å±€é‡ç®—ï¼›ESCHER ä»…æ›´æ–°å—å½±å“åŒºåŸŸï¼ˆaffection regionï¼‰ï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡ã€‚

#### ï¼ˆ2ï¼‰Incident-vertex-based Triad Counting vs StatHyper
- **å¹³å‡åŠ é€Ÿæ¯”**ï¼š**243.2Ã—**
- **æœ€å¤§åŠ é€Ÿæ¯”**ï¼š**473.7Ã—**
- æŒ‰ triad ç±»å‹ç»†åˆ†ï¼š
  - Type 1: æœ€é«˜ 249.8Ã—
  - Type 2: æœ€é«˜ 349.8Ã—
  - Type 3: æœ€é«˜ **473.7Ã—**

> ğŸ’¡ Type 3 æœ€éš¾ï¼ˆä¸‰ä¸ªé¡¶ç‚¹åˆ†åˆ«å±äºä¸åŒ hyperedgeï¼‰ï¼Œé™æ€æ–¹æ³•éœ€æ‰«æå…¨å›¾ï¼Œè€Œ ESCHER å±€éƒ¨æ›´æ–°ä¼˜åŠ¿æ›´æ˜æ˜¾ã€‚

#### ï¼ˆ3ï¼‰Temporal Triad Counting vs THyMe+
- **å¹³å‡åŠ é€Ÿæ¯”**ï¼š**36.3Ã—**
- **æœ€å¤§åŠ é€Ÿæ¯”**ï¼š**112.5Ã—**
- å¯¹æ¯” GPU ç‰ˆ THyMe+ï¼š
  - å¹³å‡ **25Ã—**
  - æœ€é«˜ **57Ã—**

> ğŸ’¡ æ—¶é—´ä¾èµ–æ€§å¢åŠ äº†å¤æ‚åº¦ï¼Œä½† ESCHER ä¾ç„¶æ˜¾è‘—é¢†å…ˆã€‚

#### ï¼ˆ4ï¼‰ä¸å…¶ä»–åŠ¨æ€å›¾æ–¹æ³•æ¯”è¾ƒï¼ˆvs Hornetï¼‰
- åœ¨å¤„ç†é«˜å˜å¼‚æ€§ï¼ˆhigh STDï¼‰çš„ hyperedge ä¿®æ”¹æ—¶ï¼Œ**ESCHER åè¶… Hornet**
- åŸå› ï¼šHornet ä½¿ç”¨å¹‚æ¬¡å†…å­˜åˆ†é…ç­–ç•¥ï¼Œåœ¨ cardinality æ³¢åŠ¨å¤§æ—¶å¯¼è‡´å¤§é‡å¤åˆ¶ï¼›ESCHER ä½¿ç”¨ 32 å¯¹é½å— + é“¾æ¥æœºåˆ¶ï¼Œæ›´é€‚åº”ä¸è§„åˆ™ç»“æ„ã€‚

### æ¶ˆèå®éªŒä¸æ•æ„Ÿæ€§åˆ†æï¼ˆè§ Fig. 6â€“8ï¼‰
- **æ‰¹å¤§å°å½±å“**ï¼šéšç€ changed hyperedges æ•°é‡å¢åŠ ï¼ŒESCHER ç›¸å¯¹ä¼˜åŠ¿ä¸‹é™ï¼ˆå› å±€éƒ¨æ›´æ–°æ”¶ç›Šé™ä½ï¼‰ï¼Œä½†ä»è¿œä¼˜äºåŸºçº¿ã€‚
- **åˆ é™¤æ¯”ä¾‹å½±å“**ï¼šåˆ é™¤è¶Šå¤šï¼ŒESCHER è¶Šå¿«ï¼ˆå› ä¸ºæ’å…¥æ¯”åˆ é™¤ä»£ä»·é«˜ï¼Œè€Œ MoCHy ä¸åŒºåˆ†æ“ä½œç±»å‹ï¼‰ã€‚
- **è¶…è¾¹åŸºæ•°å½±å“**ï¼šé«˜ cardinality å¯¼è‡´æ›´å¤šå†…å­˜æº¢å‡ºå’Œæ–°å—åˆ†é…ï¼Œç•¥å¾®å¢åŠ å¼€é”€ï¼Œä½†æ•´ä½“ä»é«˜æ•ˆã€‚
- **å¯æ‰©å±•æ€§æµ‹è¯•**ï¼šåœ¨éšæœºç”Ÿæˆçš„ 20M~55M è¶…è¾¹æ•°æ®ä¸Šï¼Œæ‰§è¡Œæ—¶é—´è¿‘ä¼¼çº¿æ€§å¢é•¿ï¼Œè¡¨æ˜è‰¯å¥½å¯æ‰©å±•æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ESCHER æ˜¯é¦–ä¸ªçœŸæ­£é€‚ç”¨äºå¤§è§„æ¨¡åŠ¨æ€è¶…å›¾çš„ GPU æ•°æ®ç»“æ„**ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚
2. **å±€éƒ¨æ›´æ–°ç­–ç•¥ + é«˜æ•ˆå†…å­˜ç®¡ç† = æè‡´æ€§èƒ½æå‡**ï¼šç›¸æ¯”å…¨å±€é‡ç®—æ–¹æ³•ï¼ŒESCHER å°† triad è®¡æ•°ä»â€œä¸å¯è¡Œâ€å˜ä¸ºâ€œå®æ—¶å¯è¡Œâ€ã€‚
3. **é€šç”¨æ€§å¼º**ï¼šä¸€å¥—æ¶æ„æ”¯æŒå¤šç§ triad ç±»å‹ï¼ˆhyperedge-basedã€incident-vertex-basedã€temporalï¼‰ï¼Œç”šè‡³å¯ç”¨äºæ™®é€šåŠ¨æ€å›¾ï¼ˆtriangle countingï¼‰ã€‚
4. **GPU ä¼˜åŒ–æœ‰æ•ˆ**ï¼šé€šè¿‡ warp å¯¹é½ã€å¹¶è¡Œå‰ç¼€å’Œï¼ˆprefix sumï¼‰ã€CUDA Thrust åº“ä¼˜åŒ–é›†åˆäº¤é›†ç­‰æŠ€æœ¯ï¼Œå……åˆ†å‘æŒ¥ç¡¬ä»¶æ½œåŠ›ã€‚
5. **å®é™…åº”ç”¨ä»·å€¼é«˜**ï¼šé€‚ç”¨äºç¤¾äº¤ç½‘ç»œæ¼”åŒ–åˆ†æã€ç”Ÿç‰©ç½‘ç»œåŠŸèƒ½æ¨¡å—æ¢æµ‹ã€æ¨èç³»ç»ŸåŠ¨æ€å»ºæ¨¡ç­‰åœºæ™¯ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å†…å­˜å ç”¨è¾ƒé«˜**ï¼šç”±äºé‡‡ç”¨å›ºå®šå—å¤§å°ï¼ˆ32 çš„å€æ•°ï¼‰å’Œé¢„ç•™ metadataï¼Œå­˜åœ¨å†…éƒ¨ç¢ç‰‡é—®é¢˜ã€‚
- **ä¸é€‚åˆæç¨€ç–å°å›¾**ï¼šå¯¹äºå°è§„æ¨¡æˆ–é™æ€è¶…å›¾ï¼Œåˆå§‹åŒ–å¼€é”€å¯èƒ½æŠµæ¶ˆä¼˜åŠ¿ã€‚
- **ç›®å‰ä»…æ”¯æŒæ— å‘è¶…å›¾**ï¼šæœªè€ƒè™‘æœ‰å‘æˆ–å¸¦æƒè¶…å›¾æ‰©å±•ã€‚
- **ä¾èµ–é¢„çŸ¥æ‰¹é‡æ›´æ–°è§„æ¨¡**ï¼šè‹¥æ— æ³•é¢„ä¼°æ’å…¥æ•°é‡ï¼Œé¢„åˆ†é…ç­–ç•¥å¯èƒ½ä¸å¤Ÿçµæ´»ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜åˆ©ç”¨ç‡**ï¼šæ¢ç´¢æ›´ç´§å‡‘çš„å­˜å‚¨æ ¼å¼ï¼ˆå¦‚å‹ç¼©å—ã€å˜é•¿ç¼–ç ï¼‰ã€‚
- **æ”¯æŒæµå¼è¶…å›¾ï¼ˆstreaming hypergraphsï¼‰**ï¼šå®ç°å®æ—¶å¢é‡å¤„ç†è€Œéæ‰¹é‡æ›´æ–°ã€‚
- **æ‰©å±•è‡³åˆ†å¸ƒå¼ç¯å¢ƒ**ï¼šå°† ESCHER æ¨å¹¿åˆ°å¤š GPU æˆ–é›†ç¾¤æ¶æ„ï¼Œå¤„ç†è¶…å¤§è§„æ¨¡è¶…å›¾ã€‚
- **é›†æˆæœºå™¨å­¦ä¹ åº”ç”¨**ï¼šç»“åˆ hypergraph neural networksï¼ˆHGNNsï¼‰è¿›è¡ŒåŠ¨æ€åµŒå…¥å­¦ä¹ ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **ESCHER é€šè¿‡åˆ›æ–°çš„ GPU å‹å¥½å‹æ•°æ®ç»“æ„å’Œå±€éƒ¨æ›´æ–°æ¡†æ¶ï¼Œå®ç°äº†å¯¹åŠ¨æ€è¶…å›¾ triad è®¡æ•°çš„é©å‘½æ€§åŠ é€Ÿï¼ˆæœ€é«˜è¾¾ 473.7Ã—ï¼‰ï¼Œä¸ºé«˜é˜¶ç½‘ç»œåŠ¨æ€åˆ†ææä¾›äº†å¼ºæœ‰åŠ›çš„åŸºç¡€è®¾æ–½æ”¯æŒã€‚**

</details>

---

### 15. [Data-Free Pruning of Self-Attention Layers in LLMs](https://arxiv.org/abs/2512.20636)

**Authors**: Dhananjay Saikumar, Blesson Varghese  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20636v1  

#### Abstract
Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the r...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šData-Free Pruning of Self-Attention Layers in LLMs

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´é«˜å»¶è¿Ÿã€é«˜èƒ½è€—å’Œé«˜éƒ¨ç½²æˆæœ¬çš„é—®é¢˜ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶é›†ä¸­åœ¨**å‚æ•°çº§å‹ç¼©**ï¼ˆå¦‚é‡åŒ–ã€éç»“æ„åŒ–å‰ªæï¼‰ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–ä¸“ç”¨ç¡¬ä»¶æ”¯æŒï¼ˆå¦‚ä½ç²¾åº¦ç®—å­æˆ–ç¨€ç–çŸ©é˜µåŠ é€Ÿï¼‰ï¼Œä¸”å¸¸éœ€å¾®è°ƒæ¢å¤ç²¾åº¦ã€‚è€Œ**æ·±åº¦ç»´åº¦**ï¼ˆå³å±‚æ•°ï¼‰çš„å‹ç¼©ä»è¢«å¹¿æ³›å¿½è§†ã€‚

æœ¬æ–‡èšç„¦äºä¸€ä¸ªå…³é”®è§‚å¯Ÿï¼šè®¸å¤šæ·±å±‚çš„ **self-attention sublayers** åœ¨é¢„è®­ç»ƒåå‡ ä¹ä¸æ”¹å˜è¾“å…¥è¡¨ç¤ºï¼Œè¡¨ç°å‡ºé«˜åº¦å†—ä½™ã€‚å¦‚ä½•é«˜æ•ˆè¯†åˆ«å¹¶ç§»é™¤è¿™äº›å†—ä½™å±‚ï¼Œæˆä¸ºæå‡æ¨ç†æ•ˆç‡çš„å…³é”®ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯

#### ï¼ˆ1ï¼‰æå‡º **Attention Suppression Hypothesis**
ä½œè€…å‡è®¾ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·±å±‚çš„ self-attention å­å±‚ä¼šä¸»åŠ¨â€œæŠ‘åˆ¶â€è‡ªèº«çš„è¾“å‡ºæ›´æ–°ï¼Œä½¿å¾—å…¶è¾“å‡ºæ¥è¿‘é›¶ï¼Œä»è€Œè®©æ®‹å·®è·¯å¾„å’Œåç»­çš„ MLP å±‚æ‰¿æ‹…ä¸»è¦è¡¨å¾ä»»åŠ¡ã€‚è¿™ç§ç°è±¡å¯é€šè¿‡æƒé‡æœ¬èº«åæ˜ å‡ºæ¥ã€‚

#### ï¼ˆ2ï¼‰è®¾è®¡ **Gate-Norm** â€”â€” å…¨æ— æ•°æ®ï¼ˆdata-freeï¼‰ã€ä»…æƒé‡ï¼ˆweight-onlyï¼‰çš„é‡è¦æ€§è¯„åˆ†å‡†åˆ™
- **å®šä¹‰**ï¼šå¯¹æ¯ä¸ª attention å±‚ï¼Œè®¡ç®—æŸ¥è¯¢æƒé‡ $ W_q $ å’Œé”®æƒé‡ $ W_k $ çš„ä¹˜ç§¯çŸ©é˜µçš„ Frobenius èŒƒæ•°ï¼š
  $$
  m_l = \|W_{q,l} W_{k,l}\|_F
  $$
  ç§°ä¸º **Gate-Norm**ã€‚
- **åŸç†**ï¼šè‹¥è¯¥èŒƒæ•°å°ï¼Œåˆ™è¯´æ˜ query-key çš„è€¦åˆå¼±ï¼Œæ³¨æ„åŠ›æœºåˆ¶éš¾ä»¥æœ‰æ•ˆæ··åˆ token ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›è¾“å‡ºè¶‹è¿‘äºé›¶ã€‚
- **ä¼˜åŠ¿**ï¼šæ— éœ€ä»»ä½•æ ¡å‡†æ•°æ®ã€å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­æˆ–å¾®è°ƒï¼Œå³å¯å®Œæˆé‡è¦æ€§æ‰“åˆ†ã€‚

#### ï¼ˆ3ï¼‰å®ç° **One-shot Pruning ç®—æ³•**
- æ­¥éª¤ï¼šè®¡ç®—æ‰€æœ‰å±‚çš„ Gate-Norm åˆ†æ•° â†’ æŒ‰å‡åºæ’åº â†’ ç§»é™¤åˆ†æ•°æœ€ä½çš„ N ä¸ª attention sublayersã€‚
- ç‰¹ç‚¹ï¼šä¸€æ¬¡æ€§å®Œæˆï¼Œæ¯«ç§’çº§æ‰§è¡Œé€Ÿåº¦ï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡å’Œéšç§æ•æ„Ÿåœºæ™¯ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | Gate-Normï¼ˆæœ¬æ–‡ï¼‰ | æ•°æ®é©±åŠ¨æ–¹æ³•ï¼ˆå¦‚ ShortGPT, He et al.ï¼‰ |
|------|------------------|----------------------------------------|
| æ˜¯å¦éœ€è¦æ•°æ® | âŒ å®Œå…¨ä¸éœ€è¦ | âœ… éœ€è¦æ•°åƒ token çš„ calibration æ•°æ® |
| æ˜¯å¦éœ€è¦å‰å‘ä¼ æ’­ | âŒ ä¸éœ€è¦ | âœ… å¤šæ¬¡å‰å‘ä¼ æ’­ |
| æ˜¯å¦ä¾èµ–ç‰¹å®šç¡¬ä»¶ | âŒ å¯åœ¨ CPU ä¸Šè¿è¡Œ | âœ… é€šå¸¸éœ€ GPU æ˜¾å­˜åŠ è½½å¤§æ¨¡å‹ |
| æ‰§è¡Œæ—¶é—´ | ~300msï¼ˆGPUï¼‰ï¼Œ<30sï¼ˆCPUï¼‰ | æ•°åˆ†é’Ÿè‡³æ•°å°æ—¶ |
| é€Ÿåº¦ä¼˜åŠ¿ | **>1000Ã— æ›´å¿«** | åŸºçº¿ |
| å‡†ç¡®ç‡ä¿æŒèƒ½åŠ› | åŒ¹é…ç”šè‡³ç•¥ä¼˜äºæ•°æ®é©±åŠ¨æ–¹æ³• | åŸºçº¿ |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“**ï¼šé¦–æ¬¡å®ç°äº†**å®Œå…¨æ— æ•°æ®ã€æ— å‰å‘ä¼ æ’­ã€çº¯åŸºäºæƒé‡**çš„ attention å±‚å‰ªææ–¹æ¡ˆï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æå¤§æå‡äº†å®ç”¨æ€§ä¸éƒ¨ç½²çµæ´»æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

| ç±»å‹ | æ•°æ®é›† |
|------|-------|
| **Perplexity è¯„ä¼°** | WikiText-2ï¼ˆæ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ï¼‰ |
| **Zero-shot å‡†ç¡®ç‡è¯„ä¼°** | BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy/Challenge, OpenBookQAï¼ˆå…±7é¡¹ NLP æ¨ç†ä»»åŠ¡ï¼‰ |
| **Fine-tuning å¾®è°ƒæ•°æ®** | WikiText-2ï¼ˆç”¨äº LoRA å¾®è°ƒé˜¶æ®µï¼‰ |

> âš ï¸ æ³¨æ„ï¼š**æ‰€æœ‰å‰ªæå†³ç­–å‡æœªä½¿ç”¨ä¸Šè¿°ä»»ä½•æ•°æ®**ï¼Œä»…åœ¨è¯„ä¼°æ—¶ä½¿ç”¨ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹
- ä¸»è¦æµ‹è¯•æ¨¡å‹ï¼šLLaMA-13Bï¼ˆv1 å’Œ v2ï¼‰
- æ‰©å±•éªŒè¯æ¨¡å‹ï¼šVicuna-7B/13B, LLaMA-3.1-8B

#### å‰ªæç­–ç•¥å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **Random-Block Removal** | éšæœºåˆ é™¤å®Œæ•´ Transformer block |
| **ShortGPT** | åŸºäº block è¾“å…¥è¾“å‡º cosine similarity åˆ é™¤æ•´ä¸ª block |
| **Random-Attn Removal** | éšæœºåˆ é™¤ attention sublayer |
| **Data-driven Attention Pruning** | åŸºäº attention è¾“å…¥è¾“å‡ºçš„ cosine similarity åˆ é™¤ sublayerï¼ˆHe et al., 2024ï¼‰ |
| **Gate-Norm-Attn (Ours)** | æœ¬æ–‡æå‡ºçš„æ— æ•°æ®æ–¹æ³• |

#### è¯„ä¼°æŒ‡æ ‡
1. **Perplexity on WikiText-2**ï¼šè¡¡é‡è¯­è¨€å»ºæ¨¡èƒ½åŠ›æŸå¤±ã€‚
2. **Zero-shot Accuracy Average**ï¼šè·¨7ä¸ªä»»åŠ¡çš„å¹³å‡å‡†ç¡®ç‡ã€‚
3. **Inference Throughput Speedup**ï¼šç›¸å¯¹åŸå§‹æ¨¡å‹çš„ååé‡æå‡å€æ•°ã€‚
4. **Pruning Latency**ï¼šé‡è¦æ€§è¯„åˆ† + å‰ªææ“ä½œè€—æ—¶ã€‚
5. **Post-pruning Fine-tuning Recovery**ï¼šç»“åˆ LoRA å¾®è°ƒåçš„æ€§èƒ½æ¢å¤æƒ…å†µã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ï¼ˆ1ï¼‰Perplexity ç»“æœï¼ˆWikiText-2ï¼‰

- åœ¨ LLaMA-13B ä¸Šç§»é™¤ 4â€“16 ä¸ª attention å±‚ï¼š
  - **Gate-Norm** ä¸ **data-driven æ–¹æ³•** è¡¨ç°å‡ ä¹ä¸€è‡´ï¼Œç”šè‡³ç•¥æœ‰é¢†å…ˆã€‚
  - ç§»é™¤ 10 å±‚æ—¶ï¼ŒGate-Norm çš„ ppl â‰ˆ 10.02ï¼ˆv1ï¼‰ï¼Œdata-driven â‰ˆ 12.55ã€‚
  - ç§»é™¤è¶…è¿‡ 16 å±‚åï¼Œæ‰€æœ‰æ–¹æ³•æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼›éšæœºå‰ªæåˆ™ç«‹å³å´©æºƒï¼ˆppl > 300ï¼‰ã€‚
- åœ¨å…¶ä»–æ¨¡å‹ï¼ˆVicuna, LLaMA-3.1ï¼‰ä¸Šä¹Ÿå–å¾—ç›¸ä¼¼ç»“æœï¼ˆè§ Table 1ï¼‰ï¼Œè¯æ˜æ³›åŒ–æ€§å¼ºã€‚

> ğŸ” **å…³é”®å‘ç°**ï¼šGate-Norm èƒ½ç²¾å‡†å®šä½çœŸæ­£å†—ä½™çš„ attention å±‚ï¼Œé¿å…ç ´åå…³é”®ç»“æ„ã€‚

---

### ï¼ˆ2ï¼‰Zero-shot Accuracy å¯¹æ¯”ï¼ˆTable 2ï¼‰

| å‰ªææ•°é‡ | æ–¹æ³• | å¹³å‡å‡†ç¡®ç‡ï¼ˆv1ï¼‰ | ååæå‡ | æ•°æ®éœ€æ±‚ |
|---------|------|------------------|----------|-----------|
| 0 | Baseline | 65.66% | 1.00x | â€“ |
| 4 | Gate-Norm | 64.99% | 1.06x | âŒ |
| 8 | Gate-Norm | 64.10% | 1.12x | âŒ |
| 16 | Gate-Norm | 63.82% | **1.30x** | âŒ |
| 16 | Data-driven | 64.02% | 1.30x | âœ… |
| 16 | Random-Attn | 49.38% | 1.30x | âŒ |

- **ç»“è®º**ï¼š
  - Gate-Norm åœ¨ç§»é™¤æœ€å¤š 16 å±‚æ—¶ï¼Œå¹³å‡å‡†ç¡®ç‡ä»…ä¸‹é™çº¦ **1.8â€“2.0%**ï¼Œä¸ data-driven æ–¹æ³•ç›¸å½“ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œrandom pruning å¯¼è‡´ä¸¥é‡é€€åŒ–ï¼ˆâ†“16%ä»¥ä¸Šï¼‰ã€‚
  - block-level pruningï¼ˆå¦‚ ShortGPTï¼‰åœ¨ç›¸åŒåŠ é€Ÿä¸‹å‡†ç¡®ç‡ä¸‹é™æ›´æ˜¾è‘—ã€‚

> âœ… **æ€§èƒ½æƒè¡¡ä¼˜åŠ¿**ï¼š**æ¯å¢åŠ  6% ååï¼Œä»…ç‰ºç‰²çº¦ 1% å‡†ç¡®ç‡**ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚

---

### ï¼ˆ3ï¼‰æ¶ˆèå®éªŒä¸åˆ†æ

#### ï¼ˆaï¼‰å“ªäº›å±‚è¢«å‰ªæï¼Ÿ
- å›¾ 5 æ˜¾ç¤ºï¼šGate-Norm å’Œ data-driven æ–¹æ³•éƒ½é›†ä¸­å‰ªæ **ç¬¬ 20 å±‚ä¹‹åçš„æ·±å±‚ attention å±‚**ã€‚
- å°½ç®¡é€‰æ‹©ç•¥æœ‰å·®å¼‚ï¼ˆGate-Norm æ›´æ—©å¼€å§‹å‰ªæï¼‰ï¼Œä½†é‡å åº¦é«˜ï¼Œè¯´æ˜æƒé‡ä¸­ç¡®å®è•´å«äº†åŠŸèƒ½å†—ä½™ä¿¡å·ã€‚

#### ï¼ˆbï¼‰Pruning Efficiencyï¼ˆæ•ˆç‡å¯¹æ¯”ï¼‰
- **Gate-Norm** åœ¨ NVIDIA A6000 ä¸Šè¯„åˆ†ä»…éœ€ **~300ms**ã€‚
- æ•°æ®é©±åŠ¨æ–¹æ³•éœ€æ•°åƒ token å‰å‘ä¼ æ’­ï¼Œè€—æ—¶ **>5åˆ†é’Ÿ** â†’ **>1000Ã— åŠ é€Ÿ**ã€‚
- åœ¨ CPU ä¸Šï¼ŒGate-Norm å¯åœ¨ **30 ç§’å†…å®Œæˆ 13B æ¨¡å‹å‰ªæ**ï¼Œæ— éœ€ GPU æ˜¾å­˜ã€‚

#### ï¼ˆcï¼‰LoRA å¾®è°ƒæ¢å¤èƒ½åŠ›ï¼ˆTable 3ï¼‰
| æ¨¡å‹ | Pruned (no FT) | + LoRA å¾®è°ƒ |
|------|---------------|-------------|
| Vicuna-13B | 14.13 â†’ 6.57 | âœ”ï¸ æˆåŠŸæ¢å¤ |
| LLaMA-2-13B | 14.11 â†’ 6.35 | âœ”ï¸ æˆåŠŸæ¢å¤ |
| Gate-Norm vs Data-driven | **ç›¸å½“æˆ–æ›´å¥½** | âœ”ï¸ å…¼å®¹æ€§å¼º |

> ğŸ’¡ è¡¨æ˜ Gate-Norm å‰ªæåçš„æ¨¡å‹ä»å…·å¤‡è‰¯å¥½å¯å¾®è°ƒæ€§ï¼Œå¯ç”¨äºæ›´æ¿€è¿›å‹ç¼© + å¿«é€Ÿé€‚é…ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **Attention Suppression æ˜¯çœŸå®å­˜åœ¨çš„ç°è±¡**ï¼š
   - æ·±å±‚ attention å±‚çš„è¾“å‡ºæ›´æ–°è¶‹è¿‘äºé›¶ï¼Œå…¶è¾“å…¥-è¾“å‡º cosine similarity æ¥è¿‘ 1ã€‚
   - attention-to-input norm ratio $ r_l $ éšæ·±åº¦å•è°ƒé€’å‡ï¼Œåœ¨æœ€åå‡ å±‚é™è‡³ <0.1ã€‚

2. âœ… **æƒé‡ä¸­è•´å«è¶³å¤Ÿä¿¡æ¯åˆ¤æ–­å†—ä½™æ€§**ï¼š
   - Gate-Norm åˆ©ç”¨ $ \|W_q W_k\|_F $ æˆåŠŸé¢„æµ‹äº† attention å±‚çš„åŠŸèƒ½æƒ°æ€§ã€‚
   - æ— éœ€ä»»ä½•æ•°æ®å³å¯åšå‡ºé«˜è´¨é‡å‰ªæå†³ç­–ã€‚

3. âœ… **å‰ªæå¯å¸¦æ¥æ˜¾è‘—æ¨ç†åŠ é€Ÿ**ï¼š
   - ç§»é™¤ 16 ä¸ª attention å±‚ â†’ **æœ€é«˜ 1.30Ã— ååæå‡**ã€‚
   - å›  attention å æ®ä¸»è¦è¿è¡Œæ—¶é—´ï¼ˆå°¤å…¶é•¿åºåˆ—ï¼‰ï¼Œå‰ªææ•ˆæœç›´æ¥è½¬åŒ–ä¸º latency ä¸‹é™ã€‚

4. âœ… **æ€§èƒ½åª²ç¾æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œé€Ÿåº¦å¿«åƒå€**ï¼š
   - å‡†ç¡®ç‡å’Œå›°æƒ‘åº¦è¡¨ç°ä¸æœ€å¼º baseline ç›¸å½“ã€‚
   - æ‰§è¡Œæ—¶é—´ä»åˆ†é’Ÿçº§é™è‡³æ¯«ç§’çº§ï¼Œé€‚åˆåŠ¨æ€ã€åœ¨çº¿ã€è¾¹ç¼˜éƒ¨ç½²ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

1. ğŸ›‘ **é€‚ç”¨èŒƒå›´é™äº dense decoder-only Transformers**ï¼š
   - å½“å‰æœªæ‰©å±•åˆ° MoE æ¶æ„ï¼ˆå¦‚ Mixtralï¼‰ã€encoder-decoder æ¨¡å‹ï¼ˆå¦‚ T5ï¼‰æˆ– vision transformersã€‚

2. ğŸ›‘ **æç«¯å‰ªæï¼ˆ>1/3 å±‚ï¼‰ä¼šå¯¼è‡´æ€§èƒ½å´©å¡Œ**ï¼š
   - è™½ç„¶ 16/40 å±‚å¯è¡Œï¼Œä½†ç»§ç»­å‰ªæå°†è¿…é€Ÿæ¶åŒ–æ€§èƒ½ï¼Œè¡¨æ˜å­˜åœ¨ç»“æ„æ€§ç“¶é¢ˆã€‚

3. ğŸ›‘ **æ— æ³•å¤„ç† layer-wise åŠ¨æ€è·¯ç”±ç±»æ–¹æ³•**ï¼š
   - å¦‚ SkipGPTã€AdaInfer ç­‰åŠ¨æ€è·³è¿‡æœºåˆ¶ï¼Œä¸é™æ€å‰ªæäº’è¡¥è€Œéæ›¿ä»£ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. ğŸ”® å°† Gate-Norm æ€æƒ³æ¨å¹¿è‡³ **MoE æ¨¡å‹ä¸­çš„ expert selection æˆ– router å‰ªæ**ã€‚
2. ğŸ” æ¢ç´¢ **hybrid pruning**ï¼šç»“åˆ Gate-Normï¼ˆæ·±åº¦ï¼‰ä¸é‡åŒ–/ç¨€ç–åŒ–ï¼ˆå®½åº¦/ç²¾åº¦ï¼‰è¿›è¡Œå¤šè½´å‹ç¼©ã€‚
3. ğŸ§  è®¾è®¡ **è‡ªé€‚åº”æ¶æ„**ï¼šåœ¨è®­ç»ƒé˜¶æ®µå°±å‡å°‘åæœŸ attention å®¹é‡ï¼Œé¿å…â€œå­¦å®Œå†å‰ªâ€ã€‚
4. ğŸŒ æ‰©å±•è‡³ **å¤šæ¨¡æ€æ¨¡å‹**ï¼ˆå¦‚ CLIPã€Flamingoï¼‰ä¸­ cross-attention å±‚çš„å†—ä½™åˆ†æã€‚
5. âš™ï¸ å¼€å‘ **è‡ªåŠ¨åŒ–å·¥å…·é“¾**ï¼Œé›†æˆ Gate-Norm åˆ° HuggingFace æˆ– ONNX ç¼–è¯‘æµç¨‹ä¸­ï¼Œå®ç°ä¸€é”®å‹ç¼©ã€‚

---

## æ€»ç»“

âœ… **æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç®€æ´è€Œå¼ºå¤§çš„æ´è§**ï¼š  
> â€œæ·±å±‚ attention å±‚ä¹‹æ‰€ä»¥å¯å‰ªï¼Œæ˜¯å› ä¸ºå®ƒä»¬è‡ªå·±å­¦ä¼šäº†â€˜æ²‰é»˜â€™ã€‚â€

ğŸš€ åŸºäºæ­¤ï¼Œä½œè€…æå‡º **Gate-Norm**â€”â€”é¦–ä¸ªå®Œå…¨ data-freeã€weight-onlyã€one-shot çš„ attention å±‚å‰ªææ–¹æ³•ï¼Œåœ¨ **ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç° >1000Ã— çš„å‰ªææ•ˆç‡æå‡**ï¼Œä¸ºå¤§è§„æ¨¡ LLM çš„è½»é‡åŒ–éƒ¨ç½²æä¾›äº†æå…·å®ç”¨ä»·å€¼çš„æ–°è·¯å¾„ã€‚

</details>

---

### 16. [Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions](https://arxiv.org/abs/2512.20974)

**Authors**: Jingyang You, Hanna Kurniawati  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20974v1  

#### Abstract
Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applic...

---

### 17. [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)

**Authors**: Yaowei Bai, Ruiheng Zhang, Yu Lei, Xuhua Duan, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, Qian Yuan, Lei Chen, Wenjuan Tang, Biqiang Zhu, Xinggang Wang, Tao Sun, Wei Zhou, Dacheng Tao, Yongchao Xu, Chuansheng Zheng, Huangxuan Zhao, Bo Du  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.20344v1  

#### Abstract
A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous pr...

---

### 18. [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)

**Authors**: Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.21323v1  

#### Abstract
We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregres...

---

### 19. [Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering](https://arxiv.org/abs/2512.21301)

**Authors**: Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H. M. Abou-El-Enien  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.21301v1  

#### Abstract
Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computat...

---

### 20. [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)

**Authors**: Divij Dudeja, Mayukha Pal  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.21280v1  

#### Abstract
The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material ...

---

### 21. [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)

**Authors**: Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.20795v1  

#### Abstract
Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent ...

---

### 22. [Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models](https://arxiv.org/abs/2512.20633)

**Authors**: MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.20633v1  

#### Abstract
Accurate prediction of treatment outcomes in lung cancer remains challenging due to the sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models often fail to capture semantic information across multimodal streams, while large-scale fine-tuning approa...

---

### 23. [GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface](https://arxiv.org/abs/2512.20813)

**Authors**: Miguel Esparza, Vamshi Battal, Ali Mostafavi  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.20813v1  

#### Abstract
As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driv...

---

### 24. [LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics](https://arxiv.org/abs/2512.21010)

**Authors**: Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.21010v1  

#### Abstract
The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring,...

---

### 25. [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)

**Authors**: Ze Gong, Pradeep Varakantham, Akshat Kumar  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20173v1  

#### Abstract
Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previ...

---

### 26. [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)

**Authors**: Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20618v1  

#### Abstract
Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We pro...

---

### 27. [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)

**Authors**: Kaiyuan Liu, Shaotian Yan, Rui Miao, Bing Wang, Chen Shen, Jun Zhang, Jieping Ye  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20908v1  

#### Abstract
Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed ana...

---

### 28. [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)

**Authors**: H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20074v1  

#### Abstract
Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We p...

---

### 29. [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)

**Authors**: Ziyi Zhu, Olivier Tieleman, Caitlin A. Stamatis, Luka Smyth, Thomas D. Hull, Daniel R. Cahn, Matteo Malgaroli  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20773v1  

#### Abstract
Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. W...

---

### 30. [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)

**Authors**: Shivraj Singh Bhatti  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20877v1  

#### Abstract
We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and mu...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
