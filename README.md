# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-02 06:47:28 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)

**Authors**: Haoyun Jiang, Junqi He, Feng Hong, Xinlong Yang, Jianwei Zhang, Zheng Li, Zhengyang Zhuge, Zhiyong Chen, Bo Han, Junyang Lin, Jiangchao Yao  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2601.23180v1  

#### Abstract
Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šTriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶å—é™äºå…¶**ä¸²è¡Œã€è‡ªå›å½’ç”Ÿæˆæœºåˆ¶**ï¼Œå¯¼è‡´å»¶è¿Ÿè¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†ã€è¾“å‡ºåºåˆ—è¾ƒé•¿çš„ä»»åŠ¡ä¸­ã€‚å°½ç®¡ç°æœ‰çš„ **Speculative Decoding (SD)** æŠ€æœ¯é€šè¿‡â€œèµ·è‰-éªŒè¯â€èŒƒå¼æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œä½†å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ– **drafting æ•ˆç‡** å’Œ **acceptance rate** ä¸Šï¼Œè€Œå¯¹ **verification é˜¶æ®µçš„è®¡ç®—å¼€é”€** å…³æ³¨ä¸è¶³ã€‚

æœ¬æ–‡æŒ‡å‡ºï¼Œéšç€ drafting æˆæœ¬è¢«å•å±‚ drafterï¼ˆå¦‚ EAGLEï¼‰å¤§å¹…é™ä½ï¼Œ**æ¯è½®éªŒè¯æ—¶é—´ $t_v$ å·²æˆä¸ºæ–°çš„æ€§èƒ½ç“¶é¢ˆ**ã€‚å› æ­¤ï¼Œå¦‚ä½•å‡å°‘æ˜‚è´µçš„ target model åœ¨éªŒè¯é˜¶æ®µçš„è°ƒç”¨é¢‘ç‡ï¼Œæˆä¸ºä¸€ä¸ªå…³é”®ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„æ–¹å‘ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº† **TriSpec** â€”â€”ä¸€ç§å…¨æ–°çš„**ä¸‰å…ƒ speculative decoding æ¡†æ¶**ï¼Œå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„ **proxy verifier** æ¥åˆ†æ‹…éªŒè¯ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åè°ƒä¸‰ä¸ªè§’è‰²ï¼š
- **Drafter**ï¼šå•å±‚ç»“æ„ï¼Œè´Ÿè´£é«˜é€Ÿç”Ÿæˆå€™é€‰ tokenã€‚
- **Proxy Verifier**ï¼šæ¥è‡ªåŒä¸€å®¶æ—çš„å°è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ Qwen3-1.7Bï¼‰ï¼Œç”¨äºå¿«é€Ÿé¢„éªŒè¯å¹¶å±€éƒ¨çº æ­£è‰æ¡ˆã€‚
- **Target Model**ï¼šå¤§è§„æ¨¡ç›®æ ‡æ¨¡å‹ï¼ˆå¦‚ Qwen3-32Bï¼‰ï¼Œä»…åœ¨ proxy åˆ¤å®šä¸ºâ€œä¸å¯ä¿¡â€æ—¶è¿›è¡Œæƒå¨éªŒè¯ã€‚

æ ¸å¿ƒæœºåˆ¶æ˜¯åŸºäº **margin-based routing rule**ï¼šåˆ©ç”¨ proxy æ¨¡å‹ top-1 ä¸ top-2 è¾“å‡ºæ¦‚ç‡ä¹‹é—´çš„å·®è·ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡ã€‚è‹¥ margin è¶³å¤Ÿå¤§ï¼Œåˆ™è®¤ä¸º proxy çš„åˆ¤æ–­å¯ä¿¡ï¼Œå¯è·³è¿‡ target éªŒè¯ï¼›å¦åˆ™å°†æ§åˆ¶æƒäº¤è¿˜ç»™ target modelã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ä¼˜åŒ– verification æˆæœ¬**ï¼Œè€Œéä»…ä»…æå‡ drafting æˆ– acceptance lengthã€‚
- å¼•å…¥ **same-family smaller model ä½œä¸º proxy verifier**ï¼Œå……åˆ†åˆ©ç”¨å…¶ä¸ target çš„å¼ºå¯¹é½æ€§ï¼ˆtoken-level alignment è¾¾ 82%ï¼‰ï¼Œå®ç°é«˜æ•ˆä¸”å¯é çš„è½»é‡éªŒè¯ã€‚
- é‡‡ç”¨åŠ¨æ€è·¯ç”±ç­–ç•¥ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶**æ˜¾è‘—å‡å°‘ target model çš„è°ƒç”¨æ¬¡æ•°**ã€‚
- å¯æ— ç¼é›†æˆåˆ°ç°æœ‰å…ˆè¿› SD æ–¹æ³•ï¼ˆå¦‚ EAGLE-3ï¼‰ä¹‹ä¸Šï¼Œè¿›ä¸€æ­¥æå‡åŠ é€Ÿæ•ˆæœã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
å®éªŒè¦†ç›–å¤šä¸ªé¢†åŸŸå’Œéš¾åº¦çº§åˆ«çš„åŸºå‡†æµ‹è¯•é›†ï¼š
- **æ•°å­¦æ¨ç†**ï¼šGSM8Kã€MATH500ã€AIME24/AIME25
- **ä»£ç ç”Ÿæˆ**ï¼šHumanEvalã€MBPP
- **å¤šè¯­è¨€ä¸ç»¼åˆèƒ½åŠ›**ï¼šGaokao2023-enï¼ˆGK23-enï¼‰ã€Polymath
- **é—®ç­”ä¸æ£€ç´¢å¢å¼º**ï¼šHotpotQAã€SpecBenchã€GPQA-Diamond

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹å®¶æ—**ï¼š
  - Qwen3 ç³»åˆ—ï¼štarget (32B), proxy (1.7B), drafter (EAGLE-3/HASS)
  - DeepSeek-R1-Distill-Qwen (DSQ)ï¼štarget (32B), proxy (1.5B)
  - DeepSeek-R1-Distill-LLaMA (DSL)ï¼štarget (70B), proxy (8B)
- **è®­ç»ƒç»†èŠ‚**ï¼š
  - Drafter ä½¿ç”¨ HASS/EAGLE-3 æ¶æ„ï¼Œå¹¶è”åˆè®­ç»ƒæˆ– adapter å¾®è°ƒä»¥é€‚é… proxy ç‰¹å¾ã€‚
  - Margin threshold $\lambda = 0.5$ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Accuracy**ï¼špass@1 å‡†ç¡®ç‡æˆ– LLM-judge åˆ†æ•°ã€‚
  - **æ•ˆç‡**ï¼šååé‡ï¼ˆTokens/s, TPSï¼‰ã€ç›¸å¯¹ speedupï¼ˆvs. target-onlyï¼‰ã€‚
  - **éªŒè¯å¼€é”€**ï¼štarget-invocation ratio ($r_t$)ï¼Œå³æ¯ä¸ªç”Ÿæˆ token ä¸­è°ƒç”¨ target çš„æ¯”ä¾‹ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Single Model (Target/Proxy)**ï¼šå•ç‹¬ä½¿ç”¨ target æˆ– proxy æ¨ç†ã€‚
- **æ ‡å‡† SD æ–¹æ³•**ï¼šHASSã€EAGLE3ã€‚
- **æ¾å¼›éªŒè¯æ–¹æ³•**ï¼šSpecCascade åŠå…¶å˜ä½“ï¼ˆChow rule, OPT rule, Token ruleï¼‰ã€‚
- æ‰€æœ‰ drafters å‡åœ¨ç»Ÿä¸€æ•°æ®ä¸Šé‡æ–°è®­ç»ƒä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| æœ€é«˜åŠ é€Ÿæ¯” | **æœ€é«˜è¾¾ 35% çš„é¢å¤–åŠ é€Ÿ**ï¼ˆç›¸æ¯”æ ‡å‡† SDï¼‰ |
| Target è°ƒç”¨å‡å°‘ | **æœ€å¤šå‡å°‘è¶…è¿‡ 50% çš„ target model invocation** |
| å¹³å‡å‡†ç¡®ç‡æŸå¤± | **å°äº 1%**ï¼ˆtemperature=0ï¼‰æˆ– **å°äº 2%**ï¼ˆtemperature=1ï¼‰ |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
#### è¡¨æ ¼æ‘˜è¦ï¼ˆä»¥ Qwen3 + EAGLE3 ä¸ºä¾‹ï¼ŒTemperature=0ï¼‰
| æ–¹æ³• | Avg Acc. | Speedup |
|------|----------|---------|
| EAGLE3 | 84.4% | 2.85Ã— |
| **EAGLE3 + TriSpec** | **84.2%** | **3.55Ã—** |

- åœ¨æ‰€æœ‰ benchmark ä¸Šï¼ŒTriSpec åœ¨å‡ ä¹ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå¸¦æ¥ **15%-35% çš„é¢å¤–åŠ é€Ÿå¢ç›Š**ã€‚
- ç›¸æ¯” SpecCascade ç±»æ–¹æ³•ï¼ŒTriSpec åœ¨ accuracy-speedup trade-off ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œé¿å…äº†å› è¿‡åº¦æ”¾æ¾éªŒè¯è€Œå¯¼è‡´çš„ä¸¥é‡ç²¾åº¦ä¸‹é™ã€‚

#### éªŒè¯æ•ˆç‡åˆ†æï¼ˆTable 3ï¼‰
åœ¨ MATH500 ä¸Šï¼ŒEAGLE3+TriSpec ç›¸æ¯” EAGLE3ï¼š
- $r_t$ ä» 21.38% â†’ **9.95%**
- æ¯è½®éªŒè¯æ—¶é—´ $t_v$ ä» 0.094s â†’ **0.071s**
- ç«¯åˆ°ç«¯å»¶è¿Ÿ $L$ æ˜¾è‘—é™ä½ï¼Œååé‡ä» 38.96 â†’ **49.74 Tok/s**

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰Draft Training ç­–ç•¥
| è®­ç»ƒæ–¹å¼ | Speedup (EAGLE3+TriSpec) |
|--------|--------------------------|
| Joint from-scratch | 3.49Ã— |
| Adapter-only finetune | 3.44Ã— |

- ä¸¤ç§ç­–ç•¥å‡èƒ½æœ‰æ•ˆåŠ é€Ÿï¼Œjoint training æ€§èƒ½ç•¥ä¼˜ï¼Œä½† adapter finetuning æ›´èŠ‚çœèµ„æºã€‚

#### ï¼ˆ2ï¼‰Token Pruning æœºåˆ¶
- å¯ç”¨ token pruning åï¼Œå¹³å‡ acceptance length æå‡ï¼Œè¯´æ˜ proxy çš„éƒ¨åˆ†éªŒè¯ç»“æœå¯é ï¼Œå…è®¸ target å®‰å…¨è·³è¿‡å†—ä½™è®¡ç®—ã€‚

#### ï¼ˆ3ï¼‰Routing Criteria å¯¹æ¯”ï¼ˆFigure 5bï¼‰
- æ¯”è¾ƒäº† top-1 æ¦‚ç‡ã€entropyã€marginã€R2R-router ç­‰ç­–ç•¥ã€‚
- **margin-based routing** åœ¨ accuracy-speedup æ›²çº¿ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œæˆä¸ºé»˜è®¤é€‰æ‹©ã€‚

#### ï¼ˆ4ï¼‰Draft Length å½±å“ï¼ˆFigure 5cï¼‰
- TriSpec çš„æœ€ä¼˜ draft length ä¸æ ‡å‡† SD ä¸åŒï¼Œè¡¨æ˜å…¶è·¯å¾„å†³ç­–å— proxy è¡Œä¸ºå½±å“ã€‚
- åœ¨ä¸åŒé•¿åº¦ä¸‹ TriSpec å§‹ç»ˆä¼˜äº baselineã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **Verification cost æ˜¯ SD åŠ é€Ÿçš„æ–°çªç ´å£**ï¼šå½“ drafting æˆæœ¬å·²æä½æ—¶ï¼Œä¼˜åŒ– $t_v$ æˆä¸ºç»§ç»­æå‡æ•ˆç‡çš„å…³é”®ã€‚
2. **Same-family smaller models æ˜¯ç†æƒ³çš„ proxy verifier**ï¼š
   - ä¸ target æ¨¡å‹å…·æœ‰é«˜åº¦ token-level alignmentï¼ˆé«˜è¾¾ 82% exact matchï¼‰ã€‚
   - è¾“å‡ºè´¨é‡å¯ä¿¡ï¼Œè½»å¾®åå·®ä¸å½±å“æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ã€‚
   - å…¶é¢„æµ‹ç½®ä¿¡åº¦ï¼ˆtop-1/top-2 marginï¼‰å¯æœ‰æ•ˆåŒºåˆ†â€œå¯ä¿¡â€ä¸â€œéœ€å‡çº§â€çš„éªŒè¯åœºæ™¯ã€‚
3. **TriSpec å®ç°é«˜æ•ˆä¸‰å…ƒååŒ**ï¼š
   - é€šè¿‡ margin-based routing åŠ¨æ€åˆ†é…éªŒè¯ä»»åŠ¡ã€‚
   - å¤§å¹…å‡å°‘ target model è°ƒç”¨ï¼ŒåŒæ—¶ç»´æŒæ¥è¿‘åŸç”Ÿ target çš„è¾“å‡ºè´¨é‡ã€‚
   - å¯ä¸ EAGLE-3 ç­‰å…ˆè¿›æ–¹æ³•ç»“åˆï¼Œå®ç°å åŠ åŠ é€Ÿã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–åŒä¸€å®¶æ—å­˜åœ¨ proxy æ¨¡å‹**ï¼šè‹¥æ— åˆé€‚è§„æ¨¡çš„ smaller modelï¼Œéš¾ä»¥éƒ¨ç½²ã€‚
- **margin threshold $\lambda$ éœ€è¦è°ƒå‚**ï¼šè™½ç„¶ $\lambda=0.5$ è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸åŒä»»åŠ¡æˆ–æ¨¡å‹ä¸Šå¯èƒ½éœ€è¦è°ƒæ•´ã€‚
- **å¯¹æç«¯é”™è¯¯å®¹å¿æœ‰é™**ï¼šè‹¥ proxy åœ¨å…³é”®å†³ç­–ç‚¹å‡ºç°ä½ç½®ä¿¡è¯¯åˆ¤ï¼Œä»å¯èƒ½å¯¼è‡´é”™è¯¯ä¼ æ’­ï¼ˆå°½ç®¡å®éªŒæ˜¾ç¤ºæ•´ä½“é²æ£’æ€§å¼ºï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´é€šç”¨çš„ proxy selection æœºåˆ¶ï¼Œä¸é™äºåŒä¸€å®¶æ—æ¨¡å‹ã€‚
- è®¾è®¡è‡ªé€‚åº” margin thresholdï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ä¿¡ä»»é˜ˆå€¼ã€‚
- å°† TriSpec æ€æƒ³æ‰©å±•è‡³å…¶ä»–ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘ï¼‰ä¸­çš„ speculative inferenceã€‚
- ç»“åˆ KV cache reuseã€layer-skipping ç­‰æŠ€æœ¯ï¼Œæ„å»ºç«¯åˆ°ç«¯æ›´é«˜æ•ˆçš„æ¨ç†æ ˆã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> TriSpec é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„ **same-family proxy verifier** å’ŒåŸºäº **margin çš„åŠ¨æ€è·¯ç”±æœºåˆ¶**ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ä¼˜åŒ–äº† speculative decoding ä¸­çš„ **verification å¼€é”€**ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹å®ç°äº†é«˜è¾¾ **35% çš„é¢å¤–åŠ é€Ÿ**ï¼Œå¹¶å‡å°‘äº† **è¶… 50% çš„ target model è°ƒç”¨**ï¼Œä¸º LLM é«˜æ•ˆæ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 2. [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)

**Authors**: Bailin Wang, Dan Friedman, Tao Lei, Chong Wang  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.22379v1  

#### Abstract
Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSPLA: Block Sparse Plus Linear Attention for Long Context Modeling

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰çš„ **block-wise sparse attention** æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š
1. **ä½é€‰æ‹©ä¿çœŸåº¦ï¼ˆLow Selection Fidelityï¼‰**ï¼šåŸºäºå‹ç¼©è¡¨ç¤ºçš„å—é€‰æ‹©æœºåˆ¶ï¼ˆå¦‚å‡å€¼æ± åŒ–ã€MLPï¼‰éš¾ä»¥å‡†ç¡®è¯†åˆ«çœŸæ­£ç›¸å…³çš„ KV blocksï¼Œå¯¼è‡´å…³é”®ä¿¡æ¯è¢«é—æ¼ã€‚
2. **ç´¯ç§¯ä¸Šä¸‹æ–‡æŸå¤±ï¼ˆCumulative Contextual Lossï¼‰**ï¼šé€šè¿‡å®Œå…¨ä¸¢å¼ƒæœªé€‰ä¸­çš„â€œé•¿å°¾â€blocksï¼Œéšç€åºåˆ—å¢é•¿ï¼Œè¿™äº›è¢«å¿½ç•¥çš„æ¦‚ç‡è´¨é‡é€æ¸ç´¯ç§¯ï¼Œé€ æˆä¸ dense attention çš„è¾“å‡ºä¸¥é‡åç¦»ï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡º **SPLA (Sparse Plus Linear Attention)**ï¼Œä¸€ç§ç»“åˆç²¾ç¡®ç¨€ç–æ³¨æ„åŠ›ä¸è¿‘ä¼¼çº¿æ€§æ³¨æ„åŠ›çš„æ–°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸Šä¸‹æ–‡ä¸¥æ ¼åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†è¿›è¡Œå¤„ç†ï¼š

- **Exact Componentï¼ˆç²¾ç¡®éƒ¨åˆ†ï¼‰**ï¼šä½¿ç”¨ principled çš„é€‰æ‹©æœºåˆ¶é€‰å‡ºæœ€é‡è¦çš„ blocksï¼Œå¹¶å¯¹å®ƒä»¬æ‰§è¡Œæ ‡å‡†çš„ exact attentionã€‚
- **Approximate Componentï¼ˆè¿‘ä¼¼éƒ¨åˆ†ï¼‰**ï¼šå°†æœªé€‰ä¸­çš„â€œé•¿å°¾â€blocks é€šè¿‡ **Residual Linear Attention (RLA)** å‹ç¼©ä¸ºä¸€ä¸ªç´§å‡‘çš„é€’å½’çŠ¶æ€ï¼Œè€Œéç›´æ¥ä¸¢å¼ƒã€‚

#### åˆ›æ–°ç‚¹è¯¦è§£ï¼š
1. **åŸºäºäºŒé˜¶æ³°å‹’å±•å¼€çš„å—é€‰æ‹©æœºåˆ¶ï¼ˆPrincipled Block Selectionï¼‰**
   - å°† block çš„é‡è¦æ€§å®šä¹‰ä¸ºå…¶å†…éƒ¨æ‰€æœ‰ token æ³¨æ„åŠ›å¾—åˆ†çš„ç§¯åˆ†ã€‚
   - æ¨å¯¼å‡ºä¸€ä¸ªåŸºäº **block-level mean å’Œ covariance ç»Ÿè®¡é‡** çš„ selection metricï¼Œæ¥æºäºå¯¹åŸå§‹ attention ç›®æ ‡çš„äºŒé˜¶æ³°å‹’è¿‘ä¼¼ã€‚
   - ç›¸æ¯”ä»…ç”¨å‡å€¼ï¼ˆfirst-orderï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°ä¼°è®¡ block è´¡çŒ®ï¼Œæå‡å¬å›ç‡ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚

2. **æ®‹å·®çº¿æ€§æ³¨æ„åŠ›ï¼ˆResidual Linear Attention, RLAï¼‰**
   - å¼•å…¥çº¿æ€§ attention æ¥é«˜æ•ˆå‹ç¼©æœªé€‰ blocks çš„ä¿¡æ¯ã€‚
   - å…³é”®è®¾è®¡ï¼šé‡‡ç”¨ **å‡æ³•å½¢å¼å®ç° RLA**ï¼š
     $$
     o_{\text{res}} = o_{\text{global}} - o_{\text{selected}}
     $$
     å…¶ä¸­ $ o_{\text{global}} $ æ˜¯å…¨å±€çº¿æ€§ attention è¾“å‡ºï¼Œ$ o_{\text{selected}} $ æ˜¯å·²åŠ è½½ blocks çš„å±€éƒ¨çº¿æ€§ attention è¾“å‡ºã€‚
   - è¿™æ ·å¯ä»¥åœ¨ä¸æ˜¾å¼è®¿é—®æœªé€‰ blocks çš„å‰æä¸‹å®Œæˆè®¡ç®—ï¼Œé¿å…äº†é¢å¤–çš„ I/O å¼€é”€ï¼Œä¿æŒäº† sparse decoding çš„æ•ˆç‡ä¼˜åŠ¿ã€‚

3. **ç»Ÿä¸€èåˆæ¶æ„**
   - æœ€ç»ˆè¾“å‡ºç”± exact sparse attention å’Œ RLA æ®‹å·®é¡¹åŠ æƒç»„åˆï¼š
     $$
     o = o_{\text{sparse}} + \text{RMSNorm}(o_{\text{res}})
     $$
   - ä»…å¼•å…¥å°‘é‡å¯å­¦ä¹ å‚æ•°ï¼ˆRMSNorm çš„ scale å‚æ•°ï¼‰ï¼Œä¾¿äºä»é¢„è®­ç»ƒ dense æ¨¡å‹è¿ç§»é€‚é…ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | æ˜¯å¦ä¿ç•™é•¿å°¾ | æ˜¯å¦æœ‰å†—ä½™è®¡ç®— | é€‰æ‹©æœºåˆ¶ |
|------|---------------|----------------|-----------|
| **InfLLM-v2 / Truncation** | âŒ å®Œå…¨ä¸¢å¼ƒ | âœ… æ—  | å¯å‘å¼ï¼ˆå¦‚å‡å€¼ï¼‰ |
| **NSA / Overlapping** | âœ… å‹ç¼©æ‰€æœ‰å— | âŒ åŒé‡è®¡æ•°ï¼ˆselected blocks è¢«é‡å¤å¤„ç†ï¼‰ | å¯å‘å¼æˆ–å­¦ä¹  |
| **SPLA (Ours)** | âœ… å‹ç¼©æœªé€‰å— | âœ… æ— ï¼ˆä¸¥æ ¼åˆ’åˆ†ï¼‰ | **Principledï¼ˆäºŒé˜¶æ³°å‹’ï¼‰** |

- **ç†è®ºæ›´ä¼˜**ï¼šä¸¥æ ¼åˆ’åˆ† + æ— æŸå‹ç¼© â†’ æ›´æ¥è¿‘ dense attention è¾“å‡ºã€‚
- **å®è·µæ›´å¼º**ï¼šåœ¨é•¿åºåˆ—ä¸Šæ€§èƒ½è¡°å‡æ˜¾è‘—ç¼“è§£ï¼Œå°¤å…¶åœ¨ >64k é•¿åº¦æ—¶åè¶… dense baselineã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒå›´ç»• **Continual Pretraining (CPT)** åœºæ™¯å±•å¼€ï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ç±»ä»»åŠ¡ï¼š

1. **General Knowledge**
   - æ•°æ®ï¼šå†…éƒ¨é«˜è´¨é‡æ··åˆæ•°æ®ï¼ˆæ•°å­¦ã€ä»£ç ã€é€šç”¨çŸ¥è¯†ï¼‰
   - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š32k
   - æµ‹è¯•é›†ï¼šARC-C/E, HellaSwag, LAMBADA, PIQA, SciQ, WinoGrande, TriviaQA, WebQS, MMLU, GSM8K

2. **Long-Context**
   - æ•°æ®ï¼šé•¿æ–‡æ¡£ + åˆæˆ QA å¯¹
   - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š128kï¼ˆè®­ç»ƒï¼‰ï¼Œæµ‹è¯•è‡³ 256k
   - æµ‹è¯•é›†ï¼š**RULER benchmark**

3. **Reasoning**
   - æ•°æ®ï¼šOpenReasoning Nemotron + OpenScienceReasoning
   - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š32k
   - æµ‹è¯•é›†ï¼šAIME 2024/2025, HMMT 2025, LiveCodeBench, HumanEval, GPQA, MMLU Pro

### å®éªŒè®¾ç½®
- **æ¨¡å‹è§„æ¨¡**ï¼š14B å‚æ•°ï¼ˆè¯¦è§ Table 1ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼šCloud TPU v6eï¼ˆ1024 chipsï¼‰
- **å®ç°æ¡†æ¶**ï¼šJAX
- **å¹¶è¡Œç­–ç•¥**ï¼šFSDP + Sequence Parallelism
- **ä¼˜åŒ–å™¨**ï¼šAdamW
- **è®­ç»ƒæµç¨‹**ï¼š
  - å…ˆåœ¨ 10T tokens ä¸Šé¢„è®­ç»ƒ dense æ¨¡å‹
  - å†åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œ CPT å¹¶é›†æˆ sparsification
- **ç¨€ç–é…ç½®**ï¼š
  - Block size $ B = 128 $
  - Stride $ s = 16 $, Window $ C = 32 $
  - Top-$k = 32$ blocks selected
  - å¼ºåˆ¶åŒ…å« initial blockï¼ˆattention sinkï¼‰å’Œæœ€è¿‘ 4 ä¸ª blocksï¼ˆlocal windowï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **DENSE** | Dense Attention | å…¨é‡ KV cache åŠ è½½ï¼Œæ€§èƒ½ä¸Šé™ |
| **NSA (Yuan et al., 2025)** | Sparse + Overlapping | å¤šåˆ†æ”¯ç»“æ„ï¼Œå­˜åœ¨åŒé‡è®¡æ•°é—®é¢˜ |
| **InfLLM-v2 (Zhao et al., 2025)** | Sparse + Truncation | ä»…ä¿ç•™ top-kï¼Œå…¶ä½™ç½®é›¶ |
| **SPA (Ablation)** | SPLA w/o RLA | ä½¿ç”¨ç›¸åŒ selection ä½†æ—  RLA æ¨¡å— |

> æ‰€æœ‰ baseline åœ¨ç»Ÿä¸€ä»£ç åº“ä¸­å®ç°ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### è¡¨1ï¼šé€šç”¨çŸ¥è¯†ä»»åŠ¡å¹³å‡è¡¨ç°ï¼ˆæ¥è‡ª Table 2ï¼‰
| Method | General Knowledge Avg | MMLU (5-shot) | GSM8K (8-shot) |
|--------|------------------------|----------------|------------------|
| DENSE | 67.5 | 77.3 | 90.2 |
| NSA | 66.5 | 77.4 | 90.5 |
| INF-V2 | 67.3 | 77.8 | 90.0 |
| SPA | 67.3 | 78.1 | 90.2 |
| **SPLA** | **67.7** | **78.6** | **90.5** |

âœ… SPLA åœ¨é€šç”¨çŸ¥è¯†å’Œæ¨ç†ä»»åŠ¡ä¸Šå…¨é¢ä¼˜äº dense å’Œå…¶ä»– sparse æ–¹æ³•ã€‚

---

#### è¡¨2ï¼šé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡è¡¨ç°ï¼ˆRULER, Table 3ï¼‰
| Model \ Length | 4k | 8k | 16k | 32k | 64k | 128k | **256k** |
|----------------|----|-----|-----|-----|-----|-------|---------|
| DENSE | 95.8 | 94.9 | 93.6 | 91.4 | 87.1 | 83.2 | **69.3** |
| NSA | 92.3 | 91.4 | 90.7 | 84.6 | 83.2 | 51.3 | 32.5 |
| INF-V2 | 95.9 | 94.1 | 92.1 | 89.3 | 86.7 | 61.6 | 42.6 |
| SPA | 95.9 | 94.2 | 93.3 | 90.4 | 87.1 | 62.4 | 44.5 |
| **SPLA** | **95.9** | **94.7** | **94.2** | **91.7** | **88.3** | **85.2** | **72.3** |

ğŸ”¥ **å…³é”®å‘ç°**ï¼š
- åœ¨ â‰¤64k æ—¶ï¼ŒSPLA ä¸ dense æ€§èƒ½ç›¸å½“ï¼›
- åœ¨ â‰¥128k æ—¶ï¼Œ**SPLA ä¸ä»…è¿œè¶…å…¶ä»– sparse æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº† dense baseline**ï¼›
- åœ¨ **256k** ä¸Šï¼ŒSPLA å¾—åˆ† **72.3** vs DENSE çš„ **69.3**ï¼Œé¦–æ¬¡å®ç°â€œè¶Šç¨€è¶Šå¼ºâ€ã€‚

---

#### è¡¨3ï¼šå¤æ‚æ¨ç†ä»»åŠ¡è¡¨ç°ï¼ˆTable 4ï¼‰
| Method | AIME 2024 | AIME 2025 | HMMT 2025 | LiveCodeBench | HumanEval | GPQA | MMLU Pro |
|--------|------------|------------|-------------|----------------|------------|--------|----------|
| DENSE | 77.1 | 73.9 | 60.0 | 61.6 | 85.4 | 68.5 | 78.9 |
| NSA | 73.1 | 64.2 | 53.3 | 49.1 | 78.7 | 59.6 | 68.8 |
| INF-V2 | 76.8 | 75.1 | 60.0 | 64.2 | 86.6 | 68.7 | 79.3 |
| SPA | 77.2 | 73.0 | 60.0 | 62.0 | 86.0 | 69.2 | 79.1 |
| **SPLA** | **78.3** | **75.1** | **63.3** | **62.4** | **86.6** | **69.5** | **79.3** |

âœ… SPLA åœ¨å¤šé¡¹é«˜éš¾åº¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä¼˜ï¼Œå°¤å…¶åœ¨ AIME å’Œ HMMT æ•°å­¦ç«èµ›é¢˜ä¸Šæ˜¾è‘—é¢†å…ˆã€‚

---

### æ¶ˆèå®éªŒç»“æœ
- **SPLA vs SPA**ï¼šä¸¤è€…å…±äº«ç›¸åŒçš„ selection æœºåˆ¶ï¼Œå”¯ä¸€åŒºåˆ«æ˜¯æ˜¯å¦å¯ç”¨ RLAã€‚
  - åœ¨ RULER 256k ä¸Šï¼ŒSPLA (72.3) > SPA (44.5)ï¼Œå·®è·è¾¾ **+27.8**ã€‚
  - ç»“è®ºï¼š**RLA æ¨¡å—å¯¹é•¿å°¾å‹ç¼©è‡³å…³é‡è¦**ï¼Œæ˜¯ SPLA æˆåŠŸçš„å…³é”®ã€‚
- **SPA vs INF-V2**ï¼šä¸¤è€…éƒ½åªåš truncationï¼Œä½† SPA ä½¿ç”¨æ›´ä¼˜çš„äºŒé˜¶ selectionã€‚
  - SPA åœ¨å¤šæ•°ä»»åŠ¡ä¸Šç•¥ä¼˜äº INF-V2ã€‚
  - ç»“è®ºï¼š**principled selection æœ¬èº«ä¹Ÿèƒ½å¸¦æ¥å¢ç›Š**ï¼Œä½†è¿œä¸å¦‚ RLA æ˜¾è‘—ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **â€œä¸¢å¼ƒâ€ä¸æ˜¯å‡ºè·¯ï¼Œâ€œå‹ç¼©â€æ‰æ˜¯å…³é”®**  
   å•çº¯ truncation æˆ– overlapping æ¶æ„æ— æ³•è§£å†³é•¿å°¾åˆ†å¸ƒå¸¦æ¥çš„æ€§èƒ½é€€åŒ–ã€‚SPLA é€šè¿‡ **residual linear attention** å®ç°æ— æŸå‹ç¼©ï¼Œåœ¨ä¸å¢åŠ  I/O çš„å‰æä¸‹ä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡ã€‚

2. **principled selection æ˜¾è‘—ä¼˜äº heuristic æ–¹æ³•**  
   åŸºäºäºŒé˜¶æ³°å‹’å±•å¼€çš„é€‰æ‹©æœºåˆ¶èƒ½æ›´å‡†ç¡®æ•æ‰ block é‡è¦æ€§ï¼Œå‡å°‘è¯¯åˆ é£é™©ï¼Œå°¤å…¶åˆ©äºå¤æ‚æ¨ç†ä»»åŠ¡ã€‚

3. **SPLA å¯æ— ç¼é€‚é…é¢„è®­ç»ƒ dense æ¨¡å‹**  
   ä»…éœ€æå°‘é‡æ–°å¢å‚æ•°ï¼ˆRMSNorm scalesï¼‰ï¼Œå³å¯å°† dense æ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆç¨€ç–ç‰ˆæœ¬ï¼Œé€‚åˆåœ¨ CPT é˜¶æ®µéƒ¨ç½²ã€‚

4. **åœ¨æé•¿åºåˆ—ä¸Šåè¶… dense attention**  
   åœ¨ RULER 256k ä¸Šï¼ŒSPLA æ€§èƒ½è¶…è¶Š dense baselineï¼Œè¡¨æ˜å…¶ä¸ä»…èƒ½é€¼è¿‘ denseï¼Œè¿˜èƒ½å› æ›´å¥½çš„é•¿ç¨‹å»ºæ¨¡è€Œè¶…è¶Šã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è®­ç»ƒæ•ˆç‡ç•¥ä½**ï¼šå½“å‰å®ç°ç›¸æ¯” dense attention æœ‰ä¸€å®šè®¡ç®—å¼€é”€ï¼ˆstep time 11s vs 9s at 16kï¼‰ï¼Œä¸»è¦æ¥è‡ª selection overhead å’Œè¾ƒå°çš„ GQA group sizeï¼ˆG=5ï¼‰é™åˆ¶å¹¶è¡Œã€‚
- **ä¾èµ– high-precision accumulation**ï¼šRLA çš„é€’å½’çŠ¶æ€éœ€ä»¥ float32 ç§¯ç´¯ä»¥ä¿è¯æ•°å€¼ç¨³å®šï¼Œå¯èƒ½å¢åŠ å†…å­˜å‹åŠ›ã€‚
- **ç›®å‰èšç„¦äº adaptation**ï¼šä»å¤´è®­ç»ƒï¼ˆscratch trainingï¼‰ä¸‹çš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **è§£è€¦ sparse ä¸ linear åˆ†æ”¯** çš„å‚æ•°åŒ–æ–¹å¼ï¼Œä¾‹å¦‚ä¸º RLA è®¾è®¡ä¸“ç”¨ projection æˆ– feature mapã€‚
- å°è¯•æ›´çµæ´»çš„ grouping ç­–ç•¥ï¼ˆå¦‚ multi-value head ç»“æ„ï¼‰ä»¥è¿›ä¸€æ­¥æå‡è®­ç»ƒæ•ˆç‡ã€‚
- å°† SPLA åº”ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ>100Bï¼‰å’ŒçœŸå®åº”ç”¨åœºæ™¯ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆã€æ–‡æ¡£æ‘˜è¦ç­‰ï¼‰ã€‚
- ç ”ç©¶å¦‚ä½•åŠ¨æ€è°ƒæ•´ sparsity budget $k$ ä»¥é€‚åº”ä¸åŒè¾“å…¥å¤æ‚åº¦ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **SPLA é€šè¿‡ principled selection + residual linear attentionï¼Œåœ¨å‡ ä¹ä¸å¢åŠ  I/O çš„å‰æä¸‹å®ç°äº†å¯¹â€œé•¿å°¾â€çš„æœ‰æ•ˆå‹ç¼©ï¼Œé¦–æ¬¡ä½¿ sparse attention åœ¨è¶…é•¿åºåˆ—ä¸Šå…¨é¢åª²ç¾ç”šè‡³è¶…è¶Š dense attentionï¼Œä¸ºä¸‹ä¸€ä»£é•¿ä¸Šä¸‹æ–‡ LLM æä¾›äº†ä¸€æ¡é«˜æ•ˆå¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚**

</details>

---

### 3. [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)

**Authors**: Julien Delavande, Regis Pierrard, Sasha Luccioni  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.22362v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šUnderstanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
éšç€ Large Language Modelsï¼ˆLLMsï¼‰ä»ç ”ç©¶é˜¶æ®µè¿›å…¥ç”Ÿäº§éƒ¨ç½²ï¼Œ**æ¨ç†é˜¶æ®µçš„èƒ½è€—å·²æˆä¸ºAIå¯æŒç»­æ€§çš„å…³é”®ç“¶é¢ˆ**ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å…³æ³¨è®­ç»ƒé˜¶æ®µçš„ç¢³è¶³è¿¹ï¼Œä½†å¯¹å®é™…éƒ¨ç½²ä¸­ç³»ç»Ÿçº§è®¾è®¡ï¼ˆå¦‚æ•°å€¼ç²¾åº¦ã€æ‰¹å¤„ç†ç­–ç•¥ã€è¯·æ±‚è°ƒåº¦ï¼‰å¦‚ä½•å½±å“èƒ½æ•ˆçš„ç ”ç©¶ä»ä¸è¶³ã€‚

æœ¬æ–‡èšç„¦äºï¼š  
> **åœ¨ç›¸åŒæ¨¡å‹ä¸‹ï¼Œç³»ç»Ÿå±‚é¢çš„è®¾è®¡é€‰æ‹©å¦‚ä½•å¯¼è‡´é«˜è¾¾æ•°é‡çº§å·®å¼‚çš„èƒ½æºæ¶ˆè€—ï¼Ÿ**

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§**ç›¸ä½æ„ŸçŸ¥ï¼ˆphase-awareï¼‰çš„ç»†ç²’åº¦èƒ½æ•ˆåˆ†ææ¡†æ¶**ï¼Œå°†LLMæ¨ç†åˆ†è§£ä¸ºä¸¤ä¸ªæ ¸å¿ƒé˜¶æ®µè¿›è¡Œç‹¬ç«‹å»ºæ¨¡ä¸ä¼˜åŒ–ï¼š

- **Prefill Phase**ï¼ˆé¢„å¡«å……ï¼‰ï¼šå¤„ç†å®Œæ•´è¾“å…¥promptï¼Œè®¡ç®—å¯†é›†å‹ï¼ˆcompute-boundï¼‰
- **Decode Phase**ï¼ˆè§£ç ï¼‰ï¼šè‡ªå›å½’ç”Ÿæˆè¾“å‡ºtokenï¼Œå†…å­˜å¯†é›†å‹ï¼ˆmemory-boundï¼‰

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†ä¸‰å¤§ç³»ç»Ÿçº§å› ç´ å¯¹èƒ½æ•ˆçš„å½±å“ï¼š
1. **Numerical Precisionï¼ˆé‡åŒ–ï¼‰**
2. **Batching Strategyï¼ˆæ‰¹å¤„ç†ï¼‰**
3. **Serving Configurationï¼ˆæœåŠ¡æ¶æ„ä¸è¯·æ±‚è°ƒåº¦ï¼‰**

ç‰¹åˆ«å¼ºè°ƒâ€œ**åˆ°è¾¾æ•´å½¢ï¼ˆarrival shapingï¼‰**â€â€”â€”å³é€šè¿‡æ§åˆ¶ç”¨æˆ·è¯·æ±‚çš„æ—¶é—´åˆ†å¸ƒæ¥æå‡æ‰¹å¤„ç†è´¨é‡ï¼Œä»è€Œæ˜¾è‘—é™ä½èƒ½è€—ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿåšæ³• | æœ¬æ–‡æ”¹è¿› |
|------|--------|---------|
| èƒ½è€—æµ‹é‡ç²’åº¦ | æ•´ä½“æ¨ç†å¹³å‡èƒ½è€— | åˆ†è§£ä¸º prefill / decode é˜¶æ®µï¼Œå®ç° phase-level profiling |
| é‡åŒ–æ•ˆæœç†è§£ | é»˜è®¤â€œä½ç²¾åº¦=æ›´èŠ‚èƒ½â€ | æ­ç¤ºå…¶ä»…åœ¨ compute-bound åœºæ™¯æœ‰æ•ˆï¼Œåœ¨ memory-bound ä¸‹å¯èƒ½é€‚å¾—å…¶å |
| æ‰¹å¤„ç†åˆ†æ | å¿½ç•¥paddingå¼€é”€ | åŒºåˆ† effective vs. computed tokensï¼Œæ­ç¤º padding å¯¹ prefill çš„è´Ÿé¢å½±å“ |
| è¯·æ±‚æ¨¡å¼å‡è®¾ | éšæœºåˆ°è¾¾ | å¼•å…¥ fixed inter-arrival delaysï¼ˆåˆ°è¾¾æ•´å½¢ï¼‰ï¼Œè¯æ˜å¯å¤§å¹…æå‡èƒ½æ•ˆ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- ä½¿ç”¨ä¸€ä¸ªåŒ¿åè¯„å®¡ä¸­çš„å­é›†æ•°æ®é›†ï¼ˆAnonymous & Anonymous, 2025ï¼‰ï¼Œæºè‡ª **UltraChat-200k** çš„ç²¾é€‰ç‰ˆæœ¬ `ultrachat_10k`ã€‚
- åŒ…å« **10,000 æ¡ç¤¼è²Œæ€§æç¤ºï¼ˆä»¥ "thank you" ç»“å°¾ï¼‰**
- è¾“å…¥é•¿åº¦èŒƒå›´ï¼š200â€“4000 tokensï¼ˆå‡å€¼çº¦ 1200ï¼‰
- è¾“å‡ºè¾ƒçŸ­ï¼š10â€“300 tokensï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯åœºæ™¯
- æ‰€æœ‰æç¤ºé€‚é…å„æ¨¡å‹çš„è¾“å…¥æ ¼å¼

### âš™ï¸ å®éªŒè®¾ç½®
#### ç¡¬ä»¶å¹³å°
- **GPU**: NVIDIA H100 SXMï¼ˆ80GBï¼‰
- **CPU**: 8æ ¸ AMD EPYC 7R13
- æ— å…¶ä»–å…±é©»ä»»åŠ¡ï¼Œç¡®ä¿æµ‹é‡çº¯å‡€

#### æ¨¡å‹é€‰æ‹©
æ¶µç›–ä¸»æµå¼€æºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼š
- **Qwen 2.5ç³»åˆ—**ï¼š0.5B, 1.5B, 3B, 7B, 14B
- **Mistral-7B-Instruct-v0.3**
- **LLaMA 3.1-8B-Instruct å’Œ 70B**

#### æ•°å€¼ç²¾åº¦é…ç½®ï¼ˆdtypeï¼‰
æµ‹è¯•äº”ç§æ ¼å¼ï¼š
- `float32`, `bfloat16`, `float16`
- `int8`ï¼ˆLLM.int8()ï¼Œoutlier-aware mixed precisionï¼‰
- `int4`ï¼ˆNF4 æ ¼å¼ + bitsandbytes é‡åŒ–ï¼‰

#### æ¨ç†å¼•æ“å¯¹æ¯”
- **Baseline**: Hugging Face `transformers` åº“ï¼ˆé™æ€æ‰¹å¤„ç†ï¼‰
- **Optimized**: Hugging Face **Text Generation Inference (TGI)** æœåŠ¡å™¨ v3.3.4ï¼ˆæ”¯æŒ continuous batchingã€kernel fusionã€paged attentionï¼‰

#### æµ‹é‡å·¥å…·
- **èƒ½æºç›‘æ§**ï¼šCodeCarbon + NVMLï¼ˆGPUï¼‰ã€pyRAPLï¼ˆCPUï¼‰ï¼ŒRAMèƒ½è€—é€šè¿‡å¯å‘å¼ä¼°ç®—
- **å»¶è¿Ÿè®°å½•**ï¼šCUDA kernel çº§åˆ«æ—¶é—´æˆ³
- æ¯ä¸ªé…ç½®é‡å¤è¿è¡Œ10æ¬¡ï¼Œå–å¹³å‡å€¼å‡å°‘æ–¹å·®

#### è¯·æ±‚åˆ°è¾¾æ¨¡å¼ï¼ˆç”¨äºTGIå®éªŒï¼‰
| ç±»å‹ | æè¿° |
|------|------|
| Random delays | â–³ ~ Uniform(k, l)ï¼Œæ¨¡æ‹Ÿéšæœºæµé‡ |
| Fixed intervals | å›ºå®šé—´éš”å‘é€è¯·æ±‚ï¼ˆå¦‚æ¯50msã€300msã€500msï¼‰|

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ”¢ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **Prefill èƒ½è€—é™å¹…ï¼ˆå¤§æ¨¡å‹ï¼‰** | float32 â†’ bfloat16/int8 å¯è¾¾ **4Ã— èƒ½æºèŠ‚çœ**ï¼ˆå› Tensor CoreåŠ é€Ÿï¼‰ |
| **Decode é˜¶æ®µé‡åŒ–å½±å“** | int8 æ¯” float32 **å¤šè€— 2â€“3Ã— èƒ½æº**ï¼›int4 æ— æ˜æ˜¾ä¼˜åŠ¿ |
| **æœ€ä¼˜ batch sizeï¼ˆdecodeï¼‰** | åœ¨ LLaMA-8B ä¸Šä¸º **b=4**ï¼Œæ›´å¤§æ‰¹æ¬¡æ”¶ç›Šé€’å‡ |
| **èƒ½é‡/æœ‰æ•ˆè¾“å…¥token æœ€å°ç‚¹** | å‡ºç°åœ¨ **b=2**ï¼Œæƒè¡¡ prefill padding ä¸ decode å¹¶è¡Œå¢ç›Š |
| **TGI ç›¸æ¯” transformers èƒ½è€—ä¸‹é™** | è¾¾åˆ° **12.5Ã— æ›´ä½èƒ½è€—**ï¼ˆLLaMA-8Bï¼‰ |
| **å›ºå®šåˆ°è¾¾é—´éš” vs éšæœº** | åœ¨ TGI ä¸­è¿›ä¸€æ­¥é™ä½è‡³ **åŸå§‹ baseline çš„ 1/100**ï¼ˆå³ 100Ã— èŠ‚èƒ½ï¼‰ |
| **LLaMA-70B å¤šå¡æ‰©å±•è¡¨ç°** | åŒæ ·è·å¾—æ˜¾è‘—èŠ‚èƒ½ï¼ŒéªŒè¯å¯æ‰©å±•æ€§ |

### ğŸ“Š ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ

| è®¾ç½® | å¹³å‡æ¯è¯·æ±‚ GPU èƒ½è€—ï¼ˆWhï¼‰ | ç›¸å¯¹æ”¹å–„ |
|------|--------------------------|----------|
| Baseline: float32 + transformersï¼ˆé¡ºåºå¤„ç†ï¼‰ | 1.2 Ã— 10â»Â¹ Wh | Ã—1ï¼ˆåŸºå‡†ï¼‰ |
| Optimized: bfloat16 + TGI + éšæœºåˆ°è¾¾ | 9.6 Ã— 10â»Â³ Wh | **12.5Ã— æ”¹å–„** |
| Fully Optimized: + å›ºå®šåˆ°è¾¾ï¼ˆ500msï¼‰ | 1.1 Ã— 10â»Â³ Wh | **>100Ã— æ”¹å–„** |

> ğŸ’¡ è¿™æ„å‘³ç€ï¼šæ¯å¤©å¤„ç†ç™¾ä¸‡è¯·æ±‚æ—¶ï¼Œæ€»èƒ½è€—ä» **120 kWh/day**ï¼ˆç›¸å½“äº10æˆ·æ³•å›½å®¶åº­æ—¥ç”¨ç”µï¼‰é™è‡³ **1.1 kWh/day**

### ğŸ” æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æ•°å€¼ç²¾åº¦æ¶ˆèï¼ˆè§ Figure 1ï¼‰
- **Prefill**ï¼šå¤§æ¨¡å‹ï¼ˆ>7Bï¼‰åœ¨ä½ç²¾åº¦ä¸‹æ˜¾è‘—èŠ‚èƒ½ï¼ˆå¾—ç›ŠäºTensor Coreï¼‰
- **Small models (<1.5B)**ï¼šä¿æŒ memory-boundï¼Œfloat16 åè€Œè½»å¾®å¢åŠ èƒ½è€—ï¼ˆkernel overheadï¼‰
- **Decode**ï¼šæ‰€æœ‰æ¨¡å‹å‡ memory-boundï¼Œä½ç²¾åº¦å‡ ä¹æ— ç›Šï¼›**int8/int4 å›  dequantization kernels å¯¼è‡´æ›´é«˜èƒ½è€—**

#### ï¼ˆ2ï¼‰Batch Size æ¶ˆèï¼ˆè§ Figure 2ï¼‰
- **æŒ‰ input token å½’ä¸€åŒ–ï¼ˆå«paddingï¼‰**
  - Prefillï¼šéš batch å¢å¤§è€Œä¸Šå‡ï¼ˆpaddingæµªè´¹ï¼‰
  - Decodeï¼šUå‹æ›²çº¿ï¼Œæœ€ä½³åœ¨ b=4
- **æŒ‰ output token å½’ä¸€åŒ–ï¼ˆæ— paddingï¼‰**
  - æ‰€æœ‰é˜¶æ®µæŒç»­æ”¹å–„ï¼Œlogarithmic ä¸‹é™è¶‹åŠ¿
  - è¡¨æ˜ batching æœ¬è´¨æœ‰ç›Šï¼Œä½†éœ€é¿å… padding æ±¡æŸ“

#### ï¼ˆ3ï¼‰Serving Stack æ¶ˆèï¼ˆè§ Figure 3ï¼‰
- åˆ‡æ¢åˆ° TGI â†’ è‡ªåŠ¨å®ç°è¿ç»­æ‰¹å¤„ç† â†’ èƒ½è€—éª¤é™
- åŠ å…¥ arrival shapingï¼ˆå›ºå®šé—´éš”ï¼‰â†’ æé«˜ batch consistency â†’ GPU idle time â†“ â†’ èƒ½æ•ˆå†è·ƒå‡

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **é‡åŒ–å¹¶éæ€»æ˜¯èŠ‚èƒ½**  
   - ä»…åœ¨ **compute-bound åœºæ™¯ï¼ˆå¦‚ prefill å¤§æ¨¡å‹ï¼‰** æœ‰æ•ˆ
   - åœ¨ **memory-bound åœºæ™¯ï¼ˆå¦‚ decodeï¼‰**ï¼Œint8/int4 å›  dequantization å¼€é”€åè€Œæ›´è€—ç”µ

2. **æ‰¹å¤„ç†æ˜¯èƒ½æ•ˆæ ¸å¿ƒæ æ†**  
   - æ˜¾è‘—é™ä½å•ä½ token èƒ½è€—ï¼Œå°¤å…¶åœ¨ decode é˜¶æ®µ
   - ä½† **padding æ˜¯ prefill çš„ä¸»è¦æ•ˆç‡æ€æ‰‹**ï¼Œéœ€ç»“åˆ bucketing æˆ– shaped batching ç¼“è§£

3. **æœåŠ¡æ ˆè®¾è®¡å†³å®šå¯æŒç»­æ€§ä¸Šé™**  
   - å•çº¯æ›´æ¢æ¨¡å‹æˆ–ç¡¬ä»¶ä¸å¦‚ä¼˜åŒ– serving infrastructure
   - **TGI + continuous batching + kernel fusion** å¯å¸¦æ¥æ•°é‡çº§æ”¹è¿›

4. **è¯·æ±‚åˆ°è¾¾æ¨¡å¼è‡³å…³é‡è¦ï¼ˆarrival shapingï¼‰**  
   - ç”¨æˆ·ä¾§è½»é‡çº§è°ƒåº¦ï¼ˆå¦‚å›ºå®šå»¶è¿Ÿï¼‰å³å¯æå¤§æå‡ batching è´¨é‡
   - **per-request energy å¯é™ä½è¾¾ 100Ã—**

5. **å¿…é¡»è¿›è¡Œ phase-aware energy profiling**  
   - Prefill ä¸ decode å…·æœ‰æ ¹æœ¬ä¸åŒçš„è®¡ç®—ç‰¹å¾
   - æŠ¥å‘Šæ•´ä½“èƒ½è€—ä¼šæ©ç›–å…³é”®ç“¶é¢ˆï¼Œåº”åˆ†åˆ«æµ‹é‡ä¸ä¼˜åŒ–

---

### âš ï¸ å±€é™æ€§

| é™åˆ¶ | è¯´æ˜ |
|------|------|
| Promptå¤šæ ·æ€§æœ‰é™ | å½“å‰ä½¿ç”¨çŸ­promptï¼Œæœªè¦†ç›–é•¿ä¸Šä¸‹æ–‡æˆ–å¤šè½®å¯¹è¯ |
| ç¡¬ä»¶å•ä¸€ | ä»…åŸºäº NVIDIA H100ï¼Œç»“æœåœ¨ AMD/TPU/AWS Inferentia ä¸Šå¯èƒ½ä¸åŒ |
| èƒ½è€—æµ‹é‡ç²’åº¦ | ä¸»è¦å…³æ³¨ GPUï¼Œæœªå®Œå…¨è®¡å…¥ CPUã€å†…å­˜ä¼ è¾“ã€ç½‘ç»œI/Oç­‰ç³»ç»Ÿçº§å¼€é”€ |
| å¤šèŠ‚ç‚¹æ‰©å±•æœªæµ‹è¯• | å½“å‰å®éªŒé™äºå•å¡æˆ–4å¡å†…ï¼Œåˆ†å¸ƒå¼è°ƒåº¦æœªæ¶‰åŠ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **è·¨å¹³å°èƒ½æ•ˆå»ºæ¨¡**  
   å°†è¯¥ phase-aware åˆ†ææ¨å¹¿è‡³ TPUã€Inferentiaã€ç§»åŠ¨è®¾å¤‡ç­‰å¼‚æ„å¹³å°

2. **åŠ¨æ€ arrival shaping æœºåˆ¶**  
   è®¾è®¡å®¢æˆ·ç«¯ SDK æˆ–ä»£ç†å±‚ï¼Œä¸»åŠ¨è°ƒèŠ‚è¯·æ±‚èŠ‚å¥ä»¥åŒ¹é…åç«¯è´Ÿè½½

3. **è”åˆä¼˜åŒ–æ¡†æ¶**  
   æ„å»º end-to-end pipelineï¼ŒååŒä¼˜åŒ– quantizationã€batching policyã€traffic scheduling

4. **æ ‡å‡†åŒ–èƒ½æ•ˆæŠ¥å‘Šåè®®**  
   æ¨åŠ¨ç¤¾åŒºé‡‡ç”¨ phase-separatedã€token-normalizedã€request-pattern-aware çš„ energy reporting standardï¼ˆç±»ä¼¼ MLPerf Powerï¼‰

5. **ç»¿è‰²APIè®¾è®¡åŸåˆ™**  
   æå‡ºâ€œå¯æŒç»­LLM APIâ€çš„è®¾è®¡æŒ‡å—ï¼Œä¾‹å¦‚æ¨è batch-friendly æ¥å£ã€é¼“åŠ±éå®æ—¶è°ƒç”¨ç­‰

---

## æ€»ç»“è¯­

> â€œ**Sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack.**â€

æœ¬æ–‡é¢ è¦†äº†â€œæ¨¡å‹è¶Šå°è¶ŠèŠ‚èƒ½â€çš„ç®€å•è®¤çŸ¥ï¼ŒæŒ‡å‡ºï¼š  
âœ… **ç³»ç»Ÿçº§ä¼˜åŒ–ï¼ˆå¦‚ batchingã€serving æ¶æ„ã€è¯·æ±‚è°ƒåº¦ï¼‰å¯ä»¥åœ¨ä¸ä¿®æ”¹æ¨¡å‹çš„å‰æä¸‹ï¼Œå®ç°è¶…è¿‡100å€çš„èƒ½æ•ˆæå‡ã€‚**

è¿™ä¸ºæ„å»ºçœŸæ­£ç»¿è‰²ã€å¯æ‰©å±•çš„AIæœåŠ¡ä½“ç³»æä¾›äº†å®è¯åŸºç¡€å’Œå·¥ç¨‹æŒ‡å¯¼ã€‚

</details>

---

### 4. [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)

**Authors**: Chuxue Cao, Jinluan Yang, Haoran Li, Kunhao Pan, Zijian Zhao, Zhengyu Chen, Yuchen Tian, Lijun Wu, Conghui He, Sirui Han, Yike Guo  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.22642v1  

#### Abstract
Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves form...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶åŸºäºæ¦‚ç‡çš„ç”Ÿæˆæœºåˆ¶ï¼Œåœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°**é€»è¾‘ä¸ä¸€è‡´**ï¼ˆlogical inconsistencyï¼‰å’Œ**å¥–åŠ±æ¬ºéª—**ï¼ˆreward hackingï¼‰ç°è±¡ã€‚å³ä½¿æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ï¼Œä¸­é—´æ¨ç†æ­¥éª¤ä¹Ÿå¯èƒ½å­˜åœ¨ä¸¥é‡é”™è¯¯ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚çº¯è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNatural Language Reasoningï¼‰ç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„æœ‰æ•ˆéªŒè¯æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½é€šè¿‡è¡¨é¢æ¨¡å¼â€œçŒœâ€å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè€Œéå»ºç«‹ä¸¥è°¨çš„é€»è¾‘é“¾æ¡ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**å½¢å¼åŒ–é€»è¾‘éªŒè¯å¼•å¯¼çš„æ¨ç†æ¡†æ¶**ï¼ˆFormal Logic Verification-Guided Reasoningï¼‰ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š

- **åŠ¨æ€äº¤é”™éªŒè¯**ï¼ˆDynamic Interleavingï¼‰ï¼šå°†å½¢å¼åŒ–é€»è¾‘éªŒè¯ï¼ˆå¦‚ SMT æ±‚è§£å™¨ã€z3-solverï¼‰å®æ—¶åµŒå…¥åˆ°è‡ªç„¶è¯­è¨€ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå½¢æˆâ€œç”Ÿæˆ-éªŒè¯-ä¿®æ­£â€çš„é—­ç¯ï¼Œè€Œéä¼ ç»Ÿçš„åéªŒè¿‡æ»¤ï¼ˆpost-hoc filteringï¼‰ã€‚
- **ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶**ï¼š
  1. **ç›‘ç£å¾®è°ƒ**ï¼ˆSFTï¼‰ï¼šé€šè¿‡ä¸€ä¸ªåˆ†å±‚çš„æ•°æ®åˆæˆç®¡é“ï¼Œç”ŸæˆåŒ…å«è‡ªç„¶è¯­è¨€æ¨ç†ã€å½¢å¼åŒ–è¯æ˜å’Œæ‰§è¡Œåé¦ˆçš„é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚
  2. **ç­–ç•¥ä¼˜åŒ–**ï¼ˆPolicy Optimizationï¼‰ï¼šé‡‡ç”¨ **Group Relative Policy Optimization (GRPO)**ï¼Œç»“åˆå¤åˆå¥–åŠ±å‡½æ•°ï¼Œå¯¹é€»è¾‘æ­£ç¡®æ€§ã€ç»“æ„å®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡è¿›è¡Œè”åˆä¼˜åŒ–ã€‚
- **æ‰§è¡Œé©±åŠ¨çš„éªŒè¯æœºåˆ¶**ï¼šåœ¨æ•°æ®åˆæˆé˜¶æ®µå¼•å…¥æ‰§è¡Œç»“æœéªŒè¯ï¼ˆexecution-based validationï¼‰ï¼Œç¡®ä¿è‡ªç„¶è¯­è¨€ã€å½¢å¼åŒ–ä»£ç å’Œæ‰§è¡Œè¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæå‡æ•°æ®è´¨é‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ä¸»åŠ¨çº é”™**ï¼šèƒ½å®æ—¶æ£€æµ‹å¹¶çº æ­£ä¸­é—´æ¨ç†è°¬è¯¯ï¼Œé˜²æ­¢é”™è¯¯ä¼ æ’­ã€‚
- **è·¨é¢†åŸŸé€‚ç”¨**ï¼šä¸é™äºç‰¹å®šé¢†åŸŸï¼ˆå¦‚çº¯æ•°å­¦å®šç†è¯æ˜ï¼‰ï¼Œå¯æ¨å¹¿è‡³ä¸€èˆ¬é€»è¾‘å’Œå¸¸è¯†æ¨ç†ã€‚
- **é«˜å¯é æ€§**ï¼šåˆ©ç”¨å½¢å¼åŒ–å·¥å…·æä¾›æœºå™¨å¯éªŒè¯çš„åé¦ˆï¼Œé¿å… LLM-as-a-judge çš„ä¸»è§‚åå·®ã€‚
- **æ•°æ®é«˜æ•ˆ**ï¼šä»…ç”¨çº¦ 17k æ ·æœ¬å³è¾¾åˆ° SOTA æ€§èƒ½ï¼Œè¿œå°‘äºä¾èµ–å¤§è§„æ¨¡æ•°æ®çš„æ–¹æ³•ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
- **SFT æ•°æ®é›†**ï¼š
  - WebInstruct-Verified
  - K&K
  - NuminaMath-TIR
- **RL æ•°æ®é›†**ï¼šä»ä¸Šè¿°æ•°æ®é›†ä¸­ç­›é€‰æ•™å¸ˆæ¨¡å‹ï¼ˆDeepSeek-R1ï¼‰é€šè¿‡ç‡ä½äº 50% çš„éš¾é¢˜ç»„æˆï¼Œå…± 3,525 ä¸ªæ ·æœ¬ã€‚
- **è¯„ä¼°åŸºå‡†**ï¼ˆå…­å¤§è¯„æµ‹é›†ï¼‰ï¼š
  - **é€»è¾‘æ¨ç†**ï¼šKOR-Benchã€BBH
  - **æ•°å­¦æ¨ç†**ï¼šMATH-500ã€AIME 2024
  - **é€šç”¨æ¨ç†**ï¼šGPQA-Diamondã€TheoremQA

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹æ¶æ„**ï¼šåŸºäº Qwen2.5-7B å’Œ Qwen2.5-14B è¿›è¡Œå¾®è°ƒã€‚
- **è®­ç»ƒæµç¨‹**ï¼š
  - **SFT é˜¶æ®µ**ï¼šå­¦ä¹ ç‡ 1e-5ï¼Œcosine è°ƒåº¦ï¼Œè®­ç»ƒ 3 è½®ã€‚
  - **RL é˜¶æ®µ**ï¼šä½¿ç”¨ verl æ¡†æ¶ï¼ŒGRPO ç®—æ³•ï¼Œæ¯è½®ç”Ÿæˆ 8 æ¡è½¨è¿¹ï¼ŒKL ç³»æ•° 0.05ï¼Œclip ratio 0.3ï¼Œè®­ç»ƒ 120 æ­¥ã€‚
- **è¯„ä¼°æ–¹å¼**ï¼š
  - ä½¿ç”¨ OpenCompass å¹³å°è¿›è¡Œè¯„æµ‹ã€‚
  - é™¤ AIME24 æŠ¥å‘Š avg@16 å¤–ï¼Œå…¶ä½™å‡é‡‡ç”¨è´ªå©ªè§£ç ï¼ˆgreedy decodingï¼‰ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼ŒæŒ‰é¢†åŸŸå®å¹³å‡ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | åŸºçº¿æ¨¡å‹ |
|------|----------|
| åŸºç¡€æ¨¡å‹ | Qwen2.5-7B/14B |
| å¼ºåŒ–å­¦ä¹ æ¨¡å‹ | SimpleRL-Zoo, General-Reasoner, RLPR |
| é€»è¾‘å¢å¼ºæ¨¡å‹ | SynLogic-7B |
| å·¥å…·é›†æˆæ¨¡å‹ | ZeroTIR, SimpleTIR |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æ¨¡å‹ | KOR-Bench | BBH | MATH-500 | AIME24 | TheoremQA | General AVG |
|------|-----------|-----|----------|--------|------------|-------------|
| **Qwen2.5-7B-FLV-RL (Ours)** | **51.0** | **70.0** | **78.6** | **20.8** | **55.7** | **51.9** |
| **Qwen2.5-14B-FLV-RL (Ours)** | **57.0** | **78.0** | **81.4** | **30.2** | **63.5** | **58.6** |

- åœ¨ 7B å’Œ 14B å°ºå¯¸ä¸Šï¼Œç›¸æ¯”å½“å‰æœ€ä¼˜åŸºçº¿ï¼ˆSOTAï¼‰ï¼Œå¹³å‡æ€§èƒ½åˆ†åˆ«æå‡äº† **10.4%** å’Œ **14.2%**ã€‚
- åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ AIME24 ä¸Šï¼Œ14B æ¨¡å‹è¾¾åˆ° **30.2%**ï¼Œå‡ ä¹æ˜¯ General-Reasonerï¼ˆ17.5%ï¼‰çš„ä¸¤å€ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **è¶…è¶Šæ‰€æœ‰åŸºçº¿**ï¼šFLV-SFT å³ä½¿åœ¨ SFT é˜¶æ®µå°±å·²è¶…è¿‡å¤šæ•° RL åŸºçº¿ï¼›FLV-RL åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚
- **åœ¨é€»è¾‘å¯†é›†å‹ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾**ï¼š
  - KOR-Bench ä¸Šæ¯” SimpleTIR é«˜å‡º 14 åˆ†ï¼ˆ51.0 vs 37.0ï¼‰ã€‚
  - TheoremQA ä¸Šæ¯”æœ€è¿‘ç«äº‰è€…é«˜å‡ºè¶… 8 åˆ†ã€‚
- **å·¥å…·ä½¿ç”¨èŒƒå¼è½¬å˜**ï¼šåˆ†ææ˜¾ç¤ºï¼ŒFLV-RL æ›´å¤šåœ°è°ƒç”¨ **Symbolic & Logic** ç±»åº“ï¼ˆå¦‚ z3ã€sympyï¼‰ï¼Œå æ¯”è¾¾ **62.5%**ï¼Œè€Œ SimpleTIR ä»…ä¸º 42.5%ï¼Œè¡¨æ˜å…¶ä»â€œè®¡ç®—æ±‚è§£â€è½¬å‘â€œç¬¦å·æ¨ç†â€ã€‚

### æ¶ˆèå®éªŒç»“æœ
| æ–¹æ³• | General AVG | KOR-Bench | TheoremQA |
|------|------------|-----------|------------|
| Natural-SFT | 36.5 | 30.4 | 41.2 |
| FLV-SFT (Ours) | **49.8** | **48.0** | **53.0** |
| Natural-RL | 37.0 | 35.7 | 41.2 |
| FLV-RL (Ours) | **51.9** | **51.0** | **55.7** |

- **å½¢å¼åŒ–éªŒè¯çš„ä½œç”¨**ï¼šFLV-SFT ç›¸æ¯” Natural-SFT æå‡æ˜¾è‘—ï¼ˆ+13.3%ï¼‰ï¼Œå°¤å…¶åœ¨é€»è¾‘ä»»åŠ¡ä¸Šï¼ˆ+16.2%ï¼‰ï¼Œè¯´æ˜å½¢å¼åŒ–éªŒè¯èƒ½æœ‰æ•ˆæå‡æ¨ç†ä¸€è‡´æ€§ã€‚
- **å¤šé˜¶æ®µè®­ç»ƒçš„ä»·å€¼**ï¼šFLV-RL åœ¨ FLV-SFT åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡ï¼Œè€Œ Natural-RL å‡ ä¹æ— å¢ç›Šï¼Œè¡¨æ˜å½¢å¼åŒ–éªŒè¯ä¸º RL æä¾›äº†æ›´ç¨³å®šå¯é çš„å¥–åŠ±ä¿¡å·ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **å½¢å¼åŒ–éªŒè¯æ˜¯æå‡ LLM æ¨ç†èƒ½åŠ›çš„å…³é”®å‚¬åŒ–å‰‚**ï¼šé€šè¿‡å®æ—¶åé¦ˆï¼Œèƒ½æœ‰æ•ˆæŠ‘åˆ¶ reward hackingï¼Œæ¨åŠ¨æ¨¡å‹å‘å±•å‡ºçœŸæ­£ä¸¥è°¨çš„æ¨ç†èƒ½åŠ›ã€‚
2. **äº¤é”™å¼éªŒè¯ä¼˜äºåéªŒè¿‡æ»¤**ï¼šåŠ¨æ€ä»‹å…¥ç”Ÿæˆè¿‡ç¨‹å¯åœ¨é”™è¯¯å‘ç”Ÿæ—¶ç«‹å³çº æ­£ï¼Œé¿å…é”™è¯¯ç´¯ç§¯ã€‚
3. **ç¬¦å·æ¨ç†ä¼˜äºæš´åŠ›è®¡ç®—**ï¼šæ¨¡å‹å­¦ä¼šä½¿ç”¨ z3 ç­‰å·¥å…·è¿›è¡ŒæŠ½è±¡æ¨ç†å’Œåä¾‹æœç´¢ï¼Œè€Œéä¾èµ–ç©·ä¸¾æˆ–æ•°å€¼æ¨¡æ‹Ÿã€‚
4. **æ•°æ®æ•ˆç‡é«˜**ï¼šå°‘é‡é«˜è´¨é‡ã€ç»è¿‡éªŒè¯çš„æ•°æ®å³å¯å¸¦æ¥å·¨å¤§æ€§èƒ½æå‡ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **è®¡ç®—å¼€é”€å¤§**ï¼šé›†æˆå®æ—¶å½¢å¼åŒ–éªŒè¯ä½¿ RL è®­ç»ƒæ—¶é—´å¢åŠ çº¦ 2 å€ã€‚
2. **è‡ªåŠ¨å½¢å¼åŒ–ï¼ˆautoformalizationï¼‰ç“¶é¢ˆ**ï¼šå°†æ¨¡ç³Šæˆ–å¸¸è¯†æ€§æè¿°è½¬åŒ–ä¸ºå½¢å¼åŒ–è¡¨è¾¾ä»å…·æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´æ˜ å°„é”™è¯¯ã€‚
3. **å¯¹å¼€æ”¾åŸŸä»»åŠ¡æ³›åŒ–æœ‰é™**ï¼šç›®å‰ä¸»è¦é€‚ç”¨äºç»“æ„åŒ–è¾ƒå¼ºçš„æ•°å­¦ã€é€»è¾‘ç­‰é¢†åŸŸï¼Œå¯¹é«˜åº¦å¼€æ”¾çš„é—®é¢˜æ”¯æŒè¾ƒå¼±ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘æ›´é²æ£’çš„ **autoformalization** æŠ€æœ¯ï¼Œé™ä½è‡ªç„¶è¯­è¨€åˆ°å½¢å¼è¯­è¨€çš„è½¬æ¢è¯¯å·®ã€‚
- æ¢ç´¢è½»é‡åŒ–éªŒè¯æœºåˆ¶ï¼Œå‡å°‘è®¡ç®—å»¶è¿Ÿï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
- å°†è¯¥æ¡†æ¶æ‰©å±•è‡³æ›´å¤šé¢†åŸŸï¼Œå¦‚æ³•å¾‹ã€åŒ»å­¦ç­‰éœ€è¦ä¸¥æ ¼é€»è¾‘ä¿è¯çš„åœºæ™¯ã€‚
- ç ”ç©¶å¦‚ä½•è®©æ¨¡å‹è‡ªä¸»å†³å®šä½•æ—¶å¯ç”¨å½¢å¼åŒ–éªŒè¯ï¼Œå®ç°æ›´æ™ºèƒ½çš„â€œè®¤çŸ¥æ§åˆ¶â€ã€‚

> **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºçš„ FLV æ¡†æ¶æˆåŠŸåœ°å°†ç¥ç»ç½‘ç»œçš„æµç•…æ€§ä¸ç¬¦å·ç³»ç»Ÿçš„ä¸¥è°¨æ€§ç›¸ç»“åˆï¼Œä¸ºæ„å»ºå¯ä¿¡ã€å¯é ã€å¯è§£é‡Šçš„é«˜çº§æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚å…¶å®éªŒç»“æœå¼ºæœ‰åŠ›åœ°è¯æ˜äº†**å½¢å¼åŒ–éªŒè¯å¯ä»¥ä½œä¸ºå¯æ‰©å±•æœºåˆ¶ï¼Œæ˜¾è‘—çªç ´å½“å‰ LLM æ¨ç†æ€§èƒ½çš„è¾¹ç•Œ**ã€‚

</details>

---

### 5. [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)

**Authors**: Yuan Li, Jun Hu, Bryan Hooi, Bingsheng He, Cheng Chen  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.22949v1  

#### Abstract
Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šAutonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰åŸºäºå›¾çš„æ¬ºè¯ˆæ£€æµ‹ï¼ˆGraph-based fraud detectionï¼‰åœ¨å¤„ç†**æ–‡æœ¬å±æ€§å›¾ï¼ˆText-Attributed Graphs, TAGsï¼‰**æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
1. **ç¼ºä¹è‡ªä¸»æ¨ç†èƒ½åŠ›**ï¼šç°æœ‰ LLM-enhanced GNN æ–¹æ³•ä¾èµ–é¢„å®šä¹‰æç¤ºï¼ˆpredefined promptingï¼‰ï¼Œé™åˆ¶äº† LLM è¿›è¡Œå¤šè·³ã€è·¨é‚»åŸŸçš„æ·±å±‚æ¨ç†ï¼Œä»…èƒ½è¿›è¡Œæµ…å±‚æ¨¡å¼åŒ¹é…ã€‚
2. **è¯­ä¹‰-ç»“æ„å¯¹é½å¼±åŒ–**ï¼šå¤šæ•°æ–¹æ³•é‡‡ç”¨è§£è€¦è®­ç»ƒï¼ˆdecoupled trainingï¼‰ï¼Œå³å…ˆç”¨ LLM å¤„ç†æ–‡æœ¬ï¼Œå†ç”¨ GNN å­¦ä¹ ç»“æ„ï¼Œå¯¼è‡´è¯­ä¹‰è¡¨ç¤ºä¸å›¾ç»“æ„ä¹‹é—´ç¼ºä¹è”åˆä¼˜åŒ–ï¼Œå‰Šå¼±äº†è”åˆå»ºæ¨¡èƒ½åŠ›ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ï¼šFraudCoT**
ä½œè€…æå‡ºäº† **FraudCoT**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæœºåˆ¶æå‡æ¬ºè¯ˆæ£€æµ‹æ€§èƒ½ï¼š

#### âœ… **(1) æ¬ºè¯ˆæ„ŸçŸ¥çš„é€‰æ‹©æ€§ CoT è’¸é¦ï¼ˆFraud-Aware Selective CoT Distillationï¼‰**
- åˆ©ç”¨æ•™å¸ˆ LLMï¼ˆå¦‚ DeepSeek-R1ï¼‰å¯¹æ ‡æ³¨èŠ‚ç‚¹åŠå…¶é‚»å±…ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼ˆChain-of-Thought, CoTï¼‰ï¼ŒåŒ…æ‹¬æ­£ç¡®å’Œé”™è¯¯è·¯å¾„ã€‚
- è®¾è®¡**æ­£è´Ÿè’¸é¦ç›®æ ‡**ï¼š
  - æ­£æ ·æœ¬ï¼šæ•™å¸ˆæ¨ç†ä¸çœŸå®æ ‡ç­¾ä¸€è‡´ â†’ é¼“åŠ±å­¦ç”Ÿæ¨¡ä»¿ï¼ˆCross-Entropy Lossï¼‰
  - è´Ÿæ ·æœ¬ï¼šä¸ä¸€è‡´ â†’ é¼“åŠ±å­¦ç”ŸæŠ‘åˆ¶ï¼ˆUnlikelihood Lossï¼‰
- å­¦ç”Ÿ LLM ç»è¿‡è’¸é¦åï¼Œèƒ½ç”Ÿæˆæ›´é²æ£’ã€æŠ—åè§çš„æ¨ç†è·¯å¾„ï¼Œå¹¶å°†å…¶åµŒå…¥èŠ‚ç‚¹æ–‡æœ¬ä¸­ï¼Œå½¢æˆ **CoT-augmented è¡¨ç¤º**ã€‚

#### âœ… **(2) é«˜æ•ˆçš„éå¯¹ç§°ååŒè®­ç»ƒï¼ˆEfficient Asymmetric Co-trainingï¼‰**
- ä¸ºå®ç°ç«¯åˆ°ç«¯ä¼˜åŒ–åŒæ—¶é¿å…é«˜æ˜‚è®¡ç®—æˆæœ¬ï¼Œæå‡º**éå¯¹ç§°ç¼–ç ç­–ç•¥**ï¼š
  - **ç›®æ ‡èŠ‚ç‚¹**ï¼šæ¯è½®è®­ç»ƒéƒ½ç”¨ LLM åŠ¨æ€ç¼–ç å…¶ CoT å¢å¼ºæ–‡æœ¬ã€‚
  - **é‚»å±…èŠ‚ç‚¹**ï¼šä½¿ç”¨åˆå§‹å‚æ•°ä¸€æ¬¡æ€§ç¼–ç å¹¶ç¼“å­˜ï¼ˆcacheï¼‰ï¼Œåç»­å›ºå®šä¸å˜ã€‚
- æ˜¾è‘—é™ä½ LLM æ¨ç†å¤æ‚åº¦ä» $O(|N(v)|)$ åˆ° $O(1)$ æ¯ä¸ªç›®æ ‡èŠ‚ç‚¹ï¼Œå®ç°é«˜è¾¾ **1,066Ã— çš„è®­ç»ƒåååŠ é€Ÿ**ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ TAPE, FLAGï¼‰ | FraudCoT |
|------|----------------------------|---------|
| **æ¨ç†æ–¹å¼** | é¢„å®šä¹‰æ¨¡æ¿å¼•å¯¼ï¼Œæµ…å±‚ç‰¹å¾æå– | è‡ªä¸»ã€è‡ªç”±å½¢å¼çš„å¤šæ­¥ CoT æ¨ç† |
| **è®­ç»ƒèŒƒå¼** | è§£è€¦è®­ç»ƒï¼ˆdecoupledï¼‰ | ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼ˆend-to-endï¼‰ |
| **è¯­ä¹‰-ç»“æ„å¯¹é½** | å¼±ï¼Œåˆ†ç¦»å­¦ä¹  | å¼ºï¼Œæ¢¯åº¦éšå¼è°ƒåˆ¶ç»“æ„ä¿¡æ¯ |
| **æ•ˆç‡** | é«˜ï¼ˆå› è§£è€¦ï¼‰ | åŒæ ·é«˜æ•ˆï¼Œä¸”ä¿æŒç«¯åˆ°ç«¯ä¼˜åŠ¿ |
| **å¯è§£é‡Šæ€§** | æœ‰é™ | æ¯ä¸ªé¢„æµ‹é™„å¸¦äººç±»å¯è¯»çš„æ¨ç†è·¯å¾„ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
| æ•°æ®é›† | ç±»å‹ | #Nodes | #Edges | ç‰¹ç‚¹ |
|-------|------|--------|--------|------|
| **InstantVideo** | å…¬å…±ï¼ˆAmazon Reviewsï¼‰ | ~37K | ~9.9M | ç”¨æˆ·è¯„è®ºæ˜¯å¦â€œæœ‰å¸®åŠ©â€åˆ†ç±»ï¼Œæ„å»ºå¼‚æ„å›¾ï¼ˆR-U-R, R-P-R, R-S-Rï¼‰ |
| **DigitalMusic** | å…¬å…±ï¼ˆAmazon Reviewsï¼‰ | ~65K | ~7.7M | åŒä¸Šï¼ŒéŸ³ä¹å“ç±»è¯„è®º |
| **PromotionAbuse** | å·¥ä¸šç§æœ‰ï¼ˆByteDance æä¾›ï¼‰ | ~371K | ~1.4M | çœŸå®ä¿ƒé”€æ»¥ç”¨åœºæ™¯ï¼Œæ£€æµ‹æ¶æ„åˆ·å•è¡Œä¸º |

> æ‰€æœ‰æ•°æ®é›†å‡æ„é€ æˆåŒ…å«ä¸‰ç§è¾¹ç±»å‹çš„å¼‚æ„å›¾ã€‚

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹éª¨å¹²**ï¼šä½¿ç”¨ Qwen3-8B ä½œä¸º LLM ä¸»å¹²ï¼ŒGNN ä½¿ç”¨ Heterogeneous GraphSAGEã€‚
- **å¾®è°ƒæŠ€æœ¯**ï¼šæ‰€æœ‰ LLM å¾®è°ƒå‡é‡‡ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰ã€‚
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œå­¦ä¹ ç‡ç½‘æ ¼æœç´¢ã€‚
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼šNVIDIA A100 GPUï¼ˆ80GBï¼‰ï¼ŒLinux ç³»ç»Ÿã€‚
- **è®­ç»ƒç­–ç•¥**ï¼šStage 1 å…ˆè¿›è¡Œ CoT è’¸é¦ï¼›Stage 2 è¿›è¡Œéå¯¹ç§°ååŒè®­ç»ƒã€‚

### **è¯„ä¼°æŒ‡æ ‡**
- **Macro-F1**ï¼šç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„å¹³å‡ F1 åˆ†æ•°
- **AUROC**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯
- **AUPRC**ï¼šç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯ï¼ˆå°¤å…¶é€‚ç”¨äºé«˜åº¦ä¸å¹³è¡¡ä»»åŠ¡ï¼‰

> æŠ¥å‘Š 5 æ¬¡éšæœºç§å­çš„å‡å€¼ Â± æ ‡å‡†å·®ï¼Œæ˜¾è‘—æ€§æ£€éªŒä½¿ç”¨é…å¯¹ t-testï¼ˆp < 0.05ï¼‰ã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
åˆ†ä¸ºå››ç±»ï¼š
1. **çº¯ GNN æ–¹æ³•**ï¼šGraphSAGE, HGT, ConsisGAD, PMP, GAAP
2. **æ— å›¾æ¨¡å‹ï¼ˆGraph-agnosticï¼‰**ï¼šMLP, LLM, LLM-SFT
3. **å›¾å¢å¼º LLMï¼ˆGraph-enhanced LLMsï¼‰**ï¼šLLaGA, GraphGPT, HiGPT, InstructGLM
4. **LLM-enhanced GNNs**ï¼šTAPE, FLAGï¼ˆä»£è¡¨å½“å‰ SOTAï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 3ï¼‰**
FraudCoT åœ¨æ‰€æœ‰æ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šå‡å–å¾—æœ€ä¼˜è¡¨ç°ï¼Œå°¤å…¶åœ¨å·¥ä¸šæ•°æ®é›† **PromotionAbuse** ä¸Šä¼˜åŠ¿æ˜¾è‘—ï¼š

| æ–¹æ³• | InstantVideo (AUPRC) | DigitalMusic (AUPRC) | PromotionAbuse (AUPRC) |
|------|------------------------|------------------------|--------------------------|
| **TAPE** | 79.28% | 81.99% | 67.21% |
| **FLAG** | 41.75% | 38.59% | 42.17% |
| **FraudCoT (Ours)** | **84.10%** (+4.82%) | **84.49%** (+2.50%) | **73.65%** (+6.44%) |

> æœ€é«˜æå‡è¾¾ **8.8% AUPRC**ï¼ˆç›¸å¯¹æå‡çº¦ 13%ï¼‰ï¼Œä¸”ç»Ÿè®¡æ˜¾è‘—ï¼ˆ*p < 0.05*ï¼‰ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- **ä¼˜äºçº¯ GNN å’Œ LLM æ–¹æ³•**ï¼šè¯´æ˜èåˆè¯­ä¹‰ä¸ç»“æ„çš„é‡è¦æ€§ã€‚
- **ä¼˜äºå›¾å¢å¼º LLM**ï¼šè¡¨æ˜ç®€å•æ³¨å…¥ç»“æ„å…ˆéªŒä¸è¶³ä»¥æ•æ‰ç»†ç²’åº¦æ¬ºè¯ˆæ¨¡å¼ã€‚
- **ä¼˜äº LLM-enhanced GNNï¼ˆå¦‚ TAPE/FLAGï¼‰**ï¼š
  - å› å…¶å—é™äºé¢„å®šä¹‰æç¤ºï¼Œæ— æ³•è¿›è¡Œæ·±å±‚æ¨ç†ï¼›
  - è§£è€¦è®­ç»ƒå¯¼è‡´è¯­ä¹‰-ç»“æ„è„±èŠ‚ã€‚

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**
ç§»é™¤ä¸åŒç»„ä»¶åçš„æ€§èƒ½ä¸‹é™æƒ…å†µï¼ˆFigure 4ï¼‰ï¼š

| å˜ä½“ | æè¿° | æ€§èƒ½å½±å“ |
|------|------|--------|
| **w/o FASCD** | ç§»é™¤ CoT è’¸é¦é˜¶æ®µ | ä¸‹é™æœ€ä¸¥é‡ï¼Œå°¤å…¶åœ¨ AUPRC ä¸Šï¼Œè¯´æ˜ CoT å¯¹æ•è·è·¨é‚»åŸŸä¾èµ–è‡³å…³é‡è¦ |
| **w/o EAC** | å†»ç»“ LLM ç¼–ç å™¨ï¼Œç¦ç”¨ååŒè®­ç»ƒ | æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ŒéªŒè¯äº†ç«¯åˆ°ç«¯ä¼˜åŒ–å¯¹è¯­ä¹‰-ç»“æ„å¯¹é½çš„å…³é”®ä½œç”¨ |
| **w/o NegDis** | ä¸æŠ‘åˆ¶é”™è¯¯æ¨ç†è·¯å¾„ | æ€§èƒ½ä¸‹é™ï¼Œè¯´æ˜è´Ÿæ ·æœ¬è’¸é¦æœ‰åŠ©äºæé«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ› |

> ç»“æœè¡¨æ˜ï¼š**ä¸‰ä¸ªç»„ä»¶ç›¸äº’å¢å¼ºï¼Œç¼ºä¸€ä¸å¯**ã€‚

### **å…¶ä»–åˆ†æ**
- **è’¸é¦æ ·æœ¬æ•°é‡å½±å“ï¼ˆFigure 5ï¼‰**ï¼šçº¦ 100 ä¸ªè’¸é¦æ ·æœ¬å³å¯é¥±å’Œæ€§èƒ½ï¼Œè¯´æ˜å°‘é‡é«˜è´¨é‡ CoT å³å¯æœ‰æ•ˆè¿ç§»æ¨ç†èƒ½åŠ›ã€‚
- **è´Ÿè’¸é¦æƒé‡ Î» å½±å“ï¼ˆFigure 6ï¼‰**ï¼šå½“ Î» = 100 æ—¶æ•ˆæœæœ€ä½³ï¼Œè¯´æ˜**å¼ºåŠ›æŠ‘åˆ¶é”™è¯¯æ¨ç†è·¯å¾„**å¯¹æ¬ºè¯ˆæ£€æµ‹è‡³å…³é‡è¦ã€‚
- **è®­ç»ƒæ•ˆç‡ï¼ˆTable 5ï¼‰**ï¼š
  - **DigitalMusic ä¸Šè®­ç»ƒé€Ÿåº¦æå‡ 1,066Ã—**
  - **æœ€å¤§ batch size æå‡ 128Ã—**
  - ç›¸æ¯” naive joint trainingï¼Œè®¡ç®—å¼€é”€å¤§å¹…é™ä½ï¼Œä½†ä»ä¿æŒç«¯åˆ°ç«¯ä¼˜åŠ¿ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **è‡ªä¸»æ¨ç†ä¼˜äºé¢„å®šä¹‰æç¤º**ï¼š  
   è‡ªç”±å½¢å¼çš„ CoT æ¨ç†èƒ½å¤Ÿæ­ç¤ºéšè—çš„å¤šè·³æ¬ºè¯ˆæ¨¡å¼ï¼ˆå¦‚â€œåŒä¸€ç”¨æˆ·å‘å¸ƒæç«¯è¯„åˆ†â€ã€â€œè™šå‡å¥½è¯„ä¸å¤§é‡å·®è¯„å…±å­˜â€ï¼‰ï¼Œè€Œé¢„å®šä¹‰æ¨¡æ¿åªèƒ½æå–è¡¨é¢ç‰¹å¾ã€‚

2. **æ­£è´Ÿ CoT è’¸é¦æ˜¯å…³é”®æœºåˆ¶**ï¼š  
   ä¸ä»…è¦å­¦â€œæ€ä¹ˆæƒ³å¯¹â€ï¼Œè¿˜è¦å­¦â€œæ€ä¹ˆæƒ³é”™â€ï¼Œä»è€Œé¿å…ç»§æ‰¿æ•™å¸ˆ LLM çš„åè§å’Œå¹»è§‰ã€‚

3. **ç«¯åˆ°ç«¯ä¼˜åŒ– + é«˜æ•ˆè®¾è®¡å¯è¡Œä¸”å¿…è¦**ï¼š  
   éå¯¹ç§°ååŒè®­ç»ƒæ‰“ç ´äº†â€œç«¯åˆ°ç«¯=ä½æ•ˆâ€çš„å›ºæœ‰è®¤çŸ¥ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†å·¥ä¸šçº§å¯æ‰©å±•æ€§ã€‚

4. **è¯­ä¹‰ä¸ç»“æ„å¿…é¡»è”åˆä¼˜åŒ–**ï¼š  
   è§£è€¦è®­ç»ƒè™½å¿«ï¼Œä½†ç‰ºç‰²äº†å…³é”®çš„äº¤äº’ä¿¡å·ï¼›FraudCoT è¯æ˜äº†äºŒè€…å¯ä»¥å…¼å¾—ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–æ•™å¸ˆ LLM ç”Ÿæˆ CoT**ï¼šè™½ç„¶è’¸é¦åå­¦ç”Ÿè½»é‡ï¼Œä½†ç¬¬ä¸€é˜¶æ®µä»éœ€è°ƒç”¨å¤§æ¨¡å‹ï¼Œæˆæœ¬è¾ƒé«˜ã€‚
- **CoT è´¨é‡å—æç¤ºå·¥ç¨‹å½±å“**ï¼šå°½ç®¡å¼ºè°ƒâ€œè‡ªç”±å½¢å¼â€ï¼Œä½†ä»éœ€ç²¾å¿ƒè®¾è®¡ prompt æ¨¡æ¿ä»¥å¼•å¯¼æœ‰æ•ˆæ¨ç†ã€‚
- **é™æ€ç¼“å­˜å‡è®¾**ï¼šé‚»å±…åµŒå…¥è¢«ç¼“å­˜åä¸å†æ›´æ–°ï¼Œå¯èƒ½å¿½ç•¥æŸäº›åŠ¨æ€å…³ç³»æ¼”åŒ–ï¼ˆä½†åœ¨æ¬ºè¯ˆæ£€æµ‹ä¸­å˜åŒ–è¾ƒæ…¢ï¼Œå½±å“è¾ƒå°ï¼‰ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- å°† CoT è’¸é¦æ‰©å±•è‡³æ›´å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è™šå‡æ–°é—»æ£€æµ‹ã€é‡‘èåæ´—é’±ï¼‰ã€‚
- æ¢ç´¢è‡ªåŠ¨ prompt ç”Ÿæˆæˆ–å¼ºåŒ–å­¦ä¹ æ¥è¿›ä¸€æ­¥æå‡æ¨ç†è´¨é‡ã€‚
- åŠ¨æ€ç¼“å­˜æœºåˆ¶ï¼šå®šæœŸæ›´æ–°é‚»å±…è¡¨ç¤ºä»¥é€‚åº”å›¾æ¼”åŒ–ã€‚
- å¤šæ¨¡æ€æ¬ºè¯ˆæ£€æµ‹ï¼šç»“åˆå›¾åƒã€è¡Œä¸ºåºåˆ—ç­‰å¤šæºä¿¡æ¯è¿›è¡Œè”åˆæ¨ç†ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **FraudCoT é€šè¿‡â€œé€‰æ‹©æ€§ CoT è’¸é¦ + éå¯¹ç§°ååŒè®­ç»ƒâ€ï¼Œé¦–æ¬¡å®ç°äº†é«˜æ•ˆã€è‡ªä¸»ã€ç«¯åˆ°ç«¯çš„ LLM-GNN æ¬ºè¯ˆæ£€æµ‹æ¡†æ¶ï¼Œåœ¨æ€§èƒ½ä¸æ•ˆç‡ä¸Šå…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºå·¥ä¸šçº§åº”ç”¨æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 6. [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)

**Authors**: Bo Yuan, Yun Zhou, Zhichao Xu, Kiran Ramnath, Aosong Feng, Balasubramaniam Srinivasan  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.22305v1  

#### Abstract
Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow ge...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿ **agentic workflow** çš„è®¾è®¡é«˜åº¦ä¾èµ–äººå·¥ä¸“å®¶è¿›è¡Œåå¤è¯•é”™ï¼Œå¯¼è‡´å¼€å‘æˆæœ¬é«˜ã€å¯æ‰©å±•æ€§å·®ã€‚ç°æœ‰çš„è‡ªåŠ¨å·¥ä½œæµç”Ÿæˆæ–¹æ³•å¤§å¤šå°†è¯¥ä»»åŠ¡å»ºæ¨¡ä¸º**ä¼˜åŒ–é—®é¢˜**ï¼ˆå¦‚åŸºäºæœç´¢çš„ç­–ç•¥ï¼‰ï¼Œç¼ºä¹ç†è®ºåŸºç¡€ï¼Œä¸”é€šå¸¸åªè¾“å‡ºå•ä¸€æœ€ä¼˜è§£ï¼Œå¤šæ ·æ€§ä¸è¶³ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- å¦‚ä½•åœ¨æ— éœ€è®­ç»ƒçš„å‰æä¸‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ **workflow**ï¼›
- å¦‚ä½•åœ¨ä»…æœ‰ç»ˆç«¯å¥–åŠ±ï¼ˆterminal rewardï¼‰çš„æƒ…å†µä¸‹æœ‰æ•ˆå¼•å¯¼å‰ç¼€å†³ç­–ï¼›
- å¦‚ä½•ä¸ºè‡ªåŠ¨ workflow ç”Ÿæˆæä¾›**ç†è®ºä¿è¯**ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡º **Bayesian Workflow Generation (BWG)**ï¼Œå°† workflow ç”Ÿæˆè§†ä¸ºä¸€ä¸ª**è´å¶æ–¯åéªŒé‡‡æ ·é—®é¢˜**ï¼Œè€Œéä¼ ç»Ÿçš„ä¼˜åŒ–é—®é¢˜ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å°†æ¯ä¸ª workflow è§†ä¸ºä»ä¸€ä¸ªèƒ½é‡æ¨¡å‹å®šä¹‰çš„éå½’ä¸€åŒ–åéªŒåˆ†å¸ƒä¸­é‡‡æ ·çš„è½¨è¿¹ï¼š
  $$
  q(s_{1:T}) \propto p(s_{1:T}) \exp(R(s_{1:T}))
  $$
  å…¶ä¸­ï¼š
  - $p(s_{1:T})$ æ˜¯ç”± meta optimizer LLM å®šä¹‰çš„å…ˆéªŒï¼›
  - $R(s_{1:T})$ æ˜¯ä»»åŠ¡ç›¸å…³çš„å¥–åŠ±å‡½æ•°ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ï¼›
  - $\exp(R)$ æ„æˆèƒ½é‡é¡¹ï¼Œæå‡é«˜å¥–åŠ±è·¯å¾„çš„æ¦‚ç‡ã€‚

åœ¨æ­¤æ¡†æ¶ä¸‹ï¼Œå®ä¾‹åŒ–ç®—æ³• **BayesFlow**ï¼Œå…¶ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯æ¨¡å—ï¼š

1. **Parallel Look-ahead Rolloutsï¼ˆå¹¶è¡Œå‰ç» rolloutï¼‰**
   - åœ¨æ¯ä¸€æ­¥å¯¹å½“å‰éƒ¨åˆ† workflow è¿›è¡Œ K æ¬¡éšæœºè¡¥å…¨ï¼Œä¼°è®¡å…¶â€œä¸‹æ¸¸ä»·å€¼â€ï¼›
   - åˆ©ç”¨é‡è¦æ€§æƒé‡è¿›è¡Œé‡é‡‡æ ·ï¼Œç¼“è§£çº¯å…ˆéªŒé‡‡æ ·çš„é€€åŒ–é—®é¢˜ï¼›
   - ä¸ä¾èµ–é¢å¤–è®­ç»ƒçš„ä»·å€¼æ¨¡å‹æˆ–é—­æºå¼ºæ¨¡å‹ï¼Œè®¡ç®—é«˜æ•ˆä¸”å¯å¹¶è¡Œã€‚

2. **Sequential In-loop Refinementï¼ˆé¡ºåºå¾ªç¯å†…ç²¾ç‚¼ï¼‰**
   - å¼•å…¥å…¨å±€ç²¾ç‚¼æœºåˆ¶ï¼ˆrefinerï¼‰ï¼ŒåŸºäºå·²æœ‰å®Œæ•´ workflows è¿›è¡Œæ”¹è¿›ï¼ˆå¦‚ MCTS æˆ–æ–‡æœ¬æ¢¯åº¦ï¼‰ï¼›
   - å¯ä¿®å¤æ—©æœŸé”™è¯¯ï¼Œå®ç°è·¨æ­¥é•¿çš„å…¨å±€ä¼˜åŒ–ï¼›
   - ç»Ÿä¸€äº†å¤šç§ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ AFlowã€ADASï¼‰ä½œä¸ºç‰¹ä¾‹ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | BayesFlow | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ AFlow, ADASï¼‰ |
|------|----------|-----------------------------|
| **ç†è®ºåŸºç¡€** | âœ… æä¾›æ¸è¿‘æ”¶æ•›åˆ°ç›®æ ‡åéªŒçš„è¯æ˜ï¼ˆTheorem 1ï¼‰ | âŒ å¤šæ•°æ— ä¸¥æ ¼ç†è®ºä¿éšœ |
| **å¤šæ ·æ€§** | âœ… è‡ªç„¶äº§ç”Ÿå¤šæ ·åŒ–é«˜è´¨é‡ workflows | âŒ å€¾å‘äºå•ä¸€æœ€ä¼˜è§£ |
| **æ•ˆç‡** | âœ… æ›´ä½ token å¼€é”€ï¼Œæ”¯æŒå®Œå…¨å¹¶è¡ŒåŒ– | âŒ å¤šä¸ºä¸²è¡Œæœç´¢ï¼Œè€—æ—¶é«˜ |
| **çµæ´»æ€§** | âœ… è½»é‡æ¥å£ï¼Œä¸é™å®šé¢„è®¾æ¨¡å—ï¼ˆå¦‚ REVISE, ENSEMBLEï¼‰ | âŒ ä¾èµ–å›ºå®šæ¨¡æ¿ |
| **é€šç”¨æ€§** | âœ… æ”¯æŒ open-source å’Œ closed-source LLM | âœ… |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å…±å…­ä¸ª benchmark æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰å¤§é¢†åŸŸï¼š

| ç±»åˆ« | æ•°æ®é›† | æè¿° |
|------|--------|------|
| æ•°å­¦æ¨ç† | **MATH**, **GSM8K** | å¤æ‚æ•°å­¦åº”ç”¨é¢˜æ±‚è§£ |
| é—®ç­” | **HotpotQA**, **DROP** | å¤šè·³æ¨ç†ä¸ç¦»æ•£æ¨ç† |
| ä¸“ä¸šçŸ¥è¯† | **GPQA**, **MMLU-Pro** | é«˜é˜¶ç§‘å­¦çŸ¥è¯†ä¸å¤šä»»åŠ¡ç†è§£ |

æ­¤å¤–è¿˜æµ‹è¯•äº†ä»£ç ç”Ÿæˆä»»åŠ¡ **MBPP**ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹é…ç½®
- **Meta Optimizer LLM**: `Claude 3.5-Sonnet`ï¼ˆç”¨äºç”Ÿæˆ workflowï¼‰
- **Executor LLM**: `Claude 3.7-Sonnet` æˆ– `Qwen2.5-7B-Instruct`ï¼ˆæ‰§è¡Œ workflow å¹¶è¯„åˆ†ï¼‰
- æ‰€æœ‰æ¨¡å‹é€šè¿‡ API æ¥å…¥ï¼Œæ¸©åº¦è®¾ä¸º 0ï¼ˆé™¤ç‰¹å®šæ­¥éª¤å¤–ï¼‰

#### è¯„ä¼°æŒ‡æ ‡
| æ•°æ®é›† | æŒ‡æ ‡ |
|-------|------|
| GSM8K, MATH | Solve Rateï¼ˆæ­£ç¡®ç‡ï¼‰ |
| HotpotQA, DROP | F1 Score |
| GPQA, MMLU-Pro | Accuracyï¼ˆé€‰é¡¹åŒ¹é…ï¼‰ |
| MBPP | Pass@1 |

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Prompting Baselines**:
  - Zero-shot (IO)
  - Chain-of-Thought (CoT)
  - CoT with Self-Consistency (CoT-SC)
- **Automatic Workflow Methods**:
  - **ADAS**ï¼ˆçº¿æ€§å¯å‘å¼æœç´¢ï¼‰
  - **AFlow**ï¼ˆMonte Carlo Tree Searchï¼‰
  - **MaAS**ï¼ˆè¿›åŒ–æœç´¢ï¼‰

æ‰€æœ‰åŸºçº¿å°½å¯èƒ½å¤ç°åŸè®ºæ–‡è®¾ç½®ï¼Œå¹¶å…±äº«ç›¸åŒéªŒè¯é›†ç”¨äºåé¦ˆã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| æ–¹æ³• | å¹³å‡å‡†ç¡®ç‡ï¼ˆClaude æ‰§è¡Œå™¨ï¼‰ | å¹³å‡å‡†ç¡®ç‡ï¼ˆQwen æ‰§è¡Œå™¨ï¼‰ |
|------|----------------------------|----------------------------|
| AFlow | 76.2% | 59.9% |
| **BayesFlow (Ours)** | **80.8%** (+4.6pp) | **63.4%** (+3.5pp) |

#### å„æ•°æ®é›†è¡¨ç°äº®ç‚¹ï¼š
- **MATH**: **69.4%** vs AFlow çš„ 60.1% â†’ **+9.3pp**
- **GPQA**: **69.2%** vs 82.3% â†’ **+6.9pp**
- **DROP**: **90.8%** vs 89.2% â†’ **+1.6pp**
- **MBPP (Pass@1)**: **85.0%** vs AFlow çš„ 75.5% â†’ **+9.5pp**

> ğŸ’¡ BayesFlow åœ¨æœ€éš¾çš„æ•°å­¦ä¸ä¸“ä¸šçŸ¥è¯†ä»»åŠ¡ä¸Šä¼˜åŠ¿æœ€æ˜¾è‘—ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨æ‰€æœ‰å…­å¤§æ•°æ®é›†ä¸­ï¼ŒBayesFlow å‡è¾¾åˆ°**æœ€é«˜å¹³å‡æ€§èƒ½**ï¼›
- ç›¸æ¯” SOTA workflow æ–¹æ³•ï¼ˆAFlowï¼‰ï¼Œå¹³å‡æå‡ **4.6â€“9.0 pp**ï¼›
- ç›¸æ¯” zero-shot promptingï¼Œæœ€å¤§æå‡è¾¾ **65 pp**ï¼›
- å³ä½¿åœ¨å°æ¨¡å‹ï¼ˆQwen-7Bï¼‰ä¸Šä»ä¿æŒé¢†å…ˆï¼Œæ˜¾ç¤ºè‰¯å¥½æ³›åŒ–æ€§ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰ä¸åŒ Nï¼ˆæ¯è½® partial workflows æ•°é‡ï¼‰çš„å½±å“ï¼ˆTable 3ï¼‰
| N | MATH | HotpotQA | DROP | ... |
|----|-------|-----------|------|
| 5 | 70.4Â±1.9 | 77.6Â±0.1 | 90.8Â±0.2 |
| 10 | 69.4Â±4.1 | 77.5Â±0.7 | 90.8Â±1.5 |
| 15 | 71.1Â±0.9 | 77.0Â±0.7 | 90.7Â±0.3 |

âœ… ç»“æœè¡¨æ˜ BayesFlow å¯¹è¶…å‚æ•°é€‰æ‹©é²æ£’ï¼Œæ¢ç´¢-åˆ©ç”¨å¹³è¡¡çµæ´»ã€‚

#### ï¼ˆ2ï¼‰Token æ•ˆç‡å¯¹æ¯”ï¼ˆTable 2ï¼‰
| æ–¹æ³• | è¾“å…¥ tokensï¼ˆDROPï¼‰ | è¾“å‡º tokens | F1 |
|------|--------------------|-------------|-----|
| AFlow | 114M | 19.7M | 0.755 |
| **BayesFlow** | **26.6M** | **9.4M** | **0.765** |

âœ… BayesFlow æ˜¾è‘—æ›´**token-efficient**ï¼Œå°¤å…¶åœ¨è¾“å…¥ä¾§èŠ‚çœè¶…è¿‡ **75%**ã€‚

#### ï¼ˆ3ï¼‰æ¨ç†æ—¶æ‰©å±•æ€§ï¼ˆInference-time Scalingï¼‰ï¼ˆTable 4ï¼‰
ä½¿ç”¨ Top-L workflows æŠ•ç¥¨ï¼š
| L | Best@L (BayesFlow/AFlow) |
|---|-------------------------|
| 1 | 74 / 45 |
| 2 | 80 / 46 |
| 4 | 85 / 65 |
| 8 | **86 / 81** |

âœ… BayesFlow çš„ workflows æ›´**å¤šæ ·åŒ–ä¸”é«˜è´¨é‡**ï¼Œä½¿å¾—é›†æˆæ•ˆæœè¿œä¼˜äº AFlowï¼Œå°¤å…¶ä½“ç°åœ¨ Best@L ä¸Šã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å°† workflow ç”Ÿæˆå»ºæ¨¡ä¸ºè´å¶æ–¯åéªŒé‡‡æ ·æ˜¯æœ‰æ•ˆçš„**ï¼š
   - æä¾›äº† principled çš„ç†è®ºæ¡†æ¶ï¼›
   - æ”¯æŒå¤šæ ·æ€§ç”Ÿæˆä¸æ¸è¿‘æ”¶æ•›æ€§ï¼ˆTheorem 1ï¼‰ï¼›

2. **Look-ahead Rollout + In-loop Refinement æ˜¯é«˜æ•ˆç»„åˆ**ï¼š
   - å‰ç» rollout æä¾›æ— åé‡è¦æ€§æƒé‡ï¼Œé¿å…å¥–åŠ±é€€åŒ–ï¼›
   - ç²¾ç‚¼æœºåˆ¶è™½ç ´åæ¸è¿‘æ”¶æ•›ï¼Œä½†å¸¦æ¥å®è¯å¢ç›Šï¼ˆTheorem 2 åˆ†æ drift boundï¼‰ï¼›

3. **BayesFlow å®ç°å…¨é¢è¶…è¶Š**ï¼š
   - åœ¨ accuracyã€efficiencyã€robustness ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼›
   - ç‰¹åˆ«é€‚åˆå¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ MATHã€GPQAï¼‰ï¼›

4. **è½»é‡æ¥å£è®¾è®¡ä¿ƒè¿›åˆ›æ–°**ï¼š
   - ä¸å¼ºåˆ¶ä½¿ç”¨é¢„å®šä¹‰æ¨¡å—ï¼ˆå¦‚ PROGRAMMERï¼‰ï¼Œé¼“åŠ± LLM è‡ªä¸»æ¢ç´¢æ–°ç»“æ„ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **æ¨ç†å¼€é”€å¢åŠ **ï¼š
   - å¹¶è¡Œ lookahead rollouts å¢åŠ äº† inference-time è®¡ç®—è´Ÿæ‹…ï¼›
   - å½“å‰å®ç°æœªé•¿æœŸå¤ç”¨å†å²è½¨è¿¹ï¼Œå­˜åœ¨å†—ä½™é‡‡æ ·ã€‚

2. **ä¾èµ– executor LLM çš„ç¨³å®šæ€§**ï¼š
   - è‹¥ executor æ‰§è¡Œå¤±è´¥é¢‘ç¹ï¼Œä¼šå½±å“ reward ä¿¡å·è´¨é‡ã€‚

3. **è¯„ä¼°èŒƒå›´æœ‰é™**ï¼š
   - å½“å‰ä»…åœ¨ä¸ƒå¤§æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå°šæœªè¦†ç›–å¤šæ¨¡æ€æˆ–å¤šæ™ºèƒ½ä½“åä½œåœºæ™¯ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. è®¾è®¡**åŠ¨æ€æ§åˆ¶å™¨**ï¼Œæ ¹æ®åœ¨çº¿ä¿¡å·è‡ªé€‚åº”è°ƒæ•´ rollout æ•°é‡ K å’Œ refine é¢‘ç‡ Mï¼›
2. å¼•å…¥**è®°å¿†æœºåˆ¶**ï¼Œä¿ç•™é«˜ä»·å€¼è½¨è¿¹ä»¥å‡å°‘é‡å¤é‡‡æ ·ï¼›
3. æ‰©å±•è‡³ **multi-modal** å’Œ **embodied agent** åœºæ™¯ï¼›
4. æ¢ç´¢ **training-based + sampling-based hybrid** æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ä¸æ€§èƒ½è¾¹ç•Œã€‚

--- 

> ğŸ“Œ æ€»ç»“ï¼š**BayesFlow** é€šè¿‡å¼•å…¥ **Bayesian inference** è§†è§’ï¼Œä¸ºè‡ªåŠ¨ workflow ç”Ÿæˆæä¾›äº†é¦–ä¸ªå…·æœ‰ç†è®ºä¿è¯çš„é‡‡æ ·æ¡†æ¶ï¼Œåœ¨æ€§èƒ½ã€æ•ˆç‡å’Œå¤šæ ·æ€§æ–¹é¢å…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€ä»â€œsearch-based designâ€å‘â€œposterior sampling-based designâ€çš„èŒƒå¼è½¬å˜ã€‚

</details>

---

### 7. [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)

**Authors**: Abhishek Tyagi, Yunuo Cen, Shrey Dhorajiya, Bharadwaj Veeravalli, Xuanyao Fong  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.22632v1  

#### Abstract
Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜

å½“å‰ Large Language Models (LLMs) å­˜åœ¨æ˜¾è‘—çš„å‚æ•°å†—ä½™ï¼Œå°¤å…¶æ˜¯åœ¨ **Feed-Forward Networks (FFNs)** ä¸­ã€‚ç°æœ‰çš„å‰ªææ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼š

1. **ä¾èµ–æ•°æ®æ ¡å‡†ï¼ˆdataset-specific calibrationï¼‰**ï¼šéœ€è¦é¢å¤–çš„æ ¡å‡†æ•°æ®é›†æ¥ç¡®å®šé‡è¦å‚æ•°ï¼Œå¢åŠ äº†è®¡ç®—å¼€é”€å’Œæ•°æ®ä¾èµ–æ€§ã€‚
2. **é™æ€å‰ªæï¼ˆstatic pruningï¼‰**ï¼šä¸€æ—¦ç”Ÿæˆä¸Šä¸‹æ–‡å‘ç”Ÿå˜åŒ–ï¼ˆå¦‚è¯é¢˜åˆ‡æ¢ï¼‰ï¼Œå›ºå®šçš„å‰ªææ©ç ï¼ˆmaskï¼‰æ— æ³•é€‚åº”åŠ¨æ€çš„çŸ¥è¯†éœ€æ±‚ï¼Œå¯¼è‡´â€œçŸ¥è¯†æ¼‚ç§»â€ï¼ˆknowledge driftï¼‰â€”â€”å³æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å› å…³é”®ç¥ç»å…ƒè¢«æ°¸ä¹…å‰ªé™¤è€Œä¸¢å¤±è¯­ä¹‰è¿è´¯æ€§ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šDART

ä½œè€…æå‡º **DART**ï¼ˆDynamic Attention-Guided Runtime Tracingï¼‰ï¼Œä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„åŠ¨æ€è¿è¡Œæ—¶å‰ªææ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **åŠ¨æ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡å˜åŒ–**ï¼šé€šè¿‡ç›‘æµ‹ **attention è¾“å‡ºåˆ†å¸ƒçš„å˜åŒ–** æ¥æ£€æµ‹è¯­ä¹‰ä¸Šä¸‹æ–‡æ˜¯å¦å‘ç”Ÿåç§»ã€‚
- **è‡ªé€‚åº”æ›´æ–°ç¥ç»å…ƒæ©ç **ï¼šå½“æ£€æµ‹åˆ°â€œçŸ¥è¯†æ¼‚ç§»â€æ—¶ï¼Œè§¦å‘é‡æ–°é€‰æ‹©æ´»è·ƒçš„ **knowledge neurons**ï¼Œä»è€Œä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚

### ğŸ” åˆ›æ–°ç‚¹ä¸ä¼˜åŠ¿

| åˆ›æ–°ç‚¹ | è¯´æ˜ |
|------|------|
| **Context-aware Neuron Selector** | åŸºäºæ¯å±‚ FFN çš„å†…åœ¨çŸ¥è¯†å¯†åº¦åˆ†é…ç¨€ç–é¢„ç®—ï¼Œå»ºæ¨¡å±‚å†…ç›¸å¯¹é‡è¦æ€§å’Œå±‚é—´äº¤äº’ï¼Œå®ç°æ— è¾…åŠ©è®­ç»ƒçš„ç»“æ„åŒ–å‰ªæã€‚ |
| **Knowledge Drift Detection** | é¦–æ¬¡è¯†åˆ«å¹¶å½¢å¼åŒ–â€œçŸ¥è¯†æ¼‚ç§»â€è¿™ä¸€å¤±è´¥æ¨¡å¼ï¼Œå¹¶æå‡ºåŸºäº attention è¾“å‡ºåˆ†å¸ƒåç§»çš„åœ¨çº¿æ£€æµ‹æœºåˆ¶ã€‚ |
| **Runtime Mask Update Mechanism** | å¼•å…¥ä¸€ä¸ªè®¡æ•°å™¨æœºåˆ¶ï¼Œé¿å…å› ç¬æ—¶å™ªå£°é¢‘ç¹é‡å‰ªæï¼Œä»…åœ¨æŒç»­åç¦»å‚è€ƒä¸Šä¸‹æ–‡æ—¶æ‰è§¦å‘æ©ç æ›´æ–°ã€‚ |
| **è½»é‡åŒ–ä¸é«˜æ•ˆæ€§** | æ•´ä¸ªæ¡†æ¶å†…å­˜å¼€é”€å°äº 10MBï¼ˆå¯¹ LLAMA-3.1-8B è€Œè¨€ï¼‰ï¼ŒFLOPs å¼€é”€ä»… 0.1%ï¼Œé€‚åˆéƒ¨ç½²äºèµ„æºå—é™ç¯å¢ƒã€‚ |

> âœ… **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**ï¼š
> - ä¸ä¾èµ–æ ¡å‡†æ•°æ®ï¼ˆtraining-freeï¼‰
> - åŠ¨æ€é€‚åº”ä¸Šä¸‹æ–‡æ¼”åŒ–ï¼ˆéé™æ€ï¼‰
> - ä¿ç•™å…³é”®çŸ¥è¯†ç¥ç»å…ƒï¼Œé¿å…é•¿ç¨‹ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯
> - æ€§èƒ½æ¥è¿‘åŸå§‹å¯†é›†æ¨¡å‹ï¼Œè¿œè¶…é™æ€å‰ªææ–¹æ³•

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†

å®éªŒè¦†ç›–å¤šä¸ªä»»åŠ¡ç±»å‹ï¼ŒéªŒè¯æ–¹æ³•åœ¨é€šç”¨ä¸é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸Šçš„é²æ£’æ€§ï¼š

#### **é›¶æ ·æœ¬ä»»åŠ¡ï¼ˆZero-Shot Tasksï¼‰**
- **BoolQ**, **RTE**, **HellaSwag**, **Winogrande**, **ARC-E/C**, **OBQA**

#### **é¢†åŸŸç‰¹å®šå¤šè·³ä»»åŠ¡ï¼ˆDomain-Specific Multi-Shot Tasksï¼‰**
- **MMLU**ï¼ˆæ¶µç›–52ä¸ªå­¦ç§‘ï¼‰
- **GPQA**ï¼ˆç ”ç©¶ç”Ÿçº§åˆ«é—®ç­”ï¼‰
- **MEDMCQA**ï¼ˆåŒ»å­¦å¤šé€‰é¢˜ï¼‰

#### **é•¿æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼ˆSummarization Benchmarksï¼‰**
- **CNN/DailyMail**
- **Multi-News**
- **GovReport**

#### **è‡ªå®šä¹‰å¤šä¸»é¢˜ç”Ÿæˆæ•°æ®é›†ï¼ˆCustom Promptsï¼‰**
- è®¾è®¡äº† 500 ä¸ªéœ€è·¨é¢†åŸŸç”Ÿæˆçš„æç¤ºæ¨¡æ¿ï¼ˆè§ Appendix Fï¼‰ï¼Œç”¨äºæµ‹è¯•çŸ¥è¯†è¿½è¸ªèƒ½åŠ›ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®

| è®¾ç½®é¡¹ | æè¿° |
|------|------|
| **æ¨¡å‹** | ä¸»è¦ä½¿ç”¨ **LLAMA-3.2-3B** å’Œ **LLAMA-3.1-8B**ï¼›é™„å½•ä¸­æ‰©å±•è‡³ Qwen3ã€DeepSeekã€Mistral ç­‰ |
| **å‰ªæç›®æ ‡** | å¯¹ FFN å±‚è¿›è¡Œ **70% ç»“æ„åŒ–å‰ªæ**ï¼ˆstructured neuron pruningï¼‰ |
| **è¯„ä¼°æ–¹å¼** | ä½¿ç”¨ `lm-eval-harness` è¿›è¡Œ zero-shot å’Œ five-shot è¯„æµ‹ |
| **ç¡¬ä»¶å¹³å°** | åŒ AMD EPYC CPU + 8Ã—NVIDIA L40S GPU |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | ç”¨é€” |
|------|------|
| **Accuracy (%)** | åˆ†ç±»/æ¨ç†ä»»åŠ¡çš„ä¸»è¦è¯„ä»·æ ‡å‡† |
| **ROUGE-L**, **BLEU** | è¡¡é‡æ‘˜è¦ä»»åŠ¡çš„å†…å®¹å¬å›ä¸è¡¨é¢ä¸€è‡´æ€§ |
| **BERTScore (F1)** | è¡¡é‡ç”Ÿæˆå†…å®¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦ |
| **Embedding Similarity** | è¡¡é‡éšè—çŠ¶æ€è½¨è¿¹ä¸åŸæ¨¡å‹çš„ä¸€è‡´æ€§ |
| **Coverage** | è¡¡é‡æºä¿¡æ¯åˆ©ç”¨ç¨‹åº¦ |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|--------|------|------|
| **WANDA** | é™æ€ã€æƒé‡çº§å‰ªæ | åŸºäºæ ¡å‡†é›†çš„é‡è¦æ€§è¯„åˆ†ï¼Œå›ºå®šæ©ç  |
| **SparseGPT** | é™æ€ã€éç»“æ„åŒ–å‰ªæ | å•æ¬¡å‰ªæï¼Œä¸æ›´æ–°æ©ç  |
| **DEJAVU** | åŠ¨æ€ã€é¢„æµ‹å¼å‰ªæ | ä½¿ç”¨é¢å¤–è®­ç»ƒçš„å°æ¨¡å‹é¢„æµ‹é‡è¦ç¥ç»å…ƒ |
| **DLP / OWL** | é™æ€ã€åˆ†å±‚é‡è¦æ€§å‰ªæ | åŸºäºå…¨å±€ç»Ÿè®¡å¾—åˆ†å‰ªæ |

> âœ… DART æ˜¯å”¯ä¸€ **å®Œå…¨æ— éœ€è®­ç»ƒã€æ— æ ¡å‡†æ•°æ®ã€ä¸”æ”¯æŒè¿è¡Œæ—¶æ©ç æ›´æ–°** çš„æ–¹æ³•ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

åœ¨ **70% FFN ç¨€ç–ç‡ä¸‹**ï¼ŒDART åœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼š

| Benchmark | LLAMA-3.1-8B (Dense) | WANDA | DEJAVU | **DART (Ours)** |
|----------|------------------------|-------|--------|------------------|
| **HellaSwag** | 79.32 | 44.99 | 26.68 | **64.58** (+19.6â†‘ vs best baseline) |
| **ARC-E** | 82.53 | 50.34 | 25.08 | **59.43** |
| **MMLU (5-shot)** | 66.61 | 29.70 | 25.20 | **34.14** |
| **MEDMCQA (5-shot)** | 56.66 | 25.65 | 28.16 | **29.35** |

> ğŸ’¡ åœ¨ LLAMA-3.1-8B ä¸Šï¼ŒDART ç›¸æ¯”æœ€ä½³åŠ¨æ€åŸºçº¿æœ€é«˜æå‡è¾¾ **+19.6% å‡†ç¡®ç‡**ã€‚

---

### ğŸ“‰ æ‘˜è¦ä»»åŠ¡è¡¨ç°ï¼ˆFigure 6 & Table 2ï¼‰

åœ¨ **long-horizon summarization** ä»»åŠ¡ä¸­ï¼ŒDART æ˜¾è‘—ä¼˜äºé™æ€å‰ªæï¼š

| æŒ‡æ ‡ | é™æ€å‰ªæ (50% sparsity) | **DART + Tracking** |
|------|-------------------------|---------------------|
| **ROUGE-L** | ä½ï¼ˆ~0.2â€“0.3ï¼‰ | **0.38** |
| **BLEU** | 0.20 | **0.36** |
| **BERTScore (F1)** | 0.80 | **0.83** |
| **Embedding Sim** | 0.85 | **0.92** |
| **Coverage** | 0.77 | **0.89** |

> âœ… DART å®ç°äº† **é«˜è¾¾ 3Ã— çš„ ROUGE-L æå‡**ï¼Œä¸”ç”Ÿæˆè´¨é‡æ¥è¿‘åŸå§‹å¯†é›†æ¨¡å‹ã€‚

---

### ğŸ” æ¶ˆèå®éªŒä¸åˆ†æï¼ˆAppendicesï¼‰

#### âœ… **çŸ¥è¯†æ¼‚ç§»æ£€æµ‹æœ‰æ•ˆæ€§ï¼ˆAppendix Eï¼‰**
- æ„é€ æ§åˆ¶å®éªŒï¼šå…ˆå†™ç”µåŠ¨è½¦ â†’ å†å†™æ„å¤§åˆ©èœã€‚
- è‹¥åœ¨ç¬¬100 token åå‰ªæä¸”ä¸é‡Šæ”¾ï¼Œåˆ™ç¬¬äºŒæ®µå˜ä¸ºé‡å¤æ ‡é¢˜ï¼ˆgibberishï¼‰ã€‚
- è‹¥åœ¨ç¬¬400 token é‡Šæ”¾æ©ç ï¼Œå¯æ¢å¤è¿è´¯è¾“å‡ºï¼›è‹¥å»¶è¿Ÿè‡³ç¬¬500 tokenï¼Œåˆ™æ— æ³•æ¢å¤ã€‚
- è¡¨æ˜ï¼š**çŸ¥è¯†æœªä¸¢å¤±ï¼Œä½†å› ç¥ç»å…ƒä¸å¯ç”¨è€Œâ€œè¢«å›°â€åœ¨é”™è¯¯è¯­ä¹‰ç›†åœ°**ã€‚

#### âœ… **æ³¨æ„åŠ›ç©ºé—´æ¼‚ç§»ä¿¡å·ï¼ˆFigure 10ï¼‰**
- ä½¿ç”¨ **cosine similarity** è¡¡é‡å½“å‰ attention è¾“å‡ºä¸åˆå§‹ä¸Šä¸‹æ–‡ä¸­å¿ƒçš„è·ç¦»ã€‚
- å‘ç°ï¼šå³ä½¿æ¨¡å‹å°è¯•è½¬å‘æ–°ä¸»é¢˜ï¼Œå‰ªæåä»è¢«â€œæ‹‰å›â€åŸè¯­ä¹‰åŒºåŸŸã€‚
- DART çš„æ£€æµ‹æœºåˆ¶èƒ½æ•æ‰è¯¥è¶‹åŠ¿å¹¶åŠæ—¶æ›´æ–°æ©ç ã€‚

#### âœ… **å±‚æ•æ„Ÿæ€§åˆ†æï¼ˆAppendix Dï¼‰**
- æ—©æœŸå±‚æ›´æ•æ„Ÿï¼ˆå½±å“åŸºç¡€è¯­æ³•ä¸ç»“æ„ï¼‰
- åæœŸå±‚æ›´é€‚åˆå‰ªæï¼ˆç¼–ç é«˜å±‚è¯­ä¹‰ï¼Œéƒ¨åˆ†å‰ªæç”šè‡³æœ‰ç›Šäºèšç„¦ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **çŸ¥è¯†æ¼‚ç§»ï¼ˆKnowledge Driftï¼‰æ˜¯åŠ¨æ€å‰ªæçš„å…³é”®å¤±è´¥æ¨¡å¼**  
   - è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¸Šä¸‹æ–‡ä¸æ–­æ¼”å˜ï¼Œé™æ€æ©ç ä¼šå¯¼è‡´å…³é”®çŸ¥è¯†ç¥ç»å…ƒç¼ºå¤±ã€‚
   - è¿™ç§åå·®ä¼šéšæ—¶é—´ç´¯ç§¯ï¼Œæœ€ç»ˆå¯¼è‡´è¯­ä¹‰å´©æºƒã€‚

2. **attention è¾“å‡ºåˆ†å¸ƒåç§»æ˜¯çŸ¥è¯†æ¼‚ç§»çš„æœ‰æ•ˆä»£ç†ä¿¡å·**  
   - attention å­å±‚è´Ÿè´£æ•æ‰ token é—´å…³ç³»ï¼Œå…¶è¾“å‡ºå˜åŒ–æ—©äº FFN å¤±æ•ˆã€‚
   - å¯ä½œä¸ºâ€œé¢„è­¦ç³»ç»Ÿâ€æŒ‡å¯¼è¿è¡Œæ—¶å‰ªæå†³ç­–ã€‚

3. **DART å®ç°äº†é«˜ç¨€ç–ä¸‹çš„é«˜è´¨é‡æ¨ç†**  
   - åœ¨ 70% FFN å‰ªæä¸‹ï¼Œæ€§èƒ½æŸå¤±æå°ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨ç†ä¸å¤šä¸»é¢˜ä»»åŠ¡ä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚
   - æ”¯æŒ **long-context generation** è€Œä¸ç‰ºç‰²è¿è´¯æ€§ã€‚

4. **æ›´å¤§çš„æ¨¡å‹æ›´å…·çŸ¥è¯†éš”ç¦»æ€§ï¼ˆKnowledge Segregationï¼‰**  
   - å¤§æ¨¡å‹ä¸­çŸ¥è¯†ç¥ç»å…ƒæ›´ä¸“ä¸šåŒ–ï¼Œè·¨ä¸»é¢˜å¹²æ‰°å°‘ï¼Œå› æ­¤æ›´æ˜“ç²¾å‡†å‰ªæã€‚
   - å°æ¨¡å‹åŠŸèƒ½å åŠ ä¸¥é‡ï¼Œå‰ªæé£é™©æ›´é«˜ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ä¾èµ– attention è¾“å‡ºç¨³å®šæ€§** | è‹¥ attention æœ¬èº«ä¸ç¨³å®šï¼ˆå¦‚æç«¯å¯¹æŠ—è¾“å…¥ï¼‰ï¼Œå¯èƒ½å¯¼è‡´è¯¯è§¦å‘æ©ç æ›´æ–°ã€‚ |
| **æœªä¼˜åŒ–ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡** | å½“å‰è¯„ä¼°ä¾§é‡ç®—æ³•æœ‰æ•ˆæ€§ï¼Œå®é™…åŠ é€Ÿæ¯”å— kernel å®ç°é™åˆ¶ï¼ˆè§ Appendix Gï¼‰ã€‚ |
| **ä»…å‰ªæ FFNï¼Œæœªå¤„ç† attention** | è™½ç„¶ GQA å·²é™ä½ attention å‚æ•°å æ¯”ï¼Œä½†åœ¨æŸäº›æ¶æ„ä¸­ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚ |
| **æ©ç æ›´æ–°é¢‘ç‡è¾ƒä½** | å¹³å‡æ¯ 100k tokens è§¦å‘çº¦ 100 æ¬¡ï¼Œå¯èƒ½é”™è¿‡ç»†ç²’åº¦å˜åŒ–ã€‚ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **å°† DART æ‰©å±•è‡³ MoE æ¶æ„**  
   - åœ¨ä¸“å®¶é€‰æ‹©å±‚é¢å¼•å…¥åŠ¨æ€è¿½è¸ªæœºåˆ¶ã€‚

2. **ç»“åˆé‡åŒ–ä¸å‰ªæå½¢æˆè”åˆå‹ç¼©æ–¹æ¡ˆ**  
   - æ¢ç´¢ Quantization + DART çš„ååŒæ•ˆåº”ã€‚

3. **è®¾è®¡æ›´é«˜æ•ˆçš„è¿è¡Œæ—¶è°ƒåº¦å™¨**  
   - åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶æ©ç æ›´æ–°ä¸ç¼“å­˜ç®¡ç†ã€‚

4. **æ¢ç´¢å…¶ä»–æ½œåœ¨æ¼‚ç§»ä¿¡å·**  
   - å¦‚æ¢¯åº¦æµã€æ®‹å·®è¿æ¥å¼ºåº¦ç­‰ä½œä¸ºè¡¥å……ç›‘æ§ç»´åº¦ã€‚

5. **æ„å»ºæ ‡å‡†çš„â€œå¤šä¸»é¢˜ç”Ÿæˆâ€è¯„æµ‹åŸºå‡†**  
   - æ¨åŠ¨å¯¹åŠ¨æ€èƒ½åŠ›çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚

---

## âœ… æ€»ç»“

**DART** æˆåŠŸè§£å†³äº† LLM æ¨ç†ä¸­ **é™æ€å‰ªææ— æ³•é€‚åº”ä¸Šä¸‹æ–‡æ¼”åŒ–** çš„æ ¹æœ¬é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ª **æ— éœ€è®­ç»ƒã€æ— éœ€æ ¡å‡†ã€å¯è¿è¡Œæ—¶è‡ªé€‚åº”æ›´æ–°æ©ç ** çš„è½»é‡çº§å‰ªææ¡†æ¶ã€‚å…¶å®éªŒå……åˆ†è¯æ˜ï¼š

- åŠ¨æ€å‰ªæå¿…é¡»è€ƒè™‘ **çŸ¥è¯†æ¼‚ç§»**
- **attention è¾“å‡ºåˆ†å¸ƒ** æ˜¯æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡å˜åŒ–æ¢æµ‹å™¨
- é€šè¿‡å°‘é‡æ©ç æ›´æ–°å³å¯å¤§å¹…æ¢å¤ç”Ÿæˆè´¨é‡
- åœ¨å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³é€¼è¿‘åŸå§‹å¯†é›†æ¨¡å‹çš„è¡¨ç°

> ğŸŒŸ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> DART é€šè¿‡â€œç”¨ attention çœ‹ä¸–ç•Œï¼Œç”¨ FFN å†™ç­”æ¡ˆâ€çš„ç†å¿µï¼Œå®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„ **ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€è¿è¡Œæ—¶è‡ªé€‚åº”çš„çŸ¥è¯†ç¥ç»å…ƒè¿½è¸ªå‰ªæ**ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²å¤§æ¨¡å‹æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 8. [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)

**Authors**: Andrei Panferov, Erik Schultheis, Soroush Tabesh, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.22813v1  

#### Abstract
The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šQuartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰åŸºäº **NVFP4** çš„å…¨é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒè™½ç„¶èƒ½æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†åœ¨ç²¾åº¦ä¸Šä»æ˜æ˜¾è½åäº FP16 å’Œ FP8 è®­ç»ƒã€‚å…¶ä¸»è¦åŸå› åœ¨äºï¼š

- **å‰å‘ä¼ æ’­**ä¸­ä¸ºä¿ç•™åŠ¨æ€èŒƒå›´è€Œç‰ºç‰²äº†è¡¨ç¤ºèƒ½åŠ›ï¼ˆå¦‚é‡‡ç”¨ square-block quantization å¯¼è‡´æ¯ç»„ scale è¦†ç›–è¿‡å¤šå…ƒç´ ï¼‰ï¼›
- **åå‘ä¼ æ’­**ä¸­ä¾èµ–ä¼ ç»Ÿçš„ **Stochastic Rounding (SR)** è¿›è¡Œæ— åæ¢¯åº¦ä¼°è®¡ï¼Œä½† SR åœ¨ 4-bit ç²¾åº¦ä¸‹å¼•å…¥äº†è¾ƒé«˜çš„æ–¹å·®å’Œé‡åŒ–è¯¯å·®ã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿æŒç¡¬ä»¶å…¼å®¹æ€§å’Œé«˜æ•ˆæ‰§è¡Œçš„å‰æä¸‹ï¼Œæå‡ NVFP4 é‡åŒ–è®­ç»ƒçš„ç²¾åº¦ï¼Œæ˜¯å½“å‰çš„å…³é”®æŒ‘æˆ˜ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**

#### âœ… **æ ¸å¿ƒåˆ›æ–°ï¼šMS-EDEN â€”â€” æ–°å‹æ— åé‡åŒ–åŸè¯­**
- æå‡ºä¸€ç§é€‚ç”¨äºå¾®ç¼©æ”¾æµ®ç‚¹æ ¼å¼ï¼ˆmicroscaling FP4ï¼‰çš„æ–°å‹æ— åé‡åŒ–æ–¹æ¡ˆï¼š**MicroScaling EDEN (MS-EDEN)**ã€‚
- ä¸ä¼ ç»Ÿ SR å°†éšæœºæ€§æ–½åŠ åœ¨æ¯ä¸ª FP4 å…ƒç´ ä¸åŒï¼ŒMS-EDEN å°†éšæœºæ€§è½¬ç§»åˆ° **microscale å› å­ï¼ˆFP8 scalesï¼‰** ä¸Šï¼Œé€šè¿‡ç»“åˆ **Randomized Hadamard Transform (RHT)** å’Œå¸¦æ ¡æ­£çš„é‡ç¼©æ”¾æœºåˆ¶å®ç°æ— åä¼°è®¡ã€‚
- åœ¨æœŸæœ›æ„ä¹‰ä¸‹ä¿è¯æ— åæ€§ï¼ŒåŒæ—¶å¤§å¹…é™ä½å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œç›¸æ¯” SR å‡å°‘è¶…è¿‡ **2å€çš„é‡åŒ–è¯¯å·®**ã€‚

#### âœ… **å®Œæ•´è®­ç»ƒæ¡†æ¶ï¼šQuartet II**
æ„å»ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å…¨ NVFP4 çº¿æ€§å±‚è®­ç»ƒå›¾ï¼ˆcomputation graphï¼‰ï¼Œèåˆä»¥ä¸‹å…³é”®æŠ€æœ¯ï¼š
- **å‰å‘ä¼ æ’­**ï¼š
  - ä½¿ç”¨åŸç”Ÿ NVFP4 ç¼–ç ï¼ˆæ¯ 16 ä¸ªå…ƒç´ ä¸€ä¸ª FP8 scaleï¼‰è€Œé square-blockï¼›
  - å¼•å…¥ **â€œFour Over Sixâ€ (4/6)** è‡ªé€‚åº”ç½‘æ ¼é€‰æ‹©ç­–ç•¥ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–é‡åŒ– MSEã€‚
- **åå‘ä¼ æ’­**ï¼š
  - åº”ç”¨ **MS-EDEN** å¯¹æ‰€æœ‰ GEMM æ“ä½œä¸­çš„å¼ é‡è¿›è¡Œæ— åå†é‡åŒ–ï¼›
  - åˆ©ç”¨å†…ç»´åº¦ä¸Šçš„ RHT å®ç°åˆ†å¸ƒå¹³æ»‘ä¸è¯¯å·®æ§åˆ¶ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **é‡åŒ–è¯¯å·®** | MS-EDEN çš„ MSE æ¯” SR ä½ **>2x**ï¼ˆè§ Table 1ï¼‰ |
| **è¡¨ç¤ºèƒ½åŠ›** | æ”¾å¼ƒ square-block quantizationï¼Œæ¢å¤æ¯ 16 å…ƒç´ ç‹¬ç«‹ scaleï¼Œæå‡å‰å‘è¡¨ç¤ºèƒ½åŠ› |
| **æ— åæ€§ä¿éšœ** | ä¸¥æ ¼ç†è®ºæ”¯æŒ + æ•°å€¼éªŒè¯è¡¨æ˜ MS-EDEN åœ¨å®è·µä¸­æœ‰æ•ˆç»´æŒæ— åæ¢¯åº¦ä¼°è®¡ |
| **ç¡¬ä»¶æ•ˆç‡** | æä¾›é’ˆå¯¹ NVIDIA Blackwell GPU çš„å®šåˆ¶åŒ– CUDA kernelï¼Œæ”¯æŒé«˜è¾¾ **4.2x çš„é€Ÿåº¦æå‡**ï¼ˆvs BF16ï¼‰ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **C4**ï¼šç”¨äº Llama-like æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œè¯„ä¼°è¯­è¨€å»ºæ¨¡ lossã€‚
- **FineWeb-Edu**ï¼šç”¨äºæ›´å¤§è§„æ¨¡çš„ Nanochat æµæ°´çº¿è®­ç»ƒã€‚
- å¾®è°ƒé˜¶æ®µä½¿ç”¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡æ•°æ®é›†ï¼š
  - **ARC-Challenge/Easy**
  - **GSM8K**
  - **HumanEval**
  - **MMLU**

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

#### ğŸ”§ **æ¨¡å‹æ¶æ„**
- åŸºäº **Llama-2** æ¶æ„çš„ Transformer æ¨¡å‹ï¼Œå‚æ•°é‡ä» **30M åˆ° 1.9B** ä¸ç­‰ã€‚
- æ‰€æœ‰çº¿æ€§å±‚æ›¿æ¢ä¸ºæ‰€æ QAT æ–¹æ¡ˆï¼Œå…¶ä½™ç»“æ„ä¿æŒä¸å˜ã€‚

#### ğŸ“Š **è¯„ä¼°æŒ‡æ ‡**
| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| é¢„è®­ç»ƒè´¨é‡ | C4 ä¸Šçš„ **Validation Loss**ï¼ˆç›¸å¯¹äº BF16 çš„å·®è·ï¼‰ |
| ä¸‹æ¸¸èƒ½åŠ› | Zero-shot å‡†ç¡®ç‡ï¼ˆArc, GSM8K, HumanEval, MMLUï¼‰ |
| æ•ˆç‡ | **è®­ç»ƒååé‡ï¼ˆktok/sï¼‰**ã€**ç›¸å¯¹ BF16 çš„åŠ é€Ÿæ¯”** |
| æ— åæ€§éªŒè¯ | å¤šæ¬¡é‡åŒ–åå‘ä¼ æ’­åå¹³å‡æ¢¯åº¦ä¸çœŸå®æ¢¯åº¦çš„ç›¸å¯¹è¯¯å·®ï¼ˆè§ Figure 9ï¼‰ |

#### âš™ï¸ **è®­ç»ƒé…ç½®**
- ä½¿ç”¨ **AdamW** ä¼˜åŒ–å™¨ï¼Œcosine å­¦ä¹ ç‡è°ƒåº¦ï¼›
- æ•°æ®-å‚æ•°æ¯”è¦†ç›– 25~800ï¼Œæµ‹è¯•ä¸åŒè®­ç»ƒå¼ºåº¦ä¸‹çš„ç¨³å®šæ€§ï¼›
- æ‰€æœ‰è¶…å‚å¤ç”¨è‡ª BF16 åŸºçº¿ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿ | æè¿° |
|------|------|
| **NVIDIA et al. (2025)** | é¦–ä¸ªç«¯åˆ°ç«¯ NVFP4 é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿ç”¨ square-block + SR |
| **TetraJet-v2 (Chen et al., 2025b)** | æ”¹è¿›ç‰ˆ NVIDIA æ–¹æ¡ˆï¼ŒåŠ å…¥ outlier æ§åˆ¶ä¸æŒ¯è¡æŠ‘åˆ¶ |
| **FourOverSix (Cook et al., 2025)** | æå‡º 4/6 è‡ªé€‚åº” scale é€‰æ‹©ï¼Œä½†ä»…ç”¨äºå‰å‘ä¸”æœªè§£å†³åå‘æ— åæ€§é—®é¢˜ |
| **SR-only variants** | å„ç§ç»„åˆä¸‹çš„ Stochastic Rounding åŸºçº¿ï¼Œä½œä¸ºæ¶ˆèå¯¹ç…§ |

> æ³¨ï¼šä½œè€…æŒ‡å‡º TetraJet-v2 ä¸­çš„éƒ¨åˆ†è®¾è®¡ï¼ˆå¦‚ä¸­é—´ FP32 scaleï¼‰éš¾ä»¥åœ¨ GPU ä¸Šé«˜æ•ˆå®ç°ï¼Œæ•…æœ¬æ–‡é‡‡ç”¨å…¶â€œGPU å¯è¡Œå­é›†â€ä½œä¸ºå®é™…å¯¹æ¯”åŸºçº¿ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **MS-EDEN vs SR çš„ MSE** | åœ¨ N(0,1) åˆ†å¸ƒä¸‹ï¼ŒMS-EDEN MSE â‰ˆ **9.8Ã—10â»Â³**ï¼ŒSR ä¸º **23.5Ã—10â»Â³**ï¼Œé™ä½ >2xï¼ˆTable 1ï¼‰ |
| **ç«¯åˆ°ç«¯é¢„è®­ç»ƒæŸå¤±å¢ç›Š** | Quartet II ç›¸æ¯” TetraJet-v2 å¹³å‡å‡å°‘ **â‰¥20% çš„ loss å·®è·**ï¼ˆFigure 4ï¼‰ |
| **Nanochat é¢„è®­ç»ƒ BPB æå‡** | åœ¨ 1.9B æ¨¡å‹ä¸Šï¼ŒQuartet II å°†éªŒè¯ bits-per-byte (BPB) ä¸ BF16 çš„å·®è·ç¼©å° **15â€“25%**ï¼ˆFigure 5ï¼‰ |
| **æ¨ç† kernel åŠ é€Ÿ** | çº¿æ€§å±‚è®­ç»ƒé€Ÿåº¦æœ€é«˜è¾¾ **4.2x BF16**ï¼Œä¼˜äº Quartet (MXFP4) çº¦ **70%**ï¼ˆFigure 6ï¼‰ |
| **çœŸå®è®­ç»ƒååæå‡** | åœ¨ 1.1B æ¨¡å‹ä¸Šè¾¾åˆ° **245% çš„ BF16 ååé‡**ï¼ˆå¾—ç›Šäºå†…å­˜èŠ‚çœå¸¦æ¥çš„ batch size æå‡ï¼‰ |

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

| æ–¹æ³• | ç›¸å¯¹ BF16 Loss Gap â†“ | æ˜¯å¦æ— å |
|------|------------------------|----------|
| NVIDIA (2025) | é«˜ | æ˜¯ |
| TetraJet-v2 | ä¸­é«˜ | æ˜¯ |
| FourOverSix (backbone) | ä¸­ | âŒï¼ˆå®éªŒè¯æ˜æœ‰åï¼‰ |
| **Quartet II** | **æœ€ä½** | âœ… |

- Figure 4 æ˜¾ç¤ºï¼Œåœ¨æ‰€æœ‰æ¨¡å‹å¤§å°å’Œæ•°æ®æ¯”ä¾‹ä¸‹ï¼Œ**Quartet II å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿**ï¼Œå°¤å…¶åœ¨è¿‡è®­ç»ƒåœºæ™¯ä¸­ä¼˜åŠ¿æ›´æ˜æ˜¾ã€‚
- Figure 5 è¡¨æ˜ Nanochat åœºæ™¯ä¸‹ï¼ŒQuartet II æ˜¾è‘—åŠ å¿«æ”¶æ•›å¹¶å‡å°ä¸ BF16 çš„å·®è·ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### âœ… **åå‘ä¼ æ’­é‡åŒ–æ–¹å¼çš„å½±å“ï¼ˆFigure 1ï¼‰**
- å•ç‹¬å¯ç”¨ MS-EDEN åœ¨ä»»ä¸€ GEMM ä¸­å‡ä¼˜äº SRï¼›
- å®Œæ•´å¯ç”¨ MS-EDENï¼ˆå«æƒé‡å†é‡åŒ–ï¼‰ç”šè‡³ä¼˜äºä¸å¯ç”¨å†é‡åŒ–çš„ SRï¼ˆå³ Figure 1e > Figure 1dï¼‰ï¼Œè¯´æ˜å…¶è¯¯å·®æ›´ä½ã€‚

#### âœ… **å‰å‘ä¼ æ’­ç­–ç•¥å½±å“ï¼ˆFigure 2ï¼‰**
- â€œ4/6â€ æŠ€æœ¯åœ¨ native group scalingï¼ˆ1x16gsï¼‰ä¸‹æ•ˆæœæœ€ä½³ï¼Œæå‡çº¦ä¸¤å€äº square-block ç‰ˆæœ¬ï¼›
- native scaling + 4/6 æˆä¸ºæœ€ä¼˜å‰å‘ç»„åˆã€‚

#### âœ… **æ— åæ€§éªŒè¯ï¼ˆFigure 9ï¼‰**
- å¹³å‡é‡åŒ–æ¢¯åº¦çš„è¯¯å·®éš accumulation steps å¢åŠ å‘ˆ **1/B è¡°å‡è¶‹åŠ¿**ï¼Œç¬¦åˆ CLT é¢„æœŸï¼Œè¯æ˜æ— åï¼›
- è€Œ â€œ4/6 + SRâ€ ç»„åˆå‡ºç° plateauï¼Œè¡¨æ˜å­˜åœ¨ç³»ç»Ÿæ€§åå·®ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **ä¼ ç»Ÿ SR å¹¶éæœ€ä¼˜è§£**ï¼šå°½ç®¡å¹¿æ³›ä½¿ç”¨ï¼ŒSR åœ¨ FP4 ä¸‹å¼•å…¥è¿‡é«˜æ–¹å·®ï¼›å°†éšæœºæ€§ä»å…ƒç´ çº§ç§»è‡³ scale çº§å¯æ˜¾è‘—é™ä½è¯¯å·®ã€‚
2. **MS-EDEN å®ç°æ›´ä¼˜æƒè¡¡**ï¼šåœ¨ä¿æŒæ— åæ€§çš„å‰æä¸‹ï¼Œå°†é‡åŒ–è¯¯å·®é™ä½ >2xï¼Œæ˜¯ç›®å‰æœ€å…ˆè¿›çš„æ— åé‡åŒ–åŸè¯­ä¹‹ä¸€ã€‚
3. **å‰å‘ä¸åå‘éœ€ååŒè®¾è®¡**ï¼š
   - æ”¾å¼ƒ square-block å¯é‡Šæ”¾å‰å‘è¡¨ç¤ºèƒ½åŠ›ï¼›
   - MS-EDEN çš„ä¼˜è¶Šæ€§è¶³ä»¥è¡¥å¿å†é‡åŒ–å¼€é”€ï¼Œå¹¶æ•´ä½“èƒœå‡ºã€‚
4. **å·¥ç¨‹å¯è¡Œæ€§å·²éªŒè¯**ï¼š
   - æå‡º **post-hoc range alignment** æŠ€æœ¯é¿å…é‡å¤æ—‹è½¬ï¼›
   - å®ç°é«˜æ•ˆ CUDA kernelï¼Œæ”¯æŒé«˜è¾¾ **4.2x é€Ÿåº¦æå‡**ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ– RHT æ—‹è½¬**ï¼šè¦æ±‚ microscaling group æ˜¯ rotation group çš„å­é›†ï¼Œé™åˆ¶äº†å¸ƒå±€çµæ´»æ€§ï¼›
- **ä»…é€‚ç”¨äºå†…ç»´åº¦ GEMM**ï¼šç›®å‰ä¸»è¦ç”¨äº linear å±‚ï¼Œattention ä¸­çš„åºåˆ—ç»´åº¦å°šéš¾ç›´æ¥åº”ç”¨ï¼›
- **é¢å¤– kernel å¼€é”€**ï¼šå°½ç®¡åšäº†èåˆä¼˜åŒ–ï¼Œå†é‡åŒ–ä»å¸¦æ¥ä¸€å®šå»¶è¿Ÿï¼ˆçº¦å æ€»æ—¶é—´ 13%ï¼Œè§ Table 7ï¼‰ï¼›
- **å¾®è°ƒé˜¶æ®µæœªå—ç›Š**ï¼šç”±äºçŸ­å‘¨æœŸå’Œå°æ•°æ®ï¼ŒSFT é˜¶æ®µå„ QAT æ–¹æ³•å·®å¼‚ä¸æ˜¾è‘—ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ‰©å±•è‡³å…¶ä»–æ¨¡å—**ï¼šå°† MS-EDEN æ¨å¹¿è‡³ attention softmaxã€norm å±‚ç­‰éçº¿æ€§æ“ä½œï¼›
2. **æ¢ç´¢æ··åˆç²¾åº¦ç­–ç•¥**ï¼šéƒ¨åˆ†å±‚ä¿ç•™æ›´é«˜ç²¾åº¦ä»¥è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒï¼›
3. **é€‚é…æ›´å¤šç¡¬ä»¶å¹³å°**ï¼šé™¤ Blackwell å¤–ï¼Œæ”¯æŒ AMD æˆ–å…¶ä»–æ”¯æŒ FP4 çš„èŠ¯ç‰‡ï¼›
4. **åŠ¨æ€ scale é€‰æ‹©æœºåˆ¶**ï¼šç»“åˆ 4/6 ä¸ MS-EDENï¼Œå¼€å‘è”åˆä¼˜åŒ–çš„ adaptive quantization policyï¼›
5. **ç†è®ºæ·±åŒ–**ï¼šåˆ†ææœ‰é™ç»´åº¦ä¸‹ EDEN-style æ–¹æ³•çš„æ”¶æ•›é€Ÿç‡è¾¹ç•Œã€‚

---

> âœ… **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/IST-DASLab/Quartet-II](https://github.com/IST-DASLab/Quartet-II)  
> âœ… **æ ¸å¿ƒæ€æƒ³ä¸€å¥è¯æ€»ç»“**ï¼š*é€šè¿‡å°†æ— åéšæœºæ€§ä» FP4 å…ƒç´ è½¬ç§»åˆ° FP8 scaleï¼Œå¹¶ç»“åˆæ—‹è½¬ä¸æ ¡æ­£ï¼ŒMS-EDEN å®ç°äº†æ¯” SR æ›´ä½è¯¯å·®çš„æ¢¯åº¦ä¼°è®¡ï¼Œä½¿ NVFP4 å…¨é‡åŒ–è®­ç»ƒé¦–æ¬¡æ¥è¿‘ BF16 ç²¾åº¦æ°´å¹³ã€‚*

</details>

---

### 9. [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)

**Authors**: Jiaxuan Gao, Jiaao Chen, Chuyi He, Wei-Chen Wang, Shusheng Xu, Hanrui Wang, Di Jin, Yi Wu  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22607v1  

#### Abstract
Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-qua...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**å¤šè½®äº¤äº’å¼å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“**ï¼ˆinteractive tool-using agentsï¼‰çš„åè®­ç»ƒï¼ˆpost-trainingï¼‰éš¾é¢˜ï¼Œè§£å†³äº†ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒç“¶é¢ˆï¼š
- **é«˜è´¨é‡åˆæˆæ•°æ®éš¾ä»¥è§„æ¨¡åŒ–è·å–**ï¼šæ„å»ºå¤æ‚çš„å¤šè½®ã€å¤šæ­¥å·¥å…·è°ƒç”¨å¯¹è¯éœ€è¦æ»¡è¶³é¢†åŸŸè§„åˆ™ã€æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼Œå¹¶ä¿è¯ä»»åŠ¡å¯è§£ï¼Œäººå·¥æ ‡æ³¨æˆæœ¬æé«˜ï¼Œè€Œè‡ªåŠ¨åŒ–åˆæˆæ˜“äº§ç”Ÿæ— æ•ˆæˆ–ä½è´¨æ•°æ®ã€‚
- **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿¡å·å™ªå£°å¤§**ï¼šåœ¨äº¤äº’ç¯å¢ƒä¸­è¿›è¡ŒRLè®­ç»ƒéœ€ä¾èµ–ç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼ˆuser simulatorï¼‰ï¼Œä½†ç°æœ‰å¼€æºæ¨¡å‹åœ¨æ¨¡æ‹Ÿä¼šè°ƒç”¨å·¥å…·çš„ç”¨æˆ·æ—¶è¡Œä¸ºä¸ç¨³å®šï¼Œå¯¼è‡´rolloutå¤±è´¥ç‡é«˜ï¼Œå¼•å…¥é”™è¯¯çš„è´Ÿåé¦ˆä¿¡å·ï¼Œä¸¥é‡æŸå®³è®­ç»ƒæ•ˆç‡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆ**è‡ªæ¼”è¿›åˆæˆæ•°æ®å¼•æ“**ä¸**åŸºäºéªŒè¯å™¨çš„å¼ºåŒ–å­¦ä¹ **ï¼š
- **EigenData**ï¼šä¸€ä¸ªåˆ†å±‚çš„ã€è‡ªæˆ‘æ¼”è¿›çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€å¯éªŒè¯çš„å¤šè½®å·¥å…·ä½¿ç”¨å¯¹è¯æ•°æ®ã€‚
  - **åˆ†å±‚æ¶æ„**ï¼šç”±**ç¼–æ’å±‚**ï¼ˆorchestration layerï¼‰è®¾è®¡å·¥ä½œæµã€ä¼˜åŒ–æç¤ºè¯å¹¶é©±åŠ¨è¿­ä»£ï¼›**æ‰§è¡Œå±‚**ï¼ˆexecution layerï¼‰ç”±å¤šä¸ªä¸“ç”¨worker agentå®Œæˆå…·ä½“çš„æ•°æ®ç”Ÿæˆä¸éªŒè¯ä»»åŠ¡ã€‚
  - **é—­ç¯è‡ªæ¼”è¿›æœºåˆ¶**ï¼šé€šè¿‡Judge Agentå¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¤šç»´åº¦æ‰¹åˆ¤ï¼ˆexecutability, tool correctness, coherenceç­‰ï¼‰ï¼Œå¹¶å°†åé¦ˆç”¨äºæ›´æ–°æç¤ºè¯å’Œå·¥ä½œæµï¼Œå®ç°æ•°æ®è´¨é‡ã€è¦†ç›–åº¦å’Œéš¾åº¦çš„æŒç»­æå‡ã€‚
  - **ç”Ÿæˆå¯æ‰§è¡ŒéªŒè¯å‡½æ•°**ï¼šä¸ºæ¯ä¸ªè®­ç»ƒå®ä¾‹ç”Ÿæˆ`verification function`ï¼Œç”¨äºåç»­RLä¸­çš„outcome rewardè®¡ç®—ï¼Œç¡®ä¿å¥–åŠ±ä¿¡å·åŸºäºæœ€ç»ˆçŠ¶æ€è€Œéå¯¹è¯æ–‡æœ¬ï¼Œå¢å¼ºé²æ£’æ€§ã€‚
- **ç¨³å®šåŒ–çš„RLè®­ç»ƒæµç¨‹**ï¼š
  - **ç”¨æˆ·æ¨¡å‹å¾®è°ƒï¼ˆUser Model Fine-tuningï¼‰**ï¼šå…ˆå¯¹ç”¨æˆ·æ¨¡æ‹Ÿå™¨è¿›è¡ŒSFTï¼Œä½¿å…¶èƒ½ç¨³å®šéµå¾ªæŒ‡ä»¤å¹¶æ­£ç¡®è°ƒç”¨å·¥å…·ï¼Œé¿å…å› ç”¨æˆ·ç«¯é”™è¯¯å¯¼è‡´çš„è™šå‡å¤±è´¥ã€‚
  - **GRPO + åŠ¨æ€è¿‡æ»¤**ï¼šé‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ï¼Œä½¿ç”¨**è½¨è¿¹çº§ç»„ç›¸å¯¹ä¼˜åŠ¿**ï¼ˆtrajectory-level group-relative advantagesï¼‰è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥**åŠ¨æ€è¿‡æ»¤**ï¼ˆdynamic filteringï¼‰æœºåˆ¶ï¼Œå‰”é™¤æ‰€æœ‰è½¨è¿¹å‡æˆåŠŸæˆ–å¤±è´¥çš„ä»»åŠ¡ï¼Œä¿ç•™å…·æœ‰åŒºåˆ†æ€§çš„è®­ç»ƒæ ·æœ¬ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ— éœ€æ˜‚è´µçš„äººå·¥æ ‡æ³¨**ï¼šå®Œå…¨ä¾èµ–è‡ªæ¼”è¿›çš„åˆæˆæ•°æ®ï¼Œæ˜¾è‘—é™ä½æ•°æ®è·å–æˆæœ¬ã€‚
- **æ•°æ®è´¨é‡æ›´é«˜ä¸”å¯éªŒè¯**ï¼šé€šè¿‡å¤šé˜¶æ®µéªŒè¯ï¼ˆå¯è¡Œæ€§ã€å·¥å…·æ­£ç¡®æ€§ã€ä¸€è‡´æ€§ï¼‰å’Œä¿®å¤æœºåˆ¶ï¼Œç¡®ä¿ç”Ÿæˆæ•°æ®çœŸå®æœ‰æ•ˆã€‚
- **RLè®­ç»ƒæ›´ç¨³å®šé«˜æ•ˆ**ï¼šé€šè¿‡ç”¨æˆ·æ¨¡å‹SFTå’ŒåŠ¨æ€è¿‡æ»¤ï¼Œå¤§å¹…å‡å°‘å™ªå£°ä¿¡å·ï¼Œä½¿RLèƒ½çœŸæ­£ä»agentè¡Œä¸ºä¸­å­¦ä¹ ã€‚
- **å…¨æµç¨‹å¼€æº**ï¼šä»£ç ä¸æ•°æ®å·²å¼€æºï¼ˆ[GitHub](https://github.com/inclusionAI/AReaL)ï¼‰ï¼Œå…·å¤‡é«˜åº¦å¯å¤ç°æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **ä¸»åŸºå‡†**ï¼š**T2-bench**ï¼ˆYao et al., 2024; Barres et al., 2025ï¼‰ï¼Œä¸€ä¸ªé¢å‘çœŸå®åœºæ™¯çš„å¤šè½®å·¥å…·ä½¿ç”¨è¯„æµ‹åŸºå‡†ï¼ŒåŒ…å«ä¸‰ä¸ªé¢†åŸŸï¼š
  - **Airline**ï¼šèˆªç­é¢„è®¢ã€æ”¹ç­¾ã€å®¢æœ
  - **Retail**ï¼šç”µå•†è®¢å•ç®¡ç†ã€å•†å“å’¨è¯¢
  - **Telecom**ï¼šç§»åŠ¨å¥—é¤ç®¡ç†ã€è´¦å•æŸ¥è¯¢
- æ¯ä¸ªé¢†åŸŸæä¾›çœŸå®çš„å·¥å…·APIã€æ•°æ®åº“çŠ¶æ€å’Œæ”¿ç­–çº¦æŸï¼Œè¦æ±‚agentåœ¨ä¸æ¨¡æ‹Ÿç”¨æˆ·çš„å¤šè½®å¯¹è¯ä¸­æ­£ç¡®æ‰§è¡Œå¤šæ­¥æ“ä½œã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹åŸºç¡€**ï¼šåŸºäº **Qwen3-MoE** ç³»åˆ—æ¨¡å‹ï¼Œä¸»è¦ä½¿ç”¨ `Qwen3-30B-A3B` å’Œ `Qwen3-235B-A22B`ã€‚
- **è®­ç»ƒæ–¹å¼**ï¼š
  - **SFT**ï¼ˆSupervised Fine-Tuningï¼‰ï¼šä½¿ç”¨EigenDataç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒã€‚
  - **RL**ï¼šåœ¨SFTåŸºç¡€ä¸Šåº”ç”¨GRPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚
- **ç”¨æˆ·æ¨¡æ‹Ÿå™¨**ï¼š
  - **è®­ç»ƒæ—¶**ï¼šä½¿ç”¨ç»è¿‡SFTå¾®è°ƒçš„å¼€æºæ¨¡å‹ä½œä¸ºç”¨æˆ·æ¨¡æ‹Ÿå™¨ã€‚
  - **è¯„ä¼°æ—¶**ï¼šç»Ÿä¸€ä½¿ç”¨ **GPT-4.1** ä½œä¸ºç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼Œä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **pass^k**ï¼škæ¬¡ç‹¬ç«‹å°è¯•å…¨éƒ¨æˆåŠŸæ‰è®¡ä¸º1ï¼Œè¡¡é‡**ä¸€è‡´æ€§ä¸å¯é æ€§**ï¼ˆæ¯”pass@kæ›´ä¸¥æ ¼ï¼‰ã€‚
  - æŠ¥å‘Š `pass^1`, `pass^2`, `pass^3`, `pass^4` åŠ `pass@4`ã€‚
- **è®­ç»ƒåŸºç¡€è®¾æ–½**ï¼šä½¿ç”¨ **AReaL** æ¡†æ¶ï¼Œåœ¨64â€“80å—H200 GPUä¸Šè¿›è¡Œå¼‚æ­¥å¤§è§„æ¨¡RLè®­ç»ƒã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **å¼€æºæ¨¡å‹**ï¼šQwen3-30B-A3Bç³»åˆ—çš„ä¸åŒè®­ç»ƒé˜¶æ®µï¼ˆBaseline, +SFT, +RLï¼‰ã€‚
- **å‰æ²¿é—­æºæ¨¡å‹**ï¼š
  - Qwen3-Max-Thinking
  - Deepseek-v3.2
  - GPT-5
  - Claude-Sonnet-4.5
  - Gemini 3.0 Pro

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 1ï¼‰
åœ¨ **Separate Training** è®¾ç½®ä¸‹ï¼Œæœ€ä½³æ¨¡å‹è¡¨ç°å¦‚ä¸‹ï¼š

| é¢†åŸŸ       | æ¨¡å‹                         | pass^1 (%) |
|------------|------------------------------|------------|
| **Airline** | Qwen3-235B-A22B-2507 (+RL)    | **73.0**   |
| **Retail**  | Qwen3-235B-A22B-2507 (+RL)    | **75.0**   |
| **Telecom** | Qwen3-235B-A22B-2507 (+RL)    | **98.3**   |

- åœ¨Airlineä¸Š**æŒå¹³Gemini 3.0 Pro**ï¼ˆ73.0ï¼‰ï¼Œ**è¶…è¶ŠGPT-5**ï¼ˆ62.5ï¼‰å’ŒClaudeï¼ˆ70.0ï¼‰ã€‚
- åœ¨Telecomä¸Š**è¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³**ï¼Œè¶…è¶Šæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ã€‚
- Retailä»æ˜¯æŒ‘æˆ˜æœ€å¤§çš„é¢†åŸŸï¼ŒClaude Sonneté¢†å…ˆï¼ˆ86.2ï¼‰ï¼Œæœ¬æ–¹æ³•è¾¾75.0ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **SFTå³å¸¦æ¥å·¨å¤§æå‡**ï¼š
  - ä¾‹å¦‚åœ¨Telecomä¸Šï¼ŒQwen3-30B-A3Bä»27.1% â†’ 80.7% pass^1ã€‚
- **RLè¿›ä¸€æ­¥æ˜¾è‘—æå‡æ€§èƒ½ä¸ä¸€è‡´æ€§**ï¼š
  - Telecomä¸Špass^1ä»85.4% â†’ 95.6%ï¼Œpass^4ä»70.8% â†’ 86.0%ï¼Œè¯´æ˜RLå¢å¼ºäº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚
- **æ··åˆè®­ç»ƒï¼ˆMix Trainingï¼‰æ•ˆæœæ›´ä¼˜**ï¼š
  - åœ¨Table 2ä¸­ï¼ŒQwen3-235B-A22B-2507ç»æ··åˆè®­ç»ƒåï¼Œ**å¹³å‡pass^1è¾¾81.3%**ï¼Œè¶…è¿‡Qwen3-Max-Thinkingï¼ˆ80.7%ï¼‰å’ŒGPT-5ï¼ˆ80.0%ï¼‰ã€‚
  - å¹³å‡pass^4è¾¾68.5%ï¼Œä¹Ÿä¼˜äºQwen3-Max-Thinkingï¼ˆ66.8%ï¼‰å’ŒGPT-5ï¼ˆ64.0%ï¼‰ã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æ•°æ®æ¶ˆèï¼ˆTable 3ï¼ŒAirlineé¢†åŸŸï¼‰
| æ•°æ®æ¥æº | pass^1 (%) |
|---------|------------|
| Human Expertï¼ˆäººå·¥è®¾è®¡ï¼‰ | 52.0 |
| **SEED Full System**ï¼ˆå®Œæ•´EigenDataï¼‰ | **56.0** |
| w/o. Validationï¼ˆæ— éªŒè¯ï¼‰ | 50.0 |
| w/o. Evolutionï¼ˆæ— è‡ªæ¼”è¿›ï¼‰ | 44.0 |
| ä»…4å¥—promptï¼ˆä½å¤šæ ·æ€§ï¼‰ | 42.5 |

âœ… ç»“è®ºï¼š**æ•°æ®è´¨é‡ï¼ˆéªŒè¯ï¼‰ä¸å¤šæ ·æ€§ï¼ˆå¤šprompté›†ï¼‰å¯¹æ€§èƒ½è‡³å…³é‡è¦**ï¼Œä¸”è‡ªåŠ¨åŒ–çš„EigenDataå¯è¶…è¶Šäººå·¥è®¾è®¡ã€‚

#### ï¼ˆ2ï¼‰ç”¨æˆ·æ¨¡å‹æ¶ˆèï¼ˆFigure 3ï¼ŒTelecomé¢†åŸŸï¼‰
- ä½¿ç”¨**æœªç»SFTçš„ç”¨æˆ·æ¨¡å‹**è¿›è¡ŒRLè®­ç»ƒï¼šæ€§èƒ½ä»SFTçš„85.4% **ä¸‹é™è‡³75.6%**ã€‚
- ä½¿ç”¨**SFTåçš„ç”¨æˆ·æ¨¡å‹**ï¼šæ€§èƒ½**æå‡è‡³95.6%**ã€‚
- âœ… ç»“è®ºï¼š**ç”¨æˆ·æ¨¡æ‹Ÿå™¨çš„è´¨é‡æ˜¯RLæˆåŠŸçš„å…³é”®**ï¼Œå¦åˆ™ä¼šå› ç”¨æˆ·é”™è¯¯æƒ©ç½šæ­£ç¡®çš„agentè¡Œä¸ºã€‚

#### ï¼ˆ3ï¼‰ç®—æ³•æ¶ˆèï¼ˆTable 4ï¼ŒAirlineé¢†åŸŸï¼‰
- **Batch Size**ï¼šæ€»batch sizeä»256å¢è‡³512ï¼Œpass^1ä»~65% â†’ **70.5%**ï¼Œè¡¨æ˜æ›´å¤§çš„batchå¸¦æ¥æ›´ç¨³å®šçš„GRPOä¼˜åŠ¿ä¼°è®¡ã€‚
- **Dynamic Filtering**ï¼šå…³é—­è¯¥æœºåˆ¶ï¼Œpass^1ä»70.5% â†’ **65.0%**ï¼Œpass^4ä»52.0% â†’ 40.0%ã€‚
- âœ… ç»“è®ºï¼š**å¤§batch size** å’Œ **åŠ¨æ€è¿‡æ»¤** æ˜¯ç¨³å®šRLè®­ç»ƒçš„é‡è¦ç»„ä»¶ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è‡ªæ¼”è¿›åˆæˆæ•°æ®ï¼ˆEigenDataï¼‰å¯åª²ç¾ç”šè‡³è¶…è¶Šäººå·¥æ ‡æ³¨æ•°æ®**ï¼Œä¸”å…·å¤‡é«˜åº¦å¯æ‰©å±•æ€§ã€‚
2. **åŸºäºéªŒè¯å™¨çš„outcome reward + GRPO + åŠ¨æ€è¿‡æ»¤** æ„æˆäº†ç¨³å®šé«˜æ•ˆçš„RLè®­ç»ƒèŒƒå¼ã€‚
3. **ç”¨æˆ·æ¨¡æ‹Ÿå™¨çš„è´¨é‡ç›´æ¥å½±å“RLæˆè´¥**ï¼Œå¿…é¡»é€šè¿‡SFTç¡®ä¿å…¶è¡Œä¸ºå¯é ã€‚
4. è¯¥æ¡†æ¶èƒ½åœ¨**å…¨å¼€æºæ¨¡å‹**ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šå‰æ²¿é—­æºæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†**å¼€æ”¾ç”Ÿæ€åœ¨å¤æ‚agentè®­ç»ƒä¸Šçš„æ½œåŠ›**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡çš„åŸºç¡€æ¨¡å‹**ï¼šè‹¥åˆå§‹LLMèƒ½åŠ›ä¸è¶³ï¼Œåˆæˆæ•°æ®è´¨é‡å’ŒRLæ•ˆæœå¯èƒ½å—é™ã€‚
- **é¢†åŸŸè¿ç§»æˆæœ¬**ï¼šè™½ç„¶å£°ç§°â€œdomain-agnosticâ€ï¼Œä½†ä»éœ€æä¾›é¢†åŸŸæ–‡æ¡£å’Œå·¥å…·schemaï¼Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æœªå……åˆ†éªŒè¯ã€‚
- **è®¡ç®—èµ„æºéœ€æ±‚é«˜**ï¼šå¤§è§„æ¨¡RLè®­ç»ƒéœ€æ•°åè‡³ä¸Šç™¾å¼ é«˜ç«¯GPUï¼Œå¯¹ä¸€èˆ¬ç ”ç©¶è€…é—¨æ§›ä»é«˜ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´è½»é‡åŒ–çš„è‡ªæ¼”è¿›æœºåˆ¶ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚
- å°†æ¡†æ¶æ‰©å±•åˆ°æ›´å¤šç°å®ä¸–ç•Œåœºæ™¯ï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰ã€‚
- ç ”ç©¶å¦‚ä½•è®©agentä¸»åŠ¨å‘ç°å¹¶åˆ©ç”¨æ–°å·¥å…·ï¼ˆopen-world tool discoveryï¼‰ã€‚
- åŠ å¼ºå®‰å…¨ä¸å¯æ§æ€§æœºåˆ¶ï¼Œé˜²æ­¢å·¥å…·æ»¥ç”¨ã€‚

--- 

> **æ€»ä½“è¯„ä»·**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€æ¡**å¯æ‰©å±•ã€å¯å¤ç°ã€é«˜æ€§èƒ½**çš„è·¯å¾„ï¼Œç”¨äºè®­ç»ƒå¤æ‚çš„å¤šè½®äº¤äº’å¼å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“ã€‚å…¶æ ¸å¿ƒæ€æƒ³â€”â€”**ç”¨è‡ªæ¼”è¿›åˆæˆæ•°æ®è§£å†³æ•°æ®ç“¶é¢ˆï¼Œç”¨å¯éªŒè¯å¥–åŠ±å’Œç¨³å®šåŒ–RLè§£å†³è®­ç»ƒå™ªå£°**â€”â€”ä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ä½“ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦èŒƒå¼ã€‚

</details>

---

### 10. [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)

**Authors**: Xin Jennifer Chen, Yunjin Tong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22315v1  

#### Abstract
Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šGaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**æ˜‚è´µåé¦ˆä¸‹çš„è¿ç»­ç©ºé—´ä¼˜åŒ–é—®é¢˜**ï¼Œå°¤å…¶æ˜¯åœ¨ç§‘å­¦å‡è®¾ç”Ÿæˆã€å·¥ç¨‹è®¾è®¡ç­‰åœºæ™¯ä¸­ï¼Œå­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **Ground-truth oracleï¼ˆçœŸå®åé¦ˆï¼‰æˆæœ¬é«˜æ˜‚**ï¼šå¦‚äººç±»è¯„ä¼°ã€ç‰©ç†å®éªŒç­‰ï¼Œéš¾ä»¥é¢‘ç¹è°ƒç”¨ã€‚
- **å»‰ä»·ä½†æœ‰åçš„é¢„æµ‹æ¨¡å‹å¯ç”¨**ï¼šå¦‚æœºå™¨å­¦ä¹ æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œèƒ½å¿«é€Ÿæä¾›ä½ä¿çœŸåº¦ï¼ˆlow-fidelityï¼‰é¢„æµ‹ï¼Œä½†å¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§åå·®ã€‚
- **å·²æœ‰å¤§é‡ç¦»çº¿æ•°æ®**ï¼šå¯ç”¨äºé¢„è®­ç»ƒé¢„æµ‹æ¨¡å‹å¹¶æ„å»ºå…ˆéªŒçŸ¥è¯†ã€‚

ä¼ ç»Ÿæ–¹æ³•å¦‚ Vanilla GP-UCB ä»…ä¾èµ–æ˜‚è´µçš„çœŸå®åé¦ˆï¼Œæ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼›è€Œå¤šä¿çœŸåº¦ï¼ˆmulti-fidelityï¼‰æ–¹æ³•é€šå¸¸éœ€åœ¨ä¸åŒä¿çœŸåº¦ä¹‹é—´æƒè¡¡æŸ¥è¯¢æˆæœ¬ï¼Œä¸é€‚ç”¨äºâ€œé¢„æµ‹è¿‘ä¹å…è´¹â€çš„åœºæ™¯ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šPA-GP-UCB
ä½œè€…æå‡º **Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB)**ï¼Œä¸€ç§ç»“åˆé«˜æˆæœ¬çœŸå®åé¦ˆã€ä½æˆæœ¬é¢„æµ‹ä¸ç¦»çº¿æ•°æ®çš„è´å¶æ–¯ä¼˜åŒ–ç®—æ³•ã€‚

#### æ ¸å¿ƒæ€æƒ³ä¸åˆ›æ–°ï¼š
1. **åŒé˜¶æ®µæ¡†æ¶**ï¼š
   - **Offline é˜¶æ®µ**ï¼šåˆ©ç”¨ç¦»çº¿æ•°æ®æˆ–ä¸»åŠ¨æŸ¥è¯¢é¢„æµ‹ oracleï¼Œåœ¨ç©ºé—´å¡«å……è®¾è®¡ï¼ˆspace-filling designï¼‰ä¸Šè·å–å¤§é‡å»‰ä»·çš„ `f_ML` é¢„æµ‹å€¼ï¼Œç”¨äºæ„å»ºå¼ºå…ˆéªŒã€‚
   - **Online é˜¶æ®µ**ï¼šæ¯è½®åŒæ—¶æŸ¥è¯¢çœŸå® oracle å’Œé¢„æµ‹ oracleï¼Œåˆ©ç”¨ä¸¤è€…ç›¸å…³æ€§è¿›è¡Œè”åˆå»ºæ¨¡ã€‚

2. **Control-Variates Estimatorï¼ˆæ§åˆ¶å˜é‡ä¼°è®¡å™¨ï¼‰**ï¼š
   - å¼•å…¥ä¸€ä¸ªåŸºäº GP åéªŒçš„åå·®æ ¡æ­£æœºåˆ¶ï¼š
     $$
     \hat{\mu}(x) = \mu^{\text{true}}(x) - \rho(x)\frac{\sigma^{\text{true}}(x)}{\sigma^{\text{ML}}(x)}(\mu^{\text{ML}}(x) - f^{\text{ML}}(x))
     $$
   - åˆ©ç”¨é¢„æµ‹æ®‹å·®æ¥ä¿®æ­£çœŸå®å‡½æ•°çš„åéªŒå‡å€¼ï¼Œå®ç°**æ–¹å·®ç¼©å‡ä¸åå·®æ ¡æ­£**ã€‚

3. **Joint Multi-task GP Prior**ï¼š
   - å°†çœŸå®å‡½æ•° `f_true` å’Œé¢„æµ‹å‡½æ•° `f_ML` è§†ä¸ºä¸¤ä¸ªç›¸å…³ä»»åŠ¡ï¼Œé€šè¿‡ multi-task GP å»ºæ¨¡å…¶åæ–¹å·®ç»“æ„ï¼Œæ˜¾å¼æ•æ‰äºŒè€…ä¹‹é—´çš„ç»Ÿè®¡ä¾èµ–å…³ç³»ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | å±€é™æ€§ | PA-GP-UCB çš„æ”¹è¿› |
|------|--------|------------------|
| **Vanilla GP-UCB** | å¿½ç•¥æ‰€æœ‰é¢„æµ‹ä¿¡æ¯ï¼Œæ”¶æ•›æ…¢ | æ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡ |
| **Naive Prediction-Augmented GP-UCB** | ç›´æ¥èåˆé¢„æµ‹ä½œä¸ºå…ˆéªŒï¼Œæ— æ³•çº æ­£å±€éƒ¨åå·® | ä½¿ç”¨ control-variates åŠ¨æ€çº åï¼Œé¿å…é™·å…¥é”™è¯¯åŒºåŸŸ |
| **MLA-UCB (Ji et al., 2025)** | ä»…é€‚ç”¨äºç¦»æ•£åŠ¨ä½œç©ºé—´ | æ”¯æŒè¿ç»­ç©ºé—´ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–å…³ç³» |
| **Multi-fidelity Bandits** | å…³æ³¨æˆæœ¬åˆ†é…ï¼Œä¸å‡å°‘é«˜ä¿çœŸæŸ¥è¯¢æ•° | åœ¨å›ºå®šé«˜ä¿çœŸé¢„ç®—ä¸‹ä¸¥æ ¼é™ä½ regret |

> âœ… **ç†è®ºä¼˜åŠ¿**ï¼šåœ¨æ ‡å‡†å‡è®¾ä¸‹ï¼ŒPA-GP-UCB ä¿æŒä¸ GP-UCB ç›¸åŒçš„ $O(\sqrt{T d \gamma_T})$ ç´¯ç§¯ regret ä¸Šç•Œï¼Œä½†**å‰å¯¼å¸¸æ•°æ›´å°**ï¼Œä¸”è¯¥å¸¸æ•°ç”±é¢„æµ‹è´¨é‡ï¼ˆç›¸å…³ç³»æ•° $\rho$ï¼‰å’Œç¦»çº¿æ•°æ®è¦†ç›–ç¨‹åº¦ï¼ˆ$R$ï¼‰æ˜¾å¼æ§åˆ¶ï¼š
> $$
> R_T \leq \sqrt{C \beta_T \gamma_T [1 - (1-R)\rho^2]} + \delta
> $$
> å½“ $\rho > 0$ ä¸” $R < 1$ æ—¶ï¼Œæ€§èƒ½ä¸¥æ ¼ä¼˜äº Vanilla GP-UCBã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
1. **åˆæˆåŸºå‡†ï¼ˆSynthetic Benchmarksï¼‰**ï¼š
   - ä» RBF æ ¸çš„ GP ä¸­é‡‡æ ·çœŸå®å‡½æ•° $f$ã€‚
   - æ„é€ ç›¸å…³ä½†æœ‰åçš„é¢„æµ‹å‡½æ•°ï¼š$f_{\text{ML}} = \rho f + \sqrt{1-\rho^2} g$ï¼Œå…¶ä¸­ $g$ æ˜¯ç‹¬ç«‹ GP æ ·æœ¬ã€‚
   - æ•…æ„å¼•å…¥**å±€éƒ¨åç›¸å…³åŒºåŸŸ**ï¼ˆå¦‚ $[0.4, 0.6]$ åŒºé—´ç¿»è½¬ç¬¦å·ï¼‰ï¼Œæµ‹è¯•å¯¹é¢„æµ‹è¯¯è®¾çš„é²æ£’æ€§ã€‚

2. **çœŸå®ä¸–ç•Œå‡è®¾ç”Ÿæˆä»»åŠ¡ï¼ˆReal-world Hypothesis Generationï¼‰**ï¼š
   - åŸºäº Milkman et al. (2021) çš„å¤§è§„æ¨¡è¡Œä¸ºå®éªŒæ•°æ®é›†ï¼ŒåŒ…å« 54 ç§å¹²é¢„æªæ–½åŠå…¶å¯¹åº”çš„ç”¨æˆ·è®¿é—®ç‡ï¼ˆground-truth rewardï¼‰ã€‚
   - æ„å»ºä¸¤ç§ç¯å¢ƒï¼š
     - **Finite-Arm Setting**ï¼šç›´æ¥åœ¨ 54 ä¸ªç¦»æ•£å‡è®¾ä¸Šè¿è¡Œ banditã€‚
     - **Continuous Benchmark**ï¼šä½¿ç”¨ TF-IDF + UMAP å°†å¹²é¢„æè¿°åµŒå…¥åˆ°äºŒç»´è¯­ä¹‰æµå½¢ä¸Šï¼Œå¹¶æ‹Ÿåˆ GP å¾—åˆ°è¿ç»­å¥–åŠ±å‡½æ•° $f_{\text{GP}}(x)$ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **Horizon**: $T = 200$
- **é‡å¤æ¬¡æ•°**: 50 æ¬¡ç‹¬ç«‹è¿è¡Œå–å¹³å‡
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **ç´¯ç§¯ regret**: $R_T = \sum_{t=1}^T [f(x^*) - f(x_t)]$
  - **ç¬æ—¶ regret æ›²çº¿**
  - **æœ€ä¼˜è§£å®šä½ç²¾åº¦**

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Vanilla GP-UCB**ï¼šä»…ä½¿ç”¨çœŸå®åé¦ˆã€‚
- **GP-UCB with offline prediction only**ï¼šä»…å°†ç¦»çº¿é¢„æµ‹åŠ å…¥å…ˆéªŒï¼Œæ— åœ¨çº¿çº åã€‚
- **GP-UCB with offline + online prediction**ï¼šèåˆæ‰€æœ‰é¢„æµ‹æ•°æ®ä½†ä¸çº åã€‚
- **PA-GP-UCB variants**ï¼šä¸åŒç¦»çº¿æ•°æ®é‡ $(M, N)$ã€ä¸åŒé¢„æµ‹ç›¸å…³æ€§ $\rho$ è®¾ç½®ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
#### ï¼ˆ1ï¼‰åˆæˆå®éªŒï¼ˆå›¾2ï¼‰
| æ–¹æ³• | ç´¯ç§¯ regret ($T=200$) | ç›¸å¯¹æå‡ |
|------|------------------------|----------|
| Vanilla GP-UCB | ~7.8 | â€” |
| GP-UCB + offline pred | ~6.5 | ~17% â†“ |
| GP-UCB + offline+online pred | ~6.3 | ~19% â†“ |
| **PA-GP-UCB ($\rho=0.8$)** | **~4.2** | **~46% â†“** âœ… |

- å³ä½¿åœ¨é¢„æµ‹å±€éƒ¨åç›¸å…³çš„æƒ…å†µä¸‹ï¼ŒPA-GP-UCB èƒ½å¤Ÿé€šè¿‡åœ¨çº¿åé¦ˆåŠ¨æ€çº åï¼Œæœ€ç»ˆæ”¶æ•›è‡³çœŸå®æœ€ä¼˜ã€‚
- å›¾1æ˜¾ç¤ºï¼šPA-GP-UCB çš„é‡‡æ ·é›†ä¸­åœ¨çœŸå®æœ€ä¼˜é™„è¿‘ï¼Œè€Œ naive æ–¹æ³•è¢«è¯¯å¯¼è‡³é¢„æµ‹åå¥½åŒºã€‚

#### ï¼ˆ2ï¼‰çœŸå®å‡è®¾ç”Ÿæˆä»»åŠ¡ï¼ˆå›¾3ï¼‰
##### Finite-Arm Settingï¼ˆå›¾3aï¼‰
- é¢„æµ‹æ¨¡å‹ä¸çœŸå® reward çš„ç»éªŒç›¸å…³æ€§ $\rho = 0.66$
- PA-GP-UCB åœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äº Vanilla GP-UCB
- éšç€ç¦»çº¿é¢„æµ‹å‰¯æœ¬æ•°å¢åŠ ï¼ˆ$N=1 \to 100$ï¼‰ï¼Œæ€§èƒ½æŒç»­æå‡ï¼Œè¡¨æ˜ç¦»çº¿æ•°æ®æœ‰æ•ˆé™ä½äº† posterior uncertainty

##### Continuous Settingï¼ˆå›¾3bï¼‰
- ä½¿ç”¨ LLMï¼ˆOpenAI o4-miniï¼‰ä½œä¸ºé¢„æµ‹ oracle
- ä¸‰ç§æç¤ºæ–¹å¼ï¼š
  - **Scale-only**ï¼šä»…æä¾›å…¨å±€ç»Ÿè®¡ï¼ˆmin/max/meanï¼‰
  - **K-shot**ï¼šæä¾› $K=4$ æˆ– $12$ ä¸ªå¸¦æ ‡ç­¾ç¤ºä¾‹
- æ‰€æœ‰å˜ä½“ä¸‹ PA-GP-UCB å‡å¤§å¹…é¢†å…ˆ Vanilla GP-UCB
- **Scale-only è¡¨ç°æœ€ä½³**ï¼šè¯´æ˜å…¨å±€å°ºåº¦ä¿¡æ¯å³å¯æœ‰æ•ˆå¼•å¯¼æ¢ç´¢ï¼Œé¿å…è¿‡æ‹Ÿåˆå°‘é‡ç¤ºä¾‹

> ğŸ“Œ **å…³é”®å‘ç°**ï¼šå³ä½¿é¢„æµ‹è´¨é‡ä¸­ç­‰ï¼ˆ$\rho \approx 0.66 \sim 0.8$ï¼‰ã€ç¦»çº¿æ•°æ®ç¨€ç–ï¼ˆ$M=N=1$ï¼‰ï¼ŒPA-GP-UCB ä»èƒ½å–å¾—æ˜¾è‘—æ”¶ç›Šã€‚

---

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰é¢„æµ‹ç›¸å…³æ€§ $\rho$ å½±å“ï¼ˆå›¾2bï¼‰
- $\rho = 0.5$: ä»æœ‰æ˜æ˜¾å¢ç›Š
- $\rho = 0.9$: æ€§èƒ½è¿›ä¸€æ­¥æå‡
- è¡¨æ˜æ–¹æ³•å¯¹å¼±ç›¸å…³é¢„æµ‹ä¹Ÿå…·æœ‰é²æ£’æ€§

#### ï¼ˆ2ï¼‰ç¦»çº¿æ•°æ®è§„æ¨¡å½±å“ï¼ˆå›¾2cï¼‰
- $M=1, N=1$ï¼ˆæå°ç¦»çº¿é¢„ç®—ï¼‰å·²å¯å¸¦æ¥æ€§èƒ½æå‡
- å¢åŠ  $M, N$ å¯è¿›ä¸€æ­¥å‹ç¼© posterior varianceï¼ŒåŠ é€Ÿæ”¶æ•›
- å®é™…æ€§èƒ½æå‡è¿œè¶…ç†è®ºä¿å®ˆæ¡ä»¶è¦æ±‚ï¼ˆå…¬å¼5ï¼‰

#### ï¼ˆ3ï¼‰å™ªå£°é²æ£’æ€§ï¼ˆé™„å½•å›¾5ï¼‰
- åœ¨é«˜ observation noise ä¸‹ï¼ŒPA-GP-UCB ç›¸å¯¹ Vanilla GP-UCB çš„ä¼˜åŠ¿æ›´å¤§
- å¯¹ prediction noise å…·æœ‰è¾ƒå¼ºé²æ£’æ€§ï¼Œå³ä½¿ $\sigma^2_{\text{pred}} = 0.5$ ä»ä¼˜äº baseline

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **PA-GP-UCB æ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡**ï¼šåœ¨ç›¸åŒé«˜ä¿çœŸæŸ¥è¯¢é¢„ç®—ä¸‹ï¼Œç›¸æ¯” Vanilla GP-UCB å®ç°æ›´ä½çš„ç´¯ç§¯ regretã€‚
2. âœ… **control-variates estimator æœ‰æ•ˆçº å**ï¼šèƒ½å¤Ÿåœ¨é¢„æµ‹å­˜åœ¨å±€éƒ¨è¯¯å¯¼æ—¶ä»æ­£ç¡®æ”¶æ•›ï¼Œé¿å…â€œç›²ç›®ä¿¡ä»»â€é¢„æµ‹ã€‚
3. âœ… **ç¦»çº¿æ•°æ®æ˜¯å…³é”®åŠ é€Ÿå™¨**ï¼šå³ä½¿æ˜¯å°‘é‡å»‰ä»·é¢„æµ‹ä¹Ÿèƒ½æ˜¾è‘—æ”¹å–„ posterior ç»“æ„ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç–åŒºåŸŸã€‚
4. âœ… **é€‚ç”¨äºç°å®å‡è®¾ç”Ÿæˆæµç¨‹**ï¼šåœ¨åŸºäºäººç±»è¡Œä¸ºæ•°æ®çš„çœŸå®ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸè¯†åˆ«å‡ºé«˜è´¨é‡å¹²é¢„ç­–ç•¥ï¼ˆå¦‚â€œé«˜é¢‘å°é¢æ¿€åŠ± + æé†’æœºåˆ¶â€æœ€æœ‰æ•ˆï¼‰ã€‚
5. âœ… **å¯¹æ ¸å‡½æ•°è¯¯è®¾æ›´å…·é²æ£’æ€§**ï¼šç”±äºå¼•å…¥è¾…åŠ©ä¿¡å·ï¼ŒPA-GP-UCB å¯¹ GP kernel çš„é€‰æ‹©ä¸æ•æ„Ÿï¼Œè€Œ Vanilla GP-UCB åœ¨éå…‰æ»‘ reward ä¸‹è¡¨ç°ä¸ç¨³å®šã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **ç†è®ºæ¡ä»¶è¾ƒä¿å®ˆ**ï¼šæ‰€éœ€ç¦»çº¿æ•°æ®å¯†åº¦ $(\epsilon, N)$ æ¡ä»¶è¿‡äºä¸¥æ ¼ï¼Œå®é™…ä¸­è¿œä½äºæ­¤ä¹Ÿå¯å¥æ•ˆã€‚
2. **ä¾èµ–é¢„æµ‹ä¸çœŸå®å‡½æ•°çš„ç›¸å…³æ€§**ï¼šè‹¥ $\rho \approx 0$ï¼Œåˆ™å¢ç›Šæœ‰é™ã€‚
3. **æœªè€ƒè™‘é¢„æµ‹æˆæœ¬**ï¼šå‡è®¾é¢„æµ‹ oracle å®Œå…¨å…è´¹ï¼Œä¸é€‚ç”¨äºè®¡ç®—å¼€é”€è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå¦‚å¤§æ¨¡å‹æ¨ç†ï¼‰ã€‚
4. **é™æ€ç¦»çº¿è®¾è®¡**ï¼šå½“å‰ä½¿ç”¨å‡åŒ€ $\epsilon$-netï¼Œæœªæ¥å¯æ‰©å±•ä¸ºè‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼±åŒ–ç¦»çº¿æ•°æ®å‡è®¾**ï¼šç ”ç©¶æ›´é«˜æ•ˆçš„ adaptive æˆ– non-uniform offline è®¾è®¡ï¼Œæå‡æ•°æ®åˆ©ç”¨ç‡ã€‚
2. **è½¨è¿¹ä¾èµ–çš„ regret åˆ†æ**ï¼šåŸºäºå®é™…æ¢ç´¢è·¯å¾„è€Œé worst-case uniform bound è¿›è¡Œåˆ†æã€‚
3. **cost-aware offline sampling**ï¼šåœ¨é¢„æµ‹ä¹Ÿæœ‰æˆæœ¬æ—¶ï¼Œä¼˜åŒ– offline-online æŸ¥è¯¢æƒè¡¡ã€‚
4. **æ‰©å±•è‡³å¤§è§„æ¨¡ç§‘å­¦å‘ç°**ï¼šåº”ç”¨äºè¯ç‰©è®¾è®¡ã€ææ–™ç§‘å­¦ç­‰é¢†åŸŸï¼Œæ”¯æŒæµ·é‡å‡è®¾ç©ºé—´ä¸­çš„é«˜æ•ˆæœç´¢ã€‚
5. **ç†è®ºåˆ†æ kernel/prediction è¯¯è®¾ä¸‹çš„é²æ£’æ€§**ï¼šå»ºç«‹æ›´å¼ºçš„ç¨³å¥æ€§ä¿è¯ã€‚

---

> ğŸ” **æ€»ä½“è¯„ä»·**ï¼š  
> PA-GP-UCB æ˜¯é¦–ä¸ªå°† **prediction-powered inference** ä¸ **Bayesian optimization** åœ¨è¿ç»­ç©ºé—´ä¸­æœ‰æœºç»“åˆçš„æ¡†æ¶ï¼Œä¸ºâ€œäººæœºååŒå‘ç°â€æä¾›äº†**ç†è®ºå¯è¯ã€å®è·µé«˜æ•ˆ**çš„æ–°èŒƒå¼ï¼Œç‰¹åˆ«é€‚åˆäº**æ•°æ®ç¨€ç¼ºã€åé¦ˆæ˜‚è´µã€é¢„æµ‹å¯ç”¨**çš„ç§‘å­¦æ¢ç´¢åœºæ™¯ã€‚

</details>

---

### 11. [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)

**Authors**: Babak Shahbaba, Zahra Moslemi  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22539v1  

#### Abstract
Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compu...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠNeural-Inspired Posterior Approximation (NIPA)ã€‹æ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è´å¶æ–¯æ¨æ–­ï¼ˆBayesian inferenceï¼‰åœ¨å¤æ‚é«˜ç»´æ¨¡å‹ï¼ˆå¦‚ Bayesian Deep Learningï¼‰ä¸­é¢ä¸´ä¸¥é‡çš„**è®¡ç®—æ•ˆç‡ç“¶é¢ˆ**ã€‚ä¼ ç»Ÿçš„ MCMC æ–¹æ³•ï¼ˆå¦‚ HMCï¼‰è™½ç„¶æ— åï¼Œä½†è®¡ç®—ä»£ä»·é«˜æ˜‚ï¼›è€Œå˜åˆ†æ¨æ–­ï¼ˆVBï¼‰ç­‰è¿‘ä¼¼æ–¹æ³•è™½å¿«ï¼Œå´å¸¸å¼•å…¥åå·®å¹¶ä½ä¼°ä¸ç¡®å®šæ€§ã€‚å¦‚ä½•åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹æå‡å¤§è§„æ¨¡è´å¶æ–¯æ¨æ–­çš„å¯æ‰©å±•æ€§ï¼Œæ˜¯å½“å‰çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„é€šç”¨æ¡†æ¶â€”â€”**Neural-Inspired Posterior Approximation (NIPA)**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ¨¡æ‹Ÿäººç±»å¤§è„‘ä¸­ä¸‰ç§äº’è¡¥çš„å­¦ä¹ ä¸å†³ç­–æœºåˆ¶æ¥é«˜æ•ˆæ¢ç´¢åéªŒåˆ†å¸ƒç©ºé—´ï¼š

- **Model-Based (MB) æ¨¡å—**ï¼šåŸºäºç›®æ ‡åˆ†å¸ƒè¿›è¡Œç²¾ç¡®ä½†ç¼“æ…¢çš„é‡‡æ ·ï¼ˆç±»æ¯”â€œæ·±æ€ç†Ÿè™‘â€çš„è§„åˆ’ï¼‰ï¼Œä½¿ç”¨æ ‡å‡† HMC ç®—æ³•ã€‚
- **Model-Free (MF) æ¨¡å—**ï¼šåˆ©ç”¨å†å²æ ·æœ¬å­¦ä¹ å‚æ•°ç©ºé—´ä¸­çš„æ¨¡å¼ï¼Œæ„å»ºä»£ç†å‡½æ•°ï¼ˆsurrogate functionï¼‰å®ç°å¿«é€Ÿé‡‡æ ·ï¼ˆç±»æ¯”â€œä¹ æƒ¯æ€§ååº”â€ï¼‰ï¼Œé‡‡ç”¨ autoencoder + DNN æ„å»ºä½ç»´ç©ºé—´ä¸‹çš„ log-posterior è¿‘ä¼¼ã€‚
- **Episodic Control (EC) æ¨¡å—**ï¼šé€šè¿‡æ£€ç´¢æœ€è¿‘çš„è®°å¿†ï¼ˆå³å·²æœ‰çš„æ ·æœ¬ï¼‰ç›´æ¥å¤ç”¨å…¶ log-posterior å€¼ï¼Œå®ç°â€œä¸€æ¬¡æ€§â€å¿«é€Ÿé‡‡æ ·ï¼ˆç±»æ¯”â€œæƒ…æ™¯è®°å¿†â€ï¼‰ã€‚

NIPA åŠ¨æ€é€‰æ‹©è¿™ä¸‰ä¸ªæ¨¡å—ä¹‹ä¸€æ‰§è¡Œé‡‡æ ·ï¼Œä¾æ®æè®®çŠ¶æ€ä¸å·²æœ‰æ ·æœ¬æ± çš„è·ç¦»å†³å®šæ¿€æ´»å“ªä¸ªç»„ä»¶ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç»Ÿä¸€æ¡†æ¶**ï¼šå°† MCMCã€VBã€CES ç­‰å¤šç§æ–¹æ³•è§†ä¸ºç‰¹ä¾‹ï¼Œå®ç°äº†çµæ´»é›†æˆã€‚
- **é«˜æ•ˆæ€§**ï¼šé€šè¿‡ MF å’Œ EC æ¨¡å—å¤§å¹…å‡å°‘æ˜‚è´µçš„ç›®æ ‡å‡½æ•°åŠå…¶æ¢¯åº¦è®¡ç®—æ¬¡æ•°ã€‚
- **å‡†ç¡®æ€§ä¿éšœ**ï¼šä¿ç•™ MB æ¨¡å—ç¡®ä¿å¯¹æœªçŸ¥åŒºåŸŸçš„ç¨³å¥æ¢ç´¢ï¼Œé¿å…é™·å…¥å±€éƒ¨è¿‘ä¼¼è¯¯å·®ã€‚
- **è‰¯å¥½çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰èƒ½åŠ›**ï¼šç›¸æ¯” VIã€MC-Dropout ç­‰æ–¹æ³•ï¼Œèƒ½æ›´å‡†ç¡®åœ°ä¼°è®¡é¢„æµ‹åŒºé—´è¦†ç›–æ¦‚ç‡ï¼ˆCP95ï¼‰å’Œæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **å›å½’ä»»åŠ¡**ï¼š
  - **åˆæˆæ•°æ®é›†**ï¼šä¸¤å±‚ ReLU ç½‘ç»œç”Ÿæˆï¼Œå« 5,000 ä¸ªæ ·æœ¬ï¼Œ100 ç»´ç‰¹å¾ï¼Œçº¦ 3.5k å‚æ•°ã€‚
  - **Year Prediction MSD æ•°æ®é›†**ï¼šé¢„æµ‹æ­Œæ›²å‘å¸ƒå¹´ä»½ï¼Œå« 515,345 ä¸ªæ ·æœ¬ï¼Œ90 ç»´éŸ³é¢‘ç‰¹å¾ï¼Œæ¨¡å‹å‚æ•°è¾¾ 309,761ã€‚
- **åˆ†ç±»ä»»åŠ¡**ï¼š
  - **åˆæˆäºŒåˆ†ç±»æ•°æ®é›†**ï¼šä¸¤å±‚ ReLU ç½‘ç»œç”Ÿæˆï¼Œ20,000 æ ·æœ¬ï¼Œ512 ç»´ç‰¹å¾ï¼Œ147,841 å‚æ•°ã€‚
  - **MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†**ï¼šå¥‡å¶æ•°å­—åˆ†ç±»ä»»åŠ¡ï¼Œ70,000 å›¾åƒï¼Œ784 ç»´åƒç´ è¾“å…¥ï¼Œ217,538 å‚æ•°ã€‚

æ‰€æœ‰æ•°æ®å‡åˆ’åˆ†ä¸ºå›ºå®šè®­ç»ƒ/æµ‹è¯•é›†ç”¨äºå…¬å¹³æ¯”è¾ƒã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **é‡‡æ ·æ•°é‡**ï¼šæ¯ä¸ªç®—æ³•ç”Ÿæˆ $ S = 2000 $ ä¸ª posterior samplesã€‚
- **è¯„ä¼°ç»´åº¦**ï¼š
  - **é¢„æµ‹æ€§èƒ½**ï¼š
    - å›å½’ï¼š**RMSE**ï¼ˆRoot Mean Squared Errorï¼‰
    - åˆ†ç±»ï¼š**Accuracy**
  - **ä¸ç¡®å®šæ€§è´¨é‡**ï¼š
    - å›å½’ï¼š**CP95**ï¼ˆ95% é¢„æµ‹åŒºé—´çš„å®é™…è¦†ç›–ç‡ï¼‰
    - åˆ†ç±»ï¼š**ECE**ï¼ˆExpected Calibration Errorï¼‰ï¼Œè¶Šå°è¶Šå¥½
  - **è®¡ç®—æ•ˆç‡ä¸é‡‡æ ·è´¨é‡**ï¼š
    - **Wall-clock time (ç§’)**
    - **Effective Sample Size (ESS)**ï¼šæœ€å°ã€ä¸­ä½æ•°ã€æœ€å¤§å€¼ï¼Œåæ˜ æ··åˆé€Ÿåº¦
    - **minESS/s**ï¼šè¡¡é‡å•ä½æ—¶é—´æœ€æ…¢æ–¹å‘çš„æœ‰æ•ˆæ ·æœ¬æ•°
    - **Speedup**ï¼šç›¸å¯¹äº BNN-HMC çš„åŠ é€Ÿæ¯”

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ç¡®å®šæ€§æ¨¡å‹**ï¼š
  - DNNï¼ˆæ—  UQï¼‰
  - DNN-Ensembleï¼ˆé›†æˆæ³•æä¾›ç»éªŒä¸ç¡®å®šæ€§ï¼‰
- **è´å¶æ–¯åŸºçº¿**ï¼š
  - **MCMC ç±»**ï¼šBNN-HMCï¼ˆåŸºå‡†ï¼‰ã€BNN-SGHMCã€BNN-pCN
  - **VI ç±»**ï¼šBNN-VIã€BNN-MCDï¼ˆMC-Dropoutï¼‰ã€BNN-LASSO
  - **å…¶ä»–è¿‘ä¼¼æ–¹æ³•**ï¼šSWAGã€BNN-RNSï¼ˆRandom Network Surrogateï¼‰

æ­¤å¤–è¿˜è¿›è¡Œäº† NIPA å„æ¨¡å—å•ç‹¬è¿è¡Œçš„æ¶ˆèå®éªŒï¼ˆNIPA-MB only, MF only, EC onlyï¼‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Tables 1 & 2ï¼‰

| æ–¹æ³• | ä»»åŠ¡ | Time (s) | Speedup vs HMC | RMSE / Accuracy | CP95 / ECE | minESS/s |
|------|------|----------|----------------|------------------|-------------|-----------|
| **BNN-HMC (Baseline)** | Year Pred | 10,875 | 1.00 | 8.96 | 85.89 | 0.0053 |
| **NIPA (full)** | Year Pred | **1,225** | **6.99Ã—** | 8.89 | **83.15** | **0.0370** |
| **BNN-HMC (Baseline)** | MNIST | 12,043 | 1.00 | 98.07 | 0.51 | 0.0051 |
| **NIPA (full)** | MNIST | **1,579** | **8.65Ã—** | 98.01 | **0.39** | **0.028** |

> æ³¨ï¼šSpeedup å®šä¹‰ä¸º `(minESS/s)_NIPA / (minESS/s)_HMC`

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **æ˜¾è‘—æé€Ÿ**ï¼š
  - åœ¨ Year Prediction ä¸Šæ¯” BNN-HMC å¿« **6.99 å€**
  - åœ¨ MNIST ä¸Šå¿« **8.65 å€**
  - æ˜¾è‘—ä¼˜äº SGHMCã€pCNã€SWAG ç­‰ä¸»æµæ–¹æ³•
- **ä¿æŒç”šè‡³æå‡é¢„æµ‹æ€§èƒ½**ï¼š
  - RMSE ä¸ HMC ç›¸å½“æˆ–æ›´ä¼˜
  - åˆ†ç±»å‡†ç¡®ç‡æ¥è¿‘æœ€ä¼˜æ°´å¹³
- **æ›´å¥½çš„ä¸ç¡®å®šæ€§é‡åŒ–**ï¼š
  - **CP95 æ›´é«˜**ï¼šè¡¨æ˜é¢„æµ‹åŒºé—´æ›´å…·åŒ…å®¹æ€§å’Œå¯é æ€§
  - **ECE æ›´ä½**ï¼šè¯´æ˜æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡æ›´åŠ æ ¡å‡†ï¼ˆwell-calibratedï¼‰
- **é‡‡æ ·æ•ˆç‡æ›´é«˜**ï¼š
  - **minESS/s è¿œé«˜äº HMC åŠå¤šæ•°åŸºçº¿**ï¼Œè¯´æ˜å•ä½æ—¶é—´å†…è·å¾—çš„æœ‰æ•ˆæ ·æœ¬æ›´å¤š

### æ¶ˆèå®éªŒç»“æœ
- **NIPA-MF only**ï¼š
  - é€Ÿåº¦å¿«ï¼ˆSpeedup â‰ˆ 7.99 on Year Predï¼‰ï¼Œä½† CP95 ä¸‹é™æ˜æ˜¾ â†’ è¡¨æ˜ä»…ä¾èµ–ä»£ç†å‡½æ•°å¯èƒ½å¯¼è‡´è¿‡åº¦è‡ªä¿¡
- **NIPA-EC only**ï¼š
  - æå¿«ï¼ˆSpeedup â‰ˆ 15.66 on synthetic regï¼‰ï¼Œä½† ESS å¾ˆä½ â†’ è¡¨æ˜çº¯è®°å¿†æ£€ç´¢ç¼ºä¹å…¨å±€æ¢ç´¢èƒ½åŠ›
- **NIPA-MB only**ï¼š
  - æ€§èƒ½æ¥è¿‘åŸå§‹ HMCï¼Œä½†é€Ÿåº¦ç•¥æ…¢ï¼ˆå› é¢å¤–å¼€é”€ï¼‰â†’ éªŒè¯äº† MB æ¨¡å—çš„ç¨³å®šæ€§
- **å®Œæ•´ NIPA (MB/MF/EC)**ï¼š
  - **ç»¼åˆè¡¨ç°æœ€ä½³**ï¼šå…¼å…·é«˜é€Ÿåº¦ã€é«˜è´¨é‡é‡‡æ ·ä¸è‰¯å¥½ UQ â†’ è¯æ˜ä¸‰è€…ååŒä½œç”¨è‡³å…³é‡è¦

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
- **ç”Ÿç‰©å­¦å¯å‘æœ‰æ•ˆ**ï¼šå€Ÿé‰´äººè„‘ä¸­ model-basedã€model-free ä¸ episodic memory æ§åˆ¶ç³»ç»Ÿçš„åˆ†å·¥åä½œæœºåˆ¶ï¼Œå¯ä»¥è®¾è®¡å‡ºé«˜æ•ˆçš„è´å¶æ–¯æ¨ç†ç®—æ³•ã€‚
- **åŠ¨æ€æ¨¡å—åˆ‡æ¢å¯è¡Œä¸”é«˜æ•ˆ**ï¼šé€šè¿‡è·ç¦»é˜ˆå€¼æ§åˆ¶æ¨¡å—æ¿€æ´»ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ¢ç´¢ï¼ˆexplorationï¼‰ä¸åˆ©ç”¨ï¼ˆexploitationï¼‰ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚
- **NIPA å®ç°äº†é€Ÿåº¦ä¸ç²¾åº¦çš„åŒèµ¢**ï¼šåœ¨å¤šä¸ªçœŸå®ä¸åˆæˆä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—è¶…è¶Šä¼ ç»Ÿ MCMC ä¸ VI æ–¹æ³•ï¼Œåœ¨ä¿æŒ high minESS çš„åŒæ—¶å¤§å¹…æå‡è®¡ç®—æ•ˆç‡ï¼Œå¹¶æ”¹å–„ä¸ç¡®å®šæ€§æ ¡å‡†ã€‚
- **é€‚ç”¨äºå¤§è§„æ¨¡ BNN æ¨æ–­**ï¼šå°¤å…¶é€‚åˆå‚æ•°é‡å·¨å¤§ã€æ•°æ®è§„æ¨¡åºå¤§çš„æ·±åº¦å­¦ä¹ åœºæ™¯ï¼Œè§£å†³äº†ä¼ ç»Ÿ HMC éš¾ä»¥è½åœ°çš„å®é™…é—®é¢˜ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šé˜ˆå€¼ $ t_1, t_2 $ éœ€æ‰‹åŠ¨è°ƒèŠ‚ï¼Œç¼ºä¹è‡ªé€‚åº”æœºåˆ¶ã€‚
- **åˆå§‹æ± ä¾èµ–**ï¼šMF æ¨¡å—çš„ surrogate å‡½æ•°è´¨é‡ä¾èµ–äºåˆå§‹ SGHMC ç”Ÿæˆçš„æ ·æœ¬æ± ã€‚
- **ä»£ç†æ¨¡å‹æ³›åŒ–é£é™©**ï¼šDNN surrogate åœ¨è¿œç¦»è®­ç»ƒåŒºåŸŸå¯èƒ½å¤±æ•ˆï¼Œéœ€ MB æ¨¡å—å…œåº•ã€‚
- **å†…å­˜å ç”¨å¢åŠ **ï¼šéœ€å­˜å‚¨æ ·æœ¬æ± åŠ surrogate æ¨¡å‹ï¼Œå¸¦æ¥é¢å¤–å†…å­˜å¼€é”€ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **è‡ªé€‚åº”é—¨æ§æœºåˆ¶**ï¼šå‚è€ƒç¥ç»ç§‘å­¦ä¸­çš„ä¸ç¡®å®šæ€§ç«äº‰æœºåˆ¶ [17]ï¼Œè®©ç³»ç»Ÿæ ¹æ®å„æ¨¡å—çš„ç½®ä¿¡åº¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„ã€‚
- **æ”¹è¿› surrogate å»ºæ¨¡**ï¼šå°è¯•æ›´è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ ELMï¼‰ã€æµå½¢å­¦ä¹ æˆ– kernel æ–¹æ³•æ›¿ä»£ DNNã€‚
- **å¢å¼º EC æ£€ç´¢æœºåˆ¶**ï¼šä»æœ€è¿‘é‚»æ‰©å±•åˆ° k-NN åŠ æƒä¼°è®¡ï¼Œæé«˜è®°å¿†åˆ©ç”¨ç‡ã€‚
- **æ‹“å±•è‡³å…¶ä»–é¢†åŸŸ**ï¼šåº”ç”¨äº Gaussian Processã€Bayesian Inverse Problems ç­‰åŒæ ·å—é™äºè®¡ç®—æˆæœ¬çš„è´å¶æ–¯å»ºæ¨¡ä»»åŠ¡ã€‚
- **ç†è®ºåˆ†æ**ï¼šç ”ç©¶è¯¥æ¡†æ¶çš„æ”¶æ•›æ€§ã€åå·®-æ–¹å·®æƒè¡¡ç­‰ç†è®ºæ€§è´¨ã€‚

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> NIPA æˆåŠŸå°†è®¤çŸ¥ç¥ç»ç§‘å­¦åŸç†è½¬åŒ–ä¸ºä¸€ç§æ–°å‹ã€é«˜æ•ˆã€å¯æ‰©å±•çš„è´å¶æ–¯æ¨æ–­æ¡†æ¶ï¼Œåœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº† posterior sampling çš„æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡ BNN çš„å®ç”¨åŒ–æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚

</details>

---

### 12. [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)

**Authors**: Zhanglu Yan, Kaiwen Tang, Zixuan Zhu, Zhenyu Bai, Qianhui Liu, Weng-Fai Wong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22876v1  

#### Abstract
Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 8...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding  
â€”â€” æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰ Spiking Neural Networksï¼ˆSNNsï¼‰åœ¨ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ—¶ï¼Œè™½ç„¶ç†è®ºä¸Šå…·å¤‡é«˜èƒ½æ•ˆæ½œåŠ›ï¼Œä½†**ç°æœ‰çš„èƒ½æ•ˆè¯„ä¼°å¤šåŸºäºç®€åŒ–å‡è®¾**ï¼Œå¦‚ä»…ç»Ÿè®¡ MAC æˆ– ACC æ“ä½œæ•°ï¼Œè€Œå¿½ç•¥äº†å®é™…ç¡¬ä»¶ä¸­å ä¸»å¯¼åœ°ä½çš„**æ•°æ®ç§»åŠ¨å¼€é”€**ï¼ˆdata movementï¼‰ï¼Œå°¤å…¶æ˜¯ï¼š
- **Spike ä¼ è¾“èƒ½è€—**ï¼ˆè·¨æ ¸é€šä¿¡ï¼‰
- **æƒé‡è®¿é—®èƒ½è€—**ï¼ˆSRAM è¯»å–ï¼‰

ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å…ˆè¿›å·¥è‰ºä¸‹ï¼Œè¿™äº›æ•°æ®ç§»åŠ¨æˆæœ¬å¯å æ€»èƒ½è€—çš„ **70%ä»¥ä¸Š**ï¼Œè€Œä¼ ç»Ÿè¯„ä¼°æ–¹å¼ä¸¥é‡ä½ä¼°äº†è¿™éƒ¨åˆ†å¼€é”€ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹

#### ï¼ˆ1ï¼‰**Masked Time-to-First-Spike (M-TTFS) ç¼–ç **
- **æ ¸å¿ƒæ€æƒ³**ï¼šé‡æ–°å®šä¹‰â€œé™é»˜çŠ¶æ€â€ï¼ˆall-zero spike trainï¼‰æ‰€ä»£è¡¨çš„å€¼ã€‚
  - ä¼ ç»Ÿ TTFS å°†é™é»˜çŠ¶æ€åˆ†é…ç»™æœ€å°è†œç”µä½ï¼ˆæå°‘è§çš„å€¼ï¼‰ï¼Œå¯¼è‡´èŠ‚èƒ½æœºä¼šæµªè´¹ã€‚
  - M-TTFS åˆ™å°†é™é»˜çŠ¶æ€åˆ†é…ç»™**æœ€é¢‘ç¹å‡ºç°çš„æ¿€æ´»å€¼å¯¹åº”çš„æ—¶é—´æ­¥ `Imax`**ï¼Œä»è€Œæœ€å¤§åŒ–ç¨€ç–æ€§ã€‚
- å¼•å…¥ **â€œæ­»åŒºâ€ç­–ç•¥ï¼ˆDead Zone Strategyï¼‰**ï¼š
  - å®šä¹‰ä¸€ä¸ªä»¥ `Imax` ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º `k` çš„æ—¶é—´çª—å£ `[Imaxâˆ’k, Imax+k]`ï¼Œåœ¨æ­¤åŒºé—´å†…æ‰€æœ‰è¾“å…¥å‡è¢«æ˜ å°„ä¸ºé™é»˜çŠ¶æ€ã€‚
  - æ˜¾è‘—æå‡ sparsityï¼Œé™ä½ spike movement èƒ½è€—ã€‚

> ğŸ’¡ ä¼˜åŠ¿ï¼šæ— éœ€ç‰ºç‰²ä¿¡æ¯å®Œæ•´æ€§å³å¯æ˜¾è‘—å‡å°‘ spike æ•°é‡ï¼Œä¸” spike åºåˆ—ä»ä¿æŒ deterministically å¯è§£ç ã€‚

#### ï¼ˆ2ï¼‰**Memristive Synapse Unit (MSU)** â€”â€” åŸºäº CIM çš„æ¨¡æ‹Ÿå­˜ç®—ä¸€ä½“å•å…ƒ
- é‡‡ç”¨ **Compute-in-Memory (CIM)** æ¶æ„ï¼Œåˆ©ç”¨ **nT1R crossbar + RRAM** å®ç°æ¨¡æ‹ŸåŸŸå‘é‡çŸ©é˜µä¹˜æ³•ï¼ˆVMMï¼‰ã€‚
- æ‰€æœ‰æƒé‡é©»ç•™åœ¨å†…å­˜ä¸­ï¼Œé¿å…äº†é¢‘ç¹çš„ SRAM æƒé‡è¯»å–ï¼Œ**å½»åº•æ¶ˆé™¤ weight access å¼€é”€**ã€‚
- æ”¯æŒ bit-serial æ··åˆä¿¡å·è®¡ç®—ï¼Œå…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡ã€‚

#### ï¼ˆ3ï¼‰ç®—æ³•-ç¡¬ä»¶ååŒè®¾è®¡ï¼ˆAlgorithm-Hardware Co-designï¼‰
- M-TTFS çš„é«˜ç¨€ç–æ€§ç›´æ¥è½¬åŒ–ä¸ºç¡¬ä»¶å±‚é¢çš„ç‰©ç†åŠŸè€—é—¨æ§ï¼ˆpower gatingï¼‰ï¼šé›¶è¾“å…¥ â†’ é›¶ç”µæµ â†’ é›¶åŠŸè€—ã€‚
- MSU çš„æ¨¡æ‹Ÿç‰¹æ€§å¤©ç„¶é€‚é…ç¨€ç– spike è¾“å…¥ï¼Œå®ç°çœŸæ­£çš„â€œæŒ‰éœ€è®¡ç®—â€ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿ SNN æ–¹æ³•ï¼ˆå¦‚ SpikingLM, Sorbetï¼‰ | Matterhorn |
|------|----------------------------------------|------------|
| ç¼–ç æ–¹å¼ | Rate / Standard TTFS | **M-TTFS + Dead Zone** |
| Spike Rate | è¾ƒé«˜ï¼ˆ5â€“33%ï¼‰ | æä½ï¼ˆæœ€ä½è¾¾ **1.65%**ï¼‰ |
| æ•°æ®ç§»åŠ¨èƒ½è€— | ä¸»å¯¼é¡¹ï¼ˆ40â€“55%ï¼‰ | å¤§å¹…å‹ç¼©ï¼ˆâ†“59%ï¼‰ |
| æƒé‡è®¿é—® | é«˜é¢‘ SRAM è¯»å– | **CIM æ¶ˆé™¤è®¿é—®å¼€é”€** |
| èƒ½æ•ˆè¯„ä¼° | å¿½ç•¥ç¡¬ä»¶ç»†èŠ‚ | åŸºäº **22nm å·¥è‰ºå®æµ‹å»ºæ¨¡** |
| å‡†ç¡®ç‡ | ä½äº BERT åŸºçº¿ | æ¥è¿‘ç”šè‡³è¶…è¶ŠåŒç±» SNN |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **GLUE Benchmark** ä¸­çš„ 7 ä¸ªä»»åŠ¡ï¼š
  - **SST-2**ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰
  - **QQP**, **MNLI-m**, **QNLI**, **RTE**, **MRPC**, **STS-B**
- ä¸»è¦åˆ†æé›†ä¸­åœ¨ **SST-2** ä¸Šè¿›è¡Œç»†ç²’åº¦æ¶ˆèç ”ç©¶ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
| å‚æ•° | è®¾ç½® |
|------|------|
| æ¨¡å‹æ¶æ„ | BERT-baseï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰è’¸é¦å¾—åˆ° SNN |
| æ—¶é—´æ­¥é•¿ $T$ | 16ï¼ˆå¯¹åº” 4-bit é‡åŒ–ï¼‰ |
| é‡åŒ–æ–¹å¼ | å¯¹ç§°é‡åŒ–ï¼ˆsymmetric quantizationï¼‰ |
| è®­ç»ƒæ–¹å¼ | QNN-to-SNN conversion + Knowledge Distillation |
| æ­»åŒºåŠå¾„ $k$ | 0, 1, 2, ...ï¼ˆæ§åˆ¶ sparsity-accuracy trade-offï¼‰ |
| ç¡¬ä»¶å»ºæ¨¡ | å•†ç”¨ **22nm å·¥è‰º**ï¼ŒVerilog ç»¼åˆæµ‹é‡å•å…ƒèƒ½è€— |
| æ¨¡æ‹Ÿ CIM èƒ½è€— | åŸºäº Ye et al. (2023) çš„ç‰©ç† crossbar å®æ¨¡å‹ |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šå„ GLUE å­ä»»åŠ¡å¾—åˆ†åŠå¹³å‡åˆ†
- **èƒ½é‡æ¶ˆè€—ï¼ˆEnergy Consumptionï¼‰**
  - æ€»èƒ½è€—ï¼ˆmJ / transformer blockï¼‰
  - åˆ†é¡¹æ‹†è§£ï¼šSpike Movement, Weight Access, Computing, Leakage
- **Spike Rate**ï¼šå¹³å‡æ¯ä¸ªç¥ç»å…ƒäº§ç”Ÿçš„ spike æ¯”ä¾‹
- **èƒ½æ•ˆæ¯”ï¼ˆEnergy Efficiency Gainï¼‰**ï¼šç›¸å¯¹äºåŸºçº¿çš„èŠ‚èƒ½å€æ•°

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ¨¡å‹ | ç±»å‹ | ç‰¹ç‚¹ |
|---------|------|------|
| **Spiking Otters** | SNN, TTFS | å½“å‰æœ€ä¼˜ spiking transformer |
| **SpikingLM / Sorbet** | SNN, Rate Encoding | é«˜ spike rateï¼Œä¾èµ–æ“ä½œè®¡æ•°èŠ‚èƒ½ |
| **SpikingBERT** | SNN | æ›´å¤§è§„æ¨¡ï¼ˆ50M å‚æ•°ï¼‰ |
| **Q2BERT / BiT** | Quantized ANN | æ•°å­—ä½æ¯”ç‰¹æ¨¡å‹ä½œä¸ºèƒ½æ•ˆå‚ç…§ |
| **BERT-base** | Full-precision ANN | å‡†ç¡®ç‡ä¸Šé™å‚è€ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 å’Œ Figure 7ï¼‰

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **å¹³å‡å‡†ç¡®ç‡ï¼ˆGLUE avgï¼‰** | **84.64%**ï¼ˆk=1ï¼‰<br>**85.87%**ï¼ˆk=0ï¼‰ |
| **æœ€é«˜å‡†ç¡®ç‡ vs SOTA SNN** | **+1.42%** è¶…è¶Š Spiking Ottersï¼ˆ83.22%ï¼‰ |
| **å•å— Transformer èƒ½è€—** | **6.14 mJ**ï¼ˆå« MSUï¼‰ |
| **ç›¸æ¯”ä¼ ç»Ÿ TTFS èŠ‚èƒ½** | **2.7Ã— æ€»ä½“èŠ‚èƒ½** |
| **ç›¸æ¯” Spiking Otters èŠ‚èƒ½** | **57% é™ä½**ï¼ˆ14.21 â†’ 6.14 mJï¼‰ |
| **ç›¸æ¯”å…¶ä»– SNN æ¨¡å‹** | <ul><li>æ¯” SpikingLM é«˜ **4.24Ã— èƒ½æ•ˆ**</li><li>æ¯” Sorbet é«˜ **6.98Ã—**</li><li>æ¯” SpikingBERT é«˜ **>13Ã—**</li></ul> |
| **Spike Rateï¼ˆSST-2ï¼‰** | ä»æ ‡å‡† TTFS çš„ ~4.07% â†“ è‡³ **1.65%**ï¼ˆk=1ï¼‰ |
| **èŠ¯ç‰‡é¢ç§¯** | å• transformer block <10 mmÂ²<br>æ•´æ¨¡å‹ **<120 mmÂ²**ï¼ˆåª²ç¾ç°ä»£ CPU/GPUï¼‰ |

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰M-TTFS + Dead Zone å¯¹ Spike åˆ†å¸ƒçš„å½±å“ï¼ˆFigure 5ï¼‰
- **æ ‡å‡† TTFS**ï¼šä»… 0.26% é™é»˜ç‡ï¼Œå°–å³°å¯†é›†åˆ†å¸ƒ
- **M-TTFS (k=0)**ï¼šé™é»˜ç‡è¾¾ **34.0%**
- **M-TTFS (k=1)**ï¼šé™é»˜ç‡å‡è‡³ **61.2%**
- **M-TTFS (k=2)**ï¼šè¾¾ **76.4%**

> â¤ è¡¨æ˜é€šè¿‡é‡å®šå‘é™é»˜çŠ¶æ€ï¼Œå¯ç³»ç»Ÿæ€§åœ°å‹ç¼© spike æµé‡ã€‚

#### ï¼ˆ2ï¼‰èƒ½é‡-å‡†ç¡®ç‡æƒè¡¡åˆ†æï¼ˆFigure 6ï¼‰
| é…ç½® | èƒ½è€—ï¼ˆmJï¼‰ | å‡†ç¡®ç‡ | ç›¸å¯¹èŠ‚èƒ½ |
|------|-----------|--------|----------|
| Standard TTFS | 6.98 | 92.55% | - |
| M-TTFS (k=0) | 4.75 | 92.55% | â†“32% |
| M-TTFS (k=1) | **2.84** | 91.63% | â†“59% âœ… æœ€ä½³å¹³è¡¡ç‚¹ |
| M-TTFS (k=2) | 1.80 | 88.42% | â†“74%ï¼Œä½†ç²¾åº¦ä¸‹é™æ˜æ˜¾ |

> â¤ **k=1 æ˜¯æœ€ä¼˜é…ç½®**ï¼šåœ¨å‡ ä¹æ— æŸå‡†ç¡®ç‡å‰æä¸‹å®ç°æœ€å¤§èŠ‚èƒ½ã€‚

#### ï¼ˆ3ï¼‰é€çº§ä¼˜åŒ–çš„èƒ½é‡åˆ†è§£ï¼ˆTable 2ï¼‰
| æ–¹æ³• | Spike Mov. | Weight Acc. | Total Energy |
|------|------------|-------------|--------------|
| Baseline (TTFS) | 6.98 mJ | 4.65 mJ | 16.80 mJ |
| + M-TTFS | 4.75 mJ | 3.33 mJ | 12.24 mJ |
| + Dead Zone (k=1) | 2.84 mJ | 2.20 mJ | 8.31 mJ |
| + MSU | **2.84 mJ** | **0.69 mJ** | **6.14 mJ** |

> â¤ **MSU å•ç‹¬å¸¦æ¥çº¦ 2.17 mJ çš„ weight access èŠ‚çœ**ï¼Œæ˜¯ç¬¬äºŒå¤§èŠ‚èƒ½æ¥æºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **æ•°æ®ç§»åŠ¨æ˜¯ SNN èƒ½è€—ç“¶é¢ˆ**ï¼šåœ¨çœŸå®ç¡¬ä»¶ä¸­ï¼Œspike transfer å’Œ weight access åˆè®¡å æ¯”è¶… **70%**ï¼Œè¿œé«˜äºè®¡ç®—æœ¬èº«ã€‚
2. **ç¼–ç æ–¹æ¡ˆåº”åŒ¹é…æ•°æ®åˆ†å¸ƒ**ï¼šå°†é™é»˜çŠ¶æ€åˆ†é…ç»™é«˜é¢‘å€¼ï¼ˆè€Œéæå°å€¼ï¼‰å¯å¤§å¹…æå‡ç¨€ç–æ€§å’Œèƒ½æ•ˆã€‚
3. **ç®—æ³•-ç¡¬ä»¶ååŒè‡³å…³é‡è¦**ï¼š
   - M-TTFS æä¾›ç®—æ³•çº§ç¨€ç–æ€§ï¼›
   - MSU åœ¨ç¡¬ä»¶ä¸Šå°†å…¶è½¬åŒ–ä¸ºå®é™…èŠ‚èƒ½ï¼›
   - äºŒè€…ç»“åˆå½¢æˆæ­£åé¦ˆå¾ªç¯ã€‚
4. **Matterhorn å®ç° SOTA å¹³è¡¡**ï¼šåœ¨ GLUE ä¸Šè¾¾åˆ° **84.64% å¹³å‡å‡†ç¡®ç‡**ï¼ŒåŒæ—¶èƒ½è€—ä»…ä¸º **6.14 mJ/block**ï¼Œ**èƒ½æ•ˆæå‡ 2.31Ã—**ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ– QNN-to-SNN è½¬æ¢æ¡†æ¶**ï¼šæœªå®ç°ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¯èƒ½é™åˆ¶æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚
2. **æ­»åŒºå‚æ•° $k$ éœ€è°ƒä¼˜**ï¼šè¿‡å¤§ $k$ å¯¼è‡´ç²¾åº¦æ˜¾è‘—ä¸‹é™ï¼Œéœ€ä»»åŠ¡è‡ªé€‚åº”é€‰æ‹©ã€‚
3. **æ¨¡æ‹Ÿå™ªå£°å®¹å¿åº¦è™½å¼ºä½†ä»å­˜åœ¨è¾¹ç•Œé£é™©**ï¼šæç«¯éç†æƒ³æ¡ä»¶ä¸‹çš„é²æ£’æ€§æœ‰å¾…å®æµ‹éªŒè¯ã€‚
4. **ç›®å‰ä»…é’ˆå¯¹ BERT-like æ¶æ„**ï¼šæ˜¯å¦é€‚ç”¨äºæ›´å¤§è§„æ¨¡ LLMï¼ˆå¦‚ Llamaï¼‰å°šå¾…æ‰©å±•ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ”¯æŒåŠ¨æ€è°ƒæ•´æ­»åŒºèŒƒå›´ $k$**ï¼šæ ¹æ®è¾“å…¥åºåˆ—å¤æ‚åº¦è‡ªé€‚åº”è°ƒèŠ‚ sparsityã€‚
2. **æ‰©å±•è‡³ Vision Transformer å’Œ Long-sequence Modeling**ï¼šæ¢ç´¢åœ¨å›¾åƒå’Œé•¿æ–‡æœ¬ä¸­çš„åº”ç”¨ã€‚
3. **æ„å»ºå®Œæ•´ SoC åŸå‹å¹¶æµç‰‡éªŒè¯**ï¼šæ¨åŠ¨ä»ä»¿çœŸåˆ°å®ç‰©éƒ¨ç½²ã€‚
4. **æ¢ç´¢å¼‚æ­¥äº‹ä»¶é©±åŠ¨æ‰§è¡Œæœºåˆ¶**ï¼šè¿›ä¸€æ­¥é‡Šæ”¾ SNN çš„ event-driven æ½œåŠ›ã€‚
5. **ç»“åˆè„‰å†²å‰ªæï¼ˆSpike Pruningï¼‰ä¸é€šé“ç¨€ç–åŒ–**ï¼šå®ç°å¤šå±‚æ¬¡ç¨€ç–ä¼˜åŒ–ã€‚

---

> ğŸ”ï¸ **å‘½åå¯“æ„**ï¼šâ€œMatterhornâ€ ä¸ä»…ä»£è¡¨å…¶é«˜æ€§èƒ½ä¸ç¨³å®šæ€§ï¼Œä¹Ÿè±¡å¾ç€æ”€ç™»ç±»è„‘è®¡ç®—é«˜å³°çš„å†³å¿ƒã€‚è¯¥å·¥ä½œä¸ºæ„å»º**é«˜ç²¾åº¦ã€è¶…ä½åŠŸè€—çš„è„‰å†²å¼ LLM æ¨ç†å¼•æ“**æä¾›äº†åšå®çš„æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 13. [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)

**Authors**: Morten I. K. Munk, Arturo Valdivia, Paolo Burelli  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.23206v1  

#### Abstract
Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these pract...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHigh-quality generation of dynamic game content via small language models: A proof of concept

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åŸºäº **Large Language Models (LLMs)** çš„åŠ¨æ€æ¸¸æˆå†…å®¹ç”Ÿæˆé¢ä¸´ä»¥ä¸‹å…³é”®æŒ‘æˆ˜ï¼š
- **Narrative incoherence**ï¼šLLMs éš¾ä»¥ç»´æŒå¤æ‚è™šæ‹Ÿä¸–ç•Œä¸­çš„å™äº‹ä¸€è‡´æ€§ï¼Œå®¹æ˜“äº§ç”Ÿé€»è¾‘é”™è¯¯æˆ–è„±ç¦»èƒŒæ™¯ã€‚
- **é«˜æˆæœ¬ä¸ä¾èµ–äº‘ç«¯**ï¼šLLMs é€šå¸¸ä½“ç§¯å¤§ã€éœ€äº‘æœåŠ¡æ”¯æŒï¼Œå¯¼è‡´å•æœºæ¸¸æˆæ— æ³•ç¦»çº¿è¿è¡Œï¼Œä¸”å­˜åœ¨è¿è¥æˆæœ¬ä¸å¯æ§ã€æœåŠ¡å™¨åœæœé£é™©ç­‰é—®é¢˜ã€‚
- **ç¼ºä¹å¯æ§æ€§**ï¼šLLMs å¯¹æç¤ºè¯æ•æ„Ÿï¼Œè¾“å‡ºéš¾ä»¥ç¨³å®šæ§åˆ¶ï¼Œä¸åˆ©äºæ¸¸æˆè®¾è®¡ä¸­å¯¹å†…å®¹é£æ ¼ã€ç»“æ„çš„ä¸€è‡´æ€§è¦æ±‚ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºä¸€ç§åŸºäº **Small Language Models (SLMs)** çš„æ–°å‹æ¡†æ¶ï¼Œæ ¸å¿ƒæ€æƒ³ä¸ºï¼š
> **é€šè¿‡â€œæ¿€è¿›å¾®è°ƒâ€ï¼ˆaggressive fine-tuningï¼‰å°†å¤æ‚çš„å™äº‹ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç‹­çª„ã€æ˜ç¡®çš„å­ä»»åŠ¡ï¼Œæ¯ä¸ªç”±ä¸€ä¸ªä¸“é—¨åŒ–çš„ SLM å¤„ç†ï¼Œæ„å»ºâ€œä»£ç†å¼ç½‘ç»œâ€ï¼ˆagentic networkï¼‰**ã€‚

å…·ä½“åˆ›æ–°åŒ…æ‹¬ï¼š
- **ä»»åŠ¡ä¸“ç”¨åŒ– SLM æ¶æ„**ï¼šæ‘’å¼ƒå•ä¸€é€šç”¨ LLMï¼Œè½¬è€Œä½¿ç”¨å¤šä¸ª fine-tuned SLMï¼Œæ¯ä¸ªæ¨¡å‹ä¸“æ³¨äºç‰¹å®šç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚è¾±éª‚æ€§å®£ä¼ è¯­ç”Ÿæˆï¼‰ï¼Œå®ç°æ›´é«˜çš„è´¨é‡ä¸ç¨³å®šæ€§ã€‚
- **DAG-based åˆæˆæ•°æ®ç”Ÿæˆ**ï¼šé‡‡ç”¨æœ‰å‘æ— ç¯å›¾ï¼ˆDirected Acyclic Graph, DAGï¼‰ç»“æ„è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œç¡®ä¿æ ·æœ¬åœ¨æ¸¸æˆä¸–ç•Œè§‚å†…é«˜åº¦ä¸€è‡´ï¼Œå¹¶å¯ç²¾ç»†æ§åˆ¶è¾“å…¥å˜é‡ç»„åˆã€‚
- **Loop-anchored å†…å®¹ç”ŸæˆèŒƒå¼**ï¼šå°†å†…å®¹ç”Ÿæˆé”šå®šäºå…·ä½“çš„æ¸¸æˆå¾ªç¯ï¼ˆgame loopï¼‰ä¸­ï¼ˆå¦‚å£°èª‰æˆ˜ï¼‰ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´æ˜ç¡®ï¼Œé™ä½å¤æ‚åº¦ï¼Œæå‡å¯é¢„æµ‹æ€§å’ŒåŒæ­¥æ€§ã€‚

### âš–ï¸ ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿ LLM æ–¹æ³• | æœ¬è®ºæ–‡ SLM æ–¹æ³• |
|------|----------------|------------------|
| **éƒ¨ç½²æ–¹å¼** | ä¾èµ–äº‘ç«¯ API | å¯æœ¬åœ°éƒ¨ç½²ï¼Œæ”¯æŒç¦»çº¿æ¸¸æˆ |
| **æ¨ç†å»¶è¿Ÿ** | é«˜ï¼ˆå—ç½‘ç»œå½±å“ï¼‰ | ä½ï¼ˆå°¤å…¶é‡åŒ–åï¼‰ |
| **æˆæœ¬æ§åˆ¶** | ä¸å¯æ§ï¼ˆæŒ‰ token æ”¶è´¹ï¼‰ | å›ºå®šï¼ˆä¸€æ¬¡æ€§è®­ç»ƒï¼‰ |
| **è¾“å‡ºä¸€è‡´æ€§** | æ˜“æ¼‚ç§»ã€éš¾æ§åˆ¶ | é«˜åº¦å¯æ§ï¼Œé€šè¿‡è®­ç»ƒæ•°æ®çº¦æŸ |
| **å™äº‹è¿è´¯æ€§** | å¼±ï¼ˆéœ€å¤æ‚ prompt engineeringï¼‰ | å¼ºï¼ˆä»»åŠ¡çª„ + æ•°æ®ç»“æ„åŒ–ï¼‰ |
| **æ¨¡å—åŒ–æ‰©å±•æ€§** | å•ä½“ç³»ç»Ÿï¼Œéš¾æ‹†åˆ† | æ”¯æŒæ„å»º agentic network |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ§ª å®éªŒç›®æ ‡
éªŒè¯ä¸€ä¸ª **fine-tuned SLM æ˜¯å¦èƒ½åœ¨å®æ—¶æ¸¸æˆçº¦æŸä¸‹é«˜è´¨é‡å®Œæˆæ•´ä¸ª gameplay loop çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ã€‚

### ğŸ”§ æ¨¡å‹ä¸æ•°æ®é›†
- **åŸºç¡€æ¨¡å‹**ï¼š`Llama 3.2-1B`ï¼ˆ10äº¿å‚æ•°ï¼‰
- **å¾®è°ƒæ–¹æ³•**ï¼š**LoRA**ï¼ˆLow-Rank Adaptationï¼‰ï¼Œprompt-loss weight è®¾ä¸º 5%
- **è®­ç»ƒæ•°æ®æ¥æº**ï¼š
  - ä½¿ç”¨ `ChatGPT-4o` ä½œä¸º teacher modelï¼Œé€šè¿‡å‚æ•°åŒ– prompt è‡ªåŠ¨ç”Ÿæˆ 1800 ä¸ªè¾“å…¥-è¾“å‡ºå¯¹ã€‚
  - è¾“å…¥åŒ…å«è§’è‰²å…ƒæ•°æ®ï¼ˆfaction, personality, appearanceï¼‰ã€æƒ…æŠ¥ä¿¡æ¯ï¼ˆfailure, addictionï¼‰ã€ä¿®è¾è§’åº¦ï¼ˆangleï¼‰å’Œç›®æ ‡å—ä¼—ï¼ˆaudienceï¼‰ã€‚
  - è¾“å‡ºä¸ºç¬¦åˆä¸­ä¸–çºªè®¾å®šçš„â€œæŠ¹é»‘æµ·æŠ¥â€æ–‡æ¡ˆï¼ˆâ‰¤500 å­—ç¬¦ï¼‰ã€‚
- **æ•°æ®åˆ’åˆ†**ï¼š
  - è®­ç»ƒé›†ï¼š1440 æ¡
  - æµ‹è¯•é›†ï¼š360 æ¡

### âš™ï¸ å®éªŒè®¾ç½®
- **é‡åŒ–çº§åˆ«**ï¼šæµ‹è¯•ä¸‰ç§é‡åŒ–ç‰ˆæœ¬ä»¥å¹³è¡¡é€Ÿåº¦ä¸è´¨é‡ï¼š
  - 16-bitï¼ˆ2.48 GBï¼‰
  - 8-bitï¼ˆ1.32 GBï¼‰
  - 4-bitï¼ˆ808 MBï¼‰
- **æ¨ç†ç­–ç•¥**ï¼šé‡‡ç”¨ **retry-until-success** ç­–ç•¥ï¼Œæ¸©åº¦ $ T=0.75 $ï¼Œtop-p=0.9
- **ç¡¬ä»¶å¹³å°**ï¼šæ¶ˆè´¹çº§ PCï¼ˆAMD Ryzen 9 7950X + NVIDIA RTX 3070 8GB VRAMï¼‰ï¼Œæ¨¡å‹è½¬æ¢ä¸º `.gguf` æ ¼å¼å¹¶é€šè¿‡ `Llama.cpp` éƒ¨ç½²

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
é‡‡ç”¨ **LLM-as-a-judge** èŒƒå¼ï¼Œç”± `ChatGPT-4o` å¯¹ç”Ÿæˆç»“æœè¿›è¡Œå¤šç»´åº¦è¯„åˆ†ï¼Œå…±ä¸ƒé¡¹æ ‡å‡†ï¼š
1. **Overall assessment**ï¼šæ˜¯å¦éµå¾ªæŒ‡ä»¤
2. **Angle implementation**ï¼šæ˜¯å¦æˆåŠŸä½“ç°æŒ‡å®šä¿®è¾è§’åº¦ï¼ˆå¦‚â€œè¡€ç»Ÿå‘åŠ£â€ï¼‰
3. **Intelligence implementation**ï¼šæ˜¯å¦å®Œæ•´èå…¥æ‰€æœ‰æƒ…æŠ¥ä¿¡æ¯
4. **Alignment**ï¼šæ˜¯å¦ç¬¦åˆä¸­ä¸–çºªè®¾å®šï¼Œé¿å…äº‹å®å¹»è§‰æˆ–ä¸å½“è¯­è¨€
5. **Writing quality**ï¼šæ–‡ç¬”æ˜¯å¦æµç•…ã€é£æ ¼ç»Ÿä¸€
6. **Audience targeting**ï¼šæ˜¯å¦é’ˆå¯¹æŒ‡å®šç¾¤ä½“ï¼ˆå¦‚å£«å…µï¼‰è°ƒæ•´è¯­æ°”
7. **Rhetorical targeting**ï¼šæ˜¯å¦å‡†ç¡®æ”»å‡»ç›®æ ‡äººç‰©

æœ€ç»ˆåˆ¤æ–­ä¸º **min(judge_scores)** â€”â€” æ‰€æœ‰é¡¹ç›®å‡é€šè¿‡æ‰ç®—æˆåŠŸã€‚

### ğŸ†š åŸºçº¿å¯¹æ¯”
- ä¸»è¦å¯¹æ¯”ä¸åŒé‡åŒ–ç­‰çº§ä¸‹çš„ SLM è¡¨ç°ï¼ˆ16-/8-/4-bitï¼‰
- é»„é‡‘æ ‡å‡†ä¸º `ChatGPT-4o` è‡ªèº«ç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼ˆâ€œGoldâ€ LLM outputï¼‰
- æœªç›´æ¥æ¯”è¾ƒå…¶ä»– SLM æˆ– LLM æ–¹æ³•ï¼Œè€Œæ˜¯èšç„¦äºè¯æ˜è¯¥èŒƒå¼çš„å¯è¡Œæ€§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰æ•´ä½“æˆåŠŸç‡ï¼ˆJudge Pass Rateï¼‰
| æ¨¡å‹ | æˆåŠŸç‡ | æ ‡å‡†è¯¯ |
|------|--------|--------|
| 16-bit | 92.5% | Â±1.4% |
| 8-bit | 94.2% | Â±1.2% |
| 4-bit | 78.0% | Â±2.2% |

ğŸ‘‰ **16-bit ä¸ 8-bit å·®å¼‚ä¸æ˜¾è‘—**ï¼ˆMcNemarâ€™s test: $ \chi^2=0.69, p=0.41 $ï¼‰ï¼Œè¡¨æ˜ **8-bit æ˜¯ç†æƒ³çš„â€œå³æ’å³ç”¨â€æ›¿ä»£æ–¹æ¡ˆ**ã€‚

#### ï¼ˆ2ï¼‰æœŸæœ›ç”Ÿæˆæ—¶é—´ï¼ˆTime-to-Successï¼‰
è€ƒè™‘ retry æœºåˆ¶åçš„æ€»è€—æ—¶ï¼ˆå«åŠ è½½ + å¤šæ¬¡å°è¯•ï¼‰ï¼š

| æ¨¡å‹ | ä¸­ä½æ•° | P95 | æœ€å¤§å€¼ |
|------|-------|-----|-------|
| 4-bit | **2.1 ç§’** | 3.4 ç§’ | 5.1 ç§’ |
| 8-bit | 2.5 ç§’ | 3.9 ç§’ | 6.1 ç§’ |
| 16-bit | 4.8 ç§’ | 7.5 ç§’ | **15 ç§’** |

ğŸ“Œ å°½ç®¡ 4-bit æ¨¡å‹éœ€è¦æ›´å¤šå°è¯•æ¬¡æ•°ï¼Œä½†ç”±äºå…¶ **æå¿«çš„ per-token æ¨ç†é€Ÿåº¦ï¼ˆ3.6ms/token vs 16msï¼‰**ï¼Œå®é™…å“åº”æ›´å¿«ã€‚

#### ï¼ˆ3ï¼‰ç”Ÿæˆæ•ˆç‡åˆ†è§£ï¼ˆmedianï¼‰
| ç»„ä»¶ | 4-bit | 8-bit | 16-bit |
|------|------|------|--------|
| åŠ è½½æ—¶é—´ (t_prep) | 1145ms | 1397ms | 1782ms |
| å•æ¬¡ç”Ÿæˆæ—¶é—´ (t_eval) | 1968ms | 2554ms | 4710ms |
| å¹³å‡å°è¯•æ¬¡æ•° | ~1.3 | ~1.1 | ~1.1 |

âœ… æ‰€æœ‰æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯åœ¨ **5 ç§’å†…å®Œæˆç”Ÿæˆ**ï¼Œæ»¡è¶³å®æ—¶æ¸¸æˆæ©ç çª—å£éœ€æ±‚ã€‚

#### ï¼ˆ4ï¼‰éš¾åº¦ç›¸å…³æ€§åˆ†æ
- æ‰€æœ‰é‡åŒ–æ¨¡å‹åœ¨ prompt éš¾åº¦æ’åºä¸Šå…·æœ‰å¼º **Spearman ç›¸å…³æ€§**ï¼ˆ$ \rho > 0.82 $ï¼‰ï¼Œè¯´æ˜å®ƒä»¬â€œè§‰å¾—éš¾çš„è¾“å…¥â€åŸºæœ¬ä¸€è‡´ã€‚
- ä½†åœ¨æœ€éš¾çš„ 21 ä¸ª prompts ä¸Šï¼Œ**4-bit æ¨¡å‹ä¸å…¶ä»–ä¸¤ä¸ªçš„ç›¸å…³æ€§éª¤é™**ï¼ˆ$ \rho < 0.4 $ï¼‰ï¼Œæ˜¾ç¤ºå‡º **ä¸åŒçš„å¤±è´¥æ¨¡å¼**ï¼Œå¯èƒ½æºäºè¿‡åº¦å‹ç¼©å¯¼è‡´è¯­ä¹‰ç†è§£æ–­è£‚ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å•ä¸ª fine-tuned SLM å¯ç‹¬ç«‹æ”¯æ’‘å®Œæ•´ gameplay loop**  
   â†’ è¯æ˜äº† **SLM ä½œä¸º agentic network åŸºç¡€ç»„ä»¶çš„å¯è¡Œæ€§**ã€‚

2. **æ¿€è¿›å¾®è°ƒ + ç»“æ„åŒ–è¾“å…¥å¯å®ç°é«˜è´¨é‡ç”Ÿæˆ**  
   â†’ åœ¨ç‹­çª„ä»»åŠ¡ä¸‹ï¼ŒSLM å¯è¾¾åˆ°æ¥è¿‘ LLM çš„è¾“å‡ºè´¨é‡ï¼Œç”šè‡³åª²ç¾äººç±»åˆ›ä½œï¼ˆå‚è€ƒæ–‡çŒ®[16]ï¼‰ã€‚

3. **8-bit é‡åŒ–æ˜¯ç†æƒ³æŠ˜ä¸­æ–¹æ¡ˆ**  
   â†’ å‡ ä¹æ— æŸè´¨é‡çš„åŒæ—¶å¤§å¹…å‡å°å†…å­˜å ç”¨ï¼ˆ<1.5GBï¼‰ï¼Œé€‚åˆé›†æˆè¿›ç°ä»£æ¸¸æˆå¼•æ“ã€‚

4. **retry-until-success ç­–ç•¥æœ‰æ•ˆä¸”å¯é¢„æµ‹**  
   â†’ å¤±è´¥å¤šä¸ºéšæœºè€Œéç¡®å®šæ€§å´©æºƒï¼Œå°‘é‡é‡è¯•å³å¯æ¢å¤ï¼Œé€‚ç”¨äºå®æ—¶ç³»ç»Ÿã€‚

5. **DAG-based æ•°æ®ç”Ÿæˆæä¾›å¼ºå¤§æ§åˆ¶åŠ›**  
   â†’ å¯ç²¾ç¡®è°ƒæ§è®­ç»ƒæ•°æ®å¤šæ ·æ€§ä¸é¢†åŸŸä¸€è‡´æ€§ï¼Œæ˜¯å®ç°â€œä¸–ç•Œé”šå®šâ€ï¼ˆworld-groundedï¼‰ç”Ÿæˆçš„å…³é”®ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ç¼ºä¹æœ¬åœ°è´¨é‡è¯„ä¼°æœºåˆ¶**  
   â†’ å½“å‰ä¾èµ– `LLM-as-a-judge` è¿›è¡Œè¯„ä¼°ï¼Œä»éœ€äº‘ç«¯æ”¯æŒï¼›æœªæ¥éœ€å¼€å‘è½»é‡çº§æœ¬åœ°åˆ¤åˆ«å™¨ï¼ˆå¦‚ ensemble classifierï¼‰ã€‚

2. **æ³›åŒ–èƒ½åŠ›å—é™äºè®­ç»ƒæ•°æ®èŒƒå›´**  
   â†’ æ¨¡å‹å¯¹ out-of-vocabulary è¾“å…¥ï¼ˆå¦‚è™šæ„æ´¾ç³»åç§°ï¼‰è¡¨ç°ä¸‹é™ï¼Œæ˜¾ç¤ºå…¶å­¦ä¹ çš„æ˜¯è¡¨é¢æ¨¡å¼è€Œéæ·±å±‚ä¸–ç•ŒçŸ¥è¯†ã€‚

3. **åˆ›é€ æ€§ä¸é‡å¤æ€§ä¹‹é—´çš„æƒè¡¡**  
   â†’ ç»“æ„åŒ–æ ¼å¼æå‡äº†å¯æ§æ€§ï¼Œä½†ä¹Ÿå¯¼è‡´è¾“å‡ºå½¢å¼è¾ƒå›ºå®šï¼Œç¼ºä¹å¼€æ”¾ç”Ÿæˆçš„çµæ´»æ€§ã€‚

4. **ä»…éªŒè¯äº†å°é—­å¼ä»»åŠ¡**  
   â†’ å½“å‰å®éªŒé›†ä¸­äºâ€œloop-anchoredâ€åœºæ™¯ï¼›å¼€æ”¾å¼ä»»åŠ¡ï¼ˆå¦‚è‡ªç”±å¯¹è¯ã€ä»»åŠ¡ç”Ÿæˆï¼‰å°šæœªéªŒè¯ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ„å»ºçœŸæ­£çš„ agentic network**  
   â†’ å°†å¤šä¸ª specialized SLM ç»„åˆï¼Œå¤„ç†æ›´å¤æ‚çš„å™äº‹æµç¨‹ï¼ˆå¦‚ä»»åŠ¡é“¾ã€å¤šè§’è‰²äº’åŠ¨ï¼‰ã€‚

2. **å‘å±•æœ¬åœ° runtime quality assessment**  
   â†’ å¼€å‘æ— éœ€å¤–éƒ¨ LLM çš„è‡ªåŠ¨æ£€æµ‹æœºåˆ¶ï¼Œå®ç°å®Œå…¨ç¦»çº¿é—­ç¯ç”Ÿæˆã€‚

3. **å¢å¼ºä¸–ç•ŒçŸ¥è¯†å»ºæ¨¡èƒ½åŠ›**  
   â†’ æ¢ç´¢ç»“åˆ knowledge graph æˆ– grammar rulesï¼Œè®© SLM å­¦ä¼šéšå¼æ¨ç†æ¸¸æˆè§„åˆ™ä¸ç¤¾ä¼šå…³ç³»ã€‚

4. **æ‹“å±•è‡³å¤šç§æ–‡ä½“ç”Ÿæˆ**  
   â†’ ä»â€œæŠ¹é»‘æµ·æŠ¥â€æ‰©å±•åˆ°æ¼”è®²ã€è¯—æ­Œã€æ­Œæ›²ç­‰ï¼Œä¸°å¯Œæ¸¸æˆè¡¨è¾¾å½¢å¼ã€‚

5. **æ¨åŠ¨æ³•å¾‹ä¸ä¼¦ç†æ¡†æ¶å»ºè®¾**  
   â†’ æ¢ç´¢åŸºäºå¼€æºæ¨¡å‹ï¼ˆå¦‚ Apertusï¼‰çš„ç«¯åˆ°ç«¯è®­ç»ƒè·¯å¾„ï¼Œä¿éšœè‰ºæœ¯å®¶æƒç›Šä¸ç‰ˆæƒåˆè§„æ€§ã€‚

---

## æ€»ç»“
æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæå…·å®è·µä»·å€¼çš„èŒƒå¼è½¬å˜ï¼š  
> **ç”¨ä¸€ç³»åˆ—â€œä¸“ç²¾å°æ¨¡å‹â€å–ä»£â€œå…¨èƒ½å¤§æ¨¡å‹â€æ¥é©±åŠ¨åŠ¨æ€æ¸¸æˆå™äº‹**ã€‚

é€šè¿‡ **DefameLM** è¿™ä¸€ proof-of-conceptï¼Œä½œè€…æˆåŠŸå±•ç¤ºäº†ï¼š
- SLM åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¯è¾¾åˆ° LLM çº§åˆ«çš„ç”Ÿæˆè´¨é‡ï¼›
- é€šè¿‡ç»“æ„åŒ–è®­ç»ƒä¸ retry ç­–ç•¥ï¼Œå¯åœ¨æ¶ˆè´¹çº§è®¾å¤‡ä¸Šå®ç°å®æ—¶ã€å¯é çš„å†…å®¹ç”Ÿæˆï¼›
- è¯¥æ–¹æ³•ä¸ºè§£å†³ LLM åœ¨æ¸¸æˆä¸­åº”ç”¨çš„ **æˆæœ¬ã€å»¶è¿Ÿã€ä¸€è‡´æ€§ã€æŒä¹…æ€§** å››å¤§éš¾é¢˜æä¾›äº†å¯è¡Œå‡ºè·¯ã€‚

è¿™ä¸ä»…æ˜¯æŠ€æœ¯ä¸Šçš„çªç ´ï¼Œæ›´æ˜¯æ¸¸æˆ AI æ¶æ„æ€ç»´çš„ä¸€æ¬¡é‡æ„â€”â€”ä»â€œé»‘ç›’è°ƒç”¨â€èµ°å‘â€œå¯¼æ¼”å¼æ§åˆ¶â€ï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½æ¸¸æˆå¥ å®šäº†åšå®åŸºç¡€ã€‚

</details>

---

### 14. [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)

**Authors**: Yuxuan Li, Qijun He, Mingqi Yuan, Wen-Tse Chen, Jeff Schneider, Jiayu Chen  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22475v1  

#### Abstract
Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize t...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šContinual Policy Distillation from Distributed Reinforcement Learning Teachers**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

è¯¥è®ºæ–‡èšç„¦äº**æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆContinual Reinforcement Learning, CRLï¼‰**ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š

- **ç¾éš¾æ€§é—å¿˜ï¼ˆCatastrophic Forgettingï¼‰**ï¼šæ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¼šé—å¿˜æ—§ä»»åŠ¡çš„çŸ¥è¯†ï¼Œå½±å“ç¨³å®šæ€§ï¼ˆstabilityï¼‰ã€‚
- **å¯å¡‘æ€§-ç¨³å®šæ€§å›°å¢ƒï¼ˆStability-Plasticity Dilemmaï¼‰**ï¼šå¦‚ä½•åœ¨ä¿æŒå·²æœ‰æŠ€èƒ½çš„åŒæ—¶é«˜æ•ˆå­¦ä¹ æ–°ä»»åŠ¡ã€‚

ä¼ ç»Ÿç«¯åˆ°ç«¯çš„CRLæ–¹æ³•åœ¨é¡ºåºä»»åŠ¡æµä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½ã€ä¼˜åŒ–å›°éš¾ã€æ˜“é—å¿˜ç­‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å®¹é‡æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰ä¸Šéš¾ä»¥ç¨³å®šè®­ç»ƒã€‚

---

### ğŸš€ **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„**æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼ˆteacher-student frameworkï¼‰**ï¼Œå°†CRLè§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹è¿‡ç¨‹ï¼š

1. **åˆ†å¸ƒå¼RLæ•™å¸ˆè®­ç»ƒï¼ˆDistributed RL Teachersï¼‰**  
   - æ¯ä¸ªä»»åŠ¡ç”±ä¸€ä¸ªç‹¬ç«‹çš„RL workerå¹¶è¡Œè®­ç»ƒå‡ºä¸“å®¶çº§ç­–ç•¥ï¼ˆsingle-task teacher policyï¼‰ï¼Œä½¿ç”¨PPOç­‰æ ‡å‡†RLç®—æ³•ã€‚
   - æ•™å¸ˆç”Ÿæˆé«˜è´¨é‡çš„ç¤ºèŒƒè½¨è¿¹ï¼ˆdemonstration trajectoriesï¼‰ã€‚

2. **æŒç»­ç­–ç•¥è’¸é¦ï¼ˆContinual Policy Distillationï¼‰**  
   - ä¸­å¿ƒåŒ–çš„**å­¦ç”Ÿæ¨¡å‹ï¼ˆcentral student modelï¼‰**é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼ˆimitation learningï¼‰ä¸æ–­ä»æ–°åˆ°æ¥çš„æ•™å¸ˆä¸­è’¸é¦çŸ¥è¯†ã€‚
   - å­¦ç”Ÿé‡‡ç”¨åŸºäº**Transformerçš„Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„**ï¼Œå…·å¤‡åŠ¨æ€æ‰©å±•èƒ½åŠ›ã€‚

#### ğŸ”§ åˆ›æ–°æœºåˆ¶è®¾è®¡ï¼š
- **å¢é‡å¼ä¸“å®¶æ‰©å±•ï¼ˆIncremental MoEï¼‰**ï¼šæ¯é˜¶æ®µæ–°å¢ä¸€ä¸ªä¸“å®¶æ¨¡å—ï¼Œæå‡æ¨¡å‹é•¿æœŸå¯å¡‘æ€§ã€‚
- **å…¨å±€ä¸»å¹²å†»ç»“ï¼ˆGlobal Backbone Freezingï¼‰**ï¼šç¬¬ä¸€é˜¶æ®µåå†»ç»“å…±äº«ç»„ä»¶ï¼ˆå¦‚Embeddingã€Attentionå±‚ï¼‰ï¼Œé˜²æ­¢è¡¨å¾æ¼‚ç§»ã€‚
- **ä¸¤é˜¶æ®µä¸“å®¶æ©ç è®­ç»ƒï¼ˆTwo-Phase Expert Maskingï¼‰**ï¼š
  - Phase 1ï¼šå†»ç»“æ—§ä¸“å®¶ï¼Œä»…è®­ç»ƒæ–°ä¸“å®¶ä¸é—¨æ§ç½‘ç»œ â†’ é¿å…å¹²æ‰°å†å²çŸ¥è¯†ã€‚
  - Phase 2ï¼šå†»ç»“é—¨æ§ï¼Œè§£å†»æ‰€æœ‰ä¸“å®¶è¿›è¡ŒååŒå¾®è°ƒ â†’ æå‡æ•´ä½“æ€§èƒ½ã€‚
- **å¤šæ ·æ€§æ„ŸçŸ¥å›æ”¾ï¼ˆDiversity-Aware Trajectory Replayï¼‰**ï¼š
  - ä½¿ç”¨**Determinantal Point Process (DPP)** ä»æ•™å¸ˆæ•°æ®ä¸­é€‰æ‹©æœ€å…·å¤šæ ·æ€§å’Œä»£è¡¨æ€§çš„è½¨è¿¹åŠ å…¥å›æ”¾ç¼“å†²åŒºï¼ˆreplay bufferï¼‰ï¼Œå æ¯” <10%ï¼Œæœ‰æ•ˆç¼“è§£é—å¿˜ã€‚

æ­¤å¤–ï¼Œå¼•å…¥**å¯¹æ¯”ä»»åŠ¡åµŒå…¥ï¼ˆContrastive Task Embeddingï¼‰**æ›¿ä»£å›ºå®šone-hot IDï¼Œä½¿æ¨¡å‹èƒ½ä»çŠ¶æ€åºåˆ—ä¸­è‡ªåŠ¨æ¨æ–­ä»»åŠ¡è¯­ä¹‰ã€‚

---

### âš–ï¸ **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **å¯æ‰©å±•æ€§** | å°†å¤æ‚çš„CRLè½¬åŒ–ä¸ºç¨³å®šçš„ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒæ•™å¸ˆæ¨¡å‹ã€‚ |
| **ç¨³å®šæ€§** | å›æ”¾æœºåˆ¶ + å‚æ•°æ©ç æ˜¾è‘—é™ä½ç¾éš¾æ€§é—å¿˜ï¼Œå®ç°æ­£å‘åå‘è¿ç§»ï¼ˆpositive BWTï¼‰ã€‚ |
| **å¯å¡‘æ€§** | MoEç»“æ„å…è®¸åŠ¨æ€æ‰©å®¹ï¼Œé¿å…å®¹é‡é¥±å’Œå¯¼è‡´çš„ä»»åŠ¡å¹²æ‰°ã€‚ |
| **å…¼å®¹æ€§** | é€‚ç”¨äºç°ä»£å¤§æ¨¡å‹æ¶æ„ï¼ˆå¦‚Transformerï¼‰ï¼Œè€Œä¼ ç»ŸRLå¸¸å› ä¼˜åŒ–é—®é¢˜éš¾ä»¥é€‚é…ã€‚ |

> ğŸ’¡ **æ ¸å¿ƒæ€æƒ³è½¬å˜**ï¼šä¸å†è®©å•ä¸€agentç›´æ¥ä¸ç¯å¢ƒäº¤äº’å­¦å¤šä»»åŠ¡ï¼Œè€Œæ˜¯â€œå…ˆåˆ†ååˆâ€â€”â€”**åˆ†å¸ƒè®­ç»ƒ + é›†ä¸­è’¸é¦**ã€‚

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### ğŸ“š **ä½¿ç”¨çš„æ•°æ®é›†**

- **Meta-World MT25 Benchmark**
  - åŒ…å«25ä¸ªä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼ˆå¦‚ `peg insert`, `door open`, `button press` ç­‰ï¼‰ã€‚
  - åŸºäºMuJoCoç‰©ç†å¼•æ“ï¼Œæ¨¡æ‹ŸSawyeræœºæ¢°è‡‚ã€‚
  - æ‰€æœ‰ä»»åŠ¡å…±äº«ç›¸åŒçš„çŠ¶æ€/åŠ¨ä½œç©ºé—´ï¼Œä»…å¥–åŠ±å‡½æ•°ä¸åŒã€‚

---

### âš™ï¸ **å®éªŒè®¾ç½®**

#### å®éªŒåè®®ï¼ˆTwo Protocolsï¼‰ï¼š
1. **Two-Stage Distillation**  
   - ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ å‰10ä¸ªä»»åŠ¡ï¼ˆMT10ï¼‰
   - ç¬¬äºŒé˜¶æ®µï¼šç»§ç»­å­¦ä¹ å‰©ä½™15ä¸ªä»»åŠ¡ï¼ˆMT25ï¼‰

2. **Five-Stage Distillationï¼ˆæ›´éš¾ï¼‰**  
   - åˆ†5è½®ï¼Œæ¯è½®å­¦ä¹ 5ä¸ªæ–°ä»»åŠ¡ï¼Œé€æ­¥æµ‹è¯•æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚

#### å­¦ç”Ÿæ¨¡å‹æ¶æ„ï¼š
- **Decoder-only Transformer**
- **MoEç»“æ„**ï¼šæ¯å±‚å«å¤šä¸ªFFNä¸“å®¶ï¼Œç¨€ç–æ¿€æ´»ï¼ˆtop-k routingï¼‰
- **Sequence Length**: 20
- **Task Embedding Dimension**: 16ï¼ˆé€šè¿‡InfoNCEæŸå¤±è®­ç»ƒï¼‰

#### è’¸é¦è®­ç»ƒç»†èŠ‚ï¼š
- ä½¿ç”¨**MSE Loss**åŒ¹é…æ•™å¸ˆä¸å­¦ç”Ÿçš„åŠ¨ä½œå‡å€¼ï¼ˆä¸å¼ºåˆ¶åŒ¹é…æ–¹å·®ï¼Œå®éªŒè¯æ˜MSEä¼˜äºKLæ•£åº¦ï¼‰ã€‚
- å¼•å…¥**Load Balancing Loss**ç¡®ä¿ä¸“å®¶è´Ÿè½½å‡è¡¡ã€‚
- æ¯é˜¶æ®µæœ€å¤šæ·»åŠ 1ä¸ªæ–°ä¸“å®¶ã€‚
- Replay Bufferå¤§å°æ§åˆ¶åœ¨æ€»è’¸é¦æ•°æ®çš„<10%ã€‚

---

### ğŸ“Š **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å®šä¹‰ | å«ä¹‰ |
|------|------|------|
| **Average Accuracy (Acc)** | $ \frac{1}{K}\sum_{k=1}^{K} a_{K,k} $ | æœ€ç»ˆå¯¹æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡æˆåŠŸç‡ |
| **Backward Transfer (BWT)** | $ \frac{1}{K}\sum_{j=1}^{K}(a_{K,j} - a_{j,j}) $ | è¡¡é‡é—å¿˜ç¨‹åº¦ï¼›æ­£å€¼è¡¨ç¤ºæ— é—å¿˜ç”šè‡³æœ‰æ­£è¿ç§»ï¼Œè´Ÿå€¼è¡¨ç¤ºé—å¿˜ |

> æ³¨ï¼š$ a_{k,j} $ è¡¨ç¤ºç¬¬ $ k $ é˜¶æ®µç»“æŸååœ¨ä»»åŠ¡ $ j $ ä¸Šçš„è¡¨ç°ã€‚

---

### ğŸ” **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿ | ç±»å‹ | æè¿° |
|------|------|------|
| **Finetune** | ä¸‹ç•Œ | é¡ºåºå¾®è°ƒï¼Œæ— é˜²é—å¿˜æœºåˆ¶ |
| **Trac** | ä¼˜åŒ–å™¨æ”¹è¿› | å‚æ•°å…è´¹ä¼˜åŒ–å™¨ç”¨äºç»ˆèº«RL |
| **Independent** | ä¸Šç•Œï¼ˆå¯å¡‘æ€§ï¼‰ | æ¯é˜¶æ®µè®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œæ— é—å¿˜ä½†æ— æ³•å…±äº«çŸ¥è¯† |
| **EWC** | æ­£åˆ™åŒ–æ³• | åŸºäºFisherçŸ©é˜µä¿æŠ¤é‡è¦å‚æ•° |
| **KL Constraint** | å‡½æ•°æ­£åˆ™åŒ– | é™åˆ¶ç­–ç•¥åˆ†å¸ƒåç§»ï¼ˆå¸¸è§äºLLMï¼‰ |
| **Replay Only** | æ¶ˆè | ä»…ç”¨å›æ”¾ï¼Œæ— ä¸“å®¶æ‰©å±• |
| **Expert Expansion Only** | æ¶ˆè | ä»…æ‰©å±•ä¸“å®¶ï¼Œæ— å›æ”¾æœºåˆ¶ |

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### ğŸ“ˆ **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… åœ¨ **Five-Stage Continual Learning (MT25)** ä¸Šçš„ç»“æœï¼ˆTable 2ï¼‰ï¼š

| æ–¹æ³• | Final Acc (%) | BWT (%) |
|------|---------------|---------|
| **Ours (Proposed)** | **69.4 Â± 1.5** | **+2.3 Â± 2.8** |
| Independent | 62.8 Â± 2.9 | â€” |
| Replay Only | 57.2 Â± 1.4 | -11.0 |
| Expert Only | 28.0 Â± 3.5 | -51.3 |
| EWC | 35.9 Â± 17.3 | -37.9 |
| Finetune | 26.1 Â± 2.6 | -55.3 |

> âœ… **ç»“è®º**ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼Œä¸”å®ç°äº†**è½»å¾®æ­£å‘BWT**ï¼Œè¯´æ˜å‡ ä¹æ²¡æœ‰é—å¿˜ï¼Œç”šè‡³å­˜åœ¨è·¨ä»»åŠ¡ä¿ƒè¿›æ•ˆåº”ã€‚

---

#### âœ… åœ¨ **Two-Stage Distillation (MT10 â†’ MT25)** ä¸Šçš„è¡¨ç°ï¼ˆFigure 5aï¼‰ï¼š

- å­¦ç”Ÿæ¨¡å‹æ¢å¤äº†è¶…è¿‡ **85% çš„æ•™å¸ˆæ€§èƒ½**ã€‚
- å¿˜è®°ç¨‹åº¦è¢«æ§åˆ¶åœ¨ **10%ä»¥å†…**ï¼ˆå³BWT > -10%ï¼‰ã€‚
- æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚

#### âœ… ä¸å¤šä»»åŠ¡RLæ–¹æ³•å¯¹æ¯”ï¼ˆTable 1ï¼‰ï¼š

| æ–¹æ³• | Avg Accuracy (MT10) | è®­ç»ƒæ­¥æ•° |
|------|---------------------|----------|
| MOORE (SOTA MT-RL) | 88.7 Â± 5.6 | ~20M env steps |
| **Central Model (Ours)** | **88.9 Â± 1.9** | ~3M distill steps |
| **Avg Teachers** | **90.8 Â± 1.4** | ~1M Ã—10 teacher steps |

> âœ… **äº®ç‚¹**ï¼šå­¦ç”Ÿæ¨¡å‹åœ¨è¿œå°‘äºä¼ ç»ŸMT-RLçš„è®¡ç®—å¼€é”€ä¸‹ï¼Œè¾¾åˆ°äº†æ¥è¿‘æ•™å¸ˆé›†æˆçš„æ€§èƒ½æ°´å¹³ã€‚

---

### ğŸ” **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

| å˜ä½“ | Acc â†“ | BWT â†“ | ç»“è®º |
|------|--------|--------|------|
| **No Freeze-Gating** | 69.4 â†’ 64.5 | +2.3 â†’ +0.8 | å†»ç»“é—¨æ§æœ‰åŠ©äºç»´æŒè·¯ç”±ç¨³å®šæ€§ |
| **No Shared-Frozen** | 69.4 â†’ 60.0 | +2.3 â†’ +1.1 | ä¸»å¹²å†»ç»“å¯¹é˜²æ­¢è¡¨å¾æ¼‚ç§»è‡³å…³é‡è¦ |
| **No TE (Task Embedding)** | 69.4 â†’ 57.9 | â€” | ä»»åŠ¡åµŒå…¥æ˜¯MoEæœ‰æ•ˆè·¯ç”±çš„å‰æ |
| **Replay Only** | 69.4 â†’ 57.2 | +2.3 â†’ -11.0 | å•é å›æ”¾ä¸è¶³ä»¥ç¨³å®šå­¦ä¹  |
| **Expert Only** | 69.4 â†’ 28.0 | +2.3 â†’ -51.3 | æ— å›æ”¾ä¼šå¯¼è‡´ä¸¥é‡é—å¿˜ï¼Œç»“æ„æ‰©å±•æ— æ•ˆ |

> âœ… **å…³é”®å‘ç°**ï¼š**å›æ”¾æœºåˆ¶ + ä¸“å®¶æ‰©å±• + å‚æ•°æ©ç **ä¸‰è€…ç¼ºä¸€ä¸å¯ï¼Œæ„æˆå®Œæ•´æŠ—é—å¿˜ä½“ç³»ã€‚

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### âœ… **ä¸»è¦å‘ç°**

1. **æ•™å¸ˆ-å­¦ç”ŸèŒƒå¼æœ‰æ•ˆè§£è€¦CRLéš¾é¢˜**  
   - åˆ†å¸ƒå¼è®­ç»ƒæ•™å¸ˆä¿è¯é«˜è´¨é‡çŸ¥è¯†æºï¼›
   - é›†ä¸­å¼è’¸é¦å®ç°ç¨³å®šã€é«˜æ•ˆçš„å¤šä»»åŠ¡èšåˆã€‚

2. **MoE + åŠ¨æ€æ‰©å±• + æ©ç è°ƒåº¦ = é«˜å¯å¡‘æ€§ä¸ç¨³å®šæ€§å¹³è¡¡**  
   - æ–°å¢ä¸“å®¶å¸æ”¶æ–°ä»»åŠ¡çŸ¥è¯†ï¼›
   - å†»ç»“æ—§å‚æ•°ä¿æŠ¤å†å²èƒ½åŠ›ï¼›
   - ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¿éšœè®­ç»ƒå¹³ç¨³ã€‚

3. **è½»é‡çº§DPPå›æ”¾å³å¯å¤§å¹…ç¼“è§£é—å¿˜**  
   - ä»…éœ€ä¿ç•™<10%çš„å†å²è½¨è¿¹ï¼›
   - DPPé€‰å‡ºå¤šæ ·åŒ–æ ·æœ¬ï¼Œæ¯”éšæœºæˆ–FFSæ›´ä¼˜ã€‚

4. **å¯¹æ¯”ä»»åŠ¡åµŒå…¥æ•æ‰ä»»åŠ¡è¯­ä¹‰**  
   - PCAå¯è§†åŒ–æ˜¾ç¤ºç›¸ä¼¼ä»»åŠ¡ï¼ˆå¦‚æŒ‰é’®æŒ‰å‹ã€æŠ½å±‰å¼€å…³ï¼‰è‡ªç„¶èšç±»ï¼›
   - æ”¯æŒé›¶æ ·æœ¬ä»»åŠ¡è¯†åˆ«ä¸æ­£è¿ç§»ã€‚

5. **æ€§èƒ½é€¼è¿‘è”åˆè’¸é¦ä¸Šé™**  
   - å¤šé˜¶æ®µå­¦ä¹ æ€§èƒ½è¾¾è”åˆè®­ç»ƒï¼ˆjoint distillationï¼‰çš„90%ä»¥ä¸Šï¼›
   - è¡¨æ˜è¯¥æ¡†æ¶å…·å¤‡è‰¯å¥½æ‰©å±•æ½œåŠ›ã€‚

---

### âš ï¸ **å±€é™æ€§**

1. **ä¾èµ–æ•™å¸ˆè´¨é‡**  
   - è‹¥æŸä¸ªæ•™å¸ˆæœªèƒ½å­¦ä¼šä»»åŠ¡ï¼ˆå¦‚ `disassemble-v3` æˆåŠŸç‡ä¸º0ï¼‰ï¼Œåˆ™å­¦ç”Ÿä¹Ÿæ— æ³•æŒæ¡ã€‚
   - æ•™å¸ˆæˆä¸ºç“¶é¢ˆã€‚

2. **ä»…åœ¨Meta-WorldéªŒè¯**  
   - å°šæœªåœ¨Atariã€DMControlæˆ–å…¶ä»–RLåŸºå‡†ä¸Šæµ‹è¯•æ³›åŒ–æ€§ã€‚

3. **ç¦»çº¿è’¸é¦æ¨¡å¼é™åˆ¶å®æ—¶é€‚åº”æ€§**  
   - ä¸é€‚åˆéœ€è¦åœ¨çº¿æ¢ç´¢ä¸å³æ—¶åé¦ˆçš„åº”ç”¨åœºæ™¯ã€‚

4. **MoEå¸¦æ¥é¢å¤–å·¥ç¨‹å¤æ‚åº¦**  
   - è·¯ç”±ä¸ç¨³å®šã€è´Ÿè½½ä¸å‡ç­‰é—®é¢˜ä»éœ€è¾…åŠ©æŸå¤±ï¼ˆå¦‚auxiliary lossï¼‰è°ƒæ§ã€‚

---

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**

1. **å¼•å…¥è‡ªç›‘ç£é¢„è®­ç»ƒå¢å¼ºåˆå§‹è¡¨å¾èƒ½åŠ›**
2. **æ„å»ºé—­ç¯ç³»ç»Ÿï¼šå­¦ç”Ÿåå“ºæ•™å¸ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–**
3. **æ‰©å±•è‡³è§†è§‰è¾“å…¥ä»»åŠ¡ï¼ˆå¦‚VPT-styleï¼‰**
4. **æ¢ç´¢æ›´é«˜æ•ˆçš„MoEè·¯ç”±æœºåˆ¶ä¸ç¨€ç–è®­ç»ƒç­–ç•¥**
5. **åº”ç”¨äºçœŸå®æœºå™¨äººéƒ¨ç½²åœºæ™¯**

---

## âœ… æ€»ç»“ä¸€å¥è¯

> æœ¬æ–‡æå‡ºä¸€ç§**è§£è€¦å¼æ•™å¸ˆ-å­¦ç”ŸæŒç»­å­¦ä¹ æ¡†æ¶ï¼ˆContinual Policy Distillationï¼‰**ï¼Œç»“åˆ**MoEæ¶æ„ã€å¢é‡æ‰©å±•ã€DPPå›æ”¾ä¸å‚æ•°æ©ç æœºåˆ¶**ï¼Œåœ¨Meta-Worldä¸Šå®ç°äº†æ¥è¿‘æ•™å¸ˆæ€§èƒ½çš„æŒç»­å­¦ä¹ æ•ˆæœï¼ŒåŒæ—¶å°†é—å¿˜æ§åˆ¶åœ¨10%ä»¥å†…ï¼Œä¸ºæ„å»ºé€šç”¨å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€æ¡é«˜æ•ˆã€ç¨³å®šã€å¯æ‰©å±•çš„æ–°è·¯å¾„ã€‚

</details>

---

### 15. [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)

**Authors**: Tian-Tian Lin, Yi Liu, Xiao-Wei Tang, Yunmei Shi, Yi Huang, Zhongxiang Wei, Qingqing Wu, Yuhan Dong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22512v1  

#### Abstract
Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, w...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **UAV-assisted VLC**ï¼ˆæ— äººæœºè¾…åŠ©å¯è§å…‰é€šä¿¡ï¼‰ç³»ç»Ÿä¸­çš„ä¸‰ç»´è½¨è¿¹è§„åˆ’é—®é¢˜ï¼Œæ—¨åœ¨æœ€å°åŒ– UAV çš„é£è¡Œè·ç¦»ä»¥æå‡æ•°æ®é‡‡é›†æ•ˆç‡ã€‚è¯¥é—®é¢˜é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- ä¼ ç»Ÿæ–¹æ³•ç¼ºä¹åŸºäº **VLC channel gain** çš„ç†è®ºæœ€ä¼˜é£è¡Œé«˜åº¦æ¨å¯¼ï¼›
- ç°æœ‰åŸºäº **DRL** çš„è½¨è¿¹è§„åˆ’ç®—æ³•å¥–åŠ±æœºåˆ¶è®¾è®¡æœªå……åˆ†èåˆ VLC ç‰©ç†å±‚ç‰¹æ€§ï¼ˆå¦‚ Lambertian è¾å°„æ¨¡å‹ï¼‰ï¼Œå¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
1. **ç†è®ºæ¨å¯¼æœ€ä¼˜é£è¡Œé«˜åº¦ï¼ˆOptimal Altitudeï¼‰**  
   - åŸºäº VLC ä¿¡é“å¢ç›Šæ¨¡å‹ï¼Œåœ¨ç»™å®šä¿¡é“å¢ç›Šé˜ˆå€¼ $ H_{\text{th}} $ ä¸‹ï¼Œé¦–æ¬¡æå‡ºä¸€ä¸ª**é—­å¼è§£ï¼ˆclosed-formï¼‰çš„æœ€ä¼˜é£è¡Œé«˜åº¦ $ h^* $**ã€‚
   - è¯¥é«˜åº¦åœ¨ä¿è¯é€šä¿¡å¯é æ€§çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–æ°´å¹³è¦†ç›–èŒƒå›´ï¼Œä»è€Œå‡å°‘ UAV éœ€è¦é£è¡Œçš„è·ç¦»ã€‚

2. **è®¾è®¡æ–°å‹ä¿¡æ¯ç´ é©±åŠ¨å¥–åŠ±æœºåˆ¶ï¼ˆPheromone-driven Reward Mechanismï¼‰**  
   - å¼•å…¥â€œreward pheromoneâ€æ¦‚å¿µï¼Œå½“ UAV è¿›å…¥ GU çš„**ä¿¡å·å¯æ¥æ”¶åŒºåŸŸ**ï¼ˆå³ä½¿å°šæœªæ»¡è¶³é€šä¿¡é—¨é™ï¼‰æ—¶å³ç»™äºˆéƒ¨åˆ†å¥–åŠ±ã€‚
   - åˆ©ç”¨ $ \delta(d_{i,n}) $ å‡½æ•°åŒºåˆ†â€œä»…èƒ½æ¥æ”¶å…‰ä¿¡å·â€ä¸â€œå¯å»ºç«‹æœ‰æ•ˆé€šä¿¡â€çš„åŒºåŸŸï¼Œå¼•å¯¼ UAV å‘ç›®æ ‡ç§»åŠ¨ï¼Œç¼“è§£ç¨€ç–å¥–åŠ±ï¼ˆsparse rewardï¼‰é—®é¢˜ã€‚

3. **æ”¹è¿› TD3 ç®—æ³•æ¡†æ¶ï¼ˆImproved TD3ï¼‰**  
   - å°†ä¸Šè¿°å¥–åŠ±æœºåˆ¶åµŒå…¥ Twin Delayed Deep Deterministic Policy Gradientï¼ˆTD3ï¼‰ç®—æ³•ä¸­ï¼Œå½¢æˆ **TD3-TDCTM** ç®—æ³•ã€‚
   - ç»“åˆå»¶è¿Ÿç­–ç•¥æ›´æ–°ä¸åŒ Critic ç½‘ç»œï¼Œå¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç†è®ºåˆ†æ** | é¦–æ¬¡ç»™å‡ºåŸºäº VLC ä¿¡é“æ¨¡å‹çš„æœ€ä¼˜é«˜åº¦é—­å¼è¡¨è¾¾ï¼Œè€Œéä¾èµ–æ•°å€¼ä»¿çœŸæˆ–ç®€åŒ–è¿­ä»£ã€‚ |
| **ç®—æ³•æ•ˆç‡** | æ–°å¥–åŠ±æœºåˆ¶æ˜¾è‘—åŠ å¿« DRL æ”¶æ•›é€Ÿåº¦ï¼ˆçº¦æå‡ 50%ï¼‰ã€‚ |
| **æ€§èƒ½è¡¨ç°** | æ‰€ææ–¹æ¡ˆåœ¨å¤šç§ç”¨æˆ·å¯†åº¦ä¸‹å‡ä¼˜äº SCANã€GREEDY-RRT å’Œ ACO-RRT ç­‰åŸºå‡†æ–¹æ³•ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†ä¸ç¯å¢ƒé…ç½®
- **æ— å…¬å¼€çœŸå®æ•°æ®é›†**ï¼Œé‡‡ç”¨**è‡ªå®šä¹‰éšæœºç”Ÿæˆåœºæ™¯**è¿›è¡Œä»¿çœŸå®éªŒã€‚
- åœºæ™¯å¤§å°ï¼š$ D \times D = 100 \times 100 \, \text{m}^2 $
- Ground Users (GUs) æ•°é‡ï¼š$ I = 10, 15, 20, 25, 30 $ï¼Œä½ç½®éšæœºåˆ†å¸ƒã€‚
- UAV èµ·å§‹ç‚¹å›ºå®šï¼Œä»»åŠ¡ä¸ºéå†æ‰€æœ‰ GUs å¹¶å®Œæˆæ•°æ®æ”¶é›†ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
| å‚æ•° | è®¾ç½®å€¼ |
|------|--------|
| Transmission power $ P $ | 10 W |
| Half-power angle $ \Phi_{1/2} $ | 60Â° |
| Receiver FOV half-angle $ \Psi_c $ | 60Â° |
| Detector area $ A $ | 1 cmÂ² |
| Refractive index $ n_r $ | 1.5 |
| Noise std $ \sigma $ | 0.6 |
| Max flight speed / distance per slot $ d_{\max} $ | 2 m |
| Minimum safe altitude $ h_{\min} $ | 10 m |
| Channel capacity threshold $ C_{\text{th}} $ | 10 bps/Hz |
| Episode max length $ N_{\max} $ | 8000 slots |
| Batch size $ |B| $ | 256 |
| Discount factor $ \gamma $ | 0.999 |

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
- **æ€»é£è¡Œè·ç¦»ï¼ˆTotal flight distanceï¼‰**ï¼šæ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ã€‚
- **ç®—æ³•æ”¶æ•›é€Ÿåº¦ï¼ˆConvergence speedï¼‰**ï¼šè¾¾åˆ°ç¨³å®šç­–ç•¥æ‰€éœ€çš„æ—¶é—´æ­¥æ•°ã€‚
- **ä»»åŠ¡å®Œæˆç‡ï¼ˆTask completion rateï¼‰**ï¼šæ˜¯å¦æˆåŠŸæœåŠ¡æ‰€æœ‰ GUsã€‚
- **æ¶ˆèå®éªŒ**ï¼šéªŒè¯ reward pheromone å’Œ optimal altitude çš„ç‹¬ç«‹å½±å“ã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **SCAN** | æ‰«æå¼è·¯å¾„è§„åˆ’ï¼Œè§„åˆ™ç½‘æ ¼æœç´¢ã€‚ |
| **GREEDY-RRT** | å¿«é€Ÿæ¢ç´¢éšæœºæ ‘ï¼ˆRRTï¼‰ç»“åˆè´ªå¿ƒç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªç›®æ ‡ã€‚ |
| **ACO-RRT** | ä½¿ç”¨èšç¾¤ä¼˜åŒ–ï¼ˆACOï¼‰é€‰æ‹©è®¿é—®é¡ºåº + RRT è§„åˆ’è·¯å¾„ã€‚ |
| **Original TD3** | ä¸å« pheromone å¥–åŠ±çš„æ ‡å‡† TD3 ç®—æ³•ã€‚ |

> æ³¨ï¼šæ‰€æœ‰åŸºçº¿æ–¹æ³•ä½¿ç”¨çš„é£è¡Œé«˜åº¦ä¸æ‰€ææ–¹æ³•ä¸€è‡´ï¼ˆå³ä¹Ÿä½¿ç”¨ç†è®ºæœ€ä¼˜é«˜åº¦ $ h^* $ï¼‰ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰æœ€ä¼˜é£è¡Œé«˜åº¦çš„æœ‰æ•ˆæ€§ï¼ˆå›¾ 2aï¼‰
- å­˜åœ¨ä¸€ä¸ªä½¿é£è¡Œè·ç¦»æœ€å°çš„**æœ€ä¼˜é«˜åº¦ $ h^* \approx 13\,\text{m} $**ã€‚
- åœ¨ä¸åŒ GU æ•°é‡ï¼ˆ10ã€15ã€20ï¼‰ä¸‹ï¼Œæœ€ä¼˜é«˜åº¦åŸºæœ¬ä¸å˜ â†’ è¡¨æ˜å…¶ç”± $ m $ å’Œ $ H_{\text{th}} $ å†³å®šï¼Œå…·æœ‰æ™®é€‚æ€§ã€‚
- ç›¸æ¯”éæœ€ä¼˜é«˜åº¦ï¼ˆå¦‚ 10m æˆ– 20mï¼‰ï¼Œé£è¡Œè·ç¦»æœ€å¤šé™ä½ **35%**ã€‚

#### ï¼ˆ2ï¼‰æ”¶æ•›æ€§èƒ½æå‡ï¼ˆå›¾ 2cï¼‰
- **åŸå§‹ TD3**ï¼šçº¦éœ€ 4000 episodes æ”¶æ•›ã€‚
- **æ”¹è¿› TD3ï¼ˆmoderate rewardï¼‰**ï¼šä»…éœ€çº¦ **2000 episodes**ï¼Œ**æ”¶æ•›é€Ÿåº¦æå‡ ~50%**ã€‚
- è‹¥ reward è¿‡å¤§ï¼Œåˆ™å‡ºç°å‰§çƒˆéœ‡è¡ï¼Œè¯´æ˜éœ€é€‚åº¦è®¾è®¡ pheromone å¼ºåº¦ã€‚

#### ï¼ˆ3ï¼‰é£è¡Œè½¨è¿¹å¯è§†åŒ–ï¼ˆå›¾ 2bï¼‰
- å½“ UAV è¿›å…¥â€œä¿¡å·å¯æ¥æ”¶ä½†æœªè¾¾æ ‡â€åŒºåŸŸï¼ˆçº¢åœˆä¸é»‘åœˆä¹‹é—´ï¼‰ï¼Œå¼€å§‹åŠ é€Ÿé€¼è¿‘ç›®æ ‡ã€‚
- ä¸€æ—¦è¿›å…¥çº¢è‰²é€šä¿¡å¯è¡ŒåŒºï¼Œè¿…é€Ÿå®ŒæˆæœåŠ¡å¹¶è½¬å‘ä¸‹ä¸€ç›®æ ‡ â†’ æ˜¾ç¤º reward pheromone æˆåŠŸå¼•å¯¼æ¢ç´¢è¡Œä¸ºã€‚

#### ï¼ˆ4ï¼‰æ€»é£è¡Œè·ç¦»å¯¹æ¯”ï¼ˆå›¾ 3ï¼‰
| æ–¹æ³• | æ€§èƒ½è¡¨ç° |
|------|----------|
| SCAN | é£è¡Œè·ç¦»æœ€é•¿ä¸”ä¸éšç”¨æˆ·æ•°å˜åŒ–ï¼Œæ•ˆç‡ä½ä¸‹ã€‚ |
| GREEDY-RRT | è·ç¦»éšç”¨æˆ·æ•°è¿‘ä¼¼çº¿æ€§å¢é•¿ã€‚ |
| ACO-RRT | è¡¨ç°è¾ƒå¥½ï¼Œæ¥è¿‘æ‰€ææ–¹æ³•ã€‚ |
| **Proposed (TD3-TDCTM)** | **åœ¨æ‰€æœ‰ç”¨æˆ·æ•°é‡ä¸‹è¡¨ç°æœ€ä¼˜ï¼Œç›¸æ¯” ACO-RRT è‡³å°‘èŠ‚çœ 22.3% çš„é£è¡Œè·ç¦»**ã€‚ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å­˜åœ¨ç†è®ºæœ€ä¼˜é£è¡Œé«˜åº¦**ï¼šåŸºäº VLC ä¿¡é“æ¨¡å‹å¯è§£ææ±‚å¾—æœ€ä¼˜ $ h^* $ï¼Œæå¤§æå‡è½¨è¿¹è§„åˆ’æ•ˆç‡ã€‚
2. **reward pheromone æ˜¾è‘—åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹**ï¼šé€šè¿‡å°†ç‰©ç†å±‚å…ˆéªŒçŸ¥è¯†èå…¥å¥–åŠ±å‡½æ•°ï¼Œæœ‰æ•ˆç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œæ”¶æ•›é€Ÿåº¦æå‡çº¦ 50%ã€‚
3. **è”åˆä¼˜åŒ– altitude ä¸ horizontal trajectory å¯å®ç°å…¨å±€é«˜æ•ˆè·¯å¾„è§„åˆ’**ï¼šæ‰€æ TD3-TDCTM æ¡†æ¶åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºé€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **å‡è®¾ç†æƒ³ LOS é“¾è·¯**ï¼šæœªè€ƒè™‘é®æŒ¡ã€å¤©æ°”ã€å¤šå¾„ç­‰å®é™…å› ç´ å¯¹ VLC ä¿¡é“çš„å½±å“ã€‚
- **é™æ€ç”¨æˆ·åœºæ™¯**ï¼šæ‰€æœ‰ GUs å›ºå®šä¸åŠ¨ï¼Œæ— æ³•ç›´æ¥æ‰©å±•è‡³ç§»åŠ¨ç”¨æˆ·åœºæ™¯ã€‚
- **é›†ä¸­å¼è®­ç»ƒ**ï¼šä¾èµ–å…¨å±€çŠ¶æ€ä¿¡æ¯ï¼Œå¯èƒ½é™åˆ¶å¤§è§„æ¨¡éƒ¨ç½²æ—¶çš„å¯æ‰©å±•æ€§ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³ **åŠ¨æ€ç”¨æˆ·ç¯å¢ƒ** ä¸‹çš„åœ¨çº¿è½¨è¿¹è§„åˆ’ã€‚
2. å¼•å…¥ **multi-UAV ååŒæœºåˆ¶**ï¼Œç ”ç©¶åˆ†å¸ƒå¼ DRL æ¶æ„ã€‚
3. æ¢ç´¢ **hybrid RF/VLC** ç³»ç»Ÿä¸‹çš„å¼‚æ„é€šä¿¡ä¸è½¨è¿¹è”åˆä¼˜åŒ–ã€‚
4. è€ƒè™‘èƒ½æºçº¦æŸï¼ˆbattery lifeï¼‰ä¸‹çš„æŒä¹…åŒ–ä»»åŠ¡è°ƒåº¦ã€‚

--- 

> **æ€»ç»“ä¸€å¥è¯**ï¼šæœ¬æ–‡é€šè¿‡**ç†è®ºæ¨å¯¼æœ€ä¼˜é£è¡Œé«˜åº¦ + è®¾è®¡ç‰©ç†æ„ŸçŸ¥çš„ pheromone å¥–åŠ±æœºåˆ¶**ï¼Œæ˜¾è‘—æå‡äº† DRL åœ¨ UAV-assisted VLC è½¨è¿¹è§„åˆ’ä¸­çš„æ•ˆç‡ä¸æ€§èƒ½ï¼Œä¸ºæ™ºèƒ½ç©ºåœ°ååŒé€šä¿¡æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 16. [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)

**Authors**: Zhiyuan Cao, Zeyu Ma, Chenhao Yang, Han Zheng, Mingang Chen  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22752v1  

#### Abstract
We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Seman...

---

### 17. [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)

**Authors**: Ji Shi, Peiming Guo, Meishan Zhang, Miao Zhang, Xuebo Liu, Min Zhang, Weili Guan  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.22803v1  

#### Abstract
Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models thro...

---

### 18. [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)

**Authors**: Heehoon Kim, Jaehwan Lee, Taejeoung Kim, Jongwon Park, Jinpyo Kim, Pyongwon Suh, Ryan H. Choi, Sangwoo Lee, Jaejin Lee  
**Category**: cs.DC  
**Published**: 2026-02-02  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.22585v1  

#### Abstract
The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present ...

---

### 19. [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)

**Authors**: Gudrun Thorkelsdottir, Arindam Banerjee  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.22495v1  

#### Abstract
Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empi...

---

### 20. [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)

**Authors**: Shahria Hoque, Ahmed Akib Jawad Karim, Md. Golam Rabiul Alam, Nirjhar Gope  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22433v1  

#### Abstract
In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering ap...

---

### 21. [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)

**Authors**: Zhipeng Chen, Zhongrui Zhang, Chao Zhang, Yifan Xu, Lan Yang, Jun Liu, Ke Li, Yi-Zhe Song  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22571v1  

#### Abstract
The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual d...

---

### 22. [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)

**Authors**: Wei Zhu, Lixing Yu, Hao-Ren Yao, Zhiwen Tang, Kun Yue  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22662v1  

#### Abstract
Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability...

---

### 23. [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)

**Authors**: Shichao Ma, Zhiyuan Ma, Ming Yang, Xiaofan Li, Xing Wu, Jintao Du, Yu Cheng, Weiqiang Wang, Qiliang Liu, Zhengyang Zhou, Yang Wang  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22776v1  

#### Abstract
Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Ho...

---

### 24. [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)

**Authors**: Hamid Reza Akbari, Mohammad Hossein Sameti, Amir M. Mansourian, Mohammad Hossein Rohban, Hossein Sameti  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22786v1  

#### Abstract
The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This p...

---

### 25. [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)

**Authors**: Xuancheng Li, Haitao Li, Yujia Zhou,  YiqunLiu, Qingyao Ai  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22900v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In ...

---

### 26. [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)

**Authors**: Mingyu Lu, Soham Gadgil, Chris Lin, Chanwoo Kim, Su-In Lee  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22276v1  

#### Abstract
As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically ground...

---

### 27. [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)

**Authors**: Yixuan Wang, Omkar Sudhir Patil, Warren E. Dixon  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22284v1  

#### Abstract
We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets opt...

---

### 28. [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)

**Authors**: Santanu Subhash Rathod, Pietro Li\`o, Xiao Zhang  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.23072v1  

#### Abstract
Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capt...

---

### 29. [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)

**Authors**: Eugenia Iofinova, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.23153v1  

#### Abstract
As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesir...

---

### 30. [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)

**Authors**: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuefeng Xiao, Hongyan Xie, Li Huaqiu, Songshi Liang, Zhongxiang Dai, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22664v1  

#### Abstract
Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capt...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- kv cache, offload, State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
