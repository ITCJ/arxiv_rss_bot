# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-27 05:59:44 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)

**Authors**: Ruoqing Zheng, Chang Sun, Qibin Liu, Lauri Laatu, Arianna Cox, Benedikt Maier, Alexander Tapper, Jose G. F. Coutinho, Wayne Luk, Zhiqiang Que  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2601.17215v1  

#### Abstract
We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of j...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šJetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨é«˜èƒ½ç‰©ç†ï¼ˆHEPï¼‰å®éªŒä¸­ï¼Œ**jet tagging** æ˜¯è¯†åˆ«ç²’å­å–·æ³¨ï¼ˆjetï¼‰èµ·æºç²’å­ç±»å‹çš„å…³é”®ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå¯»æ‰¾å¸Œæ ¼æ–¯ç»è‰²å­ã€é¡¶å¤¸å…‹ç­‰ç¨€æœ‰ä¿¡å·ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµçš„é«˜æ€§èƒ½æ¨¡å‹ï¼ˆå¦‚ ParTï¼‰è™½ç„¶ç²¾åº¦é«˜ï¼Œä½†è®¡ç®—å¤æ‚åº¦å¤§ï¼Œéš¾ä»¥éƒ¨ç½²åˆ°å¯¹å»¶è¿Ÿè¦æ±‚æé«˜çš„ **FPGA-based Level-1 Trigger (L1T)** ç³»ç»Ÿä¸­ã€‚

æœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹çŸ›ç›¾ï¼š
- é«˜æ€§èƒ½æ¨¡å‹ï¼ˆå¦‚ Transformerï¼‰é€‚åˆç¦»çº¿åˆ†æï¼Œä½†ä¸é€‚ç”¨äºä½å»¶è¿Ÿåœ¨çº¿è§¦å‘ï¼›
- è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ MLPã€Deep Setsï¼‰å¯éƒ¨ç½²äº FPGAï¼Œä½†æ€§èƒ½æœ‰é™ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹
æå‡º **JetFormer** â€”â€” ä¸€ç§**å¯æ‰©å±•ä¸”é«˜æ•ˆçš„ encoder-only Transformer æ¶æ„**ï¼Œç»Ÿä¸€æ”¯æŒä»é«˜ç²¾åº¦ç¦»çº¿åˆ†æåˆ°äºšå¾®ç§’çº§åœ¨çº¿è§¦å‘çš„å…¨åœºæ™¯ jet taggingã€‚

#### ä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼š
1. **ç»Ÿä¸€æ¶æ„è®¾è®¡ï¼ˆUnified Architectureï¼‰**
   - è®¾è®¡äº†ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯ä¼¸ç¼©çš„ Transformer æ¡†æ¶ï¼Œé€šè¿‡è°ƒæ•´å±‚æ•°ã€åµŒå…¥ç»´åº¦ç­‰å‚æ•°ï¼Œçµæ´»é€‚é…ä¸åŒåº”ç”¨åœºæ™¯ï¼ˆcompact â†’ largeï¼‰ã€‚
   - å¼•å…¥ `[CLS]` token æ¥èšåˆæ•´ä¸ª jet çš„å…¨å±€è¡¨ç¤ºï¼Œç”¨äºåˆ†ç±»ï¼Œå€Ÿé‰´ BERT æ€æƒ³å¹¶é€‚åº”æ— åºç²’å­é›†åˆè¾“å…¥ã€‚

2. **ç¡¬ä»¶å‹å¥½å‹æ”¹é€ ï¼ˆHardware-Friendly Modificationsï¼‰**
   - å°†æ ‡å‡†çš„ LayerNorm æ›¿æ¢ä¸º BatchNormï¼ˆæ¨ç†æ—¶å‚æ•°å›ºå®šï¼‰ï¼Œä¾¿äº FPGA å®ç°ï¼›
   - ä½¿ç”¨ ReLU æ›¿ä»£ SiLU æ¿€æ´»å‡½æ•°ï¼Œé¿å…æŒ‡æ•°è¿ç®—ï¼Œé™ä½ç¡¬ä»¶å¼€é”€ã€‚

3. **ç«¯åˆ°ç«¯ç¡¬ä»¶éƒ¨ç½²æµç¨‹**
   - æ„å»ºåŸºäº **Allo**ï¼ˆMLIR ç¼–è¯‘æ¡†æ¶ï¼‰çš„ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–æµæ°´çº¿ï¼Œæ”¯æŒå¤šç›®æ ‡è¶…å‚æœç´¢ï¼ˆHPOï¼‰ã€ç»“æ„åŒ–å‰ªæï¼ˆstructured pruningï¼‰å’Œ 1-bit é‡åŒ–ï¼›
   - æˆåŠŸå°†å‹ç¼©åçš„ JetFormer-tiny åˆæˆè‡³ FPGA å¯æ‰§è¡Œçš„ HLS C++ ä»£ç ï¼ŒéªŒè¯å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å¯è¡Œæ€§ã€‚

4. **è‡ªåŠ¨åŒ–å‹ç¼©ç­–ç•¥**
   - ç»“åˆ Optuna è¿›è¡Œ multi-objective HPOï¼ˆæœ€å¤§åŒ– accuracyï¼Œæœ€å°åŒ– FLOPsï¼‰ï¼›
   - åº”ç”¨ **Taylor-based structured pruning** å’Œ **1-bit quantizationï¼ˆBitNetï¼‰** å®ç°æè‡´å‹ç¼©ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ€§èƒ½ vs æ•ˆç‡å¹³è¡¡** | åœ¨ JETCLASS ä¸Šæ¥è¿‘ ParT ç²¾åº¦ï¼ˆä»…å·® 0.7%ï¼‰ï¼Œä½† FLOPs å‡å°‘ 37.4% |
| **é€šç”¨æ€§** | æ”¯æŒä»å°æ¨¡å‹ï¼ˆFPGA è§¦å‘ï¼‰åˆ°å¤§æ¨¡å‹ï¼ˆç¦»çº¿åˆ†æï¼‰çš„æ— ç¼æ‰©å±• |
| **éƒ¨ç½²èƒ½åŠ›** | æ”¯æŒå®Œæ•´ FPGA åˆæˆè·¯å¾„ï¼ˆvia Alloï¼‰ï¼Œä¼˜äº hls4ml å¯¹ Transformer æ”¯æŒä¸è¶³çš„ç°çŠ¶ |
| **å‹ç¼©æ½œåŠ›** | å¯å®ç°é«˜è¾¾ 92% æ¨¡å‹ä½“ç§¯å‹ç¼©ï¼Œç²¾åº¦æŸå¤± < 3.5% |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

| æ•°æ®é›† | æè¿° |
|-------|------|
| **HLS4ML 150P Dataset** | åŒ…å« 620K è®­ç»ƒæ ·æœ¬ï¼Œæ¯ jet æœ€å¤š 150 ä¸ªç²’å­ï¼Œæ¯ä¸ªç²’å­æœ‰ 16 ä¸ªè¿åŠ¨å­¦ç‰¹å¾ï¼›ç±»åˆ«ï¼šq, g, W, Z, tï¼›ç”¨äºå°è§„æ¨¡ benchmark æµ‹è¯• |
| **JETCLASS Dataset** | å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…± 100M jetsï¼Œæ¶µç›– 10 ç±» jetï¼Œæ¯ç²’å­ 17 ä¸ªç‰¹å¾ï¼ˆå« PIDã€è½¨è¿¹åç§»ç­‰ï¼‰ï¼›æ˜¯å½“å‰æœ€å¤æ‚çš„ jet tagging benchmark |

> æ³¨ï¼šJetFormer ä¸ä½¿ç”¨ pairwise interaction featuresï¼Œè€Œ ParT ä½¿ç”¨ï¼Œå› æ­¤æ›´å…·å…¬å¹³æŒ‘æˆ˜æ€§ã€‚

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | å†…å®¹ |
|--------|------|
| **è®­ç»ƒé…ç½®** | AdamW ä¼˜åŒ–å™¨ï¼ŒOneCycleLR å­¦ä¹ ç‡è°ƒåº¦ï¼Œbatch size=256ï¼ˆ150Pï¼‰ã€128ï¼ˆJETCLASSï¼‰ |
| **è¯„ä¼°æŒ‡æ ‡** | Accuracyã€AUCï¼ˆå„ç±»åˆ« ROC-AUCï¼‰ã€FLOPsã€å‚æ•°é‡ï¼ˆParamsï¼‰ã€æ¨ç†å»¶è¿Ÿã€FPGA èµ„æºåˆ©ç”¨ç‡ï¼ˆBRAM, DSP, FF, LUTï¼‰ |
| **ç¡¬ä»¶å¹³å°æ¨¡æ‹Ÿ** | ä½¿ç”¨ Allo + Vitis HLS è¿›è¡Œ CPU simulationã€sw_emuã€hw_emu éªŒè¯ï¼Œæœªè¿›è¡Œå®æœºéƒ¨ç½² |
| **å‹ç¼©æŠ€æœ¯** |  
| - Pruning | åŸºäº torch-pruning çš„ structured pruningï¼Œglobal ratio=50%ï¼Œfine-tune 5 è½® Ã— 5 æ­¥ |
| - Quantization | 1-bit weightsï¼ˆBitNetï¼‰ï¼Œ8-bit activationsï¼ŒQAT ä»å¤´è®­ç»ƒ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ¨¡å‹ | ç±»å‹ | æ˜¯å¦ç¡¬ä»¶å‹å¥½ |
|------|------|-------------|
| MLP / Deep Sets (DS) / IN | å…¨è¿æ¥/GNN | æ˜¯ï¼ˆå·²æœ‰ FPGA å®ç°ï¼‰ |
| JEDI-net / JEDI-linear | GNN | æ˜¯ï¼ˆSOTA for FPGAï¼‰ |
| ParT | Transformerï¼ˆå¸¦ interaction biasï¼‰ | å¦ï¼ˆå¤ªå¤§ï¼Œéš¾éƒ¨ç½²ï¼‰ |
| ParticleNet | DGCNN | ä¸­ç­‰ |
| MLP-Mixer (MLPM) | æ··åˆæ¶æ„ | æ˜¯ï¼ˆæ–°å…´æ–¹æ¡ˆï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### âœ… åœ¨ HLS4ML 150P æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆTable 1 & 2ï¼‰
| æ¨¡å‹ | Constituents | Accuracy â†‘ | FLOPs â†“ | ç›¸æ¯”åŸºçº¿æå‡ |
|------|--------------|------------|---------|-------------|
| JetFormer | 8 particles | **67.1%** | 933k | +2.5â€“3.1% vs MLP/DS/IN |
| JetFormer | 32 particles | **79.9%** | 4M | +4.1% vs IN |
| JetFormer | 150 particles | **82.97%** | 24M | > JEDI-net (+~7%) |

> æ‰€æœ‰æƒ…å†µä¸‹ AUC å‡é¢†å…ˆ 2â€“4%ï¼Œå°¤å…¶åœ¨ Top/W/Z åˆ†ç±»ä¸Šæ˜¾è‘—æ›´ä¼˜ã€‚

#### âœ… åœ¨ JETCLASS æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆTable 3ï¼‰
| æ¨¡å‹ | Accuracy | AUC | Params | FLOPs |
|------|----------|-----|--------|-------|
| ParT | 0.836 | 0.9834 | 2.14M | 340M |
| **JetFormer** | **0.829** | **0.9827** | **1.66M** | **213M** |

- ç²¾åº¦ä»…æ¯” ParT ä½ **0.7%**ï¼ŒAUC å·® 0.07%ï¼Œä½† **FLOPs å‡å°‘ 37.4%**ï¼Œå‚æ•°å‡å°‘ 22.4%
- è¡¨æ˜ JetFormer åœ¨ä¸ä¾èµ– pairwise interaction çš„å‰æä¸‹ä»å…·å¼ºå¤§è¡¨è¾¾èƒ½åŠ›

#### âœ… è¶…å‚ä¼˜åŒ–ï¼ˆHPOï¼‰ç»“æœï¼ˆNSGAIISamplerï¼‰
- ä½¿ç”¨ Optuna å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆaccuracy vs FLOPsï¼‰
- æœ€ç»ˆé€‰å®š **JetFormer-tiny**: 4 transformer blocks, embed_dim=8, 2 heads â†’ **26,168 FLOPs**, val_acc=0.6525
- NSGAIISampler æ”¶æ•›æœ€å¿«ï¼Œhypervolume æœ€é«˜ï¼Œä¼˜äº TPE å’Œ BoTorch

#### âœ… å‹ç¼©å®éªŒç»“æœ

##### ï¼ˆ1ï¼‰Structured Pruningï¼ˆTable 5ï¼‰
| æŒ‡æ ‡ | JetFormer-tiny (Model 0) |
|------|---------------------------|
| FLOPs | 26,168 â†’ **13,784** (**â†“47.3%**) |
| å‚æ•°æ•° | 3,085 â†’ **1,997** (**â†“35.3%**) |
| Accuracy | 0.656 â†’ 0.653 (**â†“0.49%**) |
| GPU æ¨ç†æ—¶é—´ï¼ˆbs=10240ï¼‰ | 3.517ms â†’ **2.902ms** (**â†“17.5%**) |

> æ›´å¤§æ¨¡å‹å‰ªæåç²¾åº¦æŸå¤±æ›´å°ï¼ˆ<0.3%ï¼‰ï¼Œè¯´æ˜é²æ£’æ€§å¼º

##### ï¼ˆ2ï¼‰1-bit Quantizationï¼ˆTable 6ï¼‰
| Constituents | Model Size | Accuracy Drop |
|-------------|------------|---------------|
| 8 particles | 404KB â†’ **31KB** (**â†“92.2%**) | â†“1.49% |
| 16 particles | 414KB â†’ **41KB** (**â†“90.1%**) | â†“2.15% |
| 32 particles | 451KB â†’ **79KB** (**â†“82.6%**) | â†“3.50% |

> æ¨¡å‹å°ºå¯¸å‹ç¼©è¶…è¿‡ **82â€“92%**ï¼Œç²¾åº¦æŸå¤±å¯æ§ï¼ˆâ‰¤3.5%ï¼‰

#### âœ… FPGA ç¡¬ä»¶è¯„ä¼°ï¼ˆTable 7 & 8ï¼‰
| æ¨¡å‹ | Batch Size | Latency | BRAM/DSP/LUT åˆ©ç”¨ç‡ |
|------|------------|--------|------------------------|
| Particle MLP | 32 | 0.585 ms | å‡ <1% â†’ èµ„æºä¸¥é‡æµªè´¹ |
| JetFormer-tiny (original) | 16 | 4.767 ms | ~9.5% BRAM, ~8% LUT |
| JetFormer-tiny (pruned) | 16 | **2.705 ms** | åˆ©ç”¨ç‡å…¨é¢ä¸‹é™ |
| JetFormer-tiny (pruned) | **2** | **0.404 ms** | è¿›ä¸€æ­¥é™ä½èµ„æºå ç”¨ |

> å½“å‰æœªå¯ç”¨æµæ°´çº¿ï¼ˆpipeliningï¼‰ï¼Œä»æœ‰å·¨å¤§ä¼˜åŒ–ç©ºé—´ï¼ˆè§ Table 9ï¼šåŠ  pipeline å¯é™å»¶è¿Ÿ 4.5Ã—ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Transformer å¯å…¼é¡¾æ€§èƒ½ä¸æ•ˆç‡**  
   JetFormer åœ¨ä¸å¼•å…¥æ˜¾å¼ pairwise interaction çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½è¾¾åˆ°æ¥è¿‘ SOTA çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

2. **å•ä¸€æ¶æ„å¯è¦†ç›–å…¨åœºæ™¯éœ€æ±‚**  
   é€šè¿‡è°ƒèŠ‚è§„æ¨¡ï¼ŒJetFormer å¯æœåŠ¡äºï¼š
   - å¤§æ¨¡å‹ï¼šç¦»çº¿é«˜ç²¾åº¦åˆ†æï¼ˆâ‰ˆ ParTï¼‰
   - å°æ¨¡å‹ï¼ˆJetFormer-tinyï¼‰ï¼šFPGA è§¦å‘ç³»ç»Ÿå€™é€‰

3. **é«˜åº¦å¯å‹ç¼©æ€§æ”¯æŒè¾¹ç¼˜éƒ¨ç½²**  
   ç»“æ„åŒ–å‰ªæ + 1-bit é‡åŒ–ç»„åˆå¯å®ç° **>90% æ¨¡å‹å‹ç¼©ç‡**ï¼Œç²¾åº¦æŸå¤± < 3.5%ï¼Œæ»¡è¶³åµŒå…¥å¼éƒ¨ç½²éœ€æ±‚ã€‚

4. **Allo æ¡†æ¶æ”¯æŒç«¯åˆ°ç«¯ FPGA åˆæˆ**  
   æˆåŠŸæ‰©å±• Allo ä»¥æ”¯æŒ Transformer ç‰¹æœ‰æ“ä½œï¼ˆlog_softmax, class token concat/slice, batchnorm3dï¼‰ï¼Œæ‰“é€šä» PyTorch åˆ° HLS çš„è·¯å¾„ã€‚

5. **ç¡¬ä»¶èµ„æºè¿œæœªé¥±å’Œï¼ŒåŠ é€Ÿæ½œåŠ›å·¨å¤§**  
   å½“å‰å®ç°æœªä½¿ç”¨æµæ°´çº¿æˆ–å¹¶è¡Œä¼˜åŒ–ï¼Œèµ„æºåˆ©ç”¨ç‡æ™®éä½äº 10%ï¼Œæœªæ¥å¯é€šè¿‡æ·±åº¦ pipelining æ˜¾è‘—é™ä½å»¶è¿Ÿã€‚

---

### âš ï¸ å±€é™æ€§
1. **å°šæœªå®ç°åœ¨çœŸå® FPGA ä¸Šè¿è¡Œ**  
   å½“å‰ä»…å®Œæˆ hw_emu éªŒè¯ï¼Œç¼ºä¹å®é™…æ¿å¡æµ‹è¯•å’Œååé‡æµ‹é‡ã€‚

2. **1-bit é‡åŒ–æš‚æœªé›†æˆè¿› Allo æµæ°´çº¿**  
   ç”±äº Allo å½“å‰ä¸æ”¯æŒé‡åŒ–ç®—å­çš„ HLS åˆæˆï¼Œå› æ­¤é‡åŒ–æ¨¡å‹æœªèƒ½éƒ¨ç½²åˆ° FPGAã€‚

3. **Latency ä»é«˜äºç›®æ ‡ï¼ˆsub-Î¼sï¼‰**  
   å³ä½¿å‹ç¼©åï¼Œå½“å‰å»¶è¿Ÿä»åœ¨ ms çº§åˆ«ï¼Œè·ç¦» L1T è¦æ±‚çš„ **sub-microsecond** ä»æœ‰å·®è·ï¼Œéœ€è¿›ä¸€æ­¥ç¡¬ä»¶ä¼˜åŒ–ã€‚

4. **æœªæ¢ç´¢å…¶ä»–è½»é‡ Transformer å˜ä½“ï¼ˆå¦‚ Linformerï¼‰**  
   æ–‡çŒ® [28] å·²å±•ç¤º Linformer å¯è¾¾ O(100ns)ï¼Œæœ¬æ–‡æœªå°†å…¶çº³å…¥æ¯”è¾ƒã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ·±å…¥ç¡¬ä»¶ååŒè®¾è®¡ï¼ˆco-designï¼‰**
   - å¼•å…¥ loop unrollingã€spatial replicationã€deep pipelining ç­‰ä¼˜åŒ–æ‰‹æ®µï¼Œé€¼è¿‘ sub-Î¼s å»¶è¿Ÿç›®æ ‡ï¼›
   - æ¢ç´¢å®šåˆ¶ IP core æˆ– systolic array åŠ é€Ÿ attention è®¡ç®—ã€‚

2. **æ”¯æŒé‡åŒ–æ¨¡å‹çš„å…¨æµç¨‹éƒ¨ç½²**
   - æ‰©å±• Allo ä»¥åŸç”Ÿæ”¯æŒ BitNet ç±»å‹å’Œè¿ç®—ï¼›
   - å¼€å‘é‡åŒ–-aware çš„ HLS codegenã€‚

3. **æ‹“å±•è‡³å…¶ä»– HEP ä»»åŠ¡**
   - å¦‚ tau taggingã€event classificationã€track reconstruction ç­‰ï¼›
   - æ¢ç´¢ JetFormer åœ¨ High-Level Triggerï¼ˆHLTï¼‰ä¸­çš„åº”ç”¨ã€‚

4. **æ›´å¤§è§„æ¨¡æ¨¡å‹è¿ç§»**
   - å°† full-scale JetFormer éƒ¨ç½²è‡³ GPU/FPGA æ··åˆç³»ç»Ÿï¼Œç”¨äºå®æ—¶ filteringã€‚

5. **å¼€æ”¾å·¥å…·é“¾å»ºè®¾**
   - å°† JetFormer + Allo pipeline å¼€æºåŒ–ï¼Œæ¨åŠ¨ HEP ç¤¾åŒºé‡‡ç”¨ Transformer for triggersã€‚

---

> âœ… **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **JetFormer æä¾›äº†ä¸€æ¡â€œä¸€é±¼ä¸¤åƒâ€çš„å®ç”¨è·¯å¾„â€”â€”åŒä¸€ä¸ª Transformer æ¶æ„æ—¢èƒ½èƒœä»»ç¦»çº¿é«˜ç²¾åº¦ jet taggingï¼Œåˆèƒ½ç»å‹ç¼©åéƒ¨ç½²äº FPGA è§¦å‘ç³»ç»Ÿï¼Œé¦–æ¬¡å®ç°äº†æ€§èƒ½ä¸å¯éƒ¨ç½²æ€§çš„ç»Ÿä¸€ã€‚**

</details>

---

### 2. [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)

**Authors**: Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2601.17275v1  

#### Abstract
While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰ Large Language Modelsï¼ˆLLMsï¼‰åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è§£é¢˜ã€ç¬¦å·æ¨ç†ç­‰ï¼‰ä¸­è¡¨ç°å‡ºâ€œæµ…å±‚ç»Ÿè®¡æ‹Ÿåˆâ€è€Œéç³»ç»Ÿæ€§é€»è¾‘æ¨å¯¼çš„é—®é¢˜ã€‚å°½ç®¡ Reinforcement Learningï¼ˆRLï¼‰è¢«ç”¨äºæå‡æ¨ç†èƒ½åŠ›ï¼ˆå¦‚é€šè¿‡ RLVRï¼‰ï¼Œä½†åœ¨**ç¦»æ•£ token ç©ºé—´**ä¸­ç›´æ¥åº”ç”¨ RL å­˜åœ¨ä¸‰å¤§ç»“æ„æ€§ç“¶é¢ˆï¼š

- **Sample Inefficiency**ï¼šæ¯æ¡æ¨ç†é“¾éœ€å®Œæ•´ç”Ÿæˆæ‰èƒ½è·å¾—å¥–åŠ±ï¼Œæ¢ç´¢æˆæœ¬æé«˜ã€‚
- **High Gradient Variance**ï¼šé•¿åºåˆ—çš„é‡è¦æ€§é‡‡æ ·å¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®šã€‚
- **Catastrophic Forgetting**ï¼šå…¨å‚æ•°å¾®è°ƒæ˜“ç ´åé¢„è®­ç»ƒé˜¶æ®µè·å¾—çš„è¯­è¨€ä¸çŸ¥è¯†è¡¨å¾ã€‚

è¿™äº›é—®é¢˜ä¸¥é‡é™åˆ¶äº† RL åœ¨ LLM æ¨ç†ä¸­çš„ç¨³å®šæ€§ä¸å¯æ‰©å±•æ€§ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡º **DeepLatent Reasoning (DLR)** â€”â€”ä¸€ç§åŸºäº**è¿ç»­æ½œåœ¨ç©ºé—´**çš„åŒå‘å¯¹æ¯”å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°â€œå…ˆæ€åè¨€â€ï¼ˆ"think-before-speak"ï¼‰çš„æ–°èŒƒå¼ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
å°†ä¼ ç»Ÿçš„ â€œGeneration-Reward-Optimizationâ€ å¾ªç¯ä»é«˜ç»´ç¦»æ•£çš„ token ç©ºé—´è¿ç§»è‡³**ä½ç»´è¿ç»­ä¸”è¯­ä¹‰ä¸°å¯Œçš„ latent space**ï¼Œå…·ä½“è®¾è®¡å¦‚ä¸‹ï¼š

- **Latent-First, Token-Later**ï¼šä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„ **Assistant Model** åœ¨ latent space ä¸­é‡‡æ · $K$ æ¡â€œè½¯æ€ç»´è·¯å¾„â€ï¼ˆsoft reasoning trajectoriesï¼‰ï¼Œä»…å¯¹é«˜ä»·å€¼è·¯å¾„è¿›è¡Œè§£ç ã€‚
- **Dual Reward Mechanism**ï¼šå¼•å…¥åŒé‡å¥–åŠ±æœºåˆ¶ï¼š
  - `R_corr`ï¼šåŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„äºŒå…ƒå¥–åŠ±ï¼›
  - `R_fmt`ï¼šéªŒè¯è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼è§„èŒƒï¼ˆå¦‚ `<think>` æ ‡ç­¾å®Œæ•´æ€§ï¼‰ã€‚
- **Contrastive Exploration Objective**ï¼šé€šè¿‡ contrastive loss é¼“åŠ± latent è½¨è¿¹ä¹‹é—´çš„å¤šæ ·æ€§ï¼Œé¿å…æ¨¡å¼åç¼©ï¼ˆmode collapseï¼‰ï¼Œå®ç°**æœ‰å‘æ¢ç´¢**ï¼ˆdirected explorationï¼‰ã€‚
- **Zero-Forgetting Updates**ï¼šä¸»æ¨¡å‹ï¼ˆMain Modelï¼‰åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒ **frozen**ï¼Œä»…æ›´æ–° Assistant å’ŒæŠ•å½±å±‚å‚æ•°ï¼Œä»æ ¹æœ¬ä¸Šæ¶ˆé™¤ catastrophic forgettingã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ GRPOï¼‰ | DLR |
|------|---------------------|-----|
| æ¢ç´¢ç©ºé—´ | Discrete token space | Continuous latent space |
| é‡‡æ ·æ•ˆç‡ | æ¯æ¬¡ rollout éƒ½éœ€å®Œæ•´ decode â†’ é«˜è®¡ç®—å¼€é”€ | å¤šæ•°è½¨è¿¹åœ¨ latent å±‚è¿‡æ»¤ â†’ å‡å°‘ 82% ä¸»æ¨¡å‹å‰å‘ä¼ æ’­ |
| æ¢¯åº¦ç¨³å®šæ€§ | åºåˆ—è¶Šé•¿ï¼Œvariance è¶Šå¤§ | å°†æ•´ä¸ª chain è§†ä¸ºå•ä¸€ latent object â†’ variance æ›´ä½ |
| çŸ¥è¯†ä¿ç•™ | å…¨å‚æ•°å¾®è°ƒ â†’ æ˜“é—å¿˜é€šç”¨èƒ½åŠ› | ä¸»æ¨¡å‹å†»ç»“ â†’ å®Œæ•´ä¿ç•™ pre-trained knowledge |
| æ¢ç´¢è´¨é‡ | æ¸©åº¦é‡‡æ ·ç­‰éšæœºæ‰°åŠ¨ â†’ æ— æ–¹å‘æ€§ | Contrastive loss å¼•å¯¼è¯­ä¹‰å¤šæ ·åŒ–çš„æ¢ç´¢ |

> âœ… **æœ¬è´¨è½¬å˜**ï¼šä»â€œåœ¨å·¨å¤§è¯è¡¨ä¸­ç›²ç›®æœç´¢â€è½¬å‘â€œåœ¨ç´§å‡‘è¯­ä¹‰ç©ºé—´ä¸­è·¯å¾„è§„åˆ’â€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **GSM8K**ï¼šçº¦ 8.5K å°å­¦çº§åˆ«æ•°å­¦æ–‡å­—é¢˜ï¼Œæµ‹è¯•åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚
  - Train: 7,473 | Test: 1,319 | Avg. tokens: 185
- **MATH**ï¼šçº¦ 12.5K ç«èµ›çº§æ•°å­¦é—®é¢˜ï¼Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚
  - Train: 7,500 | Test: 5,000 | Avg. tokens: 387

> ä¸¤ä¸ªæ•°æ®é›†å‡è¦æ±‚å¤šæ­¥ chain-of-thought æ¨ç†ï¼Œé€‚åˆè¯„ä¼° long-horizon reasoning èƒ½åŠ›ã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | æè¿° |
|--------|------|
| **Main Model** | Frozen LLaMA-2-7B |
| **Assistant Model** | Trainable LLaMA-2-1.3B + Projection Layers |
| **Hardware** | 8Ã— NVIDIA A100 GPUs |
| **Latent Dimension** | 512 |
| **Group Size (G)** | 64 |
| **Batch Size (per device)** | 4 |
| **Training Epochs** | 3 |
| **Optimization** | AdamWï¼Œå¸¦ KL æ­£åˆ™ä¸æ¢¯åº¦è£å‰ª |

#### è¯„ä¼°æŒ‡æ ‡ï¼š
- **Pass@1 Accuracy**ï¼šæ¨¡å‹ä¸€æ¬¡æ€§ç”Ÿæˆå®Œå…¨æ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆçš„æ¯”ä¾‹ã€‚
- **Qualitative Analysis**ï¼šäººå·¥æ£€æŸ¥ hallucinated intermediate steps çš„æ¯”ä¾‹ã€‚
- **Computational Cost**ï¼šä¸»æ¨¡å‹ forward pass æ¬¡æ•°å æ¯”ã€‚

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦æ›´æ–°ä¸»æ¨¡å‹ | ä¸»è¦ç‰¹ç‚¹ |
|------|------|----------------|----------|
| Base LLaMA-2-7B | Zero-shot | â€“ | æ— ä»»ä½•å¾®è°ƒ |
| GRPO (token-level) | RL on token space | æ˜¯ | ä½¿ç”¨ group-relative advantageï¼Œæ— éœ€ critic |
| DeepSeekMath-RL | GRPO å˜ä½“ | æ˜¯ | å½“å‰ SOTA çš„æ•°å­¦æ¨ç† RL æ–¹æ³•ä¹‹ä¸€ |
| **DLR (Ours)** | **Latent-space RL** | **å¦ï¼ˆä»…æ›´æ–° assistantï¼‰** | å¼•å…¥ latent sampling + contrastive exploration |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 3ï¼‰
| Method | #Forward | GSM8K (Pass@1) | MATH (Pass@1) |
|--------|----------|----------------|---------------|
| Base LLaMA-2-7B | â€“ | 14.8% | 3.9% |
| GRPO (token-level) | 100% | 46.2% | 15.7% |
| DeepSeekMath-RL | 100% | 51.7% | 18.3% |
| **DLR (Ours)** | **18%** | **55.4%** | **22.1%** |

> ğŸ”º **ç»“æœäº®ç‚¹**ï¼š
> - åœ¨ä»…ä½¿ç”¨ **18% ä¸»æ¨¡å‹å‰å‘ä¼ æ’­**çš„æƒ…å†µä¸‹ï¼ŒDLR åœ¨ä¸¤ä¸ª benchmark ä¸Šå‡è¾¾åˆ° **SOTA æ€§èƒ½**ã€‚
> - ç›¸æ¯”æœ€å¼º baselineï¼ˆDeepSeekMath-RLï¼‰ï¼ŒGSM8K æå‡ **+3.7%**ï¼ŒMATH æå‡ **+3.8%**ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰å†»ç»“ä¸»æ¨¡å‹çš„å½±å“ï¼ˆTable 4ï¼‰
| è®¾ç½® | GSM8K å‡†ç¡®ç‡ | ç¨³å®šæ€§ |
|------|-------------|--------|
| ä¸»æ¨¡å‹å¯è®­ç»ƒ | 56.1% | ä½ï¼ˆå‡ºç°éœ‡è¡ä¸å´©æºƒï¼‰ |
| ä¸»æ¨¡å‹å†»ç»“ï¼ˆDLRï¼‰ | 55.4% | **é«˜ï¼ˆç¨³å®šæ”¶æ•›ï¼‰** |

> â—è™½ç„¶å¾®è°ƒä¸»æ¨¡å‹å¯èƒ½å¸¦æ¥è½»å¾®æ€§èƒ½æå‡ï¼Œä½†ä¼šå¯¼è‡´ä¸¥é‡çš„ **catastrophic forgetting** å’Œè®­ç»ƒä¸ç¨³ï¼ŒDLR ä»¥æå°ä»£ä»·æ¢å–æå¤§ç¨³å®šæ€§ã€‚

#### ï¼ˆ2ï¼‰ç§»é™¤ Contrastive Loss çš„å½±å“
- ç§»é™¤ $ \mathcal{L}_{cl} $ åï¼Œåœ¨ MATH ä¸Šç»å¯¹æ€§èƒ½ä¸‹é™ **4.7%**ã€‚
- ç”Ÿæˆçš„ reasoning chains ç¼ºä¹å¤šæ ·æ€§ï¼Œè¶‹å‘å•ä¸€è·¯å¾„ï¼Œtest-time scaling æ•ˆæœå·®ã€‚

> âœ… è¯æ˜ contrastive loss å¯¹ç»´æŒ latent space ä¸­çš„è¯­ä¹‰å¤šæ ·æ€§è‡³å…³é‡è¦ã€‚

#### ï¼ˆ3ï¼‰è®¡ç®—æˆæœ¬åˆ†æ
- ä¼ ç»Ÿ GRPOï¼šæ¯æ¬¡æŸ¥è¯¢éœ€ $ O(G \cdot C_M) $ è®¡ç®—é‡ï¼ˆ$C_M$: ä¸»æ¨¡å‹å‰å‘æˆæœ¬ï¼‰
- DLRï¼šä»…éœ€ $ O(G \cdot C_A + K \cdot C_M) $ï¼Œå…¶ä¸­ $K \ll G$
- å®é™… $K/G \approx 0.18$ï¼Œç›¸å½“äºä¸»æ¨¡å‹è®¡ç®—é‡å‡å°‘ **5.6å€**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Latent Space æ˜¯æ›´ä¼˜çš„ RL æ¢ç´¢åŸŸ**ï¼š
   - è¿ç»­ latent manifold æä¾›æ›´å¹³æ»‘çš„ä¼˜åŒ–åœ°å½¢ï¼Œæ˜¾è‘—é™ä½ gradient varianceã€‚
   - æ”¯æŒæ›´é•¿ç¨‹ã€æ›´ä¸€è‡´çš„æ¨ç†é“¾æ„å»ºã€‚

2. **Parameter Isolation æ˜¯ç¨³å®šè®­ç»ƒçš„å…³é”®**ï¼š
   - å†»ç»“ä¸»æ¨¡å‹ + æ›´æ–°è½»é‡åŠ©ç†æ¨¡å‹ï¼Œå¯åœ¨ä¸ç‰ºç‰²é€šç”¨è¯­è¨€èƒ½åŠ›çš„å‰æä¸‹å¢å¼ºä¸“é¡¹æ¨ç†æŠ€èƒ½ã€‚

3. **Contrastive Learning å®ç°æœ‰æ•ˆæ¢ç´¢**ï¼š
   - æ›¿ä»£ä¼ ç»Ÿ temperature samplingï¼Œcontrastive objective ä¸»åŠ¨æ¨åŠ¨è¯­ä¹‰åˆ†æ•£ï¼Œé˜²æ­¢ collapseã€‚

4. **DLR å®ç°é«˜æ•ˆä¸é«˜æ€§èƒ½çš„ç»Ÿä¸€**ï¼š
   - ä¸ä»…å‡†ç¡®ç‡æ›´é«˜ï¼Œè€Œä¸”è®­ç»ƒæ›´ç¨³å®šã€èµ„æºæ¶ˆè€—æ›´ä½ï¼Œå…·å¤‡è‰¯å¥½çš„å·¥ç¨‹å®ç”¨æ€§ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **æç«¯ç¬¦å·åŒ–ä»»åŠ¡é€‚åº”æ€§æœ‰é™**ï¼šå¯¹äºå½¢å¼åŒ–è¯æ˜ã€ä¸¥æ ¼è¯­æ³•çº¦æŸçš„ä»»åŠ¡ï¼Œlatent-to-token æ˜ å°„å¯èƒ½ä¸å¤Ÿç²¾ç¡®ã€‚
- **Reward Attribution è¾ƒç²—ç²’åº¦**ï¼šç›®å‰å¥–åŠ±ä»ä½œç”¨äºæ•´æ¡ latent è½¨è¿¹ï¼Œç¼ºä¹ step-level çš„ç»†ç²’åº¦ç›‘ç£ä¿¡å·ã€‚
- **ä¾èµ–é«˜è´¨é‡ frozen decoder**ï¼šè‹¥ä¸»æ¨¡å‹æœ¬èº«å­˜åœ¨åå·®æˆ–è§£ç ä¸ç¨³å®šï¼Œä¼šå½±å“ latent policy å­¦ä¹ ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. å°† DLR æ‰©å±•åˆ°å…¶ä»–å¤æ‚æ¨ç†ä»»åŠ¡ï¼š
   - Code Generation
   - Commonsense Reasoning
   - Scientific QA
2. æ¢ç´¢ **latent space çš„ scaling laws**ï¼šéšç€ latent dimension å’Œ assistant model size å¢åŠ ï¼Œæ€§èƒ½å¦‚ä½•å˜åŒ–ï¼Ÿ
3. å¼•å…¥ **finer-grained auxiliary supervision**ï¼šå¦‚ä¸­é—´æ­¥éª¤å¥–åŠ±ã€é€»è¾‘ä¸€è‡´æ€§åˆ¤åˆ«å™¨ç­‰ã€‚
4. æ„å»º **self-evolution + self-correction** æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½åœ¨ latent space ä¸­è‡ªåŠ¨ä¿®æ­£é”™è¯¯æ¨ç†è·¯å¾„ã€‚

--- 

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **DLR æˆåŠŸå°† LLM çš„å¼ºåŒ–å­¦ä¹ ä»â€œæ–‡æœ¬å±‚é¢è¯•é”™â€å‡çº§ä¸ºâ€œæ½œæ„è¯†å±‚é¢æ€è€ƒâ€ï¼Œå®ç°äº†æ›´ç¨³å®šã€é«˜æ•ˆã€å¯æŒç»­çš„æ¨ç†èƒ½åŠ›è¿›åŒ–ã€‚**

</details>

---

### 3. [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)

**Authors**: Zihan Wang, Cheng Tang, Lei Gong, Cheng Li, Chao Wang, teng wang, Wenqi Lou, Xuehai Zhou  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2601.16986v1  

#### Abstract
Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly importa...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠCrystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principleã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜  
Chain-of-Thought (CoT) æ¨ç†åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦ã€ç¼–ç¨‹ï¼‰å‡†ç¡®ç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶â€œæ€è€ƒé˜¶æ®µâ€ä¼šç”Ÿæˆå¤§é‡ä¸­é—´ tokenï¼Œå¯¼è‡´ Key-Value (KV) Cache å†…å­˜å¼€é”€æ€¥å‰§å¢åŠ ã€‚ä¼ ç»Ÿ KV ç¼“å­˜å‹ç¼©æ–¹æ³•ï¼ˆå¦‚ H2Oã€SnapKVï¼‰åŸºäº**token-uniform principle**ï¼Œå³å‡è®¾æ‰€æœ‰è¾“å‡º token åŒç­‰é‡è¦ï¼Œè¿™ä¸ CoT çš„å®é™…éœ€æ±‚â€”â€”**ä»…æœ€ç»ˆç­”æ¡ˆå¯¹ç”¨æˆ·é‡è¦**â€”â€”å­˜åœ¨æ ¹æœ¬å†²çªã€‚

å› æ­¤ï¼Œç°æœ‰æ–¹æ³•åœ¨ CoT åœºæ™¯ä¸‹ï¼š
- è¿‡åº¦ä¿ç•™æ— ç”¨çš„ä¸­é—´æ¨ç† tokenï¼ˆSlipKVï¼‰ï¼Œæµªè´¹å†…å­˜ï¼›
- æˆ–è¿‡æ—©ä¸¢å¼ƒå¯¹æœ€ç»ˆç­”æ¡ˆå…³é”®çš„ tokenï¼ˆCrystalKVï¼‰ï¼ŒæŸå®³å‡†ç¡®æ€§ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯  
æœ¬æ–‡æå‡º **Crystal-KV**ï¼Œä¸€ç§ä¸“ä¸º CoT æ¨ç†è®¾è®¡çš„é«˜æ•ˆ KV Cache ç®¡ç†æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Answer-First Principle**ï¼ˆç­”æ¡ˆä¼˜å…ˆåŸåˆ™ï¼‰ã€‚ä¸»è¦åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

1. **é¦–æ¬¡è¯†åˆ«å¹¶åŒºåˆ†ä¸¤ç§ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å¼**ï¼š
   - **CrystalKV**ï¼šçœŸæ­£å¯¹æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§æœ‰è´¡çŒ®çš„ KV æ¡ç›®ï¼Œè¡¨ç°ä¸ºåœ¨æ€è€ƒé˜¶æ®µè¢«**é—´æ­‡æ€§ä½†æŒç»­å…³æ³¨**ï¼Œç›´åˆ°æ¨ç†ç»“æŸã€‚
   - **SlipKV**ï¼šä¸»è¦ç”¨äºç»´æŒæ¨ç†æµç¨‹çš„ KV æ¡ç›®ï¼Œè¡¨ç°ä¸º**æµå¼æ³¨æ„åŠ›æ¨¡å¼**ï¼ˆå±€éƒ¨å¼ºå…³æ³¨åè¿…é€Ÿè¡°å‡ï¼‰ï¼Œå¯èƒ½å¼•å…¥è¯¯å¯¼ä¿¡æ¯ã€‚

2. **æå‡º Attention-based LRFU ç®—æ³•**ï¼š
   - åŸºäº **Least Recently Frequently Used (LRFU)** ç­–ç•¥ï¼Œç»“åˆæ³¨æ„åŠ›å¾—åˆ†åŠ¨æ€è®¡ç®—æ¯ä¸ª KV æ¡ç›®çš„ **Combined Recency and Frequency (CRF) Score**ã€‚
   - é€šè¿‡ Top-p é‡‡æ ·ç¡®å®šâ€œå‘½ä¸­â€KVï¼ŒåŠ¨æ€æ›´æ–° CRFã€‚
   - ç²¾å‡†è¯†åˆ«å¹¶æ·˜æ±°å·²å¤±æ•ˆçš„ SlipKVï¼Œç¡®ä¿ CrystalKV è¢«ä¿ç•™è€Œä¸ç ´åæ¨ç†æµã€‚

3. **æå‡ºè‡ªé€‚åº”ç¼“å­˜é¢„ç®—åˆ†é…ç®—æ³•ï¼ˆAdaptive Budget Allocationï¼‰**ï¼š
   - åŠ¨æ€ä¼°è®¡æ¯å±‚/æ¯å¤´çš„ **Cache Utilization**ï¼ˆCRF / Budgetï¼‰ï¼Œåæ˜ å…¶å¯¹æœ€ç»ˆç­”æ¡ˆçš„é‡è¦æ€§ã€‚
   - æ ¹æ®åˆ©ç”¨ç‡åŠ¨æ€è°ƒæ•´å„å±‚/å¤´çš„ KV é¢„ç®—ï¼Œæ”¾å¤§å…³é”®ç»„ä»¶çš„å½±å“ï¼Œæå‡ç¼“å­˜ç©ºé—´åˆ©ç”¨ç‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿  
- **ç›®æ ‡å¯¼å‘æ˜ç¡®**ï¼šä»¥â€œæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§â€ä¸ºæ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ï¼Œè€Œéå‡åŒ€ä¿ç•™æ‰€æœ‰ tokenã€‚
- **å‹ç¼©æ›´ç²¾å‡†**ï¼šé€šè¿‡æ³¨æ„åŠ›æ¨¡å¼åŒºåˆ† CrystalKV å’Œ SlipKVï¼Œé¿å…è¯¯åˆ å…³é”®ä¿¡æ¯ã€‚
- **èµ„æºåˆ©ç”¨æ›´é«˜æ•ˆ**ï¼šåŠ¨æ€é¢„ç®—åˆ†é…ä½¿æœ‰é™å†…å­˜é›†ä¸­åœ¨æœ€å…³é”®çš„éƒ¨åˆ†ã€‚
- **æ€§èƒ½å…¨é¢æå‡**ï¼šåœ¨å¤§å¹…é™ä½å†…å­˜çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡å‡†ç¡®ç‡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†  
- **ç¼–ç¨‹ä»»åŠ¡**ï¼š`CodeForces`ï¼ˆ10K ç«èµ›çº§ç¼–ç¨‹é¢˜ï¼‰ï¼Œé™åˆ¶éš¾åº¦ä½äº 1500 ä»¥é¿å…å‡†ç¡®ç‡é¥±å’Œã€‚
- **æ•°å­¦ä»»åŠ¡**ï¼š`MATH-500`ï¼ˆé«˜çº§ç«èµ›çº§æ•°å­¦é¢˜ï¼‰ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡  
- **æ¨¡å‹**ï¼šä¸‰ä¸ª DeepSeek-R1-Distilled æ¨¡å‹ï¼š
  - Llama-8B
  - Qwen-14B
  - Qwen-32B
- **æ¨ç†é•¿åº¦**ï¼š8K å’Œ 16K çº§åˆ«ï¼ˆå¹³å‡ 9,350 å’Œ 18,700 tokensï¼‰ã€‚
- **é‡‡æ ·å‚æ•°**ï¼šTemperature = 0.6ï¼ŒTop-p = 0.95ã€‚
- **è¯„ä¼°æ–¹å¼**ï¼šæ¯é¢˜ç”Ÿæˆ k=8 ä¸ªç­”æ¡ˆï¼ŒæŠ¥å‘Šå¹³å‡æ­£ç¡®ç‡ï¼ˆ$\sum p_i$ï¼‰ã€‚
- **ç¡¬ä»¶**ï¼šä¸‰å— NVIDIA RTX PRO 6000 Blackwell GPUã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”  
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦æ”¯æŒåŠ¨æ€é¢„ç®— |
|------|------|----------------|
| FullKV | æ— å‹ç¼© | âŒ |
| R-KV | CoT-oriented | âŒ |
| RaaS | CoT-oriented | âŒ |
| SnapKV | LCG æ–¹æ³• | âŒ |
| StreamingLLM | LCG æ–¹æ³• | âŒ |
| H2O | LCG æ–¹æ³• | âŒ |
| **Crystal-KV.Lite** | æœ¬æ–‡æ–¹æ³•ï¼ˆç¦ç”¨åŠ¨æ€é¢„ç®—ï¼‰ | âŒ |
| **Ada-SnapKV** | SnapKV + åŠ¨æ€é¢„ç®—ï¼ˆæœ¬æ–‡æ„å»ºï¼‰ | âœ… |
| **Crystal-KV** | æœ¬æ–‡å®Œæ•´æ–¹æ³• | âœ… |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®  
- **å¹³å‡å†…å­˜èŠ‚çœ**ï¼š**90.89%**
- **å¹³å‡ååé‡æå‡**ï¼š**7.57Ã—**
- **ç”¨æˆ·çº§å“åº”å»¶è¿ŸåŠ é€Ÿ**ï¼šæœ€é«˜è¾¾ **1.24Ã—**
- **å‹ç¼©æ•ˆæœ**ï¼šå®ç°**æ— æŸå‹ç¼©**ï¼ˆlossless compressionï¼‰

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ  
#### å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆå›¾4ï¼‰
- åœ¨ç›¸åŒ KV é¢„ç®—ä¸‹ï¼ŒCrystal-KV æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼š
  - **ç¼–ç¨‹ä»»åŠ¡**ï¼šå¹³å‡å‡†ç¡®ç‡å¢ç›Š 7.30% ~ 18.98%
  - **æ•°å­¦ä»»åŠ¡**ï¼šå¹³å‡å‡†ç¡®ç‡å¢ç›Š 7.97% ~ 23.87%
- åœ¨æä½é¢„ç®—ï¼ˆ10% FullKVï¼‰ä¸‹ï¼š
  - Crystal-KV è¾¾åˆ° **105% FullKV å‡†ç¡®ç‡**
  - å…¶ä»–æ–¹æ³•ä»…è¾¾åˆ° 30%â€“67% å‡†ç¡®ç‡
- å­˜åœ¨â€œå³°å€¼ç°è±¡â€ï¼šå½“é¢„ç®—åˆšå¥½è¶³å¤Ÿä¿ç•™æ‰€æœ‰ CrystalKV æ—¶ï¼Œå‡†ç¡®ç‡è¶…è¿‡ FullKVï¼Œè¯´æ˜å»é™¤ SlipKV å¯å‡å°‘å¹²æ‰°ã€‚

#### å†…å­˜ä¸ååå¯¹æ¯”ï¼ˆè¡¨ Iï¼‰
| è®¾ç½® | æ–¹æ³• | HBM èŠ‚çœ | æœ€å¤§ Batch æ•° | åå (tok/s) | å¹¶å‘å¤„ç† token æ•° |
|------|------|----------|---------------|--------------|--------------------|
| 8K, Fixed | FullKV | â€“ | 51 | 458.67 | 476,850 |
| 8K, Fixed | Crystal-KV | 89.05% | **379** | **2297.37** | **3,543,650** |
| 8K, Ratio | Crystal-KV | 90.00% | **412** | **2478.39** | **3,852,200** |
| 16K, Fixed | FullKV | â€“ | 25 | 174.95 | 467,500 |
| 16K, Fixed | Crystal-KV | 94.52% | **379** | **2141.44** | **7,087,300** |

> ç»“æœè¡¨æ˜ï¼šCrystal-KV åœ¨é•¿åºåˆ—æ¨ç†ä¸­ä¼˜åŠ¿æ›´æ˜¾è‘—ï¼Œå¯æ”¯æŒè¿œè¶… FullKV çš„å¹¶å‘è§„æ¨¡ã€‚

### æ¶ˆèå®éªŒç»“æœ  
- **Adaptive Budget Allocation** åœ¨æå°é¢„ç®—ä¸‹è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼ŒéªŒè¯å…¶æå‡ç¼“å­˜åˆ©ç”¨ç‡çš„æœ‰æ•ˆæ€§ã€‚
- **Crystal-KV.Lite**ï¼ˆæ— åŠ¨æ€é¢„ç®—ï¼‰ä»æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œè¯´æ˜ LRFU ç­–ç•¥æœ¬èº«å·²å…·å¼ºå¤§ç«äº‰åŠ›ã€‚
- **Ada-SnapKV** è¡¨ç°å¼±äº Crystal-KVï¼Œè¯´æ˜ä»…åŠ åŠ¨æ€é¢„ç®—æ— æ³•å¼¥è¡¥åº•å±‚å‹ç¼©ç­–ç•¥çš„ç¼ºé™·ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°  
1. **Answer-First Principle æˆç«‹**ï¼šCoT ä¸­ä»…æœ‰å°‘æ•° KV æ¡ç›®ï¼ˆCrystalKVï¼‰çœŸæ­£å½±å“æœ€ç»ˆç­”æ¡ˆï¼Œä¸”å…·æœ‰å¯è¯†åˆ«çš„é—´æ­‡æ€§æ³¨æ„åŠ›æ¨¡å¼ã€‚
2. **SlipKV å¯å®‰å…¨ç§»é™¤**ï¼šå¤§é‡ä¸­é—´æ¨ç† tokenï¼ˆSlipKVï¼‰è™½ç»´æŒæµç¨‹ä½†éå¿…è¦ï¼ŒåŠæ—¶æ·˜æ±°å¯é‡Šæ”¾å†…å­˜è€Œä¸å½±å“ç”šè‡³æå‡å‡†ç¡®ç‡ã€‚
3. **åŠ¨æ€é¢„ç®—åˆ†é…è‡³å…³é‡è¦**ï¼šä¸åŒå±‚/å¤´å¯¹æ¨ç†è´¡çŒ®å¼‚è´¨ï¼ŒåŠ¨æ€åˆ†é…èƒ½æ˜¾è‘—æå‡ç¼“å­˜æ•ˆç‡ã€‚
4. **æè‡´å‹ç¼©ä¸é«˜å‡†ç¡®ç‡å¯å…¼å¾—**ï¼šåœ¨ä»… 10% KV é¢„ç®—ä¸‹ï¼ŒCrystal-KV ä¸ä»…ä¸æŸå¤±ç²¾åº¦ï¼Œåè€Œè¶…è¶Š FullKVã€‚

### æ–¹æ³•çš„å±€é™æ€§  
- å½“å‰æ–¹æ³•ä¾èµ–æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œ Top-p é‡‡æ ·ï¼Œå¯èƒ½å—é‡‡æ ·éšæœºæ€§å½±å“ã€‚
- å¯¹æç«¯åˆ†æ•£çš„æ•°å­¦é—®é¢˜ï¼ˆä¿¡æ¯åˆ†å¸ƒå¹¿ï¼‰ï¼ŒCrystalKV å æ¯”æ›´é«˜ï¼Œå‹ç¼©ç‡ç›¸å¯¹å—é™ã€‚
- å‚æ•°æ•æ„Ÿæ€§ï¼šDecay Rate (Î») å’Œ Top-p éœ€åˆç†è®¾ç½®ï¼ˆæ¨è Î» âˆˆ [0.5,0.7], top-p = 0.9ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘  
- å½¢å¼åŒ–å»ºæ¨¡ CrystalKV ä¸ SlipKVï¼Œé‡åŒ– KV Cache å‹ç¼©çš„èƒ½åŠ›è¾¹ç•Œã€‚
- æ¢ç´¢æ›´é²æ£’çš„æ³¨æ„åŠ›æ¨¡å¼è¯†åˆ«æœºåˆ¶ã€‚
- å°† Answer-First æ€æƒ³æ‰©å±•è‡³å…¶ä»–æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚ Early Exitã€Speculative Decodingï¼‰ã€‚

--- 

> **æ€»ç»“**ï¼šCrystal-KV é‡æ–°å®šä¹‰äº†é¢å‘æ¨ç†çš„ KV Cache ç®¡ç†èŒƒå¼ï¼Œä»â€œå‡åŒ€ä¿ç•™â€è½¬å‘â€œç­”æ¡ˆä¼˜å…ˆâ€ï¼Œå®ç°äº†**å†…å­˜ã€é€Ÿåº¦ã€å‡†ç¡®ç‡**ä¸‰è€…çš„ååŒä¼˜åŒ–ï¼Œä¸ºå¤§è§„æ¨¡ CoT æ¨ç†çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†æ–°è·¯å¾„ã€‚ä»£ç å°†å¼€æºï¼š`github.com/xxx`ã€‚

</details>

---

### 4. [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)

**Authors**: Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2601.16991v1  

#### Abstract
Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight update...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**SALR: Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ•ˆå¾®è°ƒä¸­ï¼Œä¸»æµæ–¹æ³•å¦‚ **LoRA** è™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä½†å…¶åº•å±‚æƒé‡ä»ä¸º**ç¨ å¯†ç»“æ„**ï¼Œå¯¼è‡´å­˜å‚¨å’Œæ¨ç†å¼€é”€ä¾ç„¶å¾ˆé«˜ã€‚å¦ä¸€æ–¹é¢ï¼Œ**magnitude-based pruning** å¯ä»¥å®ç°ç¨€ç–åŒ–å‹ç¼©ï¼Œä½†ç›´æ¥åº”ç”¨äº LoRA ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

ç°æœ‰æ–¹æ³•é¢ä¸´ä»¥ä¸‹çŸ›ç›¾ï¼š
- **LoSA** ç­‰é™æ€å‰ªææ–¹æ³•èƒ½å®ç°ç¨€ç–æ€§å’ŒåŠ é€Ÿï¼Œä½†ç‰ºç‰²äº†ç²¾åº¦ï¼›
- **SparseLoRA** ç­‰åŠ¨æ€å‰ªææ–¹æ³•ä¿æŒé«˜ç²¾åº¦ï¼Œä½†åœ¨éƒ¨ç½²æ—¶ä»æ˜¯ç¨ å¯†æ¨¡å‹ï¼Œæ— æ³•çœŸæ­£å‹ç¼©æˆ–æé€Ÿã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°**çœŸæ­£çš„ç¨€ç–å‹ç¼© + é«˜æ•ˆæ¨ç†åŠ é€Ÿ**ï¼Œæ˜¯å½“å‰èµ„æºå—é™åœºæ™¯ä¸‹çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSALR

ä½œè€…æå‡º **SALR (Sparsity-Aware Low-Rank Representation)**ï¼Œä¸€ç§ç»Ÿä¸€ä½ç§©é€‚é…ä¸ç¨€ç–å‰ªæçš„æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰ç†è®ºé©±åŠ¨çš„æœ€ä¼˜å‰ªæç­–ç•¥
- æ„å»ºäº†ä¸€ä¸ªåŸºäº **Mean-Squared Error (MSE)** çš„ç»Ÿä¸€åˆ†ææ¡†æ¶ï¼›
- è¯æ˜ï¼š**ä»…å¯¹å†»ç»“çš„åŸå§‹æƒé‡ $W_0$ åº”ç”¨é™æ€æ©ç ï¼ˆstatic maskï¼‰** æ˜¯æ‰€æœ‰å‰ªææ–¹å¼ä¸­è¯¯å·®ä¸Šç•Œæœ€ä½çš„æ–¹æ¡ˆï¼›
- å› æ­¤ï¼Œåº”é¿å…å¯¹ LoRA å­ç©ºé—´ï¼ˆå³ $A$, $B$ï¼‰è¿›è¡Œå‰ªæï¼Œä»¥ä¿æŠ¤ä½ç§©ç»“æ„å®Œæ•´æ€§ã€‚

#### ï¼ˆ2ï¼‰æ®‹å·®ä¿¡æ¯ä¿ç•™æœºåˆ¶ â€”â€” Sparsity-Preservation Pruning
- å‰ªæåä¸¢å¼ƒçš„æƒé‡å¹¶éç®€å•ç½®é›¶ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ª **truncated-SVD ä½ç§©é€‚é…å™¨** æ•è·å…¶æ®‹å·®ä¿¡æ¯ï¼›
- è¯¥é€‚é…å™¨å°†å‰ªææ®‹å·®çŸ©é˜µ $E = W - \hat{W}$ è¿›è¡Œä½ç§©é€¼è¿‘ï¼Œæ¢å¤å…³é”®ä¿¡æ¯ï¼›
- ç†è®ºè¯æ˜ï¼šæ­¤æ“ä½œå¯å°†æ¯é¡¹ MSE å‡å°‘ $(1 - r/\min(d,k))$ å€ã€‚

#### ï¼ˆ3ï¼‰å¤šé€‚é…å™¨èåˆä¼˜åŒ– â€”â€” Concatenated GEMM
- å¤šä¸ªä½ç§©æ¨¡å—ï¼ˆLoRA + æ®‹å·®é€‚é…å™¨ï¼‰è¢«æ²¿ç§©ç»´åº¦æ‹¼æ¥æˆå•ä¸€å¤§å‹ GEMM æ“ä½œï¼›
- æ˜¾è‘—é™ä½ kernel launch å¼€é”€ï¼Œæå‡ç¡¬ä»¶åˆ©ç”¨ç‡ï¼ˆå°¤å…¶æ˜¯ GPU Tensor Coresï¼‰ã€‚

#### ï¼ˆ4ï¼‰çœŸå®æ¨¡å‹å‹ç¼©ä¸é«˜æ•ˆæ¨ç†ç®¡é“
- å¼•å…¥ **bitmap ç¼–ç ** å­˜å‚¨ç¨€ç–æƒé‡ï¼Œä»…ä¿å­˜éé›¶å…ƒç´ åŠå…¶ä½ç½®ï¼›
- è®¾è®¡ **ä¸¤é˜¶æ®µæµæ°´çº¿è§£ç  + GEMM æ¶æ„**ï¼š
  - ç¬¬ä¸€é˜¶æ®µï¼šCUDA core å¹¶è¡Œè§£ç  bitmapï¼Œé‡å»ºç¨€ç–å­å—ï¼›
  - ç¬¬äºŒé˜¶æ®µï¼šTensor Core æ‰§è¡Œå¯†é›† GEMMï¼›
  - ä¸¤è€…é€šè¿‡ç¯å½¢ç¼“å†²åŒºå¹¶è¡Œæ‰§è¡Œï¼Œç»´æŒè®¡ç®—é¥±å’Œã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | æ€§èƒ½ | æ¨¡å‹ç¨€ç–æ€§ | æ˜¯å¦å®ç°æ¨ç†åŠ é€Ÿ |
|------|------|------------|------------------|
| LoSA | ä½ | âœ… | âœ… |
| SparseLoRA | é«˜ | âŒï¼ˆè®­ç»ƒç¨€ç–ï¼Œéƒ¨ç½²ç¨ å¯†ï¼‰ | âŒ |
| **SALRï¼ˆæœ¬æ–‡ï¼‰** | **é«˜** | âœ… | âœ… |

> âœ… **é¦–æ¬¡åŒæ—¶è¾¾æˆâ€œé«˜æ€§èƒ½ + ç¨€ç–æ¨¡å‹ + æ¨ç†åŠ é€Ÿâ€ä¸‰é‡ç›®æ ‡**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **MetaMath**ï¼šç”¨äºæ•°å­¦é¢†åŸŸç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé€‚é… MATH ç±»ä»»åŠ¡ï¼›
- **ARC, MC-TEST, OBQA, RACE** ç­‰å¤šé€‰é¢˜æ•°æ®é›†ï¼šç”¨äºè·¨å­¦ç§‘çŸ¥è¯†è¯„ä¼°ï¼ˆMMLUï¼‰ï¼›
- å¾®è°ƒä»»åŠ¡èšç„¦äº **zero-shot å’Œ few-shot æ¨ç†èƒ½åŠ›**ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹
- Llama2-7B
- Llama3-8B
- Mixtral-8x7B
- DeepSeek-V2-Lite

#### å¾®è°ƒé…ç½®
- ä½¿ç”¨ LoRA rank = 64ï¼›
- å…¨å±€å‰ªæç‡è®¾ä¸º **50%**ï¼ˆé™¤éç‰¹åˆ«è¯´æ˜ï¼‰ï¼›
- æ‰€æœ‰æ–¹æ³•å‡åœ¨åŒä¸€è®­ç»ƒé¢„ç®—ä¸‹æ¯”è¾ƒã€‚

#### è¯„ä¼°æŒ‡æ ‡
- **MMLU**ï¼šæŠ¥å‘Š 5-shot å‡†ç¡®ç‡ï¼›
- **GSM8K**ï¼šæŠ¥å‘Š zero-shot å‡†ç¡®ç‡ï¼›
- **æ¨ç†ååé‡**ï¼štokens/sï¼ˆRTX 4090 ä¸Šæµ‹é‡ï¼‰ï¼›
- **æ¨¡å‹å¤§å° / å†…å­˜å ç”¨**ï¼šGBï¼›
- **Fine-tuning ååé‡**ï¼šTFLOPSï¼›
- **å‹ç¼©æ¯”ï¼ˆ#Compï¼‰**ï¼šåŸå§‹æ¨¡å‹ vs å‹ç¼©åå¤§å°ã€‚

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **LoRA** | ç¨ å¯†å¾®è°ƒ | å‚æ•°é«˜æ•ˆï¼Œæ€§èƒ½å¼ºåŸºçº¿ |
| **LoSA (ICLR'25)** | é™æ€å‰ªæ | å¯¹ $W_0$ å’Œ LoRA åŒæ—¶å‰ªæï¼Œé€Ÿåº¦å¿«ä½†æ€§èƒ½å·® |
| **SparseLoRA (ICML'25)** | åŠ¨æ€å‰ªæ | è¾“å…¥æ„ŸçŸ¥å‰ªæï¼Œè®­ç»ƒå¿«ï¼Œéƒ¨ç½²ä»ä¸ºç¨ å¯† |
| **DeepSparse** | ç»“æ„åŒ–å‰ªæ | é€šç”¨ç¨€ç–è®­ç»ƒæ¡†æ¶ |
| **Pretrained** | ä¸å¾®è°ƒ | æ§åˆ¶ç»„ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 2 & Figure 1ï¼‰

| æ–¹æ³• | æ¨¡å‹ | MMLU (%) | GSM8K (%) | Sparsity | æ¨¡å‹å¤§å° |
|------|------|----------|-----------|----------|---------|
| Pretrained | Llama3-8B | 66.0 | 72.0 | â€“ | 15.5 GB |
| LoRA | Llama3-8B | 69.2 | **79.5** | â€“ | 15.5 GB |
| LoSA | Llama3-8B | 64.4 | 71.4 | 50% | 7.98 GB |
| **SALRï¼ˆæœ¬æ–‡ï¼‰** | Llama3-8B | **68.2** | **79.5** | 50% | **7.98 GB** |

> âœ… åœ¨ **50% ç¨€ç–åº¦** ä¸‹ï¼ŒSALR å®Œå…¨æ¢å¤ LoRA çš„ GSM8K æ€§èƒ½ï¼ˆ79.5%ï¼‰ï¼Œè¿œè¶… LoSAï¼ˆ71.4%ï¼‰ï¼Œä¸”æ¨¡å‹ä½“ç§¯å‡åŠï¼

---

### â±ï¸ æ¨ç†é€Ÿåº¦ä¸ç³»ç»Ÿæ•ˆç‡ï¼ˆTable 4ï¼‰

| æ–¹æ³•ï¼ˆSparsityï¼‰ | GSM8K Acc (%) | Throughput (tok/s) | Speedup |
|------------------|---------------|--------------------|--------|
| LoRA (N/A)       | 79.5          | 60.1               | 1.0x   |
| SparseLoRA (N/A) | 72.0          | 60.1               | 1.0x   |
| LoSA (2:4)       | 69.4          | **113.5**          | **1.9x** |
| **SALR (2:4)**   | **78.9**      | **104.9**          | **1.7x** |

> âœ… SALR å®ç° **1.7Ã— æ¨ç†åŠ é€Ÿ**ï¼ŒåŒæ—¶å‡†ç¡®ç‡æ¥è¿‘ LoRAï¼Œæ˜¾è‘—ä¼˜äº LoSAã€‚

---

### ğŸ’¾ å¾®è°ƒé˜¶æ®µç³»ç»Ÿæ•ˆç‡ï¼ˆTable 3ï¼‰

| æ–¹æ³• | FT Mem (GB) | FT TFLOPS |
|------|-------------|-----------|
| LoRA | 26.7        | 91.9      |
| LoSA | 27.1        | 74.5      |
| **SALR** | **19.2**    | **89.2**  |

> âœ… SALR å°†å¾®è°ƒå†…å­˜å‡å°‘ **~30%**ï¼Œååæå‡ **~20%**ï¼Œå¾—ç›Šäºä½ç§©èåˆä¸é«˜æ•ˆè§£ç è®¾è®¡ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æ˜¯å¦æ›´æ–°æ®‹å·®é€‚é…å™¨ï¼Ÿï¼ˆTable 5ï¼‰
| æ–¹æ³• | Llama2-7B (MMLU) | Llama3-8B (MMLU) |
|------|------------------|------------------|
| LoRA | 56.0             | 69.2             |
| SALR w/ frozen residual | 54.2         | 66.8             |
| **SALR w/ trainable residual** | **56.0**     | **68.2**         |

> âœ… **å¿…é¡»è”åˆå¾®è°ƒæ®‹å·®é€‚é…å™¨**ï¼Œå¦åˆ™æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼›è®­ç»ƒæ®‹å·®å¯è‡ªé€‚åº”ä»»åŠ¡éœ€æ±‚ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒç¨€ç–åº¦çš„å½±å“ï¼ˆTable 7ï¼‰
| Sparsity | GSM8K Accuracy |
|---------|----------------|
| 10%     | 79.5           |
| 30%     | **80.1** âœ…     |
| 50%     | 79.5           |

> âœ… å³ä½¿è¾¾åˆ° **50% ç¨€ç–åº¦**ï¼Œæ€§èƒ½æ— æŸï¼›ç”šè‡³ **30% æ—¶ç•¥æœ‰å¢ç›Š**ï¼Œè¡¨æ˜é€‚åº¦ç¨€ç–å…·æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚

#### ï¼ˆ3ï¼‰ç»“åˆé‡åŒ–ï¼ˆQSALRï¼ŒTable 6ï¼‰
| æ–¹æ³• | DeepSeek-V2-Lite (Size) | Mixtral-8x7B (Size) | Acc Drop |
|------|--------------------------|--------------------|----------|
| LoRA | 31.8 GB                  | 93.9 GB            | â€“        |
| QSALR (20% + NF4) | **6.5 GB**       | **19.2 GB**        | â‰¤0.6     |

> âœ… ç»“åˆ **NF4 é‡åŒ–** å¯å®ç° **~5Ã— æ¨¡å‹å‹ç¼©**ï¼Œå‡ ä¹æ— æ€§èƒ½æŸå¤±ï¼Œé€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **é™æ€å‰ªæå†»ç»“æƒé‡æ˜¯æœ€ä¼˜é€‰æ‹©**  
   ç†è®ºè¯æ˜ï¼šåªå‰ªæ $W_0$ èƒ½æœ€å°åŒ– MSE ä¸Šç•Œï¼Œä¼˜äºåŠ¨æ€æˆ–è”åˆå‰ªæã€‚

2. **æ®‹å·®ä¿¡æ¯å¯é€šè¿‡ä½ç§©é€‚é…å™¨æœ‰æ•ˆæ¢å¤**  
   åˆ©ç”¨ truncated-SVD æ„é€ è¾…åŠ© LoRA æ¨¡å—ï¼Œæ˜¾è‘—ç¼“è§£å‰ªæå¸¦æ¥çš„ä¿¡æ¯ä¸¢å¤±ã€‚

3. **ç¡¬ä»¶å‹å¥½è®¾è®¡è‡³å…³é‡è¦**  
   - å¤šé€‚é…å™¨æ‹¼æ¥ â†’ å•ä¸€ GEMM æå‡è®¡ç®—æ•ˆç‡ï¼›
   - Bitmap + æµæ°´çº¿è§£ç  â†’ å®ç°çœŸå®ç¨€ç–åŠ é€Ÿä¸å‹ç¼©ã€‚

4. **SALR å®ç°â€œä¸‰èµ¢â€å¹³è¡¡**  
   > **é«˜ç²¾åº¦ + é«˜å‹ç¼© + é«˜é€Ÿæ¨ç†**ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•åªèƒ½å–å…¶äºŒçš„ç“¶é¢ˆã€‚

---

### âš ï¸ å±€é™æ€§

1. **ä¾èµ–é¢„å®šä¹‰ç¨€ç–æ¨¡å¼**  
   å½“å‰é‡‡ç”¨å…¨å±€ magnitude-based å‰ªæï¼Œæœªæ¢ç´¢æ›´å¤æ‚çš„ç»“æ„åŒ–æˆ–å­¦ä¹ å‹ç¨€ç–ã€‚

2. **SVD æ®‹å·®å¼•å…¥é¢å¤–å»¶è¿Ÿï¼ˆè™½å·²ä¼˜åŒ–ï¼‰**  
   å°½ç®¡é€šè¿‡æµæ°´çº¿æ©ç›–äº†è§£ç å¼€é”€ï¼Œä½†åœ¨æä½å»¶è¿Ÿåœºæ™¯å¯èƒ½ä»æœ‰å½±å“ã€‚

3. **æ‰©å±•åˆ° MoE æ¶æ„å°šéœ€éªŒè¯**  
   è™½åœ¨ Mixtral ä¸Šæµ‹è¯•æœ‰æ•ˆï¼Œä½†å¯¹ä¸“å®¶é€‰æ‹©è·¯å¾„çš„ç¨€ç–æ€§ååŒå°šæœªæ·±å…¥ç ”ç©¶ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **åŠ¨æ€è°ƒæ•´ç¨€ç–ç‡ä¸ç§©åˆ†é…**  
   æ ¹æ®å±‚æ•æ„Ÿåº¦è‡ªåŠ¨åˆ†é…å‰ªææ¯”ä¾‹ä¸æ®‹å·®ç§© $r$ã€‚

2. **ä¸é‡åŒ–ã€è’¸é¦ç­‰æŠ€æœ¯è¿›ä¸€æ­¥èåˆ**  
   æ„å»ºç»Ÿä¸€çš„ **Sparse + Low-Rank + Quantized (SLQ)** å‹ç¼©æ ˆã€‚

3. **æ”¯æŒæ›´å¤šç¨€ç–æ¨¡å¼ï¼ˆå¦‚ N:M, Blockï¼‰**  
   é€‚é… Tensor Core ç¡¬ä»¶çº¦æŸï¼Œæœ€å¤§åŒ–ç¨€ç–åŠ é€Ÿæ½œåŠ›ã€‚

4. **ç«¯åˆ°ç«¯ç¼–è¯‘å™¨çº§ä¼˜åŒ–**  
   å°† SALR çº³å…¥ LLM inference compilerï¼ˆå¦‚ TensorRT-LLMï¼‰ä¸­å®ç°è‡ªåŠ¨åŒ–éƒ¨ç½²ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯

> **SALR æ˜¯é¦–ä¸ªåœ¨ç†è®ºæŒ‡å¯¼ä¸‹ï¼Œé€šè¿‡â€œé™æ€å‰ªæ + æ®‹å·®ä½ç§©è¡¥å¿ + ç¡¬ä»¶æ„ŸçŸ¥èåˆâ€ï¼Œå®ç°é«˜æ€§èƒ½ã€é«˜å‹ç¼©ã€é«˜é€Ÿåº¦ä¸‰ä½ä¸€ä½“çš„ LLM é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤§æ¨¡å‹éƒ¨ç½²æä¾›äº†å…¨æ–°å¯è¡Œè·¯å¾„ã€‚**

</details>

---

### 5. [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)

**Authors**: Shahil Shaik, Jonathon M. Smereka, Yue Wang  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.17069v1  

#### Abstract
Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMulti-Agent Deep Reinforcement Learning Under Constrained Communications

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ™®éé‡‡ç”¨**é›†ä¸­å¼è®­ç»ƒã€å»ä¸­å¿ƒåŒ–æ‰§è¡Œï¼ˆCTDEï¼‰**èŒƒå¼ï¼Œå…¶ä¾èµ–äºå…¨å±€çŠ¶æ€ä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ä»¥ä¸‹ç“¶é¢ˆï¼š
- **å¯æ‰©å±•æ€§å·®**ï¼šå¤§è§„æ¨¡ç³»ç»Ÿä¸­éš¾ä»¥è·å–å’Œä¼ è¾“å…¨å±€ä¿¡æ¯ï¼›
- **é²æ£’æ€§å¼±**ï¼šé€šä¿¡å—é™ã€åŠ¨æ€æ‹“æ‰‘å˜åŒ–æˆ–é˜Ÿå‹å¢å‡æ—¶è¡¨ç°è„†å¼±ï¼›
- **æ³›åŒ–èƒ½åŠ›ä¸è¶³**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨ç‰¹æƒä¿¡æ¯ï¼ˆprivileged informationï¼‰ï¼Œè€Œæ‰§è¡Œæ—¶ä¸å¯ç”¨ï¼Œå¯¼è‡´**è®­ç»ƒ-æµ‹è¯•ä¸åŒ¹é…ï¼ˆtrain-test mismatchï¼‰**ã€‚

æ­¤å¤–ï¼Œç°æœ‰åˆ†å¸ƒå¼æ–¹æ³•ä»éƒ¨åˆ†ä¾èµ–ä¸­å¤®ç»„ä»¶ï¼ˆå¦‚å…±äº«å‚æ•°ã€é›†ä¸­å¼ä»·å€¼å‡½æ•°åˆ†è§£ç­‰ï¼‰ï¼Œæœªèƒ½å®ç°å®Œå…¨å»ä¸­å¿ƒåŒ–çš„ç­–ç•¥ä¼˜åŒ–ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**å…¨åˆ†å¸ƒå¼ï¼ˆfully distributedï¼‰MARLæ¡†æ¶â€”â€”DG-MAPPO**ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š

#### ï¼ˆ1ï¼‰åˆ†å¸ƒå¼å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆD-GATï¼‰
- è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„**å¤šè·³é€šä¿¡æ¨¡å— D-GAT**ï¼Œä½¿æ¯ä¸ªæ™ºèƒ½ä½“é€šè¿‡æœ¬åœ°é‚»å±…é—´çš„**peer-to-peeræ¶ˆæ¯ä¼ é€’**æ¥æ¨æ–­å…¨å±€çŠ¶æ€ã€‚
- æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ç»´æŠ¤å¹¶æ›´æ–°è‡ªå·±çš„**å±€éƒ¨å›¾æ³¨æ„åŠ›å‚æ•°**ï¼Œç»“åˆ**è¾“å…¥ä¾èµ–çš„æ³¨æ„åŠ›æœºåˆ¶**ï¼ˆinspired by GATv2ï¼‰å®ç°åŠ¨æ€ã€è¡¨è¾¾èƒ½åŠ›å¼ºçš„ä¿¡æ¯èšåˆã€‚
- å¼•å…¥åŸºäº**å»ä¸­å¿ƒåŒ–éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆD-SGDï¼‰** çš„å‚æ•°å¹³å‡æœºåˆ¶ï¼Œåœ¨ä¿ç•™åˆ†å¸ƒå¼ç‰¹æ€§çš„åŒæ—¶ä¿ƒè¿›é‚»è¿‘æ™ºèƒ½ä½“è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚
- åŠ å…¥**å…±è¯†æ­£åˆ™åŒ–æŸå¤±ï¼ˆconsensus regularization lossï¼‰**ï¼Œæ˜¾å¼é¼“åŠ±ç›¸é‚»æ™ºèƒ½ä½“å¯¹é½å…¶å­¦ä¹ åˆ°çš„çŠ¶æ€è¡¨å¾ã€‚

#### ï¼ˆ2ï¼‰DG-MAPPO æ¡†æ¶
- æ„å»ºäº†ä¸€ä¸ªæ— éœ€ä»»ä½•é›†ä¸­å¼æ§åˆ¶å™¨æˆ–å…¨å±€å¯è§‚æµ‹æ€§çš„ MARL æ¡†æ¶ã€‚
- æ™ºèƒ½ä½“ä»…åŸºäºï¼š
  - å±€éƒ¨è§‚æµ‹ $ o^i $
  - D-GAT æ¨ç†å‡ºçš„å…¨å±€çŠ¶æ€è¿‘ä¼¼ $ \hat{o} $
  - å…±äº«/å¹³å‡å›¢é˜Ÿå¥–åŠ± $ R $
- è¿›è¡Œæœ¬åœ° Actor-Critic æ›´æ–°ï¼Œä½¿ç”¨ PPO èŒƒå¼è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚
- å®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„â€œ**è®­ç»ƒä¸æ‰§è¡Œå‡å»ä¸­å¿ƒåŒ–**â€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | CTDE æ–¹æ³•ï¼ˆå¦‚ MAPPO, HAPPOï¼‰ | DG-MAPPO |
|------|-------------------------------|---------|
| æ˜¯å¦éœ€è¦å…¨å±€çŠ¶æ€ | âœ… æ˜¯ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰ | âŒ å¦ |
| æ˜¯å¦éœ€è¦ä¸­å¤®åè°ƒå™¨ | âœ… æ˜¯ | âŒ å¦ |
| é€šä¿¡æ¨¡å¼ | å¤šå¯¹ä¸€ï¼ˆgather + broadcastï¼‰ | é‚»å±…é—´ mesh é€šä¿¡ |
| å¯æ‰©å±•æ€§ | å—é™äºä¸­å¤®èŠ‚ç‚¹å¸¦å®½ | çº¿æ€§å¯æ‰©å±• |
| å¯¹ç¨€ç–é€šä¿¡å®¹å¿åº¦ | å·®ï¼ˆéœ€é•¿è·ç¦»é€šä¿¡ï¼‰ | å¼ºï¼ˆä»…çŸ­ç¨‹é€šä¿¡ï¼‰ |
| æ³›åŒ–é²æ£’æ€§ | å­˜åœ¨ train-test mismatch | æ›´å¼ºï¼Œè®­ç»ƒå³æ¨¡æ‹Ÿéƒ¨ç½²ç¯å¢ƒ |

> âœ… **æ®ä½œè€…æ‰€çŸ¥ï¼ŒDG-MAPPO æ˜¯é¦–ä¸ªå®Œå…¨æ¶ˆé™¤å¯¹ç‰¹æƒé›†ä¸­ä¿¡æ¯ä¾èµ–çš„ MARL æ–¹æ³•**ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨ä¸‰ä¸ªä¸»æµå¤šæ™ºèƒ½ä½“åŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼š
1. **StarCraftII Multi-Agent Challenge (SMAC)**  
   - åœ°å›¾æ¶µç›–ä»ç®€å•åŒè´¨æˆ˜æ–—ï¼ˆå¦‚ `3m`ï¼‰åˆ°å¤æ‚å¼‚æ„å¯¹æŠ—ï¼ˆå¦‚ `6h vs 8z`, `MMM2`ï¼‰
   - è§‚æµ‹ä¸ºå±€éƒ¨è§†é‡ï¼ˆpartial observabilityï¼‰ï¼Œé€‚åˆæµ‹è¯•é€šä¿¡çº¦æŸä¸‹çš„åä½œèƒ½åŠ›

2. **Google Research Football (GFootball)**  
   - Academy 3 vs 1 with Keeper åœºæ™¯
   - æµ‹è¯•è¿ç»­åŠ¨ä½œç©ºé—´ä¸é«˜ç»´è§†è§‰è¾“å…¥ä¸‹çš„å›¢é˜Ÿé…åˆ

3. **Multi-Agent MuJoCo (MA-MuJoCo)**  
   - å¦‚ `Multi-HalfCheetah (6Ã—1)`
   - å›ºå®šç‰©ç†è¿æ¥æ‹“æ‰‘ï¼Œå¼ºè°ƒå±€éƒ¨äº¤äº’ä¸è¿ç»­æ§åˆ¶

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **é€šä¿¡æ¨¡å‹** | åŠ¨æ€å›¾ $ G=(N,E) $ï¼Œè¾¹ç”±è·ç¦»å†³å®šï¼›SMAC/GFootball ä¸­ä¸ºåŠ¨æ€å›¾ï¼ŒMuJoCo ä¸­ä¸ºå›ºå®šé‚»æ¥å›¾ |
| **è§‚æµ‹è¾“å…¥** | æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒå±€éƒ¨è§‚æµ‹ï¼›CTDE å¯è®¿é—®å…¨å±€çŠ¶æ€ï¼ŒDG-MAPPO é€šè¿‡ D-GAT æ¨ç†å…¨å±€çŠ¶æ€ |
| **å¥–åŠ±æœºåˆ¶** | æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«å¹³å‡å›¢é˜Ÿå¥–åŠ±ï¼ˆshared/averaged rewardï¼‰ |
| **è¯„ä¼°æŒ‡æ ‡** |  
- SMAC: **èƒœç‡ï¼ˆWin Rateï¼‰ Â± æ ‡å‡†å·®**  
- GFootball & MuJoCo: **å¹³å‡å›åˆå¾—åˆ†ï¼ˆAverage Episode Score/Rewardï¼‰**

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
ä¸å¤šä¸ªå¼º CTDE åŸºçº¿æ¯”è¾ƒï¼š
- **MAPPO**ï¼šæ ‡å‡†å¤šæ™ºèƒ½ä½“ PPOï¼Œä½¿ç”¨å…¨å±€çŠ¶æ€è®­ç»ƒ
- **HAPPO**ï¼šå¼‚æ„æ™ºèƒ½ä½“ç‰ˆæœ¬çš„ MAPPO
- **MAT-Dec**ï¼šåŸºäºåºåˆ—å»ºæ¨¡çš„å…ˆè¿› CTDE æ–¹æ³•

> æ‰€æœ‰åŸºçº¿ç»“æœæ¥è‡ªæ–‡çŒ®æŠ¥å‘Šçš„æœ€ä½³æ€§èƒ½ï¼Œæœªé‡æ–°è®­ç»ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆSMAC èƒœç‡ %ï¼‰

| Task | Difficulty | MAT-Dec | MAPPO | HAPPO | **DG-MAPPO** |
|------|------------|--------|--------|--------|----------------|
| 3m | Easy | 100.0 | 100.0 | 100.0 | **100.0** |
| 8m | Easy | 97.2 | 96.8 | 97.5 | **100.0** |
| MMM | Easy | 98.1 | 95.6 | 81.2 | **100.0** |
| 5m vs 6m | Hard | 83.1 | 88.2 | 77.5 | **88.7** |
| 10m vs 11m | Hard | 100.0 | 96.3 | 87.5 | **100.0** |
| 25m | Hard | 86.9 | 100.0 | 95.0 | **95.3** |
| MMM2 | Hard+ | 91.2 | 81.8 | 88.8 | **98.9** |
| 6h vs 8z | Hard+ | 93.8 | 88.4 | 76.2 | **95.0** |
| 3s5z vs 3s6z | Hard+ | 85.3 | 84.3 | 82.8 | **91.9** |

> âœ… **DG-MAPPO åœ¨ç»å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šæœ€å¼º CTDE æ–¹æ³•çš„è¡¨ç°**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨ **å°è§„æ¨¡åŒè´¨å›¢é˜Ÿ** ä¸Šï¼ŒDG-MAPPO è¡¨ç°æŒå¹³ç”šè‡³ç•¥ä¼˜ï¼›
- åœ¨ **å¤§è§„æ¨¡ï¼ˆå¦‚ 25mï¼‰å’Œå¼‚æ„åœºæ™¯ï¼ˆå¦‚ MMM2, 6h vs 8zï¼‰** ä¸­ï¼Œæ˜¾è‘—ä¼˜äºå¤šæ•° CTDE æ–¹æ³•ï¼›
- å³ä½¿åœ¨é€šä¿¡èŒƒå›´è¢«è£å‰ªè‡³ â€œ4â€ çš„æç«¯ç¨€ç–æ¡ä»¶ä¸‹ï¼ˆè§ Table 2ï¼‰ï¼ŒDG-MAPPO ä»ä¿æŒç«äº‰åŠ›ï¼š
  - `6h vs 8z` @1-hop: **77.08%**
  - `MMM2` @1-hop: **90.62%**
  - å¢åŠ  hop æ•°åè¿›ä¸€æ­¥æå‡è‡³æ¥è¿‘å®Œæ•´é€šä¿¡æ°´å¹³

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰æ¶ˆæ¯èšåˆæ–¹å¼å¯¹æ¯”ï¼ˆMean vs Attentionï¼‰
| åœºæ™¯ | ç»“è®º |
|------|------|
| `6h vs 8z`ï¼ˆé«˜éš¾åº¦ï¼‰ | Attention æ˜¾è‘—ä¼˜äº Meanï¼Œå°¤å…¶åœ¨ä½ hop ä¸‹ |
| `MMM2`, `10m vs 11m` | å½“ hop â‰¥ N/2 æ—¶ï¼ŒMean å¯è¾¾ç›¸è¿‘æ€§èƒ½ï¼Œè¯´æ˜**é«˜ hop å¯è¡¥å¿ç®€å•èšåˆ** |
| æ€»ä½“è¶‹åŠ¿ | **Attention åœ¨é€šä¿¡å—é™æ—¶æ›´å…·ä¼˜åŠ¿**ï¼Œæœ‰åŠ©äºé™ä½å¼€é”€ |

#### ï¼ˆ2ï¼‰é€šä¿¡è·³æ•°ï¼ˆHop Countï¼‰çš„å½±å“
- **1-hop å³å¯è·å¾—è‰¯å¥½æ€§èƒ½**ï¼Œè¡¨æ˜å°‘é‡é€šä¿¡å³å¯æ”¯æŒæœ‰æ•ˆåä½œï¼›
- ä» 1-hop åˆ° N-hop æå‡è¾¹é™…é€’å‡ï¼›
- å®è·µå»ºè®®ï¼šä»å° hop å¼€å§‹ï¼ŒæŒ‰éœ€å¢åŠ ä»¥å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ã€‚

#### ï¼ˆ3ï¼‰å…±è¯†æŸå¤±ï¼ˆConsensus Lossï¼‰çš„ä½œç”¨
| Hop æ•° | æ•ˆæœ |
|--------|------|
| 6-hop | å¼•å…¥ Consensus Loss æ˜æ˜¾åŠ å¿«æ”¶æ•›ã€æé«˜æœ€ç»ˆèƒœç‡ |
| 3-hop | æœ‰ä¸€å®šæå‡ |
| 1-hop | æ”¹å–„è¾ƒå°ä½†ä»ç¨³å®šå­˜åœ¨ |
> âœ… **Consensus Loss åœ¨é€šä¿¡å……åˆ†æ—¶æ•ˆæœæ›´æ˜¾è‘—ï¼Œä½†åœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡æ— è´Ÿé¢å½±å“ï¼Œé€‚åˆä½œä¸ºé€šç”¨ç¨³å®šæœºåˆ¶**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç»“æ„åŒ–å±€éƒ¨é€šä¿¡è¶³ä»¥æ”¯æ’‘é«˜è´¨é‡åä½œè¡Œä¸º**  
   å³ä½¿æ²¡æœ‰å…¨å±€çŠ¶æ€æˆ–ä¸­å¤®åè°ƒï¼Œä»…é  D-GAT çš„å¤šè·³æ³¨æ„åŠ›é€šä¿¡å³å¯å®ç°åª²ç¾ç”šè‡³è¶…è¶Š CTDE çš„æ€§èƒ½ã€‚

2. **DG-MAPPO å…·å¤‡å‡ºè‰²çš„é²æ£’æ€§ä¸å¯æ‰©å±•æ€§**  
   - åœ¨é«˜è¾¾ 25 ä¸ªæ™ºèƒ½ä½“çš„å›¢é˜Ÿä¸­ä¾ç„¶æœ‰æ•ˆï¼›
   - åœ¨ç¨€ç–é€šä¿¡ï¼ˆsight range=4ï¼‰ã€åŠ¨æ€æ‹“æ‰‘ä¸‹ä»èƒ½ç»´æŒé«˜æ€§èƒ½ï¼›
   - è®­ç»ƒä¸æ‰§è¡Œä¸€è‡´ï¼Œé¿å…äº† CTDE çš„ train-test mismatchã€‚

3. **è½»é‡çº§è®¾è®¡å¸¦æ¥å®ç”¨æ½œåŠ›**  
   - å°‘é‡ hopï¼ˆå¦‚ 1 æˆ– N/2ï¼‰å³å¯å–å¾—ä¼˜å¼‚æ€§èƒ½ï¼›
   - é€šä¿¡å¼€é”€å¯æ§ï¼Œé€‚ç”¨äºæ— çº¿ã€æœºå™¨äººç­‰èµ„æºå—é™åœºæ™¯ã€‚

4. **å…±è¯†æœºåˆ¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§**  
   Consensus Loss å’Œ D-SGD å‚æ•°å¹³å‡æœ‰æ•ˆç¼“è§£äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„éå¹³ç¨³æ€§é—®é¢˜ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å¯¹åˆå§‹è¿é€šæ€§æœ‰è¦æ±‚**ï¼šå‡è®¾é€šä¿¡å›¾å§‹ç»ˆè¿é€šï¼ˆAssumption 1ï¼‰ï¼Œè‹¥ç½‘ç»œåˆ†è£‚å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›
- **å»¶è¿Ÿæ•æ„Ÿåœºæ™¯å¯èƒ½å—å½±å“**ï¼šè™½ç„¶é€šä¿¡çŸ­è·ï¼Œä½†å¤š hop ä¼šå¼•å…¥ä¸€å®šå»¶è¿Ÿï¼›
- **è¶…å‚æ•°è°ƒä¼˜å¤æ‚åº¦ä¸Šå‡**ï¼šhop æ•°ã€consensus loss æƒé‡ç­‰æ–°å¢è‡ªç”±åº¦éœ€åˆç†é…ç½®ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‹“å±•è‡³ç«äº‰æ€§ä»»åŠ¡**ï¼šå½“å‰èšç„¦äºåˆä½œå‹ MARLï¼Œæœªæ¥å¯æ¢ç´¢æ··åˆç«äº‰-åˆä½œå…³ç³»ï¼›
2. **è‡ªé€‚åº” hop æ§åˆ¶**ï¼šè®©æ™ºèƒ½ä½“åŠ¨æ€é€‰æ‹© hop æ•°ä»¥èŠ‚çœèµ„æºï¼›
3. **ç»“åˆé€šä¿¡æ•ˆç‡å‹ç¼©æŠ€æœ¯**ï¼šå¦‚é‡åŒ–ã€ç¨€ç–åŒ–æ¶ˆæ¯ä¼ é€’ï¼›
4. **çœŸå®ä¸–ç•Œéƒ¨ç½²éªŒè¯**ï¼šåœ¨æ— äººæœºç¾¤ã€è‡ªåŠ¨é©¾é©¶è½¦é˜Ÿç­‰çœŸå®ç³»ç»Ÿä¸­æµ‹è¯•ï¼›
5. **ç†è®ºåˆ†æä¿è¯**ï¼šæä¾›å…³äº D-GAT æ”¶æ•›æ€§å’Œè¡¨ç¤ºä¸€è‡´æ€§æ›´å¼ºçš„ç†è®ºæ”¯æ’‘ã€‚

---

> ğŸ”š **æ€»ç»“**ï¼š  
> DG-MAPPO æˆåŠŸè¯æ˜äº†**æ— éœ€é›†ä¸­å¼è®­ç»ƒä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½å¤šæ™ºèƒ½ä½“åä½œ**ï¼Œä¸ºæ„å»º**å¯æ‰©å±•ã€é²æ£’ã€éƒ¨ç½²å°±ç»ªï¼ˆdeployment-readyï¼‰** çš„ MARL ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚å®ƒä¸ä»…åœ¨æ€§èƒ½ä¸Šåª²ç¾ SOTA CTDE æ–¹æ³•ï¼Œæ›´é‡è¦çš„æ˜¯æ‰“ç ´äº†å¯¹å…¨å±€ä¿¡æ¯çš„ä¾èµ–ï¼Œæ¨åŠ¨ MARL å‘æ›´è´´è¿‘ç°å®åº”ç”¨åœºæ™¯çš„æ–¹å‘å‘å±•ã€‚

</details>

---

### 6. [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)

**Authors**: Xuan-Phi Nguyen, Shrey Pandit, Austin Xu, Caiming Xiong, Shafiq Joty  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.17111v1  

#### Abstract
Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirabl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

- **MoEæ¨¡å‹åœ¨å®é™…è¿è¡Œä¸­å­˜åœ¨ä¸¥é‡çš„ä¸“å®¶è·¯ç”±ä¸å¹³è¡¡**ï¼ˆimbalanced routingï¼‰ï¼Œå³ä½¿ç»è¿‡é¢„è®­ç»ƒä¹Ÿéš¾ä»¥é¿å…ã€‚
- åœ¨ **Expert Parallelism (EP)** æ¶æ„ä¸‹ï¼Œè¿™ç§ä¸å¹³è¡¡ä¼šå¯¼è‡´æŸäº›GPUè®¾å¤‡è´Ÿè½½è¿‡é‡ï¼ˆoverloadedï¼‰ï¼Œè€Œå…¶ä»–è®¾å¤‡ç©ºé—²ï¼ˆunderutilizedï¼‰ï¼Œä»è€Œå¼•å‘ï¼š
  - æ˜¾å­˜æº¢å‡ºï¼ˆOut-of-Memory, OOMï¼‰
  - è®¡ç®—å»¶è¿Ÿå¢åŠ 
  - ååé‡ä¸‹é™
- å°¤å…¶åœ¨ **post-training å’Œ inference é˜¶æ®µ**ï¼Œæ— æ³•ä½¿ç”¨è¾…åŠ©æŸå¤±å‡½æ•°ç­‰å‚æ•°ä¿®æ”¹æ–¹å¼æ¥å¼ºåˆ¶å¹³è¡¡è·¯ç”±ï¼Œä½¿å¾—ä¼ ç»Ÿè´Ÿè½½å‡è¡¡æ‰‹æ®µå¤±æ•ˆã€‚

### ğŸš€ æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **åŠ¨æ€è´Ÿè½½å‡è¡¡ç®—æ³•** â€”â€” **Least-Loaded Expert Parallelism (LLEP)**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **åŠ¨æ€é‡è·¯ç”±æœºåˆ¶**ï¼šå½“æ£€æµ‹åˆ°æŸGPUä¸Šçš„ä¸“å®¶è´Ÿè½½è¶…è¿‡å®¹é‡é˜ˆå€¼æ—¶ï¼Œå°†è¶…å‡ºéƒ¨åˆ†çš„tokensåŠå…¶å¯¹åº”çš„expert weightsè½¬ç§»åˆ°å½“å‰æœ€è½»è½½çš„GPUä¸Šè¿›è¡Œè®¡ç®—ã€‚
- **åŒé‡æ–°åˆ†é…**ï¼šä¸ä»…è½¬ç§»tokensï¼Œè¿˜åŒæ­¥è½¬ç§»æ‰€éœ€çš„expert weightsï¼Œå®ç°â€œä»»åŠ¡å…±äº«â€ã€‚
- **ç²¾ç¡®è®¡ç®—ä¿éšœ**ï¼šLLEP æ˜¯ä¸€ä¸ª **exact MoE computation algorithm**ï¼Œä¸æ”¹å˜åŸå§‹MoEçš„æ•°å­¦è¡Œä¸ºï¼Œä¿è¯ç»“æœä¸€è‡´æ€§ã€‚
- æ”¯æŒ **å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­**ï¼Œå¯ç”¨äºè®­ç»ƒå’Œæ¨ç†ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | ç¼ºé™· | LLEPä¼˜åŠ¿ |
|------|------|----------|
| å‡å°batch size | é™ä½ååé‡ï¼Œæ•ˆç‡å·® | ä¸ç‰ºç‰²batch sizeï¼Œç»´æŒé«˜åå |
| å†—ä½™ä¸“å®¶å¤åˆ¶ï¼ˆå¦‚EPLBï¼‰ | å¢åŠ æ˜¾å­˜å¼€é”€ï¼Œä»…é€‚ç”¨äºinference | æ— éœ€å¤åˆ¶ï¼Œå†…å­˜æ›´ä¼˜ï¼Œæ”¯æŒfine-tuning |
| æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰ | æ•ˆç‡ä½ï¼Œä»å—é™äºæ˜¾å­˜ä¸Šé™ | æ›´é«˜æ•ˆï¼Œçªç ´ç“¶é¢ˆ |
| è¾…åŠ©loss / è·¯ç”±åç½® | æ”¹å˜æ¨¡å‹è¡Œä¸ºï¼Œç ´åä¸“å®¶ä¸“ä¸šåŒ– | å®Œå…¨ä¿ç•™åŸæ¨¡å‹è¡Œä¸º |

> âœ… **LLEPæ˜¯åœ¨ç³»ç»Ÿå±‚é¢è§£å†³MoEä¸å¹³è¡¡é—®é¢˜çš„æ–°èŒƒå¼ï¼Œå…¼é¡¾æ€§èƒ½ã€å†…å­˜ä¸æ­£ç¡®æ€§ã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **åˆæˆä¸å¹³è¡¡åœºæ™¯æ¨¡æ‹Ÿ**ï¼š
  - æ§åˆ¶ä¸åŒæ¯”ä¾‹çš„tokensé›†ä¸­åˆ°å°‘æ•°ä¸“å®¶ï¼ˆå¦‚30%, 50%, 80%, 95%é›†ä¸­åœ¨1/4/16ä¸ªä¸“å®¶ï¼‰
  - æµ‹è¯•æç«¯ä¸å¹³è¡¡ä¸‹çš„è¡¨ç°
- **çœŸå®ä»»åŠ¡æ•°æ®**ï¼š
  - `Megatron-Math` æ•°æ®é›†
  - å¯¹è¯æ•°æ®æ¥è‡ª `DeepScaleR`
  - å“åº”ç”± `gpt-oss-20b` è‡ªèº«ç”Ÿæˆï¼ˆchain-of-thoughtï¼‰
- æ¨¡å‹ï¼š`gpt-oss-20b`, `gpt-oss-120b`, `DeepSeek-V3`, `Kimi-K2`

### âš™ï¸ å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | æè¿° |
|--------|------|
| **ç¡¬ä»¶å¹³å°** | 8Ã— H200 GPUs |
| **EP world size (P)** | 8 |
| **Batch size per GPU** | 32Kï¼ˆgpt-ossç³»åˆ—ï¼‰ï¼Œ16Kï¼ˆDeepSeek/Kimiï¼‰ |
| **MoEé…ç½®** | å¤šç§Nï¼ˆä¸“å®¶æ•°ï¼‰ã€D/Hï¼ˆç»´åº¦ï¼‰ã€Kï¼ˆæ¿€æ´»ä¸“å®¶æ•°ï¼‰ç»„åˆ |
| **è¯„ä¼°æŒ‡æ ‡** | 
| - **Speedup**ï¼ˆåŠ é€Ÿæ¯”ï¼‰ | LLEP vs. æ ‡å‡†EPçš„å‰å‘è€—æ—¶æ¯” |
| - **Peak Memory Usage**ï¼ˆå³°å€¼æ˜¾å­˜ï¼‰ | æ¯GPUæœ€å¤§æ˜¾å­˜å ç”¨ |
| - **Full-model Throughput** | ç«¯åˆ°ç«¯ååé‡ï¼ˆtokens/secï¼‰ |
| - **End-to-end Training Time** | å®Œæ•´å¾®è°ƒä»»åŠ¡æ‰€éœ€æ—¶é—´ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **Standard EP**ï¼šæ ‡å‡†Expert Parallelismï¼Œä½œä¸ºä¸»è¦baseline
- **EPLB (Expert Parallelism Load Balancer)**ï¼šé€šè¿‡å¤åˆ¶çƒ­é—¨ä¸“å®¶ç¼“è§£è´Ÿè½½ï¼ˆLiu et al., 2024ï¼‰
- ï¼ˆæ³¨ï¼šæ–‡ä¸­æœªç›´æ¥æ¯”è¾ƒEPLBåœ¨æ‰€æœ‰åœºæ™¯çš„ç»“æœï¼Œä½†æŒ‡å‡ºå…¶æœ‰é¢å¤–å†…å­˜å¼€é”€ä¸”ä¸é€‚ç”¨äºfine-tuningï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰MoEå±‚çº§æ€§èƒ½æå‡ï¼ˆå›¾1ã€å›¾4ï¼‰

| åœºæ™¯ | æœ€å¤§åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰ | æ˜¾å­˜å‡å°‘ |
|------|------------------------|---------|
| æç«¯ä¸å¹³è¡¡ï¼ˆ95% tokens â†’ 1 expertï¼‰ | **é«˜è¾¾6.11Ã—**ï¼ˆgpt-oss-120bï¼‰ | **å³°å€¼æ˜¾å­˜é™ä½è‡³1/5** |
| å¹³è¡¡æƒ…å†µ | ~1Ã—ï¼ˆæ— æŸè€—ï¼‰ | æ˜¾å­˜ç¨³å®šï¼Œæ¥è¿‘EP |

> ğŸ’¡ LLEPåœ¨ä¸¥é‡ä¸å¹³è¡¡æ—¶ä¼˜åŠ¿æ˜¾è‘—ï¼Œåœ¨å¹³è¡¡æ—¶å‡ ä¹æ— é¢å¤–å¼€é”€ã€‚

#### ï¼ˆ2ï¼‰ç«¯åˆ°ç«¯å®Œæ•´æ¨¡å‹ååï¼ˆå›¾1cï¼‰

| æ¨¡å‹ | LLEPåŠ é€Ÿæ¯” |
|------|------------|
| `gpt-oss-20b` | **æœ€é«˜2.2Ã—** |
| `gpt-oss-120b` | **æœ€é«˜1.9Ã—** |

> å³ä½¿è€ƒè™‘attentionç­‰éMoEå±‚çš„å›ºå®šå¼€é”€ï¼ŒMoEå±‚ä¼˜åŒ–ä»å¸¦æ¥æ˜¾è‘—æ•´ä½“æé€Ÿã€‚

#### ï¼ˆ3ï¼‰çœŸå®è®­ç»ƒåœºæ™¯ï¼ˆå›¾5ï¼‰

- åœ¨ `gpt-oss-20b` ä¸Šè¿›è¡ŒSFTï¼ˆSupervised Fine-Tuningï¼‰ï¼Œä½¿ç”¨Zero-3 + CPU offloading
- **LLEPæ¯”æ ‡å‡†EPå¿«1.25Ã—**ï¼Œè¾¾åˆ°ç›¸åŒå‡†ç¡®ç‡ï¼ˆAIMEâ€™25ï¼‰æ‰€éœ€å¢™é’Ÿæ—¶é—´æ›´çŸ­

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| å› ç´  | å‘ç° |
|------|------|
| **Batch Size B â†‘** | æ‰¹æ¬¡è¶Šå¤§ï¼ŒLLEPåŠ é€Ÿè¶Šæ˜æ˜¾ â†’ æ›´å¥½åœ°æ‘Šé”€é€šä¿¡å¼€é”€ |
| **factor Î± â†“** | Î±è¶Šå°ï¼ˆå…è®¸æ¯GPUè´Ÿè½½è¶Šä½ï¼‰ï¼Œé€Ÿåº¦è¶Šå¿« â†’ æ›´ç§¯æçš„è´Ÿè½½å‡è¡¡æ›´ä¼˜ |
| **imbalance ratio threshold Î»** | å½“Î»å¤ªå°æ—¶ä¼šé¢‘ç¹è§¦å‘LLEPï¼›å¤ªå¤§åˆ™é”™è¿‡ä¼˜åŒ–æœºä¼šï¼›æ¨èè®¾ä¸º1.3å·¦å³ |
| **Hidden Size (D, H) â†‘** | æ¨¡å‹è¶Šå¤§ï¼ŒLLEPæ”¶ç›Šè¶Šé«˜ â†’ å¤§æ¨¡å‹æ›´å—ç›Š |
| **Number of Experts N â†‘** | ä¸“å®¶è¶Šå¤šï¼ŒLLEPç›¸å¯¹ä¼˜åŠ¿è¶Šå¼º |

> âœ… LLEPåœ¨å¤§è§„æ¨¡ã€å¤§æ‰¹é‡ã€é«˜åº¦ä¸å¹³è¡¡åœºæ™¯ä¸‹è¡¨ç°æœ€ä½³ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **MoEè·¯ç”±ä¸å¹³è¡¡æ˜¯è‡ªç„¶ä¸”åˆç†çš„ç°è±¡**ï¼š
   - ä¸“å®¶ä¸“ä¸šåŒ–ï¼ˆspecializationï¼‰å¯¼è‡´ç‰¹å®šä»»åŠ¡åªæ¿€æ´»ç›¸å…³ä¸“å®¶
   - å¼ºè¡Œå¹³è¡¡å¯èƒ½ç ´åå·²æœ‰çŸ¥è¯†ç»“æ„

2. **æ ‡å‡†EPå¯¹ä¸å¹³è¡¡æåº¦æ•æ„Ÿ**ï¼š
   - æç«¯æƒ…å†µä¸‹å¯æ…¢4.6Ã—ä»¥ä¸Šï¼Œæ˜¾å­˜å¢é•¿è¾¾4â€“10Ã—ï¼Œæ˜“OOM

3. **LLEPå®ç°äº†â€œè®¡ç®—-æ˜¾å­˜-é€šä¿¡â€çš„ååŒä¼˜åŒ–**ï¼š
   - åŠ¨æ€è°ƒåº¦ç¡®ä¿å„GPUè´Ÿè½½å‡è¡¡
   - æ˜¾å­˜ä½¿ç”¨ç¨³å®šå¯æ§
   - é€šä¿¡ä»£ä»·è¢«å¤§batchå’Œé«˜æ•ˆkernelæ©ç›–

4. **LLEPæ˜¯é€šç”¨ã€å¯æ’æ‹”çš„ç³»ç»Ÿçº§è§£å†³æ–¹æ¡ˆ**ï¼š
   - å¯é›†æˆè¿›ç°æœ‰MoEè®­ç»ƒ/æ¨ç†æ¡†æ¶
   - æ”¯æŒbackpropagationï¼Œé€‚ç”¨äºfull fine-tuning

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

- **å¼•å…¥é¢å¤–weight transferé€šä¿¡å¼€é”€**ï¼šè™½ç„¶æ€»ä½“å‡€æ”¶ç›Šä¸ºæ­£ï¼Œä½†åœ¨æå°batchæˆ–è½»å¾®ä¸å¹³è¡¡æ—¶å¯èƒ½å¾—ä¸å¿å¤±ï¼ˆå› æ­¤è®¾è®¡äº†è‡ªé€‚åº”å¼€å…³Î»ï¼‰
- **ä¾èµ–LLAç®—æ³•å†³ç­–å»¶è¿Ÿ**ï¼šå½“å‰Pythonå®ç°æœ‰ä¸€å®šè°ƒåº¦å¼€é”€ï¼Œæœªæ¥å¯é€šè¿‡C++/Tritonå†…æ ¸èåˆä¼˜åŒ–
- **å¤šèŠ‚ç‚¹ç¯å¢ƒä¸‹éœ€è¿›ä¸€æ­¥ä¼˜åŒ–**ï¼šè·¨èŠ‚ç‚¹ä¼ è¾“æˆæœ¬æ›´é«˜ï¼Œåº”ä¼˜å…ˆé€‰æ‹©åŒèŠ‚ç‚¹è®¾å¤‡è¿›è¡Œspill

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **ä½å»¶è¿Ÿè°ƒåº¦å™¨**ï¼šå°†LLAç®—æ³•ä¸‹æ²‰è‡³CUDA kernelæˆ–Tritonä¸­ï¼Œå®ç°zero-overheadåŠ¨æ€è°ƒåº¦
2. **é¢„æµ‹å¼è´Ÿè½½åˆ†é…**ï¼šåŸºäºå†å²è·¯ç”±ç»Ÿè®¡é¢„æµ‹æœªæ¥è´Ÿè½½ï¼Œæå‰åšplacementä¼˜åŒ–
3. **ä¸Tensor Parallelismç»“åˆ**ï¼šæ¢ç´¢Hybrid TP+LLEPæ¶æ„ä»¥æ”¯æŒè¶…å¤§è§„æ¨¡MoE
4. **å¼‚æ„ç¡¬ä»¶é€‚é…**ï¼šé’ˆå¯¹ä¸åŒGPUå‹å·ã€å¸¦å®½é…ç½®è‡ªåŠ¨è°ƒä¼˜Î±ã€mã€Î»ç­‰è¶…å‚
5. **æ”¯æŒåŠ¨æ€ä¸“å®¶å¢åˆ **ï¼šç»“åˆLLEPå®ç°runtimeä¸“å®¶æ‰©å±•æˆ–æ”¶ç¼©

---

## âœ… æ€»ç»“

> **LLEPæå‡ºäº†ä¸€ç§ä¼˜é›…è€Œå®ç”¨çš„ç³»ç»Ÿçº§æ–¹æ¡ˆï¼Œè§£å†³äº†MoEåœ¨ç°å®éƒ¨ç½²ä¸­é•¿æœŸå­˜åœ¨çš„â€œç†è®ºå¹³è¡¡ vs å®é™…å¤±è¡¡â€çŸ›ç›¾ã€‚å®ƒä¸æ˜¯å»â€œçº æ­£â€ä¸å¹³è¡¡ï¼Œè€Œæ˜¯â€œæ‹¥æŠ±â€ä¸å¹³è¡¡ï¼Œå¹¶é€šè¿‡åŠ¨æ€èµ„æºè°ƒåº¦å°†å…¶è½¬åŒ–ä¸ºé«˜æ•ˆæ‰§è¡Œçš„æœºä¼šã€‚**

- **åˆ›æ–°æ€§å¼º**ï¼šé¦–æ¬¡å®ç°â€œæŒ‰éœ€è¿ç§»weights + tokensâ€çš„åŠ¨æ€EPæœºåˆ¶
- **æ•ˆæœæ˜¾è‘—**ï¼šåœ¨çœŸå®æ¨¡å‹ä¸Šå®ç°è¿‘2Ã—ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œæ˜¾å­˜é™ä½è¾¾5å€
- **å·¥ç¨‹ä»·å€¼é«˜**ï¼šå·²å¼€æºä»£ç ï¼ˆGitHub: SalesforceAIResearch/LeastLoadedEPï¼‰ï¼Œæ˜“äºé›†æˆ

ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
**LLEPè®©MoEæ¨¡å‹åœ¨é¢å¯¹ä¸å¯é¿å…çš„è·¯ç”±ä¸å¹³è¡¡æ—¶ï¼Œä¾ç„¶èƒ½è·‘å¾—åˆå¿«åˆç¨³ã€‚**

</details>

---

### 7. [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)

**Authors**: Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.17625v1  

#### Abstract
Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šBrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation**

---

## **1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰åŸºäº Transformer çš„å¤§å‹ç¥ç»è§£ç å™¨ï¼ˆneural decodersï¼‰åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶åºå¤§çš„å‚æ•°é‡å’Œé«˜è®¡ç®—éœ€æ±‚ï¼Œéš¾ä»¥éƒ¨ç½²äºåŠŸè€—å—é™çš„**å¯æ¤å…¥ç³»ç»Ÿ**ï¼ˆimplantable systemsï¼‰ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰æ–¹æ³•è¯•å›¾å®Œæ•´ä¿ç•™æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºï¼Œä½†åœ¨å­¦ç”Ÿæ¨¡å‹å®¹é‡è¿œå°äºæ•™å¸ˆæ—¶æ•ˆæœä¸ä½³ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
æœ¬æ–‡æå‡ºäº† **BrainDistill**ï¼Œä¸€ä¸ªé¢å‘å¯æ¤å…¥ç³»ç»Ÿçš„æ–°å‹è¿åŠ¨æ„å›¾è§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ä»¥ä¸‹ä¸‰ä¸ªåˆ›æ–°ç»„ä»¶ï¼š

#### **(1) ä»»åŠ¡ç‰¹å®šçš„çŸ¥è¯†è’¸é¦ï¼ˆTask-Specific Knowledge Distillation, TSKDï¼‰**
- **æ ¸å¿ƒæ€æƒ³**ï¼šä¸åŒäºä¼ ç»Ÿ KD æ–¹æ³•è¿½æ±‚å¯¹æ•™å¸ˆåµŒå…¥ï¼ˆembeddingsï¼‰çš„å…¨é¢æ¨¡ä»¿ï¼ŒTSKD é€šè¿‡**æœ‰ç›‘ç£çš„æŠ•å½±å™¨**ï¼ˆsupervised projectorï¼‰æ˜¾å¼åœ°å‹ç¼©å¹¶ä¼˜å…ˆä¿ç•™ä¸ä»»åŠ¡æœ€ç›¸å…³çš„ç‰¹å¾ã€‚
- **å®ç°æ–¹å¼**ï¼š
  - ç¬¬ä¸€æ­¥ï¼šé€šè¿‡â€œè‡ªå‹ç¼©â€ï¼ˆself-compressionï¼‰ç›®æ ‡å­¦ä¹ ä¸€ä¸ªæœ€ä¼˜æŠ•å½±çŸ©é˜µ $P^*$ï¼Œè¯¥è¿‡ç¨‹èšç„¦äºä¿ç•™åˆ†ç±»å™¨æƒé‡ $W_T$ æ‰€ä¾èµ–çš„ä¿¡æ¯ã€‚
  - ç¬¬äºŒæ­¥ï¼šå›ºå®š $P^*$ï¼Œå¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè”åˆä¼˜åŒ–ï¼ˆlogit åŒ¹é… + æŠ•å½±åç‰¹å¾åŒ¹é…ï¼‰ï¼Œç¡®ä¿å­¦ç”Ÿå­¦åˆ°çš„æ˜¯**ä»»åŠ¡ç©ºé—´ä¸­çš„å…³é”®ä¿¡æ¯**ã€‚
- **ä¼˜åŠ¿**ï¼šæœ‰æ•ˆç¼“è§£äº†å¸ˆç”Ÿæ¨¡å‹é—´çš„å®¹é‡å·®è·ï¼Œåœ¨å°æ ·æœ¬æ ¡å‡†ï¼ˆfew-shot calibrationï¼‰åœºæ™¯ä¸‹è¡¨ç°æ›´ä¼˜ã€‚

#### **(2) ä»»åŠ¡ç‰¹å®šæ¯”ç‡ï¼ˆTask-Specific Ratio, TSRï¼‰**
- **å®šä¹‰**ï¼šä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æŠ•å½±åçš„æ•™å¸ˆç‰¹å¾ä¸­æœ‰å¤šå°‘æ¯”ä¾‹çš„ä»»åŠ¡ç›¸å…³ä¿¡æ¯è¢«ä¿ç•™ã€‚
- **å…¬å¼**ï¼š$\text{TSR} = \frac{\|\Pi_{\mathcal{U}} W_T\|^2}{\|W_T\|^2}$ï¼Œå…¶ä¸­ $\Pi_{\mathcal{U}}$ æ˜¯åœ¨å­¦ç”Ÿå­ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚
- **æ„ä¹‰**ï¼šä¸ºé€‰æ‹©æœ€ä½³æŠ•å½±æ–¹æ³•æä¾›äº†ç†è®ºä¾æ®ï¼Œä¸”å®éªŒè¯æ˜ TSR ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½é«˜åº¦ç›¸å…³ï¼ˆç›¸å…³æ€§ > 0.9ï¼‰ï¼Œè€Œä¼ ç»Ÿçš„äº’ä¿¡æ¯æˆ–é‡æ„è¯¯å·®åˆ™å‡ ä¹æ— ç›¸å…³æ€§ã€‚

#### **(3) å¯æ¤å…¥ç¥ç»è§£ç å™¨ï¼ˆImplantable Neural Decoder, INDï¼‰ä¸é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰**
- **IND æ¶æ„**ï¼š
  - ä½¿ç”¨ **Continuous Wavelet Transform (CWT)** è¿›è¡Œç‰¹å¾åˆ†è¯ï¼ˆtokenizationï¼‰ï¼Œç›´æ¥ç¼–ç æ˜ç¡®çš„æ—¶é¢‘ä¿¡æ¯ã€‚
  - é‡‡ç”¨è½»é‡çº§çš„ **linear attention** ç»“æ„ï¼Œé€‚åˆä½å»¶è¿Ÿæ¨ç†ã€‚
  - æ•´ä¸ªç½‘ç»œé™¤æœ€åä¸€å±‚å¤–å‡æ— åç½®é¡¹ï¼Œä¾¿äºæ•´æ•°é‡åŒ–ã€‚
- **é‡åŒ–æ–¹æ¡ˆ**ï¼š
  - æå‡ºäº†ä¸€ç§**æ•´æ•°ä»…æ¨ç†**ï¼ˆinteger-only inferenceï¼‰çš„ QAT æ–¹æ¡ˆã€‚
  - å¼•å…¥**å¯å­¦ä¹ çš„æ¿€æ´»è£å‰ªèŒƒå›´**ï¼ˆlearnable clipping rangesï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ç½‘ç»œæƒé‡ä¸€åŒä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†é‡åŒ–å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | BrainDistill (TSKD + IND) | ä¼ ç»Ÿæ–¹æ³• |
| :--- | :--- | :--- |
| **æ•ˆç‡** | å­¦ç”Ÿæ¨¡å‹æå°ï¼ˆå¦‚ IND ä»… 30K å‚æ•°ï¼‰ï¼Œé€‚åˆæ¤å…¥ | æ•™å¸ˆæ¨¡å‹å¤§ï¼ˆç™¾ä¸‡çº§å‚æ•°ï¼‰ï¼Œæ— æ³•éƒ¨ç½² |
| **è’¸é¦æœ‰æ•ˆæ€§** | æ˜¾è‘—ä¼˜äº KD, SimKD, VkD, TOFD, TED ç­‰ | ä¼ ç»Ÿ KD åœ¨å¸ˆç”Ÿå·®è·å¤§æ—¶æå‡æœ‰é™ç”šè‡³ä¸‹é™ |
| **ç¡¬ä»¶å‹å¥½æ€§** | æ”¯æŒ W8A8 æ•´æ•°æ¨ç†ï¼ŒåŠŸè€—æä½ | å¤šæ•°æ–¹æ³•ä¾èµ–æµ®ç‚¹è¿ç®— |
| **å°æ ·æœ¬é€‚åº”** | åœ¨æå°‘æ ¡å‡†æ•°æ®ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ | å¯¹æ ¡å‡†æ•°æ®é‡è¦æ±‚é«˜ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å®éªŒè¦†ç›–å¤šç§æ¨¡æ€å’Œç‰©ç§ï¼ŒéªŒè¯æ³›åŒ–èƒ½åŠ›ï¼š

| æ•°æ®é›† | ç±»å‹ | ä»»åŠ¡ | æ¥æº | #Subjects | #Sessions |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Human-C** | ECoG | å¤šç±»è¿åŠ¨åˆ†ç±» | ç§æœ‰ | 1 | 6 |
| **Monkey-R** | ECoG | è…•éƒ¨è½¨è¿¹å›å½’ | ç§æœ‰ | 1 | 12 |
| **Human-D** | ECoG | äºŒå…ƒçŠ¶æ€æ£€æµ‹ï¼ˆåŠ¨/é™ï¼‰ | (Peterson et al., 2021) | 12 | 7 |
| **BCIC-2A** | EEG | å››ç±»è¿åŠ¨æƒ³è±¡åˆ†ç±» | å…¬å¼€ | 9 | 1 |
| **BCIC-2B** | EEG | äºŒç±»è¿åŠ¨æƒ³è±¡åˆ†ç±» | å…¬å¼€ | 9 | 1 |
| **FALCON-M1** | Spikes | EMG ä¿¡å·å›å½’ | å…¬å¼€ | 1 | 5 |

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**
- **è®­ç»ƒèŒƒå¼**ï¼š
  - **Scratch**ï¼šç›´æ¥åœ¨ `X_recalib` ä¸Šä»å¤´è®­ç»ƒã€‚
  - **Distillation**ï¼šä»é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚ EEGPT, NDT2, å¤§å‹ Transformerï¼‰è’¸é¦çŸ¥è¯†ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **åˆ†ç±»ä»»åŠ¡**ï¼šF1 Score, Average Recall, Accuracy, AUROCã€‚
  - **å›å½’ä»»åŠ¡**ï¼šRÂ² Scoreã€‚
  - **ç¡¬ä»¶æŒ‡æ ‡**ï¼šåŠŸè€—ï¼ˆmWï¼‰ã€æ•´æ•°æ¨ç†æ€§èƒ½æŸå¤±ã€‚
- **æ ¡å‡†æ•°æ®**ï¼š`X_recalib` è§„æ¨¡è¿œå°äºç¦»çº¿è®­ç»ƒé›†ï¼Œæ¨¡æ‹ŸçœŸå®ä¸´åºŠä¸­å¿«é€Ÿæ ¡å‡†çš„åœºæ™¯ã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **è§£ç å™¨æ¶æ„å¯¹æ¯”**ï¼š
  - EEGNet, EEGConformer, ATCNet, CTNet, LaBraM (5.8M), IND (30K)ã€‚
- **çŸ¥è¯†è’¸é¦æ–¹æ³•å¯¹æ¯”**ï¼š
  - KD (Hinton et al.), SimKD, VkD, RdimKD, TOFD, TED, **TSKD**, **TSKD(CE)**ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ**

#### **(1) å¤šç±»è¿åŠ¨è§£ç  (Human-C)**
- **IND æœ¬èº«æ€§èƒ½**ï¼šå³ä½¿ä»å¤´è®­ç»ƒï¼ŒIND (30K) æ€§èƒ½ä¹Ÿè¿œè¶…å…¶ä»–è½»é‡æ¨¡å‹ï¼ˆå¦‚ EEGNet, ATCNetï¼‰ï¼Œæ¥è¿‘éƒ¨åˆ†å¤§æ¨¡å‹ã€‚
- **è’¸é¦åæ€§èƒ½**ï¼š
  - åœ¨ `4-6` åˆ†å‰²ä¸Šï¼Œ**IND+TSKD** è¾¾åˆ° **77.3 F1**ï¼Œä¸ä»…è¶…è¿‡äº†æ‰€æœ‰åŸºçº¿è’¸é¦æ–¹æ³•ï¼Œè¿˜é€¼è¿‘äº†æ•™å¸ˆæ¨¡å‹ï¼ˆ77.3 F1ï¼‰ã€‚
  - åœ¨ `1-1` å’Œ `16-17` åˆ†å‰²ä¸Šï¼Œ**IND+TSKD(CE)** è¡¨ç°æœ€ä½³ï¼Œè¯´æ˜ç»“åˆä»»åŠ¡æŸå¤±ï¼ˆcross-entropyï¼‰èƒ½åº”å¯¹æ•™å¸ˆé¢„æµ‹è´¨é‡è¾ƒå·®çš„æƒ…å†µã€‚

#### **(2) ä¸åŒæŠ•å½±æ–¹æ³•æ¯”è¾ƒ (Table 3)**
| æ–¹æ³• | Session 4-4 (F1) | Session 4-5 (F1) | Session 4-6 (F1) |
| :--- | :--- | :--- | :--- |
| **Teacher** | 73.3 | 74.5 | 77.3 |
| **Inverse Projection** | 73.1 | 71.7 | 75.5 |
| **TSKD (Ours)** | **73.3** | **75.0** | **77.3** |
| **TSKD (PCA)** | 71.8 | 72.5 | 77.3 |
| **TSKD (Random)** | 71.7 | 73.8 | 76.9 |

> âœ… **ç»“è®º**ï¼šTSKD çš„ç›‘ç£æŠ•å½±æ˜¾è‘—ä¼˜äºé€†æŠ•å½±ã€PCA å’ŒéšæœºæŠ•å½±ï¼Œè¯æ˜äº†å…¶åœ¨ä¿ç•™ä»»åŠ¡ä¿¡æ¯ä¸Šçš„ä¼˜è¶Šæ€§ã€‚

#### **(3) è·¨æ¨¡æ€æ³›åŒ– (Table 4)**
- åœ¨ **BCIC-2A (EEG)** å’Œ **FALCON-M1 (Spikes)** ä¸Šï¼Œåªæœ‰ **TSKD** å’Œ **TSKD(CE)** èƒ½ç¨³å®šæå‡æ€§èƒ½ï¼Œå…¶ä»–è’¸é¦æ–¹æ³•æœ‰æ—¶åè€Œé™ä½æ€§èƒ½ã€‚
- è¯æ˜ TSKD å…·æœ‰è‰¯å¥½çš„è·¨æ¨¡æ€ï¼ˆEEG, ECoG, Spikesï¼‰å’Œè·¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

#### **(4) ç¡¬ä»¶éƒ¨ç½²å¯è¡Œæ€§ (Table 6)**
| æ¨¡å‹ | Session 4-4 (F1) | Session 4-5 (F1) | åŠŸè€— (mW) |
| :--- | :--- | :--- | :--- |
| **FP32 (Full Precision)** | 73.3 | 75.0 | - |
| **I-ViT (W8A8)** | 73.4 | 71.0 | 22.84 |
| **Ours (W8A8)** | **73.5** | **73.2** | **5.66** |

> âœ… **ç»“è®º**ï¼šæå‡ºçš„ QAT æ–¹æ¡ˆåœ¨ **W8A8** æ•´æ•°ç²¾åº¦ä¸‹ï¼Œæ€§èƒ½æŸå¤± < 3%ï¼ŒåŠŸè€—ä»…ä¸º I-ViT çš„ **1/4**ï¼Œå®Œå…¨æ»¡è¶³æ¤å…¥å¼è®¾å¤‡ < 40mW çš„å®‰å…¨è¦æ±‚ã€‚

#### **(5) æ¶ˆèå®éªŒ**
- **IND æ¶æ„æ¶ˆè**ï¼ˆMonkey-Rï¼‰ï¼š
  - IND > CWT+RNN > CWT+CNNï¼Œè¯æ˜äº† CWT åˆ†è¯ + Linear Attention çš„æœ‰æ•ˆæ€§ã€‚
- **ç‰¹å¾æ¶ˆè**ï¼ˆHuman-Dï¼‰ï¼š
  - ç§»é™¤ 30Hz æˆåˆ†å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¯æ˜ beta band (13-30Hz) å¯¹è¿åŠ¨èµ·æ­¢æ£€æµ‹è‡³å…³é‡è¦ã€‚
- **TSR éªŒè¯**ï¼ˆAppendix A.4ï¼‰ï¼š
  - TSR ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„ç›¸å…³æ€§é«˜è¾¾ **0.91â€“0.99**ï¼Œè€Œäº’ä¿¡æ¯ç­‰ä¼ ç»ŸæŒ‡æ ‡å‡ ä¹æ— ç›¸å…³æ€§ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **ä»»åŠ¡ä¿¡æ¯ä¼˜å…ˆäºç‰¹å¾ä¿çœŸ**ï¼šåœ¨å¸ˆç”Ÿæ¨¡å‹å®¹é‡å·®è·å¤§çš„æƒ…å†µä¸‹ï¼Œç›²ç›®æ¨¡ä»¿æ•™å¸ˆç‰¹å¾ä¸å¦‚**æœ‰é€‰æ‹©åœ°ä¼ é€’ä»»åŠ¡å…³é”®ä¿¡æ¯**ï¼Œè¿™æ˜¯ TSKD æˆåŠŸçš„æ ¸å¿ƒã€‚
2. **TSR æ˜¯æœ‰æ•ˆçš„è’¸é¦è¯Šæ–­å·¥å…·**ï¼šé¦–æ¬¡æå‡ºä¸€ä¸ªå¯é‡åŒ–çš„æŒ‡æ ‡æ¥æŒ‡å¯¼æŠ•å½±å™¨çš„é€‰æ‹©ï¼Œä¸ºçŸ¥è¯†è’¸é¦æä¾›äº†æ–°çš„åˆ†æè§†è§’ã€‚
3. **è½»é‡+é‡åŒ–æ˜¯æ¤å…¥å¼ BCI çš„å…³é”®**ï¼šIND + QAT çš„ç»„åˆå®ç°äº†é«˜æ€§èƒ½ä¸ä½åŠŸè€—çš„å¹³è¡¡ï¼Œä¸ºä¸´åºŠè½¬åŒ–é“ºå¹³äº†é“è·¯ã€‚
4. **æ³›åŒ–èƒ½åŠ›å¼º**ï¼šBrainDistill åœ¨ ECoG, EEG, Spikes å¤šç§æ•°æ®æ¨¡æ€ä¸Šå‡æœ‰æ•ˆï¼Œæ”¯æŒè·¨æ¨¡æ€è¿ç§»ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–é«˜è´¨é‡æ•™å¸ˆæ¨¡å‹**ï¼šTSKD çš„æ€§èƒ½ä¸Šé™å—æ•™å¸ˆæ¨¡å‹é™åˆ¶ï¼Œè‹¥æ•™å¸ˆæœ¬èº«æ€§èƒ½å·®ï¼Œåˆ™è’¸é¦æ•ˆæœæœ‰é™ã€‚
- **æŠ•å½±å™¨éœ€é¢å¤–è®­ç»ƒ**ï¼šTSKD çš„ä¸¤é˜¶æ®µæµç¨‹ï¼ˆå…ˆå­¦æŠ•å½±å™¨ï¼Œå†è’¸é¦ï¼‰å¢åŠ äº†è®­ç»ƒå¤æ‚åº¦ã€‚
- **ç›®å‰éªŒè¯é›†ä¸­åœ¨è¿åŠ¨è§£ç **ï¼šæ˜¯å¦é€‚ç”¨äºè¯­è¨€ã€è§†è§‰ç­‰å…¶ä»– BCI ä»»åŠ¡æœ‰å¾…éªŒè¯ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- å°† TSKD åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„é€šç”¨è„‘ä¿¡å·åŸºç¡€æ¨¡å‹ï¼ˆbrain foundation modelsï¼‰ã€‚
- æ¢ç´¢åœ¨çº¿è‡ªé€‚åº”çš„ TSKDï¼Œä»¥åº”å¯¹é•¿æœŸæ¤å…¥ä¸­çš„ç¥ç»ä¿¡å·æ¼‚ç§»ã€‚
- å°† IND æ¶æ„æ‰©å±•è‡³å¤šä»»åŠ¡è”åˆè§£ç ï¼ˆå¦‚è¿åŠ¨+æ„Ÿè§‰åé¦ˆï¼‰ã€‚
- å¼€å‘ä¸“ç”¨ ASIC èŠ¯ç‰‡ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ– IND çš„èƒ½æ•ˆæ¯”ã€‚

---

> **æ€»ç»“**ï¼šBrainDistill é€šè¿‡ **TSKD** å’Œ **IND+QAT** çš„ååŒè®¾è®¡ï¼ŒæˆåŠŸè§£å†³äº†å¤§å‹ç¥ç»è§£ç å™¨åœ¨**å¯æ¤å…¥ BCI** ä¸­çš„éƒ¨ç½²éš¾é¢˜ï¼Œå®ç°äº†**é«˜æ€§èƒ½ã€ä½åŠŸè€—ã€å¼ºæ³›åŒ–**çš„ç»Ÿä¸€ï¼Œæ˜¯è¿ˆå‘ä¸´åºŠå®ç”¨åŒ– BCI ç³»ç»Ÿçš„é‡è¦ä¸€æ­¥ã€‚

</details>

---

### 8. [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)

**Authors**: Zixi Chen, Tianci Bu, Chendong Song, Xin Lu, Yinyu Ye, Zijie Zhou  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.17855v1  

#### Abstract
Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šA Universal Load Balancing Principle and Its Application to Large Language Model Serving

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Large Language Model (LLM) serving** ä¸­ä¸€ä¸ªå…³é”®ç“¶é¢ˆâ€”â€”**è§£ç é˜¶æ®µï¼ˆdecode stageï¼‰çš„è´Ÿè½½ä¸å¹³è¡¡é—®é¢˜**å±•å¼€ç ”ç©¶ã€‚åœ¨ç°ä»£ LLM æ¨ç†ç³»ç»Ÿä¸­ï¼Œé‡‡ç”¨ **Data Parallelism (DP)** å’Œ **Model Parallelism (å¦‚ EP/TP)** æ¶æ„æ—¶ï¼Œæ‰€æœ‰ worker å¿…é¡»é€šè¿‡ **synchronization barrier** åŒæ­¥ï¼Œå¯¼è‡´æ•´ä¸ªæ­¥éª¤çš„å®Œæˆæ—¶é—´ç”±æœ€æ…¢çš„ worker å†³å®šã€‚

ç”±äºä»¥ä¸‹ä¸‰ä¸ªæŒ‘æˆ˜ï¼Œè¯¥é—®é¢˜å°¤ä¸ºä¸¥å³»ï¼š
- **æœªçŸ¥ä¸”åŠ¨æ€å˜åŒ–çš„å·¥ä½œé‡**ï¼šè¯·æ±‚çš„è¾“å‡ºé•¿åº¦ï¼ˆdecode lengthï¼‰åœ¨åˆ†é…æ—¶æœªçŸ¥ï¼Œä¸”éšç”Ÿæˆè¿‡ç¨‹å¢é•¿ï¼ˆKV cache çº¿æ€§å¢åŠ ï¼‰ã€‚
- **åœ¨çº¿éå¹³ç¨³åˆ°è¾¾**ï¼šè¯·æ±‚æŒ‰ prefill å®Œæˆé¡ºåºä¾æ¬¡åˆ°è¾¾ï¼Œæ— æ³•é¢„çŸ¥æœªæ¥çš„è¯·æ±‚æ··åˆã€‚
- **Sticky assignments**ï¼šä¸€æ—¦è¯·æ±‚è¢«åˆ†é…åˆ°æŸä¸ª decode workerï¼Œå…¶å®Œæ•´çš„ KV cache æ— æ³•è¿ç§»ï¼Œå› æ­¤åˆå§‹åˆ†é…é”™è¯¯ä¼šæŒç»­å¹¶ç´¯ç§¯ã€‚

è¿™äº›å› ç´ å¯¼è‡´ç³»ç»Ÿä¸­å‡ºç° **persistent stragglers**ï¼Œé€ æˆé«˜è¾¾ **40% çš„ GPU ç­‰å¾…æ—¶é—´**ï¼Œä¸¥é‡æµªè´¹è®¡ç®—èµ„æºå’Œèƒ½æºã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§ **é€šç”¨çš„è´Ÿè½½å‡è¡¡åŸåˆ™**ï¼š**Balance Future with Integer Optimization (BF-IO)**ã€‚

#### æ ¸å¿ƒæ€æƒ³
- **ä¸ä¾èµ–å¯¹å®Œæ•´ä»»åŠ¡æ‰§è¡Œæ—¶é—´çš„é¢„æµ‹**ï¼ˆè¿™åœ¨ LLM åœºæ™¯ä¸‹æä¸å¯é ï¼‰ï¼Œè€Œæ˜¯åˆ©ç”¨ **çŸ­æœŸå‰ç»ä¿¡æ¯ï¼ˆshort lookaheadï¼‰** æ¥é¢„æµ‹æ´»è·ƒè¯·æ±‚åœ¨æœªæ¥å‡ æ­¥å†…çš„å®Œæˆæƒ…å†µã€‚
- åœ¨æ¯ä¸€æ­¥åˆ†é…å†³ç­–æ—¶ï¼ŒBF-IO é€šè¿‡æ±‚è§£ä¸€ä¸ª **æœ‰é™æ—¶åŸŸæ•´æ•°ä¼˜åŒ–é—®é¢˜ï¼ˆfinite-horizon integer optimizationï¼‰**ï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å°åŒ–æœªæ¥å‡ æ­¥å†… **é¢„æœŸè´Ÿè½½ä¸å¹³è¡¡ï¼ˆpredicted imbalanceï¼‰** çš„åˆ†é…æ–¹æ¡ˆã€‚

#### æ•°å­¦å½¢å¼
åœ¨æ¯ä¸ªæ—¶é—´æ­¥ $k$ï¼ŒBF-IO æœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼š
$$
J(S(k)) = \sum_{h=0}^{H} \text{Imbalance}(k+h)
$$
å…¶ä¸­ $H$ æ˜¯å‰ç»çª—å£é•¿åº¦ï¼Œ$\text{Imbalance}(k+h)$ æ˜¯åŸºäºå½“å‰ä¿¡æ¯å¯¹æœªæ¥ç¬¬ $k+h$ æ­¥è´Ÿè½½ä¸å¹³è¡¡çš„é¢„æµ‹ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç†è®ºä¿éšœ**ï¼šåœ¨å¯¹æŠ—æ€§åˆ°è¾¾ï¼ˆadversarial arrivalsï¼‰å’Œæœ€åæƒ…å†µä¸‹ï¼ŒBF-IO èƒ½å¤Ÿå°†é•¿æœŸä¸å¹³è¡¡é™ä½ $\Omega(\sqrt{B \log G})$ å€ï¼Œå…¶ä¸­ $B$ æ˜¯æ‰¹å¤§å°ï¼Œ$G$ æ˜¯ worker æ•°é‡ã€‚è¿™æ„å‘³ç€ **ç³»ç»Ÿè§„æ¨¡è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šæ˜¾è‘—**ã€‚
- **æ™®é€‚æ€§ï¼ˆUniversalityï¼‰**ï¼šè¯¥åŸåˆ™ä¸ä»…é€‚ç”¨äº LLM servingï¼Œè¿˜å¯æ¨å¹¿åˆ°ä¸€ç±»æ›´å¹¿æ³›çš„ **éé€’å‡è´Ÿè½½æ¼‚ç§»ï¼ˆnon-decreasing workload driftï¼‰** é—®é¢˜ã€‚
- **æ— éœ€å…¨å±€æœ€ä¼˜**ï¼šBF-IO æ˜¯ä¸€ç§ **myopic but foresighted** çš„ç­–ç•¥ï¼Œè™½éå…¨å±€æœ€ä¼˜ï¼Œä½†å…¶å±€éƒ¨å†³ç­–å·²èƒ½å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **å…¬å¼€æ•°æ®é›†**ï¼š`BurstGPT` æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ç”Ÿäº§ç¯å¢ƒä¸‹çš„ LLM è¯·æ±‚æ¨¡å¼ï¼Œå…·æœ‰å…¸å‹çš„é‡å°¾ï¼ˆheavy-tailedï¼‰è¾“å‡ºé•¿åº¦åˆ†å¸ƒã€‚
- **ç§æœ‰å·¥ä¸šå·¥ä½œè´Ÿè½½**ï¼šç”¨äºéªŒè¯åœ¨å®é™…éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡æ‹Ÿå™¨**ï¼šæ„å»ºäº†ä¸€ä¸ª GPU-based è¯·æ±‚å¤„ç†çš„æ•°å€¼æ¨¡æ‹Ÿæ¡†æ¶ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼šé»˜è®¤è®¾ç½®ä¸º $G=32$ ä¸ª GPUï¼Œæ¯ä¸ª GPU æ‰¹å¤§å° $B=72$ã€‚
- **å‰ç»çª—å£ $H$**ï¼šæµ‹è¯•äº† $H=0$ï¼ˆä»…è€ƒè™‘å½“å‰æ­¥ï¼‰å’Œ $H=20$ï¼ˆè€ƒè™‘æœªæ¥ 20 æ­¥ï¼‰ä¸¤ç§æƒ…å†µã€‚
- **æ—¶é—´æ¨¡å‹**ï¼šæ¯ä¸ªæ­¥éª¤çš„å¢™é’Ÿæ—¶é—´ç”±æœ€æ…¢ worker çš„æœ¬åœ°è®¡ç®—æ—¶é—´å’ŒåŒæ­¥é€šä¿¡æ—¶é—´å†³å®šã€‚

### è¯„ä¼°æŒ‡æ ‡
1. **Average Imbalance**ï¼šå¹³å‡è´Ÿè½½ä¸å¹³è¡¡ï¼Œæ˜¯ç†è®ºåˆ†æçš„æ ¸å¿ƒæŒ‡æ ‡ã€‚
2. **Throughput**ï¼šæ¯ç§’å¤„ç†çš„ token æ•°é‡ï¼ˆtokens/sï¼‰ã€‚
3. **Time Per Output Token (TPOT)**ï¼šæ¯ä¸ªè¾“å‡º token çš„å»¶è¿Ÿï¼Œåæ˜ ç”¨æˆ·ä½“éªŒã€‚
4. **Energy Consumption**ï¼šæ€»èƒ½è€—ï¼ˆJoulesï¼‰ï¼Œé€šè¿‡ç¬æ—¶åŠŸç‡å¯¹æ—¶é—´ç§¯åˆ†å¾—åˆ°ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **FCFS (First-Come-First-Serve)**ï¼šæŒ‰åˆ°è¾¾é¡ºåºåˆ†é…ï¼Œå¿½ç•¥è´Ÿè½½å·®å¼‚ã€‚
- **JSQ (Join-Shortest-Queue)**ï¼šå°†è¯·æ±‚åˆ†é…ç»™é˜Ÿåˆ—é•¿åº¦æœ€çŸ­çš„ workerã€‚
- **BF-IO ($H=0$)**ï¼šæ— å‰ç»çš„ BF-IO ç‰ˆæœ¬ã€‚
- **BF-IO ($H=20$)**ï¼šå¸¦ 20 æ­¥å‰ç»çš„ BF-IO ç‰ˆæœ¬ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰
| Strategy | Avg Imbalance | Throughput (10â´ tok/s) | TPOT (10â»Â² s/tok) â†“ | Energy â†“ (kJ) |
| :--- | :--- | :--- | :--- | :--- |
| **FCFS** | 27.9 | 8.00 | 1.42 | 396 |
| **JSQ** | 28.2 | 7.99 | 1.42 | 396 |
| **BF-IO (H=0)** | 2.92 | 9.03 | 1.26 | 386 |
| **BF-IO (H=20)** | **1.65** | **9.13** | **1.25** | **383** |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ç›¸æ¯” FCFS**ï¼š
  - **å¹³å‡ä¸å¹³è¡¡é™ä½ 17 å€**ï¼ˆä» 27.9 åˆ° 1.65ï¼‰ã€‚
  - **ååé‡æå‡ 14%**ï¼ˆä» 8.00 åˆ° 9.13 Ã—10â´ tok/sï¼‰ã€‚
  - **å»¶è¿Ÿé™ä½ 13%**ï¼ˆTPOT ä» 1.42 é™è‡³ 1.25 Ã—10â»Â² s/tokï¼‰ã€‚
  - **èƒ½è€—é™ä½ 3.3%**ï¼ˆä» 396 kJ é™è‡³ 383 kJï¼‰ã€‚
- **JSQ è¡¨ç°ä¸ä½³**ï¼šå…¶æ€§èƒ½ä¸ FCFS å‡ ä¹ç›¸åŒï¼Œç”šè‡³ç•¥å·®ã€‚è¿™æ˜¯å› ä¸º JSQ ä½¿ç”¨â€œé˜Ÿåˆ—é•¿åº¦â€ä½œä¸ºä»£ç†ï¼Œè€Œå®é™…è´Ÿè½½ï¼ˆtoken æ•°é‡ï¼‰ä¸é˜Ÿåˆ—é•¿åº¦ä¸æˆæ­£æ¯”ï¼Œå°¤å…¶åœ¨è¯·æ±‚å¤§å°å·®å¼‚å¤§æ—¶ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **$H=0$ vs $H=20$**ï¼š
  - å³ä½¿æ²¡æœ‰å‰ç»ï¼ˆ$H=0$ï¼‰ï¼ŒBF-IO å·²èƒ½å®ç°å·¨å¤§æ”¹è¿›ï¼ˆä¸å¹³è¡¡ä» 27.9 é™è‡³ 2.92ï¼‰ã€‚
  - å¼•å…¥ $H=20$ çš„å‰ç»åï¼Œä¸å¹³è¡¡è¿›ä¸€æ­¥ **é™ä½ 44%**ï¼ˆä» 2.92 åˆ° 1.65ï¼‰ï¼Œå¹¶åœ¨ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¸Šå¸¦æ¥é¢å¤–å¢ç›Šã€‚
- **å›¾ 4 æ˜¾ç¤º**ï¼šéšç€ $H$ å¢åŠ ï¼Œæ€§èƒ½å…ˆå‡åé™ï¼Œè¡¨æ˜å­˜åœ¨ä¸€ä¸ª **æœ€ä½³å‰ç»çª—å£**ï¼Œè¿‡å¤§çš„ $H$ ä¼šå¯¼è‡´é¢„æµ‹è¿‡äºæŠ•æœºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è´Ÿè½½ä¸å¹³è¡¡æ˜¯ LLM serving çš„ä¸»è¦æ•ˆç‡ç“¶é¢ˆ**ï¼šåœ¨çœŸå®ç”Ÿäº§ trace ä¸­ï¼Œ**barrier-induced idle time è¶…è¿‡ 40%**ï¼Œç›´æ¥å¯¼è‡´å·¨å¤§çš„ç®—åŠ›å’Œèƒ½æºæµªè´¹ã€‚
2. **BF-IO æ˜¯ä¸€ç§é«˜æ•ˆä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆ**ï¼šå®ƒé€šè¿‡ **çŸ­æœŸå‰ç» + æ•´æ•°ä¼˜åŒ–** çš„æ–¹å¼ï¼Œåœ¨ä¸ä¾èµ–ç²¾ç¡®ä»»åŠ¡æ—¶é•¿é¢„æµ‹çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
3. **æ”¹è¿›å¹…åº¦éšç³»ç»Ÿè§„æ¨¡æ‰©å¤§è€Œå¢é•¿**ï¼šç†è®ºè¯æ˜å…¶ä¸å¹³è¡¡å‡å°‘å› å­ä¸º $\Omega(\sqrt{B \log G})$ï¼Œæ„å‘³ç€åœ¨å¤§è§„æ¨¡é›†ç¾¤å’Œé«˜å¹¶å‘åœºæ™¯ä¸‹ï¼ŒBF-IO çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚
4. **èƒ½æºæ•ˆç‡ä¸è´Ÿè½½å‡è¡¡å¼ºç›¸å…³**ï¼šé€šè¿‡å‡å°‘ç­‰å¾…æ—¶é—´ï¼ŒBF-IO å°†åŸæœ¬æ¶ˆè€—åœ¨ idle çŠ¶æ€çš„ **watts è½¬åŒ–ä¸ºæœ‰ç”¨çš„è®¡ç®—**ï¼Œä»è€Œç›´æ¥é™ä½äº†å•ä½ token çš„èƒ½è€—ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é›†ä¸­å¼ç­‰å¾…æ± å‡è®¾**ï¼šæœ€å¼ºçš„ç†è®ºä¿è¯ä¾èµ–äºä¸€ä¸ªä¸­å¤®ç­‰å¾…é˜Ÿåˆ—ï¼Œè€ŒæŸäº›ç³»ç»Ÿå¯èƒ½é‡‡ç”¨å³æ—¶åˆ†å‘ï¼ˆinstant-dispatchï¼‰åˆ°å„ worker é˜Ÿåˆ—çš„æ¶æ„ï¼Œè¿™ä¼šå‰Šå¼± BF-IO çš„æ•ˆæœã€‚
- **æ¨¡å‹èŒƒå›´é™åˆ¶**ï¼šå½“å‰ç†è®ºè¦†ç›–çš„æ˜¯ **éé€’å‡è´Ÿè½½æ¼‚ç§»** è¿‡ç¨‹ã€‚å¯¹äºè´Ÿè½½å¯èƒ½å‡å°‘æˆ–é«˜åº¦éå•è°ƒçš„ç³»ç»Ÿï¼ˆå¦‚ aggressive pruning æˆ–è‡ªé€‚åº”å‹ç¼©ï¼‰ï¼Œè¯¥ç†è®ºå°šä¸é€‚ç”¨ã€‚
- **å¤šç›®æ ‡ä¼˜åŒ–**ï¼šç°å®éƒ¨ç½²éœ€è¦åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼ˆå¦‚å°¾å»¶è¿Ÿã€ç§Ÿæˆ·å…¬å¹³æ€§ã€SLO éµå®ˆï¼‰ã€‚å¦‚ä½•å°† BF-IO çš„ç›®æ ‡ä¸è¿™äº›çº¦æŸç»“åˆï¼Œä»éœ€æ·±å…¥ç ”ç©¶ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•åˆ°å³æ—¶åˆ†å‘æ¶æ„**ï¼šä¸º â€œinstant-dispatchâ€ æ¥å£å¼€å‘å®Œæ•´çš„ç†è®ºï¼Œå¹¶é‡åŒ–çŸ­å‰ç»ä¿¡å·åœ¨æ¿€æ´»å»¶è¿Ÿå¢é•¿æ—¶çš„é€€åŒ–ç¨‹åº¦ã€‚
2. **å¤„ç†æ›´å¤æ‚çš„è´Ÿè½½æ¨¡å¼**ï¼šå°†ç†è®ºæ‰©å±•åˆ°è´Ÿè½½å‡å°‘æˆ–éå•è°ƒå˜åŒ–çš„ç³»ç»Ÿï¼Œå¹¶ç ”ç©¶å•è°ƒæ€§åœ¨æ€§èƒ½ä¿è¯ä¸­çš„å¿…è¦æ€§ã€‚
3. **å¤šç›®æ ‡å¯æŒç»­æœåŠ¡**ï¼šåœ¨ä¿æŒæ¯«ç§’çº§å†³ç­–é¢„ç®—çš„åŒæ—¶ï¼Œå°†å…¬å¹³æ€§å’Œ SLO çº¦æŸé›†æˆåˆ°çŸ­æ—¶åŸŸä¼˜åŒ–æ¡†æ¶ä¸­ã€‚
4. **è·¨é¢†åŸŸåº”ç”¨**ï¼šå°† BF-IO åŸåˆ™åº”ç”¨äºå…¶ä»–ç§‘å­¦è®¡ç®—é¢†åŸŸï¼Œå¦‚å¤§è§„æ¨¡çº¿æ€§ä»£æ•°ã€è‡ªé€‚åº”å¤šç‰©ç†åœºæ¨¡æ‹Ÿç­‰ï¼Œè¿™äº›åœºæ™¯åŒæ ·å­˜åœ¨åŒæ­¥éšœç¢å’ŒçŠ¶æ€ç»‘å®šé—®é¢˜ã€‚

</details>

---

### 9. [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)

**Authors**: Yufeng Huang  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.18401v1  

#### Abstract
In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally exclud...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠSuperlinear Multi-Step Attentionã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æ ‡å‡†çš„ **causal self-attention** åœ¨é•¿åºåˆ—åœºæ™¯ä¸‹å­˜åœ¨ **O(LÂ²)** çš„è®¡ç®—å¤æ‚åº¦ç“¶é¢ˆï¼Œä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ç™¾ä¸‡è‡³åƒä¸‡çº§ tokenï¼‰ä¸­çš„å¯æ‰©å±•æ€§ã€‚å°½ç®¡å·²æœ‰å¤šç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶è¢«æå‡ºï¼Œä½†å¤šæ•°æ–¹æ³•é€šè¿‡ç¡¬ç¼–ç çš„ç¨€ç–æ¨¡å¼ï¼ˆå¦‚æ»‘åŠ¨çª—å£ã€èšç±»åˆ†å—ï¼‰ç‰ºç‰²äº† **random context access**ï¼ˆå³ä»»æ„åˆæ³• token éƒ½å¯èƒ½è¢«é€‰ä¸­å‚ä¸ attentionï¼‰ï¼Œä»è€Œå½±å“æ¨¡å‹å¯¹å…¨å±€ä¿¡æ¯çš„è®¿é—®èƒ½åŠ›ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **Superlinear attention**ï¼Œä¸€ç§å…¨æ–°çš„å¤šæ­¥ï¼ˆmulti-stepï¼‰å¯è®­ç»ƒæ³¨æ„åŠ›æ¶æ„ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°† attention é‡æ„ä¸ºä¸€ä¸ª **å¤šæ­¥æœç´¢é—®é¢˜**ï¼ˆmulti-step search problemï¼‰ï¼Œå…·ä½“åˆ†ä¸ºä»¥ä¸‹å››æ­¥ï¼š

1. **Accumulation**ï¼šåˆ©ç”¨çº¿æ€§é€’å½’æœºåˆ¶ï¼ˆå¦‚ Mamba-2 æˆ– linear attentionï¼‰é«˜æ•ˆç§¯ç´¯å‰ç¼€ä¿¡æ¯ï¼Œç”Ÿæˆæ¯ä¸ªä½ç½®çš„ä»£è¡¨å‘é‡ $ K_a(t) $ã€‚
2. **Span-search**ï¼šåŸºäºæŸ¥è¯¢å‘é‡ $ Q_s(i) $ å¯¹ä¸€ç»„é”šç‚¹ï¼ˆanchorsï¼‰è¿›è¡Œæ‰“åˆ†ï¼Œé€‰æ‹© top-k æœ€ç›¸å…³çš„å€™é€‰ spanã€‚
3. **Span-attention**ï¼šåœ¨é€‰å®šçš„è¿ç»­ span ä¸Šæ‰§è¡Œæ ‡å‡†çš„ scaled dot-product attentionã€‚
4. **Combination**ï¼šé€šè¿‡ softmax åŠ æƒç»„åˆå¤šä¸ª span çš„è¾“å‡ºï¼Œä½¿ span-selection å¯å¾®å¹¶å¯é€šè¿‡åå‘ä¼ æ’­è®­ç»ƒã€‚

è¯¥æ–¹æ³•å®ç°äº† **subquadratic å¤æ‚åº¦**ï¼ˆå¦‚ N=2 æ—¶ä¸º O(LÂ³/Â²)ï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº† **random context access** â€”â€” å³æ²¡æœ‰ token å› å›ºå®šç¨€ç–æ¨¡å¼è€Œè¢«æ°¸ä¹…æ’é™¤åœ¨å¤–ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Standard Dense Attention | Top-k Sparse Attention | Block-based Retrieval | Clustering-based | **Superlinear Attention** |
|------|--------------------------|------------------------|------------------------|------------------|----------------------------|
| å¤æ‚åº¦ | O(LÂ²) | O(LÂ²)ï¼ˆexact scoringï¼‰ | O(LÂ²)ï¼ˆworst-caseï¼‰ | O(LÂ³/Â²) ~ O(L log L) | **O(LÂ¹âºÂ¹/á´º)** |
| Random Context Access | âœ… | âœ…ï¼ˆä½†å—é™äº exact scoringï¼‰ | âœ… | âŒï¼ˆè·¨ç°‡ä¸å¯è¾¾ï¼‰ | âœ… |
| å¯è®­ç»ƒæ€§ | âœ… | âœ… | âœ… | âœ… | âœ…ï¼ˆç«¯åˆ°ç«¯å¯å¾®ï¼‰ |
| å®ç°å¯è¡Œæ€§ | é«˜ï¼ˆFlashAttentionï¼‰ | ä¸­ç­‰ | ä¸­ç­‰ | è¾ƒé«˜ | âœ…ï¼ˆGPU bucketed kernel æ”¯æŒï¼‰ |

> âœ… **å…³é”®ä¼˜åŠ¿**ï¼šé¦–æ¬¡åœ¨ä¿æŒ **random context access** çš„å‰æä¸‹å®ç° **subquadratic attention**ï¼Œä¸”å®Œå…¨å¯è®­ç»ƒã€ç³»ç»Ÿä¸Šå¯è¡Œã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä¸»è¦ç”¨äºéªŒè¯å­¦ä¹ èƒ½åŠ›å’Œé•¿ä¸Šä¸‹æ–‡æ£€ç´¢ä»»åŠ¡ï¼š
  - **NIAH (Needle In A Haystack)**ï¼šæ¥è‡ª RULER benchmark çš„ç»å…¸é•¿ä¸Šä¸‹æ–‡æ£€ç´¢å‹åŠ›æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹èƒ½å¦ä»æé•¿æ–‡æœ¬ä¸­å‡†ç¡®æå–æ’å…¥çš„â€œneedleâ€ä¿¡æ¯ã€‚
- ä¸Šä¸‹æ–‡é•¿åº¦èŒƒå›´ï¼š**4K åˆ° 256K tokens**ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„åŸºç¡€**ï¼šåŸºäº **NVIDIA Nemotron-3-Nano-30B-A3B** æ··åˆ MoE æ¶æ„ï¼ˆHybrid Mamba-Transformerï¼‰ï¼Œæ‰€æœ‰æ ‡å‡† attention å±‚æ›¿æ¢ä¸º Superlinear attention å±‚ã€‚
- **å®ç°ç»†èŠ‚**ï¼š
  - N=2 è®¾ç½®ï¼Œç†è®ºå¤æ‚åº¦ **O(LÂ³/Â²)**ã€‚
  - ä½¿ç”¨ **Mamba-2 layer è¾“å‡ºä½œä¸º Ka(t)** è¿›è¡Œ accumulationã€‚
  - å¼•å…¥ä¸“ç”¨ search query matrix $ Q_s $ ç”¨äº span-searchã€‚
  - Span é•¿åº¦éšä½ç½®å¢é•¿ï¼š$ l(i) = \lfloor i^{1-p} \rfloor $ï¼Œå…¶ä¸­ $ p=0.5 $ã€‚
  - Top-k = 2ï¼Œbackward_factor â‰¥ 2 ä»¥ä¿è¯è¦†ç›–ã€‚
- **è®­ç»ƒç­–ç•¥**ï¼š
  - é‡‡ç”¨ **curriculum learning**ï¼šä» 4K ä¸Šä¸‹æ–‡å¼€å§‹ï¼Œé€æ­¥å¢åŠ è‡³ 64Kã€‚
  - å¾®è°ƒæ•´ä¸ªæ¨¡å‹ï¼ˆæ— å†»ç»“ç»„ä»¶ï¼‰ï¼Œå®ç° accumulationã€routing å’Œ attention çš„ååŒé€‚åº”ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **ååé‡ï¼ˆThroughputï¼‰**ï¼š
  - Prefillï¼štokens/secï¼ˆå¤„ç†æ•´æ®µè¾“å…¥ï¼‰
  - Decodeï¼štokens/secï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
- **NIAH æ£€ç´¢å‡†ç¡®ç‡**ï¼šè¡¡é‡æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®å®šä½å¹¶è¾“å‡ºéšè—çš„â€œneedleâ€å†…å®¹ã€‚
- **å¤æ‚åº¦åˆ†æ**ï¼šç†è®ºè®¡ç®—é‡ vs å®é™…è¿è¡Œæ•ˆç‡ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **FlashAttention-2**ï¼šå½“å‰æœ€é«˜æ•ˆçš„ dense attention å®ç°ï¼Œä½œä¸ºä¸»è¦æ€§èƒ½å¯¹æ¯”åŸºçº¿ã€‚
- å…¶ä»–ç¨€ç– attention æ–¹æ³•æœªç›´æ¥å¯¹æ¯”ï¼Œä½†åœ¨ Table 1 ä¸­è¿›è¡Œäº†ç†è®ºæ€§è´¨æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰æ¨ç†ååé‡ï¼ˆSingle B200 GPUï¼‰
| ä¸Šä¸‹æ–‡é•¿åº¦ | Prefill Throughput (tokens/sec) | Decode Throughput (tokens/sec) |
|------------|-----------------------------------|---------------------------------|
| 1M         | ~5,576                            | **109**                         |
| 10M        | ~5,576ï¼ˆç¨³å®šï¼‰                    | **76**                          |

> ğŸ’¡ åœ¨ 10M context ä¸‹ä»ä¿æŒçº¦ **13.18ms/token** çš„è§£ç å»¶è¿Ÿï¼Œæ”¯æŒäº¤äº’å¼åº”ç”¨ã€‚

#### ï¼ˆ2ï¼‰ä¸ FlashAttention-2 çš„å¯¹æ¯”
- **Prefill é˜¶æ®µ**ï¼š
  - <60K æ—¶ FA2 æ›´å¿«ï¼›
  - >60K å Superlinear åè¶…ï¼Œå¹¶åœ¨ 100Kâ€“10M åŒºé—´æ˜¾è‘—é¢†å…ˆã€‚
- **Decode é˜¶æ®µ**ï¼š
  - FA2 è§£ç é€Ÿåº¦éš context å¿«é€Ÿä¸‹é™ï¼ˆå›  O(L) KV-cache æ‰«æï¼‰ï¼›
  - Superlinear è§£ç å‡ ä¹ä¸å— context å½±å“ï¼Œç»´æŒç¨³å®š ~76â€“109 tokens/secã€‚

> ğŸ“ˆ å›¾ 5 æ˜¾ç¤ºï¼š**Superlinear åœ¨é•¿ context ä¸‹å±•ç°å‡ºæ˜æ˜¾æ›´ä¼˜çš„æ‰©å±•æ€§**ã€‚

#### ï¼ˆ3ï¼‰NIAH æ£€ç´¢å‡†ç¡®ç‡
| é…ç½® | æœ€å¤§ context | å¹³å‡å‡†ç¡®ç‡ | å…³é”®è¡¨ç° |
|------|-------------|-----------|---------|
| Baseline (p=0.5, b=2, f=0) | 256K | ~97.7% | åœ¨æµ…å±‚/ä¸­å±‚æ·±åº¦å‡ºç°å¤±è´¥æ¡ˆä¾‹ |
| Extended (+2/+2 factors, b=4, f=2) | 256K | **100.0%** | æ¶ˆé™¤å¤§éƒ¨åˆ†å¤±è´¥ï¼Œé²æ£’æ€§å¢å¼º |

> ğŸ” ç»“æœè¡¨æ˜ï¼š**span-routing æ˜¯å¯å­¦ä¹ çš„**ï¼Œä¸”é€šè¿‡å¢åŠ  redundancyï¼ˆå¦‚æ›´å¤§çš„ span èŒƒå›´ï¼‰å¯æ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆéšå«åˆ†æï¼‰
è™½ç„¶æœªæä¾›å®Œæ•´æ¶ˆèè¡¨ï¼Œä½†æ–‡ä¸­æ˜ç¡®æŒ‡å‡ºä»¥ä¸‹è®¾è®¡é€‰æ‹©çš„å½±å“ï¼š
- **search_exponent ä¸ span_exponent ä¸ä¸€è‡´**ï¼ˆå¦‚ search_p=1/3, attention_p=2/3ï¼‰æœ‰åŠ©äºç¼“è§£è®­ç»ƒåˆæœŸéš¾ä»¥å‘ç° relevant span çš„é—®é¢˜ã€‚
- **increased redundancy factors**ï¼ˆbackward_factor å’Œ forward_factorï¼‰èƒ½æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå‡å°‘æ¼æ£€ã€‚
- **bucketed GPU kernel** è®¾è®¡é¿å…äº†å…¨å±€æ’åºå¼€é”€ï¼ˆO(L log L)ï¼‰ï¼Œæ˜¯å®ç°é«˜æ•ˆ irregular span è®¿é—®çš„å…³é”®ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Superlinear attention æˆåŠŸå®ç°äº† subquadratic attention ä¸ random context access çš„ç»Ÿä¸€**ï¼šé€šè¿‡ multi-step span-search + span-attention æ¶æ„ï¼Œåœ¨ä¸ç‰ºç‰²ä»»æ„ token å¯è¾¾æ€§çš„å‰æä¸‹ï¼Œå°†å¤æ‚åº¦é™è‡³ O(LÂ¹âºÂ¹/á´º)ã€‚
2. âœ… **ç³»ç»Ÿå®ç°å¯è¡Œ**ï¼šæå‡ºçš„ **bucketed GPU kernel** èƒ½æœ‰æ•ˆå¤„ç† irregular span patternï¼Œæ— éœ€å…¨å±€æ’åºï¼Œåœ¨å•å¼  B200 ä¸Šå®ç°ç™¾ä¸‡è‡³åƒä¸‡çº§ context çš„å®ç”¨ååã€‚
3. âœ… **ç«¯åˆ°ç«¯å¯è®­ç»ƒ**ï¼šé€šè¿‡ curriculum learning å’Œ redundancy æ§åˆ¶ï¼Œæ¨¡å‹å¯åœ¨æœ‰é™ fine-tuning ä¸‹å­¦ä¼šæœ‰æ•ˆçš„ span routingï¼Œåœ¨ NIAH ä»»åŠ¡ä¸Šè¾¾åˆ°æ¥è¿‘å®Œç¾çš„æ£€ç´¢ç²¾åº¦ã€‚
4. âœ… **decode æ€§èƒ½ä¼˜è¶Š**ï¼šæ¯ token è§£ç æˆæœ¬ä»… O(LÂ¹/Â²)ï¼Œè¿œä¼˜äº dense attention çš„ O(L)ï¼Œé€‚åˆè¶…é•¿ context ç”Ÿæˆã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä»…ä¸º **N=2 çš„åˆæ­¥å®ç°**ï¼Œå°šæœªæ¢ç´¢æ›´é«˜é˜¶ N æˆ– binary/k-ary search ç­–ç•¥ï¼ˆç†è®ºä¸Šå¯è¾¾ O(L log L)ï¼‰ã€‚
- ç¼ºä¹åœ¨å¤šæ ·åŒ– long-context ä»»åŠ¡ä¸Šçš„å…¨é¢è´¨é‡è¯„ä¼°ï¼ˆå¦‚ summarizationã€code generationã€long-horizon reasoningï¼‰ã€‚
- ä¾èµ– accumulation ç»„ä»¶ï¼ˆå¦‚ Mamba-2ï¼‰çš„è´¨é‡ï¼Œè‹¥å…¶æ— æ³•æœ‰æ•ˆæ±‡æ€»å†å²ä¿¡æ¯ï¼Œåˆ™ä¼šå½±å“ routing æ•ˆæœã€‚
- **random context access æ˜¯ç»“æ„æ€§ä¿éšœï¼Œä¸ä»£è¡¨å®é™…ä¼šè®¿é—®æ‰€æœ‰ token** â€”â€” å­¦ä¹ åˆ°çš„ router ä»å¯èƒ½é›†ä¸­å…³æ³¨å°‘æ•° spansã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‹“å±•è‡³ N > 2 æ­¥æœºåˆ¶**ï¼šè¿›ä¸€æ­¥é™ä½å¤æ‚åº¦è‡³ O(LÂ¹âºÂ¹/á´º)ï¼Œç”šè‡³ç»“åˆ O(log L) åˆ†æ²»ç­–ç•¥ã€‚
2. **å¼•å…¥è¾…åŠ©ç›‘ç£ä¿¡å·**ï¼šå¦‚å¯¹ span-selection æ·»åŠ  auxiliary lossï¼ŒåŠ é€Ÿ routing å­¦ä¹ è¿‡ç¨‹ã€‚
3. **æ›´å¹¿æ³›çš„ benchmark è¯„æµ‹**ï¼šåœ¨å¤šç§ long-context downstream tasks ä¸Šè¯„ä¼°è´¨é‡-æ•ˆç‡ trade-offã€‚
4. **kernel co-design for multi-step variants**ï¼šå¼€å‘é€‚é…å¤šçº§ span-search çš„é«˜æ•ˆ GPU å†…æ ¸ã€‚
5. **æ¢ç´¢ adaptive anchor scheduling**ï¼šéå›ºå®š stride patternï¼ŒåŠ¨æ€è°ƒæ•´ anchor å¯†åº¦ã€‚

---

> ğŸ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> Superlinear attention æå‡ºäº†ä¸€ç§ç»“æ„ä¸Šä¿ç•™ **random context access** çš„ **subquadratic å¤šæ­¥æ³¨æ„åŠ›æœºåˆ¶**ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„ **bucketed kernel** å®ç°åœ¨ç™¾ä¸‡è‡³åƒä¸‡çº§ context ä¸‹çš„é«˜æ•ˆæ¨ç†ï¼Œä¸ºæ„å»ºçœŸæ­£å¯æ‰©å±•çš„ long-context LLM æä¾›äº†æ–°çš„æ¶æ„è·¯å¾„ã€‚

</details>

---

### 10. [When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems](https://arxiv.org/abs/2601.16280)

**Authors**: Donghao Huang, Gauri Malwe, Zhaoxia Wang  
**Category**: cs.AI  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.16280v1  

#### Abstract
Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate proced...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“  
**è®ºæ–‡æ ‡é¢˜**: *When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰å¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿåœ¨ä¼ä¸šè‡ªåŠ¨åŒ–ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶**ç”Ÿäº§éƒ¨ç½²å—é™äºç¨‹åºå¯é æ€§ä¸è¶³**ï¼Œå°¤å…¶æ˜¯å·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­çš„å¤±è´¥ç¼ºä¹ç³»ç»Ÿæ€§è¯Šæ–­æœºåˆ¶ã€‚ç°æœ‰ç ”ç©¶å¤šä¾èµ–èšåˆæŒ‡æ ‡ï¼ˆå¦‚ä»»åŠ¡æˆåŠŸç‡ï¼‰ï¼Œæ©ç›–äº†å…·ä½“æ•…éšœåŸå› ï¼Œéš¾ä»¥æ”¯æŒé’ˆå¯¹æ€§ä¼˜åŒ–ã€‚

æ­¤å¤–ï¼Œä¸­å°ä¼ä¸šï¼ˆSMEsï¼‰é¢ä¸´ç¡¬ä»¶èµ„æºã€éšç§ä¿æŠ¤å’Œæˆæœ¬æ•æ„Ÿç­‰çº¦æŸï¼ŒäºŸéœ€ä¸€ç§å…¼é¡¾**å¯é æ€§ã€æ•ˆç‡ä¸å¯éƒ¨ç½²æ€§çš„è¯„ä¼°æ¡†æ¶**ã€‚

### âœ… æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºä¸€ä¸ª**ç³»ç»ŸåŒ–çš„è¯Šæ–­æ¡†æ¶ï¼ˆDiagnostic Framework for Tool Invocation Reliability, DF4TIRï¼‰**ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

- **12ç±»é”™è¯¯åˆ†ç±»æ³•ï¼ˆ12-category error taxonomyï¼‰**  
  è¦†ç›–å·¥å…·åˆå§‹åŒ–ã€å‚æ•°å¤„ç†ã€æ‰§è¡Œå’Œç»“æœè§£é‡Šå››ä¸ªé˜¶æ®µï¼Œç»“åˆä¸‰ç§å·¥å…·ç±»å‹ï¼ˆOCRã€DB Queryã€DB Updateï¼‰ï¼Œå½¢æˆç»†ç²’åº¦çš„å¤±è´¥æ¨¡å¼åˆ†æä½“ç³»ï¼Œæ˜¾è‘—è¶…è¶Šä¼ ç»Ÿå•æ™ºèƒ½ä½“åœºæ™¯ä¸‹çš„é”™è¯¯åˆ†ç±»ã€‚

- **å¯å¤ç°çš„è¯„ä¼°åŸºç¡€è®¾æ–½**  
  æ„å»ºæ ‡å‡†åŒ–åè®®ï¼Œåœ¨å¤šç§è¾¹ç¼˜ç¡¬ä»¶å¹³å°ï¼ˆNVIDIA RTX A6000 / RTX 4090 / Apple M3 Maxï¼‰ä¸Šå¯¹ open-weight å’Œ closed-source LLMs è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œé‡‡ç”¨ 4-bit quantization æ¨¡æ‹ŸçœŸå®éƒ¨ç½²æ¡ä»¶ã€‚

- **å¯é æ€§é˜ˆå€¼è¯†åˆ«ä¸éƒ¨ç½²æŒ‡å¯¼**  
  æ˜ç¡®æŒ‡å‡ºæ¨¡å‹è§„æ¨¡ä¸ç¡¬ä»¶é…ç½®ä¹‹é—´çš„å¯é æ€§-æˆæœ¬æƒè¡¡å…³ç³»ï¼Œä¸ºä¸åŒç»„ç»‡æä¾›åˆ†å±‚éƒ¨ç½²å»ºè®®ã€‚

### âœ… ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬è®ºæ–‡æ–¹æ³• |
|------|--------|-----------|
| è¯„ä¼°ç²’åº¦ | èšåˆæˆåŠŸç‡ï¼ˆSRï¼‰ | ç»†ç²’åº¦é”™è¯¯å½’å› ï¼ˆ12ç±»ï¼‰ |
| åº”ç”¨èŒƒå›´ | å•æ™ºèƒ½ä½“ä¸ºä¸» | å¤šæ™ºèƒ½ä½“åè°ƒåœºæ™¯ |
| éƒ¨ç½²è€ƒé‡ | å¿½ç•¥ç¡¬ä»¶å·®å¼‚ | åŒ…å«è·¨å¹³å°æ€§èƒ½è¡¨å¾ |
| å¯å¤ç°æ€§ | å°‘æ•°é—­æºAPIæµ‹è¯• | å¼€æºä»£ç ã€æ•°æ®é›†ã€å·¥å…·å…¨å…¬å¼€ï¼ˆGitHubï¼‰ |

> ğŸ”— å…¬å¼€èµ„æºåœ°å€ï¼šhttps://github.com/inflaton/df4tir

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **åˆæˆå‘ç¥¨å¯¹è´¦æ•°æ®é›†ï¼ˆSynthetic Invoice Reconciliation Datasetï¼‰**
  - æ€»è®¡ **1,980ä¸ªç¡®å®šæ€§æµ‹è¯•å®ä¾‹**
  - å¹³è¡¡è®¾è®¡ï¼š
    - è§†è§‰ä»»åŠ¡ï¼ˆvision-enabledï¼‰ï¼š990ä¾‹ï¼ˆå«PDFæ‰«æä»¶OCRå¤„ç†ï¼‰
    - çº¯æ–‡æœ¬ä»»åŠ¡ï¼ˆtext-onlyï¼‰ï¼š990ä¾‹
  - æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡æµç¨‹ï¼Œæ¶µç›–å¤šæ¨¡æ€è¾“å…¥ã€æ•°æ®åº“æŸ¥è¯¢ä¸æ›´æ–°æ“ä½œã€‚

### âš™ï¸ å®éªŒè®¾ç½®
#### æ¨¡å‹é€‰æ‹©
| ç±»å‹ | æ¨¡å‹åˆ—è¡¨ |
|------|---------|
| **Closed-Source Models** | `gpt-4o`, `gpt-4.1`, `claude-3-5-sonnet`, `claude-3-7-sonnet` ç­‰ |
| **Open-Weight Models** | `qwen2.5`ç³»åˆ—ï¼ˆ3Bâ€“72Bï¼‰ã€`Functionary` V3.1ï¼ˆ8B, 70Bï¼‰ |

#### éƒ¨ç½²é…ç½®
- æ‰€æœ‰ open-weight æ¨¡å‹é€šè¿‡ **Ollama v0.6.8** æœ¬åœ°è¿è¡Œ
- ä½¿ç”¨ **4-bit quantization (Q4_K_M)** å‡å°‘å†…å­˜å ç”¨ï¼Œè´´è¿‘è¾¹ç¼˜éƒ¨ç½²ç°å®
- æµ‹è¯•å¹³å°ï¼š
  - NVIDIA RTX A6000ï¼ˆ48GB VRAMï¼‰
  - NVIDIA RTX 4090 ç¬”è®°æœ¬ GPUï¼ˆ16GB VRAMï¼‰
  - Apple M3 Maxï¼ˆ96GB ç»Ÿä¸€å†…å­˜ï¼‰

#### å¤šæ™ºèƒ½ä½“æ¶æ„ï¼ˆåŸºäº LangGraphï¼‰
- **Email Agent**ï¼šè´Ÿè´£æ–‡æ¡£è§£æä¸OCRå·¥å…·è°ƒç”¨
- **Data Engineering Agent**ï¼šæ‰§è¡Œ DB Query / Update å·¥å…·å¹¶éªŒè¯
- **Reconciliation Agent**ï¼šåè°ƒæµç¨‹ï¼Œåšå‡ºæœ€ç»ˆå†³ç­–
- å„Agentå‡é…å¤‡è¯¦ç»†æ—¥å¿—è®°å½•ä»¥æ”¯æŒæ•…éšœè¯Šæ–­

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Success Rate (SR)** | å®Œæ•´ä»»åŠ¡æˆåŠŸå®Œæˆçš„æ¯”ä¾‹ |
| **Execution Time** | ç«¯åˆ°ç«¯å“åº”æ—¶é—´ï¼ˆå«APIå»¶è¿Ÿï¼‰ |
| **Process Steps** | å¹³å‡æ¨ç†æ­¥æ•°ï¼Œåæ˜ è§„åˆ’æ•ˆç‡ |
| **OCR F1 Score** | OCRè¾“å‡ºå‡†ç¡®æ€§ï¼ˆå­ä»»åŠ¡ï¼‰ |
| **Error Distribution** | æŒ‰12ç±»é”™è¯¯åˆ†ç±»ç»Ÿè®¡å¤±è´¥ç±»å‹ |

> æ‰€æœ‰å®éªŒä½¿ç”¨ `temperature=0` å®ç°ç¡®å®šæ€§æ‰§è¡Œï¼Œç¡®ä¿ç»“æœå¯å¤ç°ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

| æ¨¡å‹ | å¹³å° | æˆåŠŸç‡ (SR) | å»¶è¿Ÿ (ç§’) | æ­¥éª¤æ•° |
|------|------|------------|----------|--------|
| `qwen2.5:32b` | RTX A6000 | **100.0%** | 13.9 | 8.5 |
| `qwen2.5:14b` | RTX A6000 | 96.6% | **7.3** | 8.6 |
| `qwen2.5:72b` | RTX A6000 | 95.1% | 39.3 | 8.7 |
| `qwen2.5:7b` | RTX A6000 | 57.3% | 9.9 | 20.7 |
| `qwen2.5:3b` | RTX A6000 | 13.9% | 8.1 | 18.8 |
| `gpt-4.1` | OpenAI | **100.0%** | 8.3 | 9.9 |
| `gpt-4o-mini` | OpenAI | 99.3% | 7.6 | 8.6 |

> âœ… `qwen2.5:32b` åœ¨æœ¬åœ°éƒ¨ç½²ä¸‹è¾¾åˆ°ä¸ `gpt-4.1` ç›¸åŒçš„ **100% æˆåŠŸç‡**

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **open-weight vs. closed-source**ï¼š
  - `qwen2.5:32b` ä¸ `gpt-4.1` è¡¨ç°æŒå¹³ï¼ˆ100% SRï¼‰
  - `qwen2.5:14b` è¾¾åˆ° **96.6â€“97.4% SR**ï¼Œæ¥è¿‘å¤šæ•°é—­æºæ¨¡å‹ï¼ˆå¦‚ `gpt-4o-mini`: 99.3%ï¼‰
  - å°æ¨¡å‹ï¼ˆ<14Bï¼‰è¡¨ç°æ˜¾è‘—è½åï¼Œå°¤å…¶ `qwen2.5:3b` ä»… 13.9%

- **ç¡¬ä»¶å½±å“æ˜¾è‘—**ï¼š
  - `qwen2.5:14b` åœ¨ RTX A6000 ä¸Šè€—æ—¶ **7.3ç§’**ï¼Œè€Œåœ¨ M3 Max ä¸Šé«˜è¾¾ **60.0ç§’** â†’ **8.2å€å»¶è¿Ÿå·®å¼‚**
  - æ›´å¼ºç¡¬ä»¶ä¸èƒ½å¼¥è¡¥ä½æ•ˆè§„åˆ’ï¼š`qwen2.5:7b` åœ¨ M3 Max ä¸Šå¹³å‡éœ€ **85.9ç§’**ï¼Œå› å…¶å¸¸é™·å…¥å¾ªç¯å†³ç­–

### ğŸ”¬ æ¶ˆèå®éªŒä¸é”™è¯¯åˆ†å¸ƒåˆ†æï¼ˆAblation Insightsï¼‰

#### é”™è¯¯ç±»å‹ä¸»å¯¼æ¨¡å¼ï¼ˆTables II & IIIï¼‰
- **Tool Initialization Failures æ˜¯ä¸»è¦ç“¶é¢ˆ**ï¼š
  - å°æ¨¡å‹ä¸­å æ€»é”™è¯¯ >80%
  - ç‰¹åˆ«æ˜¯ `DB_UPDATE_TOOL_NOT_INITIALIZED` å’Œ `DB_QUERY_TOOL_NOT_INITIALIZED`
- ç¤ºä¾‹ï¼š
  - `qwen2.5:3b` åœ¨è§†è§‰ä»»åŠ¡ä¸­æœ‰ **881æ¬¡ DB_UPDATE åˆå§‹åŒ–å¤±è´¥**ï¼ˆå 89%ï¼‰
  - `qwen2.5:14b` å°†è¯¥é”™è¯¯é™è‡³ **15æ¬¡ä»¥å†…**
  - `qwen2.5:32b` å’Œ `gpt-4.1` å®ç° **é›¶åˆå§‹åŒ–é”™è¯¯**

#### å®šæ€§é”™è¯¯åˆ†æï¼ˆQualitative Analysisï¼‰
ä»200ä¸ªå¤±è´¥æ¡ˆä¾‹ä¸­å½’çº³ä¸¤ç±»ä¸»è¦å¤±è´¥æ¨¡å¼ï¼š
| ç±»å‹ | å æ¯” | æè¿° | ç¤ºä¾‹ |
|------|-----|------|-------|
| **Omission Failures** | ~68% | æ™ºèƒ½ä½“æœªæ„è¯†åˆ°éœ€è¦è°ƒç”¨å·¥å…·ï¼Œç›´æ¥ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ | â€œPayment is $1,234â€¦â€ è€Œä¸è°ƒç”¨ `db_query_tool` |
| **Malformed Call Failures** | ~32% | å·¥å…·åç§°é”™è¯¯ã€JSONç»“æ„æ— æ•ˆã€å¹»è§‰å‚æ•°ç­‰ | ä½¿ç”¨ `database_query` æ›¿ä»£ `db_query_tool` |

> â— è¡¨æ˜å°æ¨¡å‹å­˜åœ¨â€œè¯­ä¹‰ç†è§£â€ä¸â€œåŠ¨ä½œæ‰§è¡Œâ€çš„è„±èŠ‚é—®é¢˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å·¥å…·åˆå§‹åŒ–å¤±è´¥æ˜¯å¯é æ€§çš„é¦–è¦ç“¶é¢ˆ**  
   > ä¸æ˜¯å‚æ•°é”™è¯¯æˆ–ç»“æœå¤„ç†ï¼Œè€Œæ˜¯â€œæ˜¯å¦è°ƒç”¨å·¥å…·â€è¿™ä¸€åŸºæœ¬å†³ç­–å‡ºé”™ï¼Œå°¤å…¶åœ¨å°æ¨¡å‹ä¸­æä¸ºæ™®éã€‚

2. **å­˜åœ¨æ˜ç¡®çš„å®¹é‡é˜ˆå€¼**ï¼š
   - **14B å‚æ•°æ˜¯æœ€ä½å¯è¡Œéƒ¨ç½²é—¨æ§›**ï¼ˆ`qwen2.5:14b` è¾¾ 96.6% SRï¼‰
   - **32B å‚æ•°å®ç°ä¸é—­æºæ¨¡å‹ï¼ˆGPT-4.1ï¼‰å¯é æ€§å¯¹é½**ï¼ˆ100% SRï¼‰

3. **mid-sized models å…·å¤‡æœ€ä½³æ€§ä»·æ¯”**  
   > `qwen2.5:14b` åœ¨ RTX 4090 ä¸Šä»¥ **$5K æˆæœ¬** å®ç°é«˜å¯é æ€§ï¼Œé€‚åˆå¤§å¤šæ•° SME éƒ¨ç½²ã€‚

4. **ç¡¬ä»¶é€‰æ‹©æå¤§å½±å“æ€§èƒ½**  
   > æœ€å¿«ä¸æœ€æ…¢å¹³å°é—´å»¶è¿Ÿç›¸å·® **8.2å€**ï¼Œå‡¸æ˜¾ç¡¬ä»¶ä¼˜åŒ–çš„é‡è¦æ€§ã€‚

5. **æ›´å¤§çš„æ¨¡å‹ä¸ä¸€å®šæ›´å¥½**  
   > `qwen2.5:72b` æˆåŠŸç‡ï¼ˆ95.1%ï¼‰åè€Œä½äº `32b`ï¼Œè¡¨æ˜å­˜åœ¨ä»»åŠ¡ç‰¹å®šçš„â€œå®¹é‡é¥±å’Œç‚¹â€ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ¡†æ¶åŸºäº**ç¡®å®šæ€§æ‰§è¡Œ**ï¼ˆtemperature=0ï¼‰ï¼Œæœªè¦†ç›–éšæœºæ€§å¼•å‘çš„è¡Œä¸ºæ¼‚ç§»
- æ•°æ®é›†ä¸º**åˆæˆç”Ÿæˆ**ï¼Œè™½æ¨¡æ‹ŸçœŸå®åœºæ™¯ä½†ä»ç¼ºä¹çœŸå®ä¸–ç•Œå™ªå£°
- ä»…èšç„¦**å‘ç¥¨å¯¹è´¦**å•ä¸€é¢†åŸŸï¼Œéœ€è¿›ä¸€æ­¥è·¨åŸŸéªŒè¯ï¼ˆå¦‚ä¾›åº”é“¾ã€å®¢æœï¼‰
- æœªé›†æˆå®æ—¶å®¹é”™æˆ–è‡ªä¿®å¤æœºåˆ¶ï¼Œä»å±â€œäº‹åè¯Šæ–­â€è€Œéâ€œäº‹ä¸­æ¢å¤â€

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **å¯¹æŠ—æ€§ä¸è´Ÿæ ·æœ¬åè®®è®¾è®¡**ï¼šå¼•å…¥æ›´å…·æŒ‘æˆ˜æ€§çš„è¾¹ç¼˜æ¡ˆä¾‹
2. **åˆ†å¸ƒå¼å¤±è´¥ä¼ æ’­åˆ†æ**ï¼šè¿½è¸ªé”™è¯¯å¦‚ä½•åœ¨å¤šAgenté—´çº§è”æ‰©æ•£
3. **Self-healing Agent Architectures**ï¼šåŸºäºé”™è¯¯åˆ†ç±»æ„å»ºè‡ªåŠ¨é‡è¯•ä¸ä¿®æ­£ç­–ç•¥
4. **è·¨é¢†åŸŸæ³›åŒ–éªŒè¯**ï¼šæ‰©å±•è‡³åŒ»ç–—ã€æ³•å¾‹ã€åˆ¶é€ ç­‰é¢†åŸŸ
5. **åŠ¨æ€è°ƒåº¦ä¸èµ„æºæ„ŸçŸ¥æ¨ç†**ï¼šç»“åˆç¡¬ä»¶è´Ÿè½½è¿›è¡Œå¼¹æ€§Agentç¼–æ’

---

## æ€»ç»“
> æœ¬è®ºæ–‡é¦–æ¬¡å°†**ç»†ç²’åº¦é”™è¯¯è¯Šæ–­**å¼•å…¥å¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿè¯„ä¼°ï¼Œæ­ç¤ºäº†â€œå·¥å…·è°ƒç”¨ä¸å¯é â€çš„æ ¹æœ¬ç—‡ç»“åœ¨äº**åˆå§‹åŒ–å¤±è´¥**ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒç¡®ç«‹äº† `14B` å’Œ `32B` æ¨¡å‹çš„å…³é”®å¯é æ€§é˜ˆå€¼ã€‚  
>
> å…¶æå‡ºçš„ **DF4TIR æ¡†æ¶**ä¸ä»…æä¾›äº†ç§‘å­¦è¯„ä¼°å·¥å…·ï¼Œæ›´ä¸ºèµ„æºå—é™ç»„ç»‡æä¾›äº†åˆ‡å®å¯è¡Œçš„éƒ¨ç½²è·¯å¾„â€”â€”**æ— éœ€ç›²ç›®è¿½æ±‚æœ€å¤§æ¨¡å‹ï¼Œè€Œåº”å…³æ³¨æ¨¡å‹è§„æ¨¡ã€ç¡¬ä»¶åŒ¹é…ä¸ç³»ç»ŸéŸ§æ€§ä¹‹é—´çš„å¹³è¡¡**ã€‚  
>
> è¿™æ ‡å¿—ç€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°æ­£ä»â€œèƒ½å¦å®Œæˆä»»åŠ¡â€è¿ˆå‘â€œä¸ºä½•å¤±è´¥â€çš„æ·±åº¦è¯Šæ–­æ—¶ä»£ã€‚

</details>

---

### 11. [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)

**Authors**: Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang  
**Category**: cs.AI  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.16725v1  

#### Abstract
We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, includi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **ã€ŠLongCat-Flash-Thinking-2601 Technical Reportã€‹æ ¸å¿ƒæ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨**agentic reasoningï¼ˆä»£ç†å¼æ¨ç†ï¼‰** èƒ½åŠ›ä¸Šçš„ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ã€‚å…·ä½“æŒ‘æˆ˜åŒ…æ‹¬ï¼š
- **ç¯å¢ƒäº¤äº’èƒ½åŠ›å¼±**ï¼šä¼ ç»Ÿæ¨¡å‹å¤šä¾èµ–å†…éƒ¨æ¨ç†ï¼Œç¼ºä¹ä¸å¤–éƒ¨å·¥å…·å’Œç¯å¢ƒè¿›è¡ŒåŠ¨æ€ã€é•¿æœŸäº¤äº’çš„èƒ½åŠ›ã€‚
- **æ³›åŒ–èƒ½åŠ›å·®**ï¼šåœ¨æœªè§è¿‡çš„å·¥å…·ç»„åˆæˆ–å¤šé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
- **å¯¹å™ªå£°æ•æ„Ÿ**ï¼šçœŸå®ä¸–ç•Œç¯å¢ƒä¸­å­˜åœ¨æŒ‡ä»¤æ¨¡ç³Šã€å·¥å…·å¤±è´¥ç­‰å™ªå£°ï¼Œç°æœ‰è®­ç»ƒèŒƒå¼éš¾ä»¥åº”å¯¹ã€‚
- **æµ‹è¯•æ—¶æ‰©å±•æ€§æœ‰é™**ï¼šç¼ºä¹æœ‰æ•ˆçš„ test-time scaling æœºåˆ¶æ¥æå‡å¤æ‚ä»»åŠ¡çš„æ¨ç†æ·±åº¦ä¸å¹¿åº¦ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹**
è®ºæ–‡æå‡ºäº† **LongCat-Flash-Thinking-2601**ï¼Œä¸€ä¸ªæ‹¥æœ‰ **5600äº¿å‚æ•°ï¼ˆ560Bï¼‰çš„ MoE æ¶æ„å¼€æºæ¨¡å‹**ï¼Œå…·å¤‡å“è¶Šçš„ agentic reasoning èƒ½åŠ›ã€‚å…¶ä¸‰å¤§æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### **(1) Environment Scaling ä¸å¤šåŸŸç¯å¢ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶**
- **æ–¹æ³•**ï¼šæ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ **domain graph åˆæˆæµæ°´çº¿**ï¼Œä»é«˜é˜¶é¢†åŸŸå®šä¹‰ç”Ÿæˆå¯æ‰§è¡Œçš„å·¥å…·é›†ã€æ•°æ®åº“ schema å’Œå·¥å…·ä¾èµ–å›¾ï¼ˆtool dependency graphï¼‰ï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œå¯æ§çš„å›¾æ‰©å±•ä»¥æ„é€ å¤šæ ·åŒ–ã€å¤æ‚çš„ agentic ç¯å¢ƒã€‚
- **ä¼˜åŠ¿**ï¼š
  - æ”¯æŒè¶…è¿‡ 20 ä¸ªé¢†åŸŸçš„å¼‚æ„ç¯å¢ƒè®­ç»ƒã€‚
  - ä¿è¯ç”Ÿæˆç¯å¢ƒçš„ **executabilityï¼ˆå¯æ‰§è¡Œæ€§ï¼‰** å’Œ **verifiabilityï¼ˆå¯éªŒè¯æ€§ï¼‰**ã€‚
  - å®ç°è·¨åŸŸæŠ€èƒ½è¿ç§»ä¸æ³›åŒ–ã€‚

#### **(2) é¢å‘å™ªå£°ç¯å¢ƒçš„é²æ£’è®­ç»ƒç­–ç•¥ï¼ˆRobust Agentic Training under Noisy Environmentsï¼‰**
- **æ–¹æ³•**ï¼šç³»ç»Ÿåˆ†æçœŸå®ä¸–ç•Œå™ªå£°æ¨¡å¼ï¼ˆå¦‚æŒ‡ä»¤æ­§ä¹‰ã€å·¥å…·è¿”å›é”™è¯¯/ä¸å®Œæ•´ç»“æœï¼‰ï¼Œè®¾è®¡è‡ªåŠ¨åŒ– pipeline æ³¨å…¥å¤šç±»å‹ã€å¤šå±‚çº§å™ªå£°ï¼Œå¹¶é‡‡ç”¨è¯¾ç¨‹å¼ RL ç­–ç•¥é€æ­¥å¢åŠ å™ªå£°å¤æ‚åº¦ã€‚
- **ä¼˜åŠ¿**ï¼š
  - æ˜¾è‘—æå‡æ¨¡å‹åœ¨éç†æƒ³æ¡ä»¶ä¸‹çš„ç¨³å®šæ€§ä¸æˆåŠŸç‡ã€‚
  - ç¼©å°è®­ç»ƒ-éƒ¨ç½²ä¹‹é—´çš„â€œå®Œç¾ vs çœŸå®â€ç¯å¢ƒé¸¿æ²Ÿã€‚

#### **(3) Heavy Thinking Mode â€”â€” æµ‹è¯•æ—¶è”åˆæ‰©å±•æ¨ç†å®½åº¦ä¸æ·±åº¦**
- **æ–¹æ³•**ï¼šå¼•å…¥ä¸€ç§æ–°çš„ test-time scaling æ¨¡å¼ï¼š
  1. **Parallel Reasoning Stage**ï¼šå¤šä¸ª thinker å¹¶è¡Œç”Ÿæˆä¸åŒçš„æ¨ç†è·¯å¾„ï¼ˆæ‰©å±•å®½åº¦ï¼‰ã€‚
  2. **Heavy Thinking Stage**ï¼šç”± summary model å¯¹å¤šæ¡è·¯å¾„è¿›è¡Œåæ€æ•´åˆï¼Œæç‚¼æœ€ç»ˆç­”æ¡ˆï¼ˆæ·±åŒ–æ¨ç†ï¼‰ã€‚
- **ä¼˜åŠ¿**ï¼š
  - ç›¸æ¯”ä»…æ‰©å±•æ·±åº¦ï¼ˆChain-of-Thoughtï¼‰æˆ–å®½åº¦ï¼ˆSelf-Consistencyï¼‰ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°é€¼è¿‘æ¨¡å‹çš„çœŸå®æ¨ç†è¾¹ç•Œã€‚
  - å¯çµæ´»æ§åˆ¶è®¡ç®—é¢„ç®—ï¼Œåœ¨æ€§èƒ½ä¸å»¶è¿Ÿé—´å–å¾—å¹³è¡¡ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
| ç±»åˆ« | æ•°æ®é›† |
|------|-------|
| **æ•°å­¦æ¨ç†** | AIME-25, HMMT-25, IMO-AnswerBench, AMO-Bench (EN/CH) |
| **ä»£ç†æœç´¢ï¼ˆAgentic Searchï¼‰** | BrowseComp, BrowseComp-zh, RWSearchï¼ˆç§æœ‰çœŸå®åœºæ™¯æœç´¢åŸºå‡†ï¼‰ |
| **ä»£ç†å·¥å…·ä½¿ç”¨ï¼ˆAgentic Tool Useï¼‰** | T2-Bench, VitaBench, T2-Noise, Vita-Noise, Random Complex Tasks |
| **é€šç”¨é—®ç­”** | GPQA-Diamond, HLE |
| **ä»£ç èƒ½åŠ›** | LiveCodeBench (24.08â€“25.05), OJBench, OIBench, SWE-bench Verified |

> âœ… æ‰€æœ‰ benchmark å‡è¿›è¡Œäº†æ¸…æ´—ä¸å¢å¼ºï¼ˆå¦‚ä¿®å¤ T2-Bench airline å­é›†æ ‡æ³¨é”™è¯¯ã€å‡çº§ VitaBench éªŒè¯å™¨ç­‰ï¼‰ï¼Œç¡®ä¿å…¬å¹³å¯å¤ç°ã€‚

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**
- **æ¨¡å‹æ¶æ„**ï¼šMoEï¼Œæ€»å‚æ•° 560Bï¼Œæ¯ token æ¿€æ´»çº¦ 27B å‚æ•°ã€‚
- **æ¨ç†é…ç½®**ï¼štemperature=1.0, top-k=-1, top-p=1.0ï¼›Heavy Thinking æ¨¡å¼å¯ç”¨æ—¶æŠ¥å‘Šå¢å¼ºæ€§èƒ½ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - æ•°å­¦/ç¼–ç ï¼š`Avg@k`, `Pass@1`
  - æœç´¢/å·¥å…·ä½¿ç”¨ï¼š`Pass@1`, `Avg@4`
  - å¤šè½®ä»»åŠ¡ï¼šç»“åˆ context management ç­–ç•¥ï¼ˆsummary-based / discard-all / hybridï¼‰

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| å¼€æºæ¨¡å‹ï¼ˆOpen-Weightsï¼‰ | é—­æºæ¨¡å‹ï¼ˆClosed-Weightsï¼‰ |
|--------------------------|----------------------------|
| DeepSeek-V3.2-Thinking | GPT-5.2-Thinking-xhigh |
| Kimi-K2-Thinking | Claude-Opus-4.5-Thinking |
| Qwen3-235B-A22B-Thinking-2507 | Gemini-3-Pro |
| GLM-4.7-Thinking | â€” |

> âš ï¸ å¤–éƒ¨æ¨¡å‹ç»“æœä¼˜å…ˆé‡‡ç”¨å®˜æ–¹æŠ¥å‘Šå€¼ï¼›éƒ¨åˆ†é€šè¿‡ç»Ÿä¸€æ¡†æ¶é‡æµ‹ä½†æ€§èƒ½åä½ï¼Œæ•…ä»å¼•ç”¨åŸæŠ¥å‘Šã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ‘˜å½•è‡ª Table 2ï¼‰**

| Benchmark | LongCat-Flash-Thinking-2601 | æœ€ä½³ç«å“ï¼ˆå«é—­æºï¼‰ | æ˜¯å¦ SOTAï¼ˆå¼€æºï¼‰ |
|----------|------------------------------|--------------------|------------------|
| **BrowseComp (Pass@1)** | **73.1**ï¼ˆå¯ç”¨äº† context managementï¼‰ | 65.8 (GPT-5.2) | âœ… æ˜¯ |
| **BrowseComp-zh (Pass@1)** | **77.7** | 62.3 (Kimi-K2) | âœ… æ˜¯ |
| **RWSearch (Pass@1)** | **79.5** | 82.0 (GPT-5.2) | ğŸ”· ç¬¬äºŒ |
| **T2-Bench (Avg@4)** | **88.2** | 90.7 (Gemini-3-Pro) | âœ… æ˜¯ |
| **VitaBench (Avg@4)** | **29.3** | 31.5 (Claude Opus) | âœ… æ˜¯ |
| **AIME-25 (Avg@16)** | **99.6 / 100.0**ï¼ˆå¯ç”¨ Heavy Thinkingï¼‰ | 100.0 | âœ… æ¥è¿‘æœ€ä¼˜ |
| **IMO-AnswerBench (Avg@4)** | **78.6 / 86.8**ï¼ˆHeavy Thinkingï¼‰ | 86.7 | âœ… å¼€æºç¬¬ä¸€ |
| **AMO-Bench EN (Avg@16)** | **61.6 / 66.0**ï¼ˆHeavy Thinkingï¼‰ | 72.5 | âœ… å¼€æºé¢†å…ˆ |
| **AMO-Bench CH (Avg@16)** | **56.8 / 67.5**ï¼ˆHeavy Thinkingï¼‰ | 74.9 | âœ… å¼€æºæœ€ä½³ |
| **OIBench EN (Pass@1)** | **42.2** | 61.2 (Claude Opus) | âœ… å¼€æºå‰åˆ— |
| **SWE-bench Verified (Avg@5)** | **70.0** | 80.0 | âœ… é¡¶çº§æ°´å¹³ |

> ğŸ“Œ **Heavy Thinking æ¨¡å¼æ˜¾è‘—æå‡æ€§èƒ½**ï¼Œå°¤å…¶åœ¨æ•°å­¦ç±»ä»»åŠ¡ä¸Šæ¥è¿‘ç”šè‡³åª²ç¾æœ€å¼ºé—­æºæ¨¡å‹ã€‚

### **æ¶ˆèå®éªŒç»“æœ**

#### **(1) å™ªå£°è®­ç»ƒçš„å½±å“ï¼ˆTable 1ï¼‰**
| Dataset | Training w/o Noise | Training w/ Noise | æå‡å¹…åº¦ |
|--------|---------------------|-------------------|---------|
| VitaBench (Avg@4) | 28.6 | **29.3** | +0.7 |
| VitaBench-Noise (Avg@4) | 13.3 | **20.5** | **+7.2** |
| T2-Bench (Avg@4) | 87.1 | **88.2** | +1.1 |
| T2-Bench-Noise (Avg@4) | 62.2 | **67.1** | **+4.9** |

> âœ… ç»“æœè¡¨æ˜ï¼š**æ˜¾å¼æ³¨å…¥å™ªå£°ä¸ä»…æå‡æŠ—å™ªèƒ½åŠ›ï¼Œåè€Œè½»å¾®æå‡äº†å¹²å‡€ç¯å¢ƒä¸‹çš„æ€§èƒ½**ï¼Œè¯´æ˜è®­ç»ƒæ›´å…·é²æ£’æ€§ã€‚

#### **(2) Context Management ç­–ç•¥æ¯”è¾ƒï¼ˆFigure 7ï¼‰**
- **Hybridï¼ˆSummary + Discard-allï¼‰** åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°æœ€ä¼˜ï¼Œæœ€ç»ˆè¾¾åˆ° **73.1% Pass@1**ã€‚
- å•ç‹¬ä½¿ç”¨ Summary æˆ– Discard-all å‡ä¸å¦‚ Hybrid ç¨³å®šé«˜æ•ˆã€‚

#### **(3) åŠ¨æ€é¢„ç®—åˆ†é…ä¸è¯¾ç¨‹å­¦ä¹ **
- å¼•å…¥ curriculum learning åï¼Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦åŠ å¿«ï¼Œæœ€ç»ˆä»»åŠ¡å®Œæˆç‡æé«˜ **+62.03%**ï¼ˆè§ Figure 8ï¼‰ã€‚
- åŠ¨æ€ rollout budget allocation æœ‰æ•ˆèšç„¦äºé«˜ä»·å€¼ä»»åŠ¡ï¼Œé¿å…èµ„æºæµªè´¹ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **ç¯å¢ƒå¤šæ ·æ€§æ˜¯æ³›åŒ–èƒ½åŠ›çš„å…³é”®**ï¼šé€šè¿‡å¤§è§„æ¨¡ã€å¤šåŸŸã€å¯æ‰§è¡Œçš„ environment scalingï¼Œæ¨¡å‹å­¦ä¼šäº†å¯è¿ç§»çš„ agentic skillã€‚
2. âœ… **çœŸå®å™ªå£°åº”è¢«ä¸»åŠ¨å»ºæ¨¡**ï¼šè¢«åŠ¨é€‚åº”ä¸å¦‚ä¸»åŠ¨è®­ç»ƒï¼Œç³»ç»Ÿæ€§æ³¨å…¥å™ªå£°å¤§å¹…æå‡æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„å¯é æ€§ã€‚
3. âœ… **test-time scaling åº”åŒæ—¶æ‰©å±•å®½åº¦ä¸æ·±åº¦**ï¼šâ€œHeavy Thinkingâ€ æ¨¡å¼è¯æ˜ï¼Œ**å¹¶è¡Œæ¢ç´¢ + åæ€èšåˆ** æ˜¯è¶…è¶Šä¼ ç»Ÿæ¨ç†æ¨¡å¼çš„æœ‰æ•ˆè·¯å¾„ã€‚
4. âœ… **ç»Ÿä¸€ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶è‡³å…³é‡è¦**ï¼šä» pre-training åˆ° mid-training å†åˆ° post-trainingï¼Œæ•°æ®ã€ç®—æ³•ã€åŸºç¡€è®¾æ–½éœ€ååŒè®¾è®¡æ‰èƒ½æ”¯æ’‘å¤§è§„æ¨¡ agentic RLã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ç¡¬ä»¶é—¨æ§›é«˜**ï¼šè®­ç»ƒä¾èµ–å¤šè¾¾ 32,000 ä¸ªå¹¶å‘ç¯å¢ƒå’Œæ•°åƒåŠ é€Ÿå™¨ï¼Œæ™®é€šç ”ç©¶æœºæ„éš¾ä»¥å¤ç°ã€‚
- **Heavy Thinking å»¶è¿Ÿè¾ƒé«˜**ï¼šè™½ç„¶æ€§èƒ½å¼ºï¼Œä½†åœ¨å®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯ä¸‹å¯èƒ½å—é™ã€‚
- **éƒ¨åˆ†åˆæˆæ•°æ®å¯èƒ½å­˜åœ¨åå·®**ï¼šå°½ç®¡å¼ºè°ƒ executabilityï¼Œä½†ç¯å¢ƒä»æ˜¯æ¨¡æ‹Ÿç”Ÿæˆï¼Œæœªå¿…å®Œå…¨åæ˜ çœŸå®ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢æ›´é«˜æ•ˆçš„ **staleness-aware asynchronous training** ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æå‡ DORA æ¡†æ¶æ•ˆç‡ã€‚
- å°† **Zigzag Attention** æŠ€æœ¯æ¨å¹¿è‡³æ›´å¤šé•¿ä¸Šä¸‹æ–‡åœºæ™¯ï¼Œæ”¯æŒé«˜è¾¾ 1M tokens çš„æ¨ç†ã€‚
- å¼€æ”¾ **Random Complex Tasks** åˆ°å…¬å…±è¯„æµ‹å¹³å°ï¼Œæ¨åŠ¨ agentic generalization çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚
- ç ”ç©¶è½»é‡åŒ–ç‰ˆæœ¬ä»¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œä½¿ Heavy Thinking æ›´å…·å®ç”¨æ€§ã€‚

---

> ğŸ”— **é¡¹ç›®é“¾æ¥**ï¼š
> - **LongCat Chat**: https://longcat.ai  
> - **HuggingFace**: https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking-2601  
> - **GitHub**: https://github.com/meituan-longcat/LongCat-Flash-Thinking-2601

âœ… æ€»ç»“ï¼š**LongCat-Flash-Thinking-2601 æ˜¯ç›®å‰æœ€å¼ºå¤§çš„å¼€æº agentic reasoning æ¨¡å‹ä¹‹ä¸€ï¼Œä»£è¡¨äº†ä»â€œé™æ€æ¨ç†â€å‘â€œåŠ¨æ€äº¤äº’â€çš„é‡è¦è·ƒè¿ã€‚**

</details>

---

### 12. [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)

**Authors**: Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.18730v1  

#### Abstract
The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF)...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠReflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scaleã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **LLM alignment** æŠ€æœ¯ï¼ˆå¦‚ RLHFã€DPOï¼‰ä¾èµ–äºå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œå¾®è°ƒï¼ˆparameter fine-tuningï¼‰ï¼Œå­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š
- **è®¡ç®—æˆæœ¬é«˜**ï¼šéœ€è¦å¤§é‡è®­ç»ƒèµ„æºã€‚
- **ä¾èµ–äººç±»æ ‡æ³¨æ•°æ®**ï¼šéš¾ä»¥è·å–ä¸”æˆæœ¬é«˜æ˜‚ã€‚
- **ç¼ºä¹çµæ´»æ€§**ï¼šéš¾ä»¥åŠ¨æ€é€‚é…ä¸åŒæ–‡åŒ–ã€ä»·å€¼è§‚æˆ–éƒ¨ç½²åœºæ™¯ä¸‹çš„å¤šæ ·åŒ–åŸåˆ™ï¼ˆprinciplesï¼‰ã€‚
- **é»‘ç®±åŒ–å¥–åŠ±å»ºæ¨¡**ï¼šreward model ç¼ºä¹é€æ˜åº¦ï¼Œéš¾ä»¥è§£é‡Šã€‚

æ­¤å¤–ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨é¢å¯¹â€œå°¾éƒ¨é£é™©â€ï¼ˆrare but severe violationsï¼‰æ—¶è¡¨ç°ä¸ä½³ï¼Œè€Œè¿™äº›è¿è§„è¡Œä¸ºå¯èƒ½å¸¦æ¥ä¸¥é‡çš„å®‰å…¨åæœã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šREFLECT
æœ¬æ–‡æå‡º **REFLECT** â€”â€”ä¸€ç§**çº¯æ¨ç†æ—¶ï¼ˆinference-timeï¼‰** çš„å¯¹é½æ¡†æ¶ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒæˆ–æ•°æ®æ ‡æ³¨ï¼Œå³å¯å°†æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼ˆinstruction-tuned modelï¼‰åŠ¨æ€å¯¹é½åˆ°ä¸€ç»„è‡ªç„¶è¯­è¨€ä¹¦å†™çš„ **constitution**ï¼ˆä»·å€¼å¯¼å‘çš„åŸåˆ™é›†åˆï¼‰ã€‚

#### REFLECT çš„å››é˜¶æ®µæµç¨‹ï¼š
1. **Constitution-Conditioned Base Response (åˆå§‹ç”Ÿæˆ)**  
   åœ¨æç¤ºä¸­ä¼ å…¥å®Œæ•´çš„ constitution å’Œç”¨æˆ· queryï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆåˆæ­¥å“åº”ã€‚
2. **Self-Evaluation (è‡ªè¯„)**  
   æ¨¡å‹åŸºäº constitution ä¸­æ¯æ¡åŸåˆ™å¯¹è‡ªèº«è¾“å‡ºæ‰“åˆ†ï¼ˆ1â€“5 Likert scaleï¼‰ã€‚è‹¥ä»»ä¸€åŸåˆ™å¾—åˆ†ä½äºé˜ˆå€¼ï¼ˆé»˜è®¤ä¸º3ï¼‰ï¼Œåˆ™è§¦å‘åç»­ä¿®æ­£ã€‚
3. **Self-Critique (è‡ªæ‰¹åˆ¤)**  
   æ¨¡å‹åˆ†æä¸ºä½•æŸäº›åŸåˆ™æœªè¢«æ»¡è¶³ï¼Œå¹¶ç»™å‡ºå…·ä½“æ”¹è¿›å»ºè®®ã€‚
4. **Final Revision (æœ€ç»ˆä¿®è®¢)**  
   ç»“åˆæ‰¹åˆ¤å†…å®¹ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–åçš„å“åº”ã€‚

è¯¥è¿‡ç¨‹å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­å®Œæˆï¼ˆin-contextï¼‰ï¼Œä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ RLHF/DPOï¼‰ | REFLECT |
|------|------------------------|-------|
| æ˜¯å¦éœ€è¦è®­ç»ƒ | âœ… éœ€è¦ | âŒ ä¸éœ€è¦ |
| æ˜¯å¦éœ€è¦æ ‡æ³¨æ•°æ® | âœ… éœ€è¦åå¥½æ•°æ® | âŒ åªéœ€è‡ªç„¶è¯­è¨€åŸåˆ™ |
| åŠ¨æ€é€‚åº”èƒ½åŠ› | å¼±ï¼ˆå›ºå®šåéš¾å˜æ›´ï¼‰ | å¼ºï¼ˆéšæ—¶æ›´æ¢ constitutionï¼‰ |
| é€æ˜æ€§ | é»‘ç®± reward model | æ˜¾å¼è‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹ |
| å°¾éƒ¨è¿è§„æ§åˆ¶ | ä¸€èˆ¬ | æ˜¾è‘—é™ä½ä¸¥é‡è¿è§„ç‡ |
| å¯æ‰©å±•æ€§ | ä½ | æ”¯æŒç”Ÿæˆé«˜è´¨é‡ SFT/DPO æ•°æ®ç”¨äºåç»­å¾®è°ƒ |

REFLECT æ˜¯é¦–ä¸ªå°† **inference-time alignment**ã€**multi-principle reasoning** å’Œ **reusable supervision generation** ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸­çš„æ–¹æ³•ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **PKU-Safe-RLHF dataset**ï¼šå•è½®å¯¹è¯çº¢é˜Ÿæµ‹è¯•é›†ï¼Œå…±éšæœºé‡‡æ · 500 æ¡ promptã€‚
- **Anthropic HH-RLHF dataset**ï¼šå¤šè½®å¯¹è¯çº¢é˜Ÿæµ‹è¯•é›†ï¼ŒåŒæ ·é‡‡æ · 500 æ¡ã€‚
- **LMSYS-Chat-1M dataset**ï¼šç”¨äº self-finetuning å®éªŒï¼Œä»ä¸­æŠ½å– 2000 æ¡æ¨¡æ‹ŸçœŸå®èŠå¤©äº¤äº’ã€‚

### å®éªŒè®¾ç½®
#### æ¨¡å‹é€‰æ‹©
- å•†ä¸šå¤§æ¨¡å‹ï¼š
  - `GPT-4.1-Mini`
  - `Claude-Haiku-3.5`
- å¼€æºä¸­å°æ¨¡å‹ï¼š
  - `Mistral-7B-Instruct-v0.3`

è¯„ä¼°ä½¿ç”¨æ›´å¼ºçš„ `GPT-4.1` ä½œä¸º **LLM-as-a-judge** è¿›è¡Œæ‰“åˆ†ã€‚

#### Constitution è®¾è®¡ï¼ˆä¸¤å¤§æŒ‘æˆ˜æ€§åŸåˆ™é›†ï¼‰
| Constitution åç§° | åŒ…å«åŸåˆ™æ•° | ç‰¹ç‚¹ |
|------------------|-----------|------|
| **SafeRLHF Constitution** | 12 æ¡ | å¼ºè°ƒå¤šå…ƒè§†è§’ã€éè¥¿æ–¹å†å²ç±»æ¯”ã€è¯—æ„è¡¨è¾¾ã€é¿å…äººç±»ä¸­å¿ƒä¸»ä¹‰ç­‰ |
| **HH-RLHF Constitution** | 10 æ¡ | è¦æ±‚èå…¥ç¾å›½å†å²äº‹ä»¶ã€æœ¬åœ°ç¤¾åŒºèµ„æºã€ä¿æŒè¯­æ°”ä¸€è‡´ç­‰ |

> âš ï¸ æ‰€æœ‰åŸåˆ™å‡è®¾è®¡ä¸ºä¸æ¨¡å‹åŸå§‹è®­ç»ƒç›®æ ‡æ­£äº¤ç”šè‡³å†²çªï¼Œä»¥å¢åŠ ä»»åŠ¡éš¾åº¦ã€‚

#### è¯„ä¼°æŒ‡æ ‡
- **å¤šç›®æ ‡è¯„ä¼°ï¼ˆMulti-objective evaluationï¼‰**ï¼š
  - å¯¹æ¯æ¡ principle è¾“å‡ºä¸€ä¸ª 1â€“5 åˆ†çš„ Likert è¯„åˆ†ã€‚
  - å¹³å‡å¾—åˆ†è¶Šé«˜è¶Šå¥½ã€‚
- **åŸåˆ™è¿åç‡ï¼ˆPrinciple Violation Rateï¼‰**ï¼š
  - å¾—åˆ† â‰¤2 è§†ä¸ºâ€œè¿åâ€ï¼Œç»Ÿè®¡è¿åæ¯”ä¾‹ã€‚
  - å…³æ³¨å°¾éƒ¨é£é™©ç¼“è§£æ•ˆæœã€‚
- **äºŒåˆ†ç±»ä¸€è‡´æ€§éªŒè¯**ï¼š
  - å°†äººç±»è¯„ä¼°è€…ä¸ LLM judge çš„åˆ¤æ–­è¿›è¡Œå¯¹æ¯”ï¼ŒéªŒè¯å…¶å¯é æ€§ï¼ˆè§ Table 2ï¼‰ã€‚

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**ï¼šä»…ä½¿ç”¨ constitution-conditioned promptingï¼ˆå³ REFLECT çš„ç¬¬ä¸€æ­¥ï¼‰ã€‚
- **CA_SELFREFINE**ï¼šå¯¹ Self-Refine æ–¹æ³•è¿›è¡Œå®ªæ³•é€‚é…åçš„ç‰ˆæœ¬ï¼Œç”¨äºæ¯”è¾ƒ token å¼€é”€å’Œæ€§èƒ½ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 3ï¼‰

| Dataset | Model | Avg. Likert Score (CCBase) | Avg. Likert Score (REFLECT) | Î” | Violation Rate â†“ |
|--------|-------|----------------------------|------------------------------|----|----------------|
| SafeRLHF | GPT-4.1-Mini | 4.554 | **4.652** | +0.098 | 7.72% â†’ **3.45%** |
| SafeRLHF | Claude-Haiku-3.5 | 2.895 | **4.155** | +1.26 | 47.30% â†’ **13.26%** |
| SafeRLHF | Mistral-7B | 4.593 | **4.628** | +0.035 | 4.02% â†’ **2.92%** |
| HH-RLHF | GPT-4.1-Mini | 4.224 | **4.596** | +0.372 | 10.36% â†’ **2.00%** |
| HH-RLHF | Claude-Haiku-3.5 | 3.944 | **4.406** | +0.462 | 16.86% â†’ **6.00%** |
| HH-RLHF | Mistral-7B | 3.961 | **4.179** | +0.218 | 13.84% â†’ **7.66%** |

> âœ… æ‰€æœ‰æ”¹è¿›å‡é€šè¿‡ **paired bootstrap test** éªŒè¯ä¸ºç»Ÿè®¡æ˜¾è‘—ï¼ˆp < 0.002ï¼‰

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ
- **å¹³å‡å¾—åˆ†æå‡æ˜æ˜¾**ï¼Œå°¤å…¶å¯¹äºåŸæœ¬ alignment è¡¨ç°è¾ƒå·®çš„æ¨¡å‹ï¼ˆå¦‚ Claudeï¼‰ã€‚
- **ä¸¥é‡è¿è§„ç‡å¤§å¹…ä¸‹é™**ï¼š
  - æœ€é«˜è¾¾ **-34.04% ç»å¯¹é™å¹…**ï¼ˆClaude on SafeRLHFï¼‰ã€‚
  - å³ä½¿å¹³å‡åˆ†å˜åŒ–ä¸å¤§ï¼ˆå¦‚ GPT on SafeRLHF +0.098ï¼‰ï¼Œè¿è§„ç‡ä»ä¸‹é™è¶… 50%ï¼Œè¯´æ˜ REFLECT æœ‰æ•ˆæ•æ‰å¹¶ä¿®å¤äº†â€œè¾¹ç¼˜æ¡ˆä¾‹â€ã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰è·¨æ¨¡å‹è§„æ¨¡æœ‰æ•ˆæ€§ï¼ˆTable 8ï¼‰
åœ¨ GPT-4.1 å®¶æ—å†…è¿›è¡Œæ¶ˆèï¼ˆNano â†’ Mini â†’ Fullï¼‰ï¼š
- æ‰€æœ‰å°ºå¯¸ä¸‹ REFLECT å‡èƒ½æå‡ alignmentã€‚
- **æœ€å°æ¨¡å‹ GPT-4.1-Nano æå‡æœ€å¤§ï¼ˆ+0.51ï¼‰**ï¼Œè¡¨æ˜å°æ¨¡å‹æ›´å—ç›Šäº post-generation reasoningã€‚

#### ï¼ˆ2ï¼‰è®¡ç®—å¼€é”€åˆ†æï¼ˆTable 5ï¼‰
| æ–¹æ³• | Token å¼€é”€å€æ•°ï¼ˆç›¸å¯¹ baselineï¼‰ | è¿è§„ç‡ |
|------|-------------------------------|--------|
| REFLECT | 3.7Ã— ~ 5.0Ã— | æä½ |
| CA_SELFREFINE (Full) | 28Ã— ~ 58Ã— | ç•¥ä½ä½†ä»£ä»·æé«˜ |

ğŸ‘‰ REFLECT åœ¨å®ç°ç›¸è¿‘ alignment æ•ˆæœçš„åŒæ—¶ï¼Œtoken æ¶ˆè€—ä»…ä¸ºæ›¿ä»£æ–¹æ¡ˆçš„ **1/5 åˆ° 1/10**ï¼Œæ•ˆç‡ä¼˜åŠ¿æ˜¾è‘—ã€‚

#### ï¼ˆ3ï¼‰äº‹å®æ¨ç†èƒ½åŠ›ä¿ç•™ï¼ˆAppendix Table 9ï¼‰
åœ¨ **GSM8K** å’Œ **MMLU** ä¸Šæµ‹è¯• REFLECT æ˜¯å¦æŸå®³ factual reasoningï¼š
| Benchmark | Base | CCBase | REFLECT |
|----------|------|--------|--------|
| GSM8K (%) | 94.69 | 95.00 | **95.07** |
| MMLU (%) | 87.60 | 85.80 | 85.80 |

âœ… REFLECT **ä¸å½±å“æ•°å­¦æ¨ç†èƒ½åŠ›**ï¼Œç”šè‡³ç•¥æœ‰æå‡ï¼›å¯¹ MMLU å‡†ç¡®ç‡å½±å“å°äº 2 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### ï¼ˆ4ï¼‰ç”Ÿæˆ fine-tuning æ•°æ®çš„èƒ½åŠ›ï¼ˆTable 4ï¼‰
ä½¿ç”¨ REFLECT è¾“å‡ºç”Ÿæˆçš„ data å¯¹ GPT-4.1-Mini è¿›è¡Œå¾®è°ƒï¼š
| æ¨¡å‹ | å¹³å‡ Likert Score |
|------|------------------|
| Baseline | 1.68 |
| DPO-finetuned | 1.70 |
| **SFT-finetuned** | **4.60** |

ğŸ‘‰ SFT å¾®è°ƒæ¨¡å‹å‡ ä¹è¾¾åˆ°å®Œæ•´ REFLECT æµç¨‹çš„æ•ˆæœï¼Œè¯æ˜å…¶å¯é«˜æ•ˆç”¨äºé•¿æœŸéƒ¨ç½²é™æœ¬ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **REFLECT æ˜¾è‘—æå‡æ¨¡å‹å¯¹å¤æ‚åŸåˆ™çš„éµå®ˆç¨‹åº¦**ï¼Œå°¤å…¶æ˜¯åœ¨åŸæœ¬ alignment è¾ƒå¼±çš„æ¨¡å‹ä¸Šæ•ˆæœæœ€æ˜æ˜¾ã€‚
2. âœ… **ç‰¹åˆ«æ“…é•¿å‡å°‘ç½•è§ä½†ä¸¥é‡çš„è¿è§„è¡Œä¸ºï¼ˆtail-case violationsï¼‰**ï¼Œè¿™å¯¹å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚
3. âœ… **æ— éœ€è®­ç»ƒå³å¯ plug-and-play åœ°é€‚é…ä»»æ„ constitution**ï¼Œæ”¯æŒå¿«é€Ÿè¿­ä»£å’Œä¸ªæ€§åŒ–éƒ¨ç½²ã€‚
4. âœ… **æ¨ç†è¿‡ç¨‹é€æ˜å¯è§£é‡Š**ï¼šæä¾›å®Œæ•´çš„ reasoning traceï¼Œä¾¿äºå®¡è®¡å’Œè°ƒè¯•ã€‚
5. âœ… **å¯ä½œä¸ºæ•°æ®å¼•æ“**ï¼šè‡ªç„¶ç”Ÿæˆé«˜è´¨é‡çš„ SFT/DPO è®­ç»ƒæ ·æœ¬ï¼Œå½¢æˆâ€œæ¨ç†â†’æ•°æ®â†’å†è®­ç»ƒâ€çš„æ­£å‘å¾ªç¯ã€‚
6. âœ… **ä¸æŸå®³ factual reasoning èƒ½åŠ›**ï¼Œå…¼é¡¾ alignment ä¸ capabilityã€‚
7. âŒ **æ— æ³•ç”¨äºâ€œå»å¯¹é½â€æˆ– jailbreaking**ï¼šå³ä½¿è¾“å…¥â€œæœ‰å®³â€constitutionï¼Œæ¨¡å‹ä¹Ÿä¸ä¼šçœŸæ­£æ‰§è¡Œæœ‰å®³è¡Œä¸ºï¼ˆè§ Table 6ï¼‰ï¼Œè¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æ¨¡å‹è‡ªèº«çš„ self-evaluation èƒ½åŠ›**ï¼šå¯èƒ½å­˜åœ¨è¯¯åˆ¤ï¼ˆfalse positive/negativeï¼‰ï¼Œå¯¼è‡´ä¸å¿…è¦çš„ä¿®è®¢æˆ–æ¼ä¿®ã€‚
- **ä¸èƒ½ä¿è¯å®Œç¾åˆè§„**ï¼šä»æœ‰å¯èƒ½å¼•å…¥æ–°çš„åŸåˆ™è¿åã€‚
- **è¯„ä¼°æœ¬èº«ä¹Ÿæœ‰åå·®**ï¼šLLM-as-a-judge å¯èƒ½åå¥½æ˜¾å¼æåŠåŸåˆ™åç§°çš„å›ç­”ï¼Œè€Œéå®è´¨ç¬¦åˆã€‚
- **å°šæœªæ¢ç´¢æ‰€æœ‰ç±»å‹çš„å¯¹æŠ—æ€§åŸåˆ™**ï¼šæœªæ¥éœ€æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§çš„ constitutionã€‚
- **å¾®è°ƒåæ¨¡å‹åå‘å½±å“ REFLECT æ€§èƒ½**ï¼šåˆæ­¥å®éªŒæ˜¾ç¤ºï¼Œç”¨ REFLECT æ•°æ®å¾®è°ƒåçš„æ¨¡å‹ï¼Œåœ¨å†æ¬¡è¿è¡Œ REFLECT æ—¶å¯èƒ½å‡ºç°è´¨é‡ä¸‹é™ï¼Œå¯èƒ½æ˜¯å› æ¨¡ä»¿ç»“æ„è€Œéç†è§£åŸåˆ™æ‰€è‡´ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ¢ç´¢æ›´å¤æ‚çš„ **principle space**ï¼Œè¯†åˆ« REFLECT çš„è¾¹ç•Œå’Œè„†å¼±ç‚¹ã€‚
2. è®¾è®¡ä¸“é—¨é’ˆå¯¹ REFLECT çš„ **fine-tuning ç­–ç•¥**ï¼Œæ—¢æå‡é¢„ç”Ÿæˆè´¨é‡ï¼Œåˆä¸å‰Šå¼±åç»­ critique èƒ½åŠ›ã€‚
3. æå‡ **self-evaluation æ¨¡å—çš„å‡†ç¡®æ€§**ï¼Œä¾‹å¦‚å¼•å…¥å¤–éƒ¨ checker æˆ– multi-agent debateã€‚
4. ç ”ç©¶ REFLECT åœ¨ **adversarial attack åœºæ™¯ä¸‹çš„é˜²å¾¡èƒ½åŠ›**ã€‚
5. æ¢ç´¢å¦‚ä½•è®©æœ€ç»ˆç”¨æˆ·æŸ¥çœ‹å®Œæ•´çš„ **reasoning trace**ï¼Œå¢å¼ºä¿¡ä»»ä¸å¯æ§æ€§ã€‚

--- 

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **REFLECT æä¾›äº†ä¸€ç§è½»é‡ã€é€æ˜ã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„ inference-time alignment æ–°èŒƒå¼ï¼Œæ—¢èƒ½å³æ—¶æå‡æ¨¡å‹å¯¹å¤šæ ·åŒ–ä»·å€¼åŸåˆ™çš„éµå¾ªèƒ½åŠ›ï¼Œåˆèƒ½åå“ºè®­ç»ƒæ•°æ®ç”Ÿäº§ï¼Œæ˜¯è¿ˆå‘ pluralisticã€safe å’Œ interpretable AI çš„é‡è¦ä¸€æ­¥ã€‚**

</details>

---

### 13. [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)

**Authors**: Zizhao Zhang, Yihan Xue, Haotian Zhu, Sijia Li, Zhijun Wang, Yujie Xiao  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.17774v1  

#### Abstract
Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communica...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡ã€ŠCondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensationã€‹æ ¸å¿ƒæ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
åˆ†å¸ƒå¼å›¾ç¥ç»ç½‘ç»œï¼ˆ**Distributed GNN**ï¼‰è®­ç»ƒé¢ä¸´ä¸¥é‡çš„**é€šä¿¡ç“¶é¢ˆ**ï¼Œä¸»è¦æºäºå›¾ç»“æ„æ•°æ®ä¸­çš„**é‚»å±…ä¾èµ–æ€§**ï¼ˆneighborhood dependencyï¼‰ã€‚å½“å›¾è¢«åˆ’åˆ†åˆ°å¤šä¸ªè®¡ç®—èŠ‚ç‚¹ï¼ˆworkersï¼‰æ—¶ï¼Œè·¨åˆ†åŒºçš„è¾¹ç•ŒèŠ‚ç‚¹ï¼ˆboundary nodesï¼‰éœ€è¦é¢‘ç¹äº¤æ¢ç‰¹å¾ä¿¡æ¯ï¼Œå¯¼è‡´é€šä¿¡å¼€é”€å è®­ç»ƒæ€»æ—¶é—´çš„ **50â€“90%**ã€‚è¿™ä¸€â€œ**é‚»å±…çˆ†ç‚¸**â€ï¼ˆneighbor explosionï¼‰é—®é¢˜ä¸¥é‡é™åˆ¶äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§ã€‚

ç°æœ‰æ–¹æ³•å¦‚é™æ€å›¾åˆ’åˆ†ï¼ˆå¦‚METISï¼‰æ— æ³•é€‚åº”åŠ¨æ€ç½‘ç»œæ¡ä»¶ï¼Œè€Œé€šç”¨æ¢¯åº¦å‹ç¼©æŠ€æœ¯æœªèƒ½å……åˆ†åˆ©ç”¨å›¾æ•°æ®çš„ç»“æ„ç‰¹æ€§ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
è®ºæ–‡æå‡ºäº† **CondenseGraph**ï¼Œä¸€ç§å…¨æ–°çš„ã€é€šä¿¡é«˜æ•ˆçš„åˆ†å¸ƒå¼ GNN è®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **On-the-Fly Graph Condensationï¼ˆè¿è¡Œæ—¶å›¾å‹ç¼©ï¼‰**  
  åœ¨é€šä¿¡å‰ï¼Œå°†æ¯ä¸ªè¾¹ç•ŒèŠ‚ç‚¹ç»„ï¼ˆboundary node groupï¼‰çš„ç‰¹å¾åŠ¨æ€å‹ç¼©ä¸ºä¸€ä¸ªç´§å‡‘çš„â€œ**super node**â€ï¼ˆè¶…èŠ‚ç‚¹ï¼‰ï¼Œæ˜¾è‘—å‡å°‘ä¼ è¾“æ•°æ®é‡ã€‚è¯¥è¿‡ç¨‹æ˜¯è½»é‡çº§ã€å®æ—¶æ‰§è¡Œçš„ï¼Œä¸åŒäºç¦»çº¿å›¾è’¸é¦ï¼ˆgraph distillationï¼‰æ–¹æ³•ã€‚

- **Gradient-based Error Compensationï¼ˆåŸºäºæ¢¯åº¦çš„è¯¯å·®è¡¥å¿æœºåˆ¶ï¼‰**  
  ä¸ºç¼“è§£å‹ç¼©å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œå¼•å…¥è¯¯å·®ç´¯ç§¯å™¨ï¼ˆerror accumulatorï¼‰ï¼Œå°†æœªæˆåŠŸä¼ è¾“çš„å‹ç¼©è¯¯å·®åé¦ˆåˆ°åç»­è¿­ä»£ä¸­ï¼Œç¡®ä¿é‡è¦æ¢¯åº¦ä¿¡æ¯ä¸ä¸¢å¤±ï¼Œç»´æŒæ¨¡å‹æ”¶æ•›æ€§ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
- **åŠ¨æ€é€‚åº”æ€§**ï¼šä¸åŒäºé™æ€å›¾åˆ’åˆ†ç­–ç•¥ï¼ŒCondenseGraph èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å‹ç¼©è¡Œä¸ºï¼Œé€‚åº”å¸¦å®½æ³¢åŠ¨ç­‰ç½‘ç»œå˜åŒ–ã€‚
- **ç»“æ„æ„ŸçŸ¥å‹ç¼©**ï¼šåˆ©ç”¨å›¾æ•°æ®çš„åŒè´¨æ€§ï¼ˆhomophilyï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§è¿›è¡Œç‰¹å¾èšåˆï¼Œæ¯”é€šç”¨æ¢¯åº¦å‹ç¼©æ›´é«˜æ•ˆã€‚
- **ä½å¼€é”€é«˜æ”¶ç›Š**ï¼šä»…å¢åŠ å°‘é‡è®¡ç®—ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œå³å¯å®ç°æ˜¾è‘—é€šä¿¡èŠ‚çœï¼Œä¸”ç²¾åº¦æŸå¤±æå°ã€‚
- **å…¼å®¹æ€§å¼º**ï¼šå¯ä¸ç°æœ‰å›¾åˆ’åˆ†ã€é‡åŒ–ç­‰æŠ€æœ¯ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å®éªŒåœ¨å››ä¸ªæ ‡å‡†å›¾å­¦ä¹ æ•°æ®é›†ä¸Šè¿›è¡Œï¼š
| æ•°æ®é›† | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç‰¹å¾ç»´åº¦ | ç±»åˆ«æ•° |
|--------|--------|------|----------|--------|
| **Reddit** | 232,965 | 114.6M | 602 | 41 |
| **ogbn-arxiv** | 169,343 | 1.17M | 128 | 40 |
| **ogbn-products** | 2.45M | 61.9M | 100 | 47 |
| **Flickr** | 89,250 | 899,756 | 500 | 7 |

> æ³¨ï¼šReddit ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬ï¼ˆçº¦1.14äº¿è¾¹ï¼‰ï¼ŒåŒºåˆ«äºéƒ¨åˆ†æ—©æœŸç ”ç©¶ä½¿ç”¨çš„ç®€åŒ–ç‰ˆã€‚

---

### **å®éªŒè®¾ç½®**
- **æ¡†æ¶å®ç°**ï¼šåŸºäº **DistDGL** æ„å»ºï¼Œä½¿ç”¨ 8 ä¸ª workerï¼Œæ¯ä¸ªé…å¤‡ NVIDIA V100 GPUã€‚
- **æ¨¡å‹æ¶æ„**ï¼š2å±‚ **GraphSAGE**ï¼Œéšè—å±‚ç»´åº¦ 256ã€‚
- **å‹ç¼©æ¯”ä¾‹**ï¼šé»˜è®¤è®¾ç½®ä¸º 0.5ï¼ˆå³é€šä¿¡é‡å‡å°‘ 50%ï¼‰ã€‚
- **Condensation æ–¹æ³•å¯¹æ¯”**ï¼š
  - Mean Condensationï¼ˆå‡å€¼å‹ç¼©ï¼‰
  - Weighted Condensationï¼ˆåŠ æƒå‹ç¼©ï¼ŒæŒ‰èŠ‚ç‚¹åº¦åŠ æƒï¼‰
  - Attention-based Condensationï¼ˆæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æƒé‡ï¼‰

---

### **è¯„ä¼°æŒ‡æ ‡**
- **é€šä¿¡å¼€é”€**ï¼ˆCommunication Volumeï¼‰ï¼šæ¯è½®è®­ç»ƒä¼ è¾“çš„æ•°æ®æ€»é‡ï¼ˆGBï¼‰ã€‚
- **è®­ç»ƒæ—¶é—´åˆ†è§£**ï¼šé€šä¿¡æ—¶é—´ vs. è®¡ç®—æ—¶é—´å æ¯”ã€‚
- **æ¨¡å‹å‡†ç¡®ç‡**ï¼ˆTest Accuracyï¼‰ï¼šä¸å…¨ç²¾åº¦åŸºçº¿å¯¹æ¯”ã€‚
- **æ”¶æ•›é€Ÿåº¦**ï¼šè®­ç»ƒæ›²çº¿å¯¹æ¯”ã€‚
- **æ¶ˆèå®éªŒ**ï¼šéªŒè¯è¯¯å·®è¡¥å¿æœºåˆ¶ã€å‹ç¼©æ¯”ã€ä¸åŒ condensation æ–¹æ³•çš„å½±å“ã€‚

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | ç®€ä»‹ |
|------|------|
| **Baseline** | æ ‡å‡†åˆ†å¸ƒå¼ GNN è®­ç»ƒï¼Œå…¨ç²¾åº¦é€šä¿¡ |
| **AdaQP** [4] | è‡ªé€‚åº”é‡åŒ– + å¹¶è¡ŒåŒ– |
| **BNS-GCN** | è¾¹ç•ŒèŠ‚ç‚¹é‡‡æ ·ï¼ˆBoundary Node Samplingï¼‰ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **é€šä¿¡æ•ˆç‡æå‡**
- **é€šä¿¡é‡å‡å°‘ 40â€“60%**ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä¸€è‡´æœ‰æ•ˆã€‚
- åœ¨ **ogbn-products** ä¸Šï¼Œæ¯è½®é€šä¿¡ä» **15.2 GB é™è‡³ 6.2 GB**ï¼Œé™å¹…è¾¾ **59%**ã€‚
- é€šä¿¡æ—¶é—´å æ¯”ä»å¹³å‡ **55% é™è‡³ 22%**ï¼Œæ˜¾è‘—ç¼“è§£é€šä¿¡ç“¶é¢ˆã€‚

#### âœ… **æ¨¡å‹å‡†ç¡®ç‡ä¿æŒ**
| æ–¹æ³• | Reddit | arxiv | products | Flickr |
|------|--------|-------|----------|--------|
| Baseline | 96.2% | 71.5% | 78.3% | 51.8% |
| **Ours (attention)** | **96.1%** | **71.4%** | **78.1%** | **51.6%** |

- æ‰€æœ‰å˜ä½“ç²¾åº¦æŸå¤±å‡åœ¨ **0.5% ä»¥å†…**ï¼Œå…¶ä¸­ **attention-based condensation** è¡¨ç°æœ€ä¼˜ã€‚
- æ˜¾è‘—ä¼˜äº AdaQP å’Œ BNS-GCNï¼Œåœ¨ç›¸åŒå‹ç¼©æ°´å¹³ä¸‹ç²¾åº¦æ›´é«˜ã€‚

#### âœ… **æ”¶æ•›æ€§åˆ†æ**
- æ”¶æ•›é€Ÿåº¦ä¸ Baseline å‡ ä¹ä¸€è‡´ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æã€‚
- **æ— è¯¯å·®è¡¥å¿æ—¶**ï¼Œåœ¨ 50% å‹ç¼©æ¯”ä¸‹ç²¾åº¦ä¸‹é™ **2â€“3%**ï¼Œè¯æ˜è¯¯å·®åé¦ˆæœºåˆ¶è‡³å…³é‡è¦ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

| å®éªŒé¡¹ | å‘ç° |
|--------|------|
| **å‹ç¼©æ¯”å½±å“**ï¼ˆ10% â†’ 60%ï¼‰ | åœ¨ 40â€“60% å‹ç¼©èŒƒå›´å†…ï¼Œç²¾åº¦æŸå¤± <1%ï¼›è¶…è¿‡ 60% åæ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼Œå»ºè®®æ“ä½œèŒƒå›´ä¸º 40â€“60%ã€‚ |
| **è¯¯å·®è¡¥å¿æœºåˆ¶** | ç§»é™¤ååœ¨ ogbn-products ä¸Šç²¾åº¦ä¸‹é™ **2.1%**ï¼Œä½†è¯¯å·®å­˜å‚¨å¼€é”€ <1%ï¼Œæ€§ä»·æ¯”æé«˜ã€‚ |
| **Condensation æ–¹æ³•å¯¹æ¯”** | - Attention-basedï¼šç²¾åº¦æœ€é«˜ï¼ˆ+0.2â€“0.3%ï¼‰ï¼Œä½†è®¡ç®—å¼€é”€ +5%<br>- Weightedï¼šç²¾åº¦ä¸ attention æ¥è¿‘ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œæ¨èä½œä¸ºé»˜è®¤é€‰æ‹©<br>- Meanï¼šæœ€ç®€å•ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯ |

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **è¾¹ç•ŒèŠ‚ç‚¹ç‰¹å¾å…·æœ‰é«˜åº¦å†—ä½™æ€§**ï¼Œå¯é€šè¿‡ç»“æ„æ„ŸçŸ¥çš„ condensation æœ‰æ•ˆå‹ç¼©ã€‚
2. **On-the-fly graph condensation** æ˜¯ä¸€ç§é«˜æ•ˆã€ä½å»¶è¿Ÿçš„é€šä¿¡ä¼˜åŒ–æ‰‹æ®µï¼Œé€‚ç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼ GNN è®­ç»ƒã€‚
3. **Gradient-based error compensation** æˆåŠŸå¼¥è¡¥äº†å‹ç¼©å¸¦æ¥çš„åå·®ï¼Œä¿éšœäº†æ¨¡å‹æ”¶æ•›æ€§å’Œæœ€ç»ˆç²¾åº¦ã€‚
4. CondenseGraph åœ¨ **40â€“60% é€šä¿¡å‰Šå‡** ä¸‹ä»èƒ½ä¿æŒä¸å…¨ç²¾åº¦è®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å‹ç¼©ä¸é‡‡æ ·æ–¹æ³•ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
1. **åˆ†ç»„ç­–ç•¥ä¾èµ–å¯å‘å¼è§„åˆ™**ï¼šå½“å‰è¾¹ç•ŒèŠ‚ç‚¹åˆ†ç»„åŸºäºç»“æ„é‚»è¿‘æ€§å’Œç‰¹å¾ç›¸ä¼¼æ€§ï¼Œå°šæœªå¼•å…¥å¯å­¦ä¹ çš„èšç±»æœºåˆ¶ï¼Œå¯èƒ½éæœ€ä¼˜ã€‚
2. **å¯¹å¼‚æ„å›¾æ”¯æŒæœ‰é™**ï¼šæœªè€ƒè™‘å¤šç±»å‹èŠ‚ç‚¹æˆ–è¾¹çš„å¤æ‚å›¾ç»“æ„ï¼ˆheterogeneous graphsï¼‰ã€‚
3. **GNN æ¶æ„æ³›åŒ–æ€§å¾…éªŒè¯**ï¼šå®éªŒä¸»è¦åŸºäº GraphSAGEï¼Œå…¶ä»– GNN æ¶æ„ï¼ˆå¦‚ GATã€GCN-IIï¼‰ä¸Šçš„è¡¨ç°éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **å­¦ä¹ å‹åˆ†ç»„ç­–ç•¥**ï¼šè®¾è®¡å¯è®­ç»ƒçš„æ¨¡å—æ¥è‡ªé€‚åº”åœ°åˆ’åˆ†è¾¹ç•ŒèŠ‚ç‚¹ç»„ã€‚
2. **æ‰©å±•è‡³å¼‚æ„å›¾**ï¼šæ”¯æŒå¤šæ¨¡æ€ã€å¤šå…³ç³»å›¾æ•°æ®çš„å‹ç¼©ä¸é€šä¿¡ä¼˜åŒ–ã€‚
3. **ä¸å…¶ä»–æŠ€æœ¯èåˆ**ï¼šä¸å›¾åˆ’åˆ†ï¼ˆgraph partitioningï¼‰ã€æ¢¯åº¦é‡åŒ–ï¼ˆgradient quantizationï¼‰ã€ç¨€ç–é€šä¿¡ç­‰ç»“åˆï¼Œæ„å»ºç«¯åˆ°ç«¯é«˜æ•ˆè®­ç»ƒç³»ç»Ÿã€‚
4. **åŠ¨æ€å‹ç¼©è°ƒåº¦**ï¼šæ ¹æ®ç½‘ç»œçŠ¶æ€ã€è´Ÿè½½æƒ…å†µåŠ¨æ€è°ƒæ•´å‹ç¼©æ¯”ä¸ condensation æ–¹å¼ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **CondenseGraph é€šè¿‡è¿è¡Œæ—¶å›¾å‹ç¼© + è¯¯å·®åé¦ˆæœºåˆ¶ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±ç²¾åº¦çš„å‰æä¸‹ï¼Œå®ç°äº† 40â€“60% çš„é€šä¿¡å‰Šå‡ï¼Œä¸ºå¤§è§„æ¨¡åˆ†å¸ƒå¼ GNN è®­ç»ƒæä¾›äº†é«˜æ•ˆã€å®ç”¨çš„æ–°èŒƒå¼ã€‚**

</details>

---

### 14. [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)

**Authors**: Byeongju Kim, Jungwan Lee, Donghyeon Han, Hoi-Jun Yoo, Sangyeob Kim  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.17063v1  

#### Abstract
Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ MoE æ¨ç†ç³»ç»Ÿï¼ˆå¦‚ Fiddlerã€DAOPï¼‰ä¾èµ–äºå°†ä¸“å®¶æ¨¡å‹å­˜å‚¨åœ¨ DRAM ä¸­è¿›è¡Œ offloadingï¼Œä½†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œç”±äºå†…å­˜å—é™ï¼ˆé€šå¸¸ä»… 16â€“64GBï¼‰ï¼Œæ— æ³•å®¹çº³æ•°ç™¾ GB çš„ MoE æ¨¡å‹æƒé‡ã€‚å› æ­¤ï¼Œ**ä¼ ç»Ÿ DRAM-based offloading ä¸é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ç¯å¢ƒ**ã€‚

æ­¤å¤–ï¼Œç°æœ‰ç¼“å­˜ç­–ç•¥ï¼ˆå¦‚ LRUã€LFUï¼‰æœªèƒ½å……åˆ†æ•æ‰ MoE æ¨¡å‹ä¸­ä¸“å®¶çš„è·¯ç”±ç‰¹æ€§ï¼ˆtemporal locality + routing-aware reuse patternsï¼‰ï¼Œå¯¼è‡´é¢‘ç¹çš„ SSD I/O å’Œæ¨ç†å»¶è¿Ÿã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹

1. **FlashMoE ç³»ç»Ÿæ¶æ„è®¾è®¡**
   - å°† MoE æ¨¡å‹ä¸­çš„ **experts ä¸ non-experts åˆ†ç¦»å­˜å‚¨**ï¼Œnon-expertsï¼ˆattentionã€normalizationã€gatingï¼‰åœ¨åˆå§‹åŒ–æ—¶åŠ è½½è‡³å†…å­˜ï¼Œè€Œ experts æŒ‰éœ€ä» SSD åŠ¨æ€åŠ è½½ã€‚
   - å®ç°ç»†ç²’åº¦çš„ **layer-level ä¸ unit-level ä¸“å®¶åˆ’åˆ†**ï¼Œæ¯ä¸ª expert å­˜å‚¨ä¸ºç‹¬ç«‹ `.pt` æ–‡ä»¶ï¼Œæ”¯æŒ on-demand åŠ è½½ã€‚

2. **åŸºäºæœºå™¨å­¦ä¹ çš„ç¼“å­˜æ›¿æ¢ç­–ç•¥ï¼ˆML-Based Cache Replacement Policyï¼‰**
   - è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§ feed-forward networkï¼ˆFFNï¼‰æ¥é¢„æµ‹ä¸“å®¶çš„â€œä¸‹ä¸€æ¬¡ä½¿ç”¨è·ç¦»â€ï¼ˆnext use distanceï¼‰ï¼Œç»“åˆ **recencyï¼ˆç±»ä¼¼ LRUï¼‰ä¸ frequencyï¼ˆç±»ä¼¼ LFUï¼‰ä¿¡å·**ï¼Œå®ç°æ›´ä¼˜çš„ eviction å†³ç­–ã€‚
   - åˆ©ç”¨ Belady æœ€ä¼˜ç®—æ³•ç”Ÿæˆ oracle label è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œé€¼è¿‘ç†è®ºæœ€ä¼˜æ›¿æ¢ç­–ç•¥ã€‚

3. **å¼‚æ­¥ç¼“å­˜ç®¡ç†æœºåˆ¶**
   - åœ¨ decoding é˜¶æ®µï¼Œå°† cache replacement æ“ä½œä¸ SSD åŠ è½½è¿‡ç¨‹é‡å ï¼ˆoverlapï¼‰ï¼Œéšè—è®¡ç®—å¼€é”€ï¼Œä½¿å¾—é«˜å¤æ‚åº¦çš„ ML ç¼“å­˜ç­–ç•¥ä»å¯å®ç”¨åŒ–ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | FlashMoE | ä¼ ç»Ÿæ–¹æ³•ï¼ˆFiddler / DAOPï¼‰ |
|------|----------|-----------------------------|
| å†…å­˜å±‚çº§ | æ”¯æŒ SSD offloading | ä»…æ”¯æŒ DRAM offloading |
| ç¼“å­˜ç­–ç•¥ | ML-drivenï¼Œèåˆ recency & frequency | åŸºäº LRU/LFU ç­‰å¯å‘å¼è§„åˆ™ |
| åˆå§‹åŒ–å¼€é”€ | æä½ï¼ˆåªåŠ è½½ non-expertsï¼‰ | é«˜ï¼ˆéœ€é¢„åŠ è½½å…¨éƒ¨æ¨¡å‹ï¼‰ |
| å®é™…éƒ¨ç½²å¯è¡Œæ€§ | å¯ç”¨äºæ¶ˆè´¹çº§ PC/è¾¹ç¼˜è®¾å¤‡ | å¯¹å†…å­˜è¦æ±‚é«˜ï¼Œéš¾ä»¥éƒ¨ç½² |

> âœ… FlashMoE æˆåŠŸå®ç°äº†åœ¨ **16GB RAM + NVMe SSD** çš„æ¡Œé¢å¹³å°ä¸Šè¿è¡Œæ•°å GB è§„æ¨¡çš„ MoE æ¨¡å‹ï¼Œçªç ´äº†ä»¥å¾€â€œMoE æ— æ³•åœ¨è¾¹ç¼˜è®¾å¤‡è¿è¡Œâ€çš„é™åˆ¶ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **TriviaQA [7]**ï¼šç”¨äºè®­ç»ƒ ML ç¼“å­˜æ¨¡å‹å’Œè¯„ä¼°æ¨ç†æ€§èƒ½ã€‚
  - ä½¿ç”¨å…¶è®­ç»ƒå­é›†æå– routing traceï¼›
  - æµ‹è¯•é˜¶æ®µä½¿ç”¨æœªå‚ä¸è®­ç»ƒçš„æ•°æ®åˆ†å‰²ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚

### âš™ï¸ å®éªŒå¹³å°é…ç½®ï¼ˆè§ Table 2ï¼‰
| ç»„ä»¶ | è§„æ ¼ |
|------|------|
| CPU | AMD Ryzen 9 9600X (Zen 5) |
| GPU | NVIDIA RTX 5070 Ti (16GB GDDR7) |
| SSD | SK hynix P51 NVMe (PCIe 5.0, 7.4 GB/s read) |
| RAM | 16GB Ã—4 DDR5-6000MHz (å…± 64GBï¼Œä¿ç•™ ~1GB å¯ç”¨) |
| OS/ç¯å¢ƒ | Ubuntu 24.04, Python 3.11, PyTorch 2.7, CUDA 12.9 |

> æ³¨ï¼šè¯¥å¹³å°ä»£è¡¨å…¸å‹çš„é«˜æ€§èƒ½æ¶ˆè´¹çº§ PCï¼Œå…·å¤‡ç°å®éƒ¨ç½²æ„ä¹‰ã€‚

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Cache Hit Rate** | è¡¡é‡ç¼“å­˜æ•ˆç‡ï¼Œè¶Šé«˜è¶Šå¥½ |
| **Model Loading Time** | åˆå§‹åŒ–é˜¶æ®µè€—æ—¶ |
| **Prefill Latency** | ä¸Šä¸‹æ–‡å¡«å……é˜¶æ®µæ€»å»¶è¿Ÿ |
| **Token Generation Speed (Throughput)** | è§£ç é˜¶æ®µæ¯ç§’ç”Ÿæˆ token æ•° |
| **End-to-End Inference Latency** | å®Œæ•´æ¨ç†æ—¶é—´ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **llama.cpp**ï¼šé€šç”¨ LLM æ¨ç†æ¡†æ¶ï¼Œä½œä¸ºåŸºç¡€å‚è€ƒ
- **Fiddler [8]**ï¼šCPU-GPU ååŒè°ƒåº¦ MoE æ¨ç†ç³»ç»Ÿ
- **DAOP [13]**ï¼šæ•°æ®æ„ŸçŸ¥ + é¢„è®¡ç®—çš„ MoE offloading æ–¹æ³•
- **ä¼ ç»Ÿç¼“å­˜ç­–ç•¥**ï¼šLRUã€LFUã€LIFOã€ARCã€LeCaRï¼ˆå­¦ä¹ å‹ç¼“å­˜ï¼‰

æ‰€æœ‰åŸºçº¿å‡é…ç½®ä¸ºä¼˜å…ˆåŠ è½½â€œpopular expertsâ€ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰ç¼“å­˜å‘½ä¸­ç‡æå‡æ˜¾è‘—ï¼ˆå›¾ 6ï¼‰
| æ¨¡å‹ | æ–¹æ³• | Cache Hit Rate æå‡ vs LRU |
|------|------|----------------------------|
| OLMoE-1B-7B | FlashMoE (ML Cache) | â†‘ **51%** |
| Qwen3-30B-A3B | FlashMoE (ML Cache) | â†‘ **47%** |

> åŒæ—¶ä¼˜äº LFUï¼ˆâ†‘35â€“51%ï¼‰ã€ARCï¼ˆâ†‘28%ï¼‰ã€LeCaRï¼ˆâ†‘21%ï¼‰

â¡ï¸ **æ„å‘³ç€ I/O å‡å°‘çº¦ 22â€“35%**

---

#### ï¼ˆ2ï¼‰æ¨ç†ååå¤§å¹…æå‡ï¼ˆå›¾ 7e/fï¼‰
| æ¨¡å‹ | æ–¹æ³• | ç›¸å¯¹ LRU çš„åŠ é€Ÿæ¯” |
|------|------|--------------------|
| OLMoE-1B-7B | FlashMoE ML Cache | **1.22Ã—** |
| Qwen3-30B-A3B | FlashMoE ML Cache | **1.07Ã—** |

> è™½ç„¶ç»å¯¹å¢ç›Šéšæ¨¡å‹è§„æ¨¡å¢å¤§ç•¥æœ‰ä¸‹é™ï¼Œä½†ä»ä¿æŒæ­£å‘æ”¶ç›Šã€‚

---

#### ï¼ˆ3ï¼‰ç«¯åˆ°ç«¯ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”ï¼ˆå›¾ 7c/fï¼‰
| æŒ‡æ ‡ | FlashMoE vs Fiddler/DAOP |
|------|-------------------------|
| åˆå§‹åŠ è½½é€Ÿåº¦ | **6.8Ã— æ›´å¿«** |
| Prefill æ€»å»¶è¿Ÿ | **4.1Ã— æ›´å¿«** |
| è§£ç åå | **æœ€é«˜è¾¾ 2.6Ã— speedup** |

> å³ä½¿è€ƒè™‘ SSD I/O å¼€é”€ï¼ŒFlashMoE ä¾ç„¶å…¨é¢é¢†å…ˆã€‚

---

#### ï¼ˆ4ï¼‰æ¶ˆèå®éªŒä¸åˆ†æ
- **LRU vs Belady åˆ†æï¼ˆå›¾ 2ï¼‰**ï¼š
  - LRU å­˜åœ¨ä¸¤å¤§ç¼ºé™·ï¼š
    1. **Eviction Delay**ï¼šåº”è¢«æ›¿æ¢çš„ä¸“å®¶å»¶è¿Ÿ 4â€“5 æ­¥æ‰è¢«æ·˜æ±°ï¼›
    2. **Evicting Hot Experts**ï¼šè¿‘æœŸæœªè®¿é—®ä½†é«˜é¢‘ä½¿ç”¨çš„ä¸“å®¶è¢«é”™è¯¯æ·˜æ±°ã€‚
  - LRU çš„ re-fetch rate è¾¾ **34.2%**ï¼Œè¿œé«˜äº Belady çš„ **0.1%**ï¼Œè¯´æ˜å¤§é‡æœ¬ä¸è¯¥æ·˜æ±°çš„ä¸“å®¶è¢«é‡æ–°åŠ è½½ã€‚

- **Recency ä¸ Frequency èåˆå¿…è¦æ€§**ï¼š
  - åœ¨ Qwen3-30B-A3B ä¸Šï¼ŒLRU å’Œ LFU å„è‡ªåšå‡ºâ€œæœ€ä¼˜é€‰æ‹©â€çš„æ¯”ä¾‹åˆ†åˆ«ä¸º 56% å’Œ 44%ï¼Œè¡¨æ˜ä¸¤è€…äº’è¡¥æ€§å¼ºã€‚
  - FlashMoE çš„ ML æ¨¡å‹æˆåŠŸèåˆäºŒè€…ä¼˜åŠ¿ï¼Œå®ç°æ›´ç²¾å‡†å†³ç­–ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **SSD-based offloading æ˜¯ MoE è¾¹ç¼˜éƒ¨ç½²çš„å…³é”®è·¯å¾„**
   - å½“å‰ MoE æ¨¡å‹å·²è¾¾æ•°ç™¾ GBï¼ŒDRAM å·²ä¸è¶³ä»¥æ‰¿è½½ inactive expertsï¼›
   - FlashMoE è¯æ˜é€šè¿‡åˆç†ç»„ç»‡æ–‡ä»¶ç»“æ„ + é«˜æ•ˆç¼“å­˜ç­–ç•¥ï¼Œå¯åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®Œæˆå¤§è§„æ¨¡ MoE æ¨ç†ã€‚

2. **ä¼ ç»Ÿç¼“å­˜ç­–ç•¥ä¸é€‚ç”¨äº MoE æ¨ç†åœºæ™¯**
   - LRU/LFU å¿½è§†äº†ä¸“å®¶è·¯ç”±çš„æ—¶é—´æ¨¡å¼å’Œé¢‘ç‡ç‰¹å¾ï¼›
   - å•çº¯ä¾èµ– recency æˆ– frequency å‡ä¼šå¯¼è‡´æ¬¡ä¼˜ eviction å†³ç­–ã€‚

3. **è½»é‡çº§ ML ç¼“å­˜ç­–ç•¥æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**
   - å°½ç®¡ SSD åŠ è½½å»¶è¿Ÿé«˜ï¼ˆ~3msï¼‰ï¼Œä½† FFN è®¡ç®—æå¿«ï¼ˆ~0.1msï¼‰ï¼Œå¯é€šè¿‡å¼‚æ­¥æ‰§è¡Œå®Œå…¨æ©ç›–ï¼›
   - æ¨¡å‹å¤§å°ä»… ~113KBï¼Œé€‚åˆ per-layer éƒ¨ç½²ï¼Œè®­ç»ƒæ—¶é—´ <2 å°æ—¶ã€‚

4. **FlashMoE æ˜¾è‘—é™ä½ç³»ç»Ÿå¯åŠ¨æ—¶é—´å’Œå†…å­˜å ç”¨**
   - ä»…åŠ è½½ non-expertsï¼ˆ<2GBï¼‰ï¼Œå¤§å¹…ç¼©çŸ­åˆå§‹åŒ–æ—¶é—´ï¼›
   - æ”¯æŒåŠ¨æ€æ‰©å±• cache sizeï¼Œçµæ´»é€‚é…ä¸åŒ VRAM å®¹é‡ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ–é«˜è´¨é‡ routing trace è¿›è¡Œè®­ç»ƒ**
   - å½“å‰ ML ç¼“å­˜æ¨¡å‹åŸºäº TriviaQA æ•°æ®åˆ†å¸ƒè®­ç»ƒï¼Œå¯èƒ½åœ¨å…¶ä»– domainï¼ˆå¦‚ä»£ç ã€æ•°å­¦ï¼‰è¡¨ç°ä¸‹é™ï¼›
   - éœ€è¦åœ¨çº¿å¾®è°ƒæˆ–é¢†åŸŸè‡ªé€‚åº”æœºåˆ¶åº”å¯¹åˆ†å¸ƒåç§»ã€‚

2. **æœªè§£å†³å¤šç”¨æˆ·å¹¶å‘åœºæ™¯ä¸‹çš„ç¼“å­˜ç«äº‰é—®é¢˜**
   - å¤šä»»åŠ¡å…±äº« SSD å’Œç¼“å­˜èµ„æºæ—¶å¯èƒ½å‡ºç°å¹²æ‰°ï¼›
   - ç¼ºä¹ QoS æ§åˆ¶æˆ–ä¼˜å…ˆçº§è°ƒåº¦æœºåˆ¶ã€‚

3. **SSD å¯¿å‘½ä¸å†™å…¥æ”¾å¤§é£é™©**
   - è™½ç„¶ä¸»è¦ä¸ºè¯»æ“ä½œï¼Œä½†é¢‘ç¹åŠ è½½ä»å¯èƒ½å½±å“ NAND è€ä¹…æ€§ï¼›
   - æœªè®¨è®º wear-leveling æˆ–å†·çƒ­æ•°æ®åˆ†å±‚ä¼˜åŒ–ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **Online Learning for Adaptive Caching**
   - å¼•å…¥åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œä½¿ç¼“å­˜ç­–ç•¥èƒ½é€‚åº”è¾“å…¥æµçš„å˜åŒ–ï¼›
   - ç»“åˆå¼ºåŒ–å­¦ä¹ å®ç°å®æ—¶è°ƒä¼˜ã€‚

2. **Cross-Layer Correlation Modeling**
   - å½“å‰ç¼“å­˜ç­–ç•¥æŒ‰ layer ç‹¬ç«‹å»ºæ¨¡ï¼Œå¿½ç•¥è·¨å±‚ä¸“å®¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼›
   - å¯æ¢ç´¢ graph-based model æ•æ‰å…¨å±€ä¾èµ–ã€‚

3. **Hardware-Software Co-design**
   - ä¸æ–°å‹å­˜å‚¨ä»‹è´¨ï¼ˆå¦‚ Compute-in-Memoryã€ZNS SSDï¼‰ç»“åˆï¼›
   - æ¢ç´¢ä¸“ç”¨ç¼“å­˜æ§åˆ¶å™¨ ASIC åŠ é€Ÿ ML æ¨ç†ã€‚

4. **Privacy-preserving Caching**
   - åœ¨æœ¬åœ°è®¾å¤‡ä¸Šä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶å®ç°é«˜æ•ˆç¼“å­˜ï¼›
   - æ”¯æŒå·®åˆ†éšç§æˆ–è”é‚¦å­¦ä¹ å¼ç¼“å­˜è®­ç»ƒã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **FlashMoE é¦–æ¬¡å®ç°äº†åœ¨æ¶ˆè´¹çº§è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œè¶…å¤§è§„æ¨¡ MoE æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§åŸºäº ML çš„æ™ºèƒ½ç¼“å­˜æ›¿æ¢æœºåˆ¶ï¼Œåœ¨ SSD I/O å—é™ç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº† cache hit rate ä¸ end-to-end æ¨ç†é€Ÿåº¦ï¼Œä¸º MoE æ¨¡å‹çš„ç»ˆç«¯éƒ¨ç½²å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚**

</details>

---

### 15. [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)

**Authors**: Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.18292v1  

#### Abstract
In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three ro...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šTriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´ä¸¥é‡çš„**å®‰å…¨å¯¹é½ï¼ˆSafety Alignmentï¼‰æŒ‘æˆ˜**ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
- ä¾èµ–é«˜æˆæœ¬çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥æ‰©å±•ï¼›
- ç°æœ‰çº¢é˜Ÿæ”»å‡»ï¼ˆRed Teamingï¼‰æ–¹æ³•æ˜“å‡ºç°**æ¨¡å¼å´©æºƒï¼ˆpattern collapseï¼‰**ï¼Œæ”»å‡»å¤šæ ·æ€§ä¸è¶³ï¼›
- é˜²å¾¡æ¨¡å‹ï¼ˆDefenderï¼‰å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç‰ºç‰²é€šç”¨æ¨ç†èƒ½åŠ›ä»¥æ¢å–å®‰å…¨æ€§ï¼›
- è¯„ä¼°å™¨ï¼ˆEvaluatorï¼‰å¤šä¸ºé™æ€ã€å›ºå®šè§„åˆ™ï¼Œç¼ºä¹åŠ¨æ€æ¼”è¿›èƒ½åŠ›ï¼Œå¯¼è‡´å¥–åŠ±è¢«â€œæ¸¸æˆåŒ–â€ï¼ˆreward hackingï¼‰ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯**
æœ¬æ–‡æå‡ºäº† **TriPlay-RL** â€”â€” ä¸€ç§**ä¸‰è§’è‰²è‡ªåšå¼ˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆTri-Role Self-Play RL Frameworkï¼‰**ï¼Œå®ç°æ”»å‡»è€…ï¼ˆMRedï¼‰ã€é˜²å¾¡è€…ï¼ˆMBlueï¼‰å’Œè¯„ä¼°è€…ï¼ˆMEvalï¼‰ä¹‹é—´çš„é—­ç¯ååŒè¿›åŒ–ã€‚

#### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š**
1. **ä¸‰è§’è‰²é—­ç¯æœºåˆ¶ï¼ˆClosed-Loop Tri-Role Collaborationï¼‰**
   - å¼•å…¥**è¯„ä¼°è€…è§’è‰² MEval**ï¼Œæ„å»º MRed â†’ MBlue â†’ MEval çš„è¿­ä»£æ›´æ–°å¾ªç¯ï¼Œå½¢æˆç¨³å®šçš„è‡ªæˆ‘æ¼”è¿›ç³»ç»Ÿã€‚
   - æ‰€æœ‰è§’è‰²é€šè¿‡**è¿‘é›¶äººå·¥æ ‡æ³¨**çš„æ–¹å¼æŒç»­ä¼˜åŒ–ï¼Œæ˜¾è‘—é™ä½å¯¹äººç±»åé¦ˆçš„ä¾èµ–ã€‚

2. **æ”»å‡»å¤šæ ·æ€§ä¿éšœæœºåˆ¶**
   - è®¾è®¡**å¤šæ ·æ€§æƒ©ç½šé¡¹ï¼ˆDiversity Penaltyï¼‰**ï¼Œç»“åˆ Self-BLEU å’Œå¥å­åµŒå…¥ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé˜²æ­¢ MRed ç”Ÿæˆé‡å¤æ¨¡æ¿ã€‚
   - é‡‡ç”¨**å¤šæ¨¡å‹æ”»å‡»å¥–åŠ±ï¼ˆMulti-Model Attack Rewardï¼‰**ï¼Œé¼“åŠ± MRed åœ¨å¤šä¸ªå¼‚æ„é˜²å¾¡æ¨¡å‹ä¸Šæ³›åŒ–ï¼Œé¿å…è¿‡æ‹Ÿåˆå•ä¸€ç›®æ ‡ã€‚

3. **é˜²å¾¡æ€§èƒ½ä¸é€šç”¨æ€§çš„å¹³è¡¡**
   - æå‡º**ä¸‰çº§å¥–åŠ±æœºåˆ¶ï¼ˆThree-Level Rewardï¼‰**ï¼š
     - `Negative`ï¼ˆå«é£é™©ï¼‰â†’ Reward = -1  
     - `Rejective`ï¼ˆç®€å•æ‹’ç»ï¼‰â†’ Reward = 0  
     - `Positive`ï¼ˆå®‰å…¨ä¸”æœ‰ç›ŠæŒ‡å¯¼ï¼‰â†’ Reward = +1
   - æ˜¾è‘—æå‡ MBlue æä¾›**å»ºè®¾æ€§å®‰å…¨å¼•å¯¼**çš„èƒ½åŠ›ï¼Œè€Œéä¸€å‘³æ‹’ç»ï¼Œæ‰“ç ´â€œå®‰å…¨ vs. å¯ç”¨æ€§â€çš„ä¼ ç»Ÿæƒè¡¡ã€‚

4. **é«˜å¯é æ€§è‡ªåŠ¨è¯„ä¼°å™¨è®­ç»ƒ**
   - æ„å»º**å¤šä¸“å®¶æŠ•ç¥¨ç³»ç»Ÿï¼ˆMulti-Expert Majority Votingï¼‰**ï¼Œèåˆå¤šä¸ªå¼‚æ„å®‰å…¨ä¸“å®¶æ¨¡å‹æ ‡ç­¾ï¼Œæå‡ MEval åˆ¤æ–­ä¸€è‡´æ€§ã€‚
   - ä½¿ç”¨**å¤šæ–¹å‘è’¸é¦æç¤ºæ¨¡æ¿ï¼ˆMulti-Directional Distillation Prompt Templateï¼‰** æ„é€ é«˜è´¨é‡ä¸‰ç±»æ ‡æ³¨æ•°æ®é›†ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | TriPlay-RL | ä¼ ç»Ÿæ–¹æ³• |
|------|-----------|---------|
| æ•°æ®éœ€æ±‚ | è¿‘é›¶äººå·¥æ ‡æ³¨ | ä¾èµ–å¤§è§„æ¨¡äººå·¥åé¦ˆï¼ˆå¦‚ RLHFï¼‰ |
| æ”»å‡»å¤šæ ·æ€§ | é«˜ï¼Œé˜²æ¨¡å¼å´©æºƒ | æ˜“æ”¶æ•›è‡³å°‘æ•°æ”»å‡»æ¨¡å¼ |
| å®‰å…¨ä¸å¯ç”¨æ€§å¹³è¡¡ | æ˜¾å¼å»ºæ¨¡ï¼Œæ”¯æŒå»ºè®¾æ€§å›åº” | å¤šæ•°ä»…è¿½æ±‚æ‹’ç»ç‡ |
| è¯„ä¼°æœºåˆ¶ | åŠ¨æ€æ¼”åŒ–ã€æŠ— reward hacking | å›ºå®šè§„åˆ™æˆ–é™æ€åˆ†ç±»å™¨ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **åˆå§‹æ”»å‡»ç§å­é›†**ï¼šä» HarmBench ä¸­é€‰å– 200 æ¡åŸºç¡€æ”»å‡»æç¤ºï¼ˆbasic attack promptsï¼‰ï¼Œé…åˆ 9 ç§åŒ…è£…æŠ€æœ¯ç”Ÿæˆå…± 1,800 æ¡åˆå§‹å¯¹æŠ—æç¤ºã€‚
- **è¯„ä¼°åŸºå‡†**ï¼š
  - å®‰å…¨æ€§æµ‹è¯•ï¼š`AIR-Bench 2024`, `JailBreakBench`, `WildJailBreak`, `S-Eval`
  - æ¨ç†èƒ½åŠ›ä¿ç•™æµ‹è¯•ï¼š`IFEval`, `GPQA`, `LiveCodeBench-v5`, `AIME 2025`
- **MEval è®­ç»ƒæ•°æ®**ï¼šç”± MRed-MBlue å¯¹æŠ—äº¤äº’äº§ç”Ÿçš„ `(prompt, response)` å¯¹ï¼Œç»å¤šä¸“å®¶æŠ•ç¥¨æ ‡æ³¨ä¸ºä¸‰ç±»æ ‡ç­¾ï¼ˆNegative / Rejective / Positiveï¼‰ï¼Œæœ€ç»ˆæ„å»º **3,000 æ¡é«˜è´¨é‡æ ·æœ¬**ã€‚

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹æ¶æ„**ï¼šåŸºäº Qwen3 ç³»åˆ—ï¼ˆ4B/8B/14Bï¼‰ä½œä¸ºåˆå§‹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
- **è®­ç»ƒæµç¨‹**ï¼šé‡‡ç”¨äº¤æ›¿æ›´æ–°ç­–ç•¥ï¼Œæ¯è½®åªæ›´æ–°ä¸€ä¸ªè§’è‰²ï¼š
  1. **PRed Phase**ï¼šMRed ç”Ÿæˆå¯¹æŠ—æç¤ºï¼Œæ”»å‡» MBlue å’Œå…¶ä»–é˜²å¾¡æ¨¡å‹ï¼›
  2. **PBlue Phase**ï¼šMBlue åœ¨å¯¹æŠ—æç¤ºä¸Šè®­ç»ƒï¼Œæ¥æ”¶æ¥è‡ª MEval çš„å¥–åŠ±ä¿¡å·ï¼›
  3. **PEval Phase**ï¼šMEval ä½¿ç”¨æ–°æ”¶é›†çš„æ•°æ®å¾®è°ƒï¼Œæå‡åˆ¤æ–­ç²¾åº¦ã€‚
- **ä¼˜åŒ–ç®—æ³•**ï¼šä½¿ç”¨ GRPOï¼ˆGeneralized Reward Policy Optimizationï¼‰ä¸ºåŸºç¡€çš„ RLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰ç›®æ ‡å‡½æ•°ã€‚

### **è¯„ä¼°æŒ‡æ ‡**
| è§’è‰² | ä¸»è¦æŒ‡æ ‡ |
|------|--------|
| **MRed** | Attack Success Rate (**ASR**)ï¼Œè¾“å‡ºå¤šæ ·æ€§å¾—åˆ†ï¼ˆODï¼‰ |
| **MBlue** | ASRï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼Œé€šç”¨æ¨ç†ä»»åŠ¡è¡¨ç°ï¼ˆIFEval/GPQAç­‰ï¼‰ |
| **MEval** | ä¸‰åˆ†ç±»å‡†ç¡®ç‡ï¼ˆAccuracy on curated datasetï¼‰ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **MRed vs. åŸºçº¿æ”»å‡»æ¨¡å‹**ï¼šæ¯”è¾ƒä¸åŒè®­ç»ƒé˜¶æ®µä¸‹ ASR æå‡å¹…åº¦ã€‚
- **MBlue vs. åŸå§‹ Qwen æ¨¡å‹**ï¼šåœ¨å¤šä¸ªå®‰å…¨ benchmark ä¸Šæ¯”è¾ƒ ASR ä¸‹é™ç¨‹åº¦åŠæ¨ç†èƒ½åŠ›ä¿æŒæƒ…å†µã€‚
- **æ¶ˆèå®éªŒè®¾è®¡**ï¼š
  - æ˜¯å¦å¯ç”¨å¤šæ¨¡å‹æ”»å‡»ï¼ˆMulti-Single Ablationï¼‰
  - æ˜¯å¦å¼•å…¥å¤šæ ·æ€§æƒ©ç½šä¸é—­ç¯è®­ç»ƒï¼ˆDiversity Ablationï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **MRed æ”»å‡»èƒ½åŠ›æ˜¾è‘—å¢å¼º**
- ç»è¿‡ 16 è½®è®­ç»ƒåï¼š
  - å¯¹ **DeepSeek-R1-0528-Qwen3-8B** çš„ ASR ä» 13.0% â†’ **32.0%**
  - å¯¹ **Qwen3-8B** çš„ ASR ä» 21.84% â†’ **67.75%**
  - å¯¹ **Llama-3.1-Nemotron-Nano-8B-v1** è¾¾åˆ° **90% ASR**
- è¾“å‡ºå¤šæ ·æ€§ï¼ˆODï¼‰ç»´æŒåœ¨è¾ƒé«˜æ°´å¹³ï¼ˆæœ€é«˜è¾¾ 0.588ï¼‰ï¼Œè¿œä¼˜äºæ— å¤šæ ·æ€§æ§åˆ¶çš„å˜ä½“ã€‚

#### âœ… **MBlue å®‰å…¨æ€§èƒ½å¤§å¹…æå‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†èƒ½åŠ›**
- åœ¨ **AIR-Bench 2024** ä¸Šï¼ŒMBlue-14B çš„ ASR ä» 13.9% â†“ è‡³ **4.4%**
- åœ¨ **JailBreakBench** ä¸Šï¼ŒASR ä» 31.5% â†“ è‡³ **4.6%**
- æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸­**æœªè§æ˜æ˜¾é€€åŒ–**ï¼Œéƒ¨åˆ†ç”šè‡³ç•¥æœ‰æå‡ï¼š

| æ¨¡å‹ | LiveCodeBench | GPQA | AIME25 | IFEval |
|------|---------------|------|-------|--------|
| Qwen3-14B | 55.87 | 64.33 | 71.67 | 86.70 |
| **MBlue-14B** | **56.15** | **64.93** | **71.56** | **85.72** |

> ğŸ’¡ è¡¨æ˜æ¨¡å‹åœ¨å¼ºåŒ–å®‰å…¨çš„åŒæ—¶ï¼Œå¹¶æœªç‰ºç‰²æ¨ç†èƒ½åŠ›ï¼Œåè€Œå› æ­£å‘æ¿€åŠ±æ›´å€¾å‘äºæä¾›æœ‰ç”¨å›ç­”ã€‚

#### âœ… **MEval è¯„ä¼°å‡†ç¡®æ€§ç¨³æ­¥ä¸Šå‡**
- MEval-14B å‡†ç¡®ç‡ä»åˆå§‹ **97.0%** æå‡è‡³ **98.2%**
- å°æ¨¡å‹ä¹Ÿå‘ˆç°ç¨³å®šå¢é•¿è¶‹åŠ¿ï¼š
  - MEval-4B: 48.2% â†’ 56.2%
  - MEval-8B: 54.9% â†’ 64.3%
- è¡¨æ˜å…¶å…·å¤‡æŒç»­ç²¾ç»†åŒ–åˆ¤æ–­èƒ½åŠ›ï¼Œèƒ½æ›´å¥½åœ°åŒºåˆ† `Unsafe`, `Simple Refusal`, `Helpful Guidance`

### **æ¶ˆèå®éªŒç»“æœ**

#### ğŸ”¹ **å¤šæ¨¡å‹æ”»å‡» vs å•ä¸€æ¨¡å‹æ”»å‡»ï¼ˆMulti-Single Ablationï¼‰**
| è®¾ç½® | DeepSeek ASR | Qwen3-8B ASR | Llama-Nano ASR |
|------|-------------|--------------|----------------|
| å¤šæ¨¡å‹è®­ç»ƒ | **85.4%** | **64.1%** | **77.6%** |
| å•æ¨¡å‹è®­ç»ƒ | 60.2% | 22.8% | 9.1% |

> âœ”ï¸ ä½¿ç”¨å¤šæ¨¡å‹è®­ç»ƒæ˜¾è‘—æå‡æ”»å‡»æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜è¯¥è®¾è®¡æœ‰æ•ˆç¼“è§£è¿‡æ‹Ÿåˆã€‚

#### ğŸ”¹ **å¤šæ ·æ€§ä¸é—­ç¯è®­ç»ƒæ¶ˆèï¼ˆDiversity Ablationï¼‰**
| è®¾ç½® | OD åˆ†æ•° | Nano-8B ASR | Llama-8B ASR |
|------|--------|------------|------------|
| w/L + w/Dï¼ˆå®Œæ•´ç‰ˆï¼‰ | **0.588** | 85.4% | **6.6%** |
| w/o L + w/Dï¼ˆæ— é—­ç¯ï¼‰ | 0.156 | 95.1% | 2.8% |
| w/L + w/o Dï¼ˆæ— å¤šæ ·æ€§ï¼‰ | 0.514 | 90.9% | 6.0% |
| w/o L + w/o D | 0.004 | 98.2% | 0.2% |

> âš ï¸ å‘ç°ä¸¥é‡ trade-offï¼šå…³é—­é—­ç¯æˆ–å¤šæ ·æ€§ä¼šå¯¼è‡´ç†µåç¼©ï¼ˆentropy collapseï¼‰ï¼Œæ”»å‡»è™½å¯¹å¼±æ¨¡å‹æœ‰æ•ˆï¼Œä½†åœ¨å¼ºæ¨¡å‹ä¸Šå¤±æ•ˆï¼Œè¯´æ˜**é—­ç¯æœºåˆ¶æ˜¯ç»´æŒå¤šæ ·æ€§ä¸é•¿æœŸæ¼”åŒ–çš„å…³é”®**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **ä¸‰è§’è‰²é—­ç¯å¯å®ç°æŒç»­å…±è¿›åŒ–**ï¼š
   - MRed ä¸æ–­ç”Ÿæˆæ–°é¢–ã€æœ‰æ•ˆçš„æ”»å‡»ï¼›
   - MBlue åœ¨å‹åŠ›ä¸‹ä¸æ–­å¢å¼ºé˜²å¾¡åŠ›å¹¶æä¾›æ›´æœ‰ä»·å€¼çš„å®‰å…¨å¼•å¯¼ï¼›
   - MEval é€æ­¥ç²¾ç»†åŒ–è¯„ä¼°æ ‡å‡†ï¼Œå½¢æˆè‰¯æ€§å¾ªç¯ã€‚

2. **æ— éœ€ç‰ºç‰²é€šç”¨èƒ½åŠ›å³å¯å®ç°å¼ºå®‰å…¨å¯¹é½**ï¼š
   - é€šè¿‡ä¸‰çº§å¥–åŠ±æœºåˆ¶ï¼ŒæˆåŠŸå¼•å¯¼ MBlue åœ¨æ‹’ç»å±é™©è¯·æ±‚çš„åŒæ—¶ï¼Œä¸»åŠ¨æä¾›æ•™è‚²æ€§ã€åˆè§„æ€§çš„æ›¿ä»£å»ºè®®ã€‚

3. **è‡ªåŠ¨åŒ–è¯„ä¼°å™¨å¯é€šè¿‡åä½œè®­ç»ƒè¾¾åˆ°é«˜ä¸€è‡´æ€§**ï¼š
   - å¤šä¸“å®¶æŠ•ç¥¨ + å¤šæ–¹å‘è’¸é¦æç¤ºæœ‰æ•ˆæå‡äº† MEval çš„é²æ£’æ€§å’ŒæŠ— reward hacking èƒ½åŠ›ã€‚

4. **å¤šæ ·æ€§æœºåˆ¶è‡³å…³é‡è¦**ï¼š
   - ç¼ºä¹å¤šæ ·æ€§çº¦æŸå°†å¯¼è‡´æ”»å‡»ç­–ç•¥å¿«é€Ÿæ”¶æ•›ï¼Œå‰Šå¼±é•¿æœŸè®­ç»ƒæ•ˆæœã€‚

---

### **å±€é™æ€§**
1. **åŒæºæ¨¡å‹åˆå§‹åŒ–é™åˆ¶**ï¼š
   - å½“å‰ MRedã€MBlueã€MEval å‡åŸºäºç›¸åŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰ï¼Œå°šæœªæ¢ç´¢å¼‚æ„èƒ½åŠ›ç»„åˆä¸‹çš„è¡¨ç°ã€‚

2. **å•å®ä¾‹å…±äº«æ¨¡å‹æœªå®ç°**ï¼š
   - ä¸‰ä¸ªè§’è‰²ç›®å‰ä¸ºç‹¬ç«‹æ¨¡å‹å®ä¾‹ï¼Œæ˜¯å¦å¯ç”¨**å•ä¸€å…±äº«æ¨¡å‹è½®æµæ‰®æ¼”ä¸‰è§’è‰²**ä»æ˜¯å¼€æ”¾é—®é¢˜ã€‚

3. **è§’è‰²é—´æ ¹æœ¬æ€§å†²çªæœªè§£**ï¼š
   - å¼ºåŒ– MRed çš„æ”»å‡»æ€§å¯èƒ½æŸå®³ MBlue çš„å®‰å…¨æ€§ï¼Œåä¹‹äº¦ç„¶ã€‚ä¸¤è€…åœ¨åŒä¸€å‚æ•°ç©ºé—´ä¸­çš„ä¼˜åŒ–å­˜åœ¨å†…åœ¨å¼ åŠ›ï¼Œæœ¬æ–‡æœªæ·±å…¥åˆ†æå…¶åšå¼ˆå‡è¡¡ç‰¹æ€§ã€‚

4. **å¤–éƒ¨æ•°æ®å½±å“æœªçŸ¥**ï¼š
   - æœªç ”ç©¶å¼•å…¥é¢å¤–ç›‘ç£æ•°æ®ï¼ˆå¦‚ SFT on safety datasetsï¼‰å¦‚ä½•å½±å“ä¸‰è§’è‰²åŠ¨æ€å¹³è¡¡ã€‚

5. **ç¼ºä¹ç†è®ºå»ºæ¨¡**ï¼š
   - æœªåˆ†æç³»ç»Ÿçš„ Nash Equilibrium æˆ– Pareto Frontierï¼Œä¹Ÿç¼ºå°‘å¯¹å„è§’è‰²èƒ½åŠ›å¢é•¿è½¨è¿¹çš„è°ƒæ§æœºåˆ¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. æ¢ç´¢**å¼‚æ„æ¨¡å‹é…ç½®**ä¸‹çš„ä¸‰è§’è‰²åä½œï¼Œä¾‹å¦‚è½»é‡çº§ MRed + å¤§è§„æ¨¡ MBlue + ä¸“ç”¨è¯„ä¼°æ¨¡å‹ã€‚
2. ç ”ç©¶**å•æ¨¡å‹ä¸‰è§’è‰²åˆ‡æ¢æœºåˆ¶**ï¼Œå®ç°èµ„æºé«˜æ•ˆåˆ©ç”¨ã€‚
3. å¼•å…¥**åšå¼ˆè®ºåˆ†æå·¥å…·**ï¼Œå»ºæ¨¡ä¸‰æ–¹äº’åŠ¨çš„åŠ¨åŠ›å­¦è¿‡ç¨‹ï¼Œè¯†åˆ«æ½œåœ¨ä¸ç¨³å®šç‚¹ã€‚
4. å¼€å‘**åŠ¨æ€è°ƒèŠ‚å™¨æ¨¡å—**ï¼Œç›‘æ§å¹¶å¹²é¢„å„è§’è‰²æˆé•¿é€Ÿåº¦ï¼Œé˜²æ­¢æŸä¸€æ–¹ä¸»å¯¼è®­ç»ƒè¿›ç¨‹ã€‚
5. ç»“åˆå¤–éƒ¨çŸ¥è¯†æ³¨å…¥ï¼ˆå¦‚æ³•å¾‹æ¡æ–‡ã€ä¼¦ç†å‡†åˆ™ï¼‰ï¼Œå¢å¼º MEval çš„è§£é‡Šèƒ½åŠ›å’Œæ–‡åŒ–æ•æ„Ÿæ€§ã€‚

---

> ğŸ”— **ä»£ç åœ°å€**ï¼šhttps://anonymous.4open.science/r/TriPlay-RL  
> ğŸ“„ **è®ºæ–‡ç±»å‹**ï¼šConference Paper (arXiv Preprint)  
> ğŸ¢ **ä½œè€…å•ä½**ï¼šåŒ—äº¬å¤§å­¦ã€å¥‡æºç§‘æŠ€ã€ä¸­å›½ç§‘å­¦é™¢å¤§å­¦

</details>

---

### 16. [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)

**Authors**: Adeeba Tarannum, Muzakkiruddin Ahmed Mohammed, Mert Can Cakmak, Shames Al Mandalawi, John Talburt  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.18014v1  

#### Abstract
Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large la...

---

### 17. [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)

**Authors**: Sukirt Thakur, Marcus Roper, Yang Zhou, Reza Akbarian Bafghi, Brahmajee K. Nallamothu, C. Alberto Figueroa, Srinivas Paruchuri, Scott Burger, Maziar Raissi  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.17192v1  

#### Abstract
Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from st...

---

### 18. [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)

**Authors**: Tiejin Chen, Xiaoou Liu, Vishnu Nandam, Kuan-Ru Liou, Hua Wei  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.17329v1  

#### Abstract
Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} b...

---

### 19. [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)

**Authors**: Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng, Yu Wang, Zhiyuan Yan, Yonghong Tian, Yu Li, Li Yuan  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.17687v1  

#### Abstract
Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models pl...

---

### 20. [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)

**Authors**: Pedram Zaree, Md Abdullah Al Mamun, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.18110v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inf...

---

### 21. [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)

**Authors**: Chenyu Zhang, Xinchen Lyu, Chenshan Ren, Shuhan Liu, Qimei Cui, Xiaofeng Tao  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.18200v1  

#### Abstract
Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining ...

---

### 22. [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)

**Authors**: Peng Sun, Xiangyu Zhang, Duan Wu  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.18253v1  

#### Abstract
Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrap...

---

### 23. [Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs](https://arxiv.org/abs/2601.17707)

**Authors**: Mekala Kiran, Apurba Das, Suman Banerjee, Tathagata Ray  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.17707v1  

#### Abstract
Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work ha...

---

### 24. [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)

**Authors**: Yifei Zhang, Meimei Liu, Zhengwu Zhang  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.17073v1  

#### Abstract
Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behav...

---

### 25. [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)

**Authors**: Hadi Salloum, Ali Jnadi, Yaroslav Kholodov, Alexander Gasnikov  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.17570v1  

#### Abstract
Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) proble...

---

### 26. [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)

**Authors**: Tunazzina Islam  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.17172v1  

#### Abstract
Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted...

---

### 27. [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)

**Authors**: Syed Muhammad Ali, Hammad Sajid, Zainab Haider, Ali Muhammad Asad, Haya Fatima, Abdul Samad  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.17664v1  

#### Abstract
Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To ad...

---

### 28. [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)

**Authors**: Jinyoung Park, Sanghyeok Lee, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.17755v1  

#### Abstract
Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-ch...

---

### 29. [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)

**Authors**: Devansh Srivastav, David Pape, Lea Sch\"onherr  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.18552v1  

#### Abstract
LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliber...

---

### 30. [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)

**Authors**: Julian Bellavita, Matthew Rubino, Nakul Iyer, Andrew Chang, Aditya Devarakonda, Flavio Vella, Giulia Guidi  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.17136v1  

#### Abstract
Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Pri...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- kv cache, offload, State Space, SSM, framework, System, Generation, Video, Linear, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
